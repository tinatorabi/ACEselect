2025-02-04 10:18:45.013 INFO: reading data
2025-02-04 10:18:48.415 INFO: Loaded 1354 training configurations from 'train_data.xyz'
2025-02-04 10:18:48.415 INFO: Using random 15.0% of training set for validation
2025-02-04 10:19:16.588 INFO: CUDA version: 12.1, CUDA device: 0
2025-02-04 10:19:16.588 INFO: device: cuda
2025-02-04 10:19:16.588 INFO: building CACE representation
2025-02-04 10:19:20.926 INFO: Representation: Cace(
  (node_onehot): NodeEncoder(num_classes=2)
  (node_embedding_sender): NodeEmbedding(num_classes=2, embedding_dim=3)
  (node_embedding_receiver): NodeEmbedding(num_classes=2, embedding_dim=3)
  (edge_coding): EdgeEncoder(directed=True)
  (radial_basis): BesselRBF(cutoff=5.5, n_rbf=6, trainable=True)
  (cutoff_fn): PolynomialCutoff(p=6.0, cutoff=5.5)
  (angular_basis): AngularComponent(l_max=3)
  (radial_transform): SharedRadialLinearTransform(
    (weights): ParameterList(
        (0): Parameter containing: [torch.float32 of size 6x12x9 (cuda:0)]
        (1): Parameter containing: [torch.float32 of size 6x12x9 (cuda:0)]
        (2): Parameter containing: [torch.float32 of size 6x12x9 (cuda:0)]
        (3): Parameter containing: [torch.float32 of size 6x12x9 (cuda:0)]
    )
  )
  (symmetrizer): Symmetrizer()
  (message_passing_list): ModuleList(
    (0): ModuleList(
      (0-1): 2 x None
      (2): MessageBchi()
    )
  )
)
2025-02-04 10:19:20.926 INFO: building CACE NNP
2025-02-04 10:19:20.926 INFO: First train loop:
2025-02-04 10:19:20.927 INFO: creating training task
2025-02-04 10:20:15.546 INFO: training
##### Step: 0 Learning rate: 0.002 #####
2025-02-04 10:23:29.425 INFO: ##### Step: 0 Learning rate: 0.002 #####
Epoch 1, Train Loss: 957.2896, Val Loss: 261.2624
2025-02-04 10:23:29.426 INFO: Epoch 1, Train Loss: 957.2896, Val Loss: 261.2624
train_e/atom_mae: 0.078234
2025-02-04 10:23:29.689 INFO: train_e/atom_mae: 0.078234
train_e/atom_rmse: 0.117625
2025-02-04 10:23:30.358 INFO: train_e/atom_rmse: 0.117625
train_f_mae: 0.568566
2025-02-04 10:23:30.362 INFO: train_f_mae: 0.568566
train_f_rmse: 0.951990
2025-02-04 10:23:30.362 INFO: train_f_rmse: 0.951990
val_e/atom_mae: 0.040025
2025-02-04 10:23:30.367 INFO: val_e/atom_mae: 0.040025
val_e/atom_rmse: 0.055902
2025-02-04 10:23:30.368 INFO: val_e/atom_rmse: 0.055902
val_f_mae: 0.335645
2025-02-04 10:23:30.368 INFO: val_f_mae: 0.335645
val_f_rmse: 0.499534
2025-02-04 10:23:30.369 INFO: val_f_rmse: 0.499534
##### Step: 1 Learning rate: 0.004 #####
2025-02-04 10:25:24.413 INFO: ##### Step: 1 Learning rate: 0.004 #####
Epoch 2, Train Loss: 166.7725, Val Loss: 149.2175
2025-02-04 10:25:24.414 INFO: Epoch 2, Train Loss: 166.7725, Val Loss: 149.2175
train_e/atom_mae: 0.036478
2025-02-04 10:25:24.415 INFO: train_e/atom_mae: 0.036478
train_e/atom_rmse: 0.050214
2025-02-04 10:25:24.415 INFO: train_e/atom_rmse: 0.050214
train_f_mae: 0.251085
2025-02-04 10:25:24.418 INFO: train_f_mae: 0.251085
train_f_rmse: 0.396834
2025-02-04 10:25:24.418 INFO: train_f_rmse: 0.396834
val_e/atom_mae: 0.052237
2025-02-04 10:25:24.421 INFO: val_e/atom_mae: 0.052237
val_e/atom_rmse: 0.058195
2025-02-04 10:25:24.421 INFO: val_e/atom_rmse: 0.058195
val_f_mae: 0.244953
2025-02-04 10:25:24.422 INFO: val_f_mae: 0.244953
val_f_rmse: 0.369734
2025-02-04 10:25:24.422 INFO: val_f_rmse: 0.369734
##### Step: 2 Learning rate: 0.006 #####
2025-02-04 10:27:31.123 INFO: ##### Step: 2 Learning rate: 0.006 #####
Epoch 3, Train Loss: 99.0575, Val Loss: 109.7414
2025-02-04 10:27:31.124 INFO: Epoch 3, Train Loss: 99.0575, Val Loss: 109.7414
train_e/atom_mae: 0.027772
2025-02-04 10:27:31.125 INFO: train_e/atom_mae: 0.027772
train_e/atom_rmse: 0.037619
2025-02-04 10:27:31.125 INFO: train_e/atom_rmse: 0.037619
train_f_mae: 0.187814
2025-02-04 10:27:31.128 INFO: train_f_mae: 0.187814
train_f_rmse: 0.306334
2025-02-04 10:27:31.128 INFO: train_f_rmse: 0.306334
val_e/atom_mae: 0.023642
2025-02-04 10:27:31.130 INFO: val_e/atom_mae: 0.023642
val_e/atom_rmse: 0.026532
2025-02-04 10:27:31.131 INFO: val_e/atom_rmse: 0.026532
val_f_mae: 0.236088
2025-02-04 10:27:31.131 INFO: val_f_mae: 0.236088
val_f_rmse: 0.327292
2025-02-04 10:27:31.132 INFO: val_f_rmse: 0.327292
##### Step: 3 Learning rate: 0.008 #####
2025-02-04 10:29:35.507 INFO: ##### Step: 3 Learning rate: 0.008 #####
Epoch 4, Train Loss: 64.6970, Val Loss: 49.5208
2025-02-04 10:29:35.508 INFO: Epoch 4, Train Loss: 64.6970, Val Loss: 49.5208
train_e/atom_mae: 0.022189
2025-02-04 10:29:35.509 INFO: train_e/atom_mae: 0.022189
train_e/atom_rmse: 0.029532
2025-02-04 10:29:35.509 INFO: train_e/atom_rmse: 0.029532
train_f_mae: 0.155691
2025-02-04 10:29:35.512 INFO: train_f_mae: 0.155691
train_f_rmse: 0.247956
2025-02-04 10:29:35.512 INFO: train_f_rmse: 0.247956
val_e/atom_mae: 0.012778
2025-02-04 10:29:35.515 INFO: val_e/atom_mae: 0.012778
val_e/atom_rmse: 0.020648
2025-02-04 10:29:35.515 INFO: val_e/atom_rmse: 0.020648
val_f_mae: 0.138278
2025-02-04 10:29:35.515 INFO: val_f_mae: 0.138278
val_f_rmse: 0.218988
2025-02-04 10:29:35.516 INFO: val_f_rmse: 0.218988
##### Step: 4 Learning rate: 0.01 #####
2025-02-04 10:31:34.074 INFO: ##### Step: 4 Learning rate: 0.01 #####
Epoch 5, Train Loss: 54.8856, Val Loss: 38.5699
2025-02-04 10:31:34.075 INFO: Epoch 5, Train Loss: 54.8856, Val Loss: 38.5699
train_e/atom_mae: 0.027262
2025-02-04 10:31:34.076 INFO: train_e/atom_mae: 0.027262
train_e/atom_rmse: 0.035134
2025-02-04 10:31:34.076 INFO: train_e/atom_rmse: 0.035134
train_f_mae: 0.144673
2025-02-04 10:31:34.079 INFO: train_f_mae: 0.144673
train_f_rmse: 0.224355
2025-02-04 10:31:34.079 INFO: train_f_rmse: 0.224355
val_e/atom_mae: 0.014936
2025-02-04 10:31:34.082 INFO: val_e/atom_mae: 0.014936
val_e/atom_rmse: 0.017262
2025-02-04 10:31:34.082 INFO: val_e/atom_rmse: 0.017262
val_f_mae: 0.123793
2025-02-04 10:31:34.083 INFO: val_f_mae: 0.123793
val_f_rmse: 0.193582
2025-02-04 10:31:34.083 INFO: val_f_rmse: 0.193582
##### Step: 5 Learning rate: 0.01 #####
2025-02-04 10:33:27.983 INFO: ##### Step: 5 Learning rate: 0.01 #####
Epoch 6, Train Loss: 44.8433, Val Loss: 34.3038
2025-02-04 10:33:27.984 INFO: Epoch 6, Train Loss: 44.8433, Val Loss: 34.3038
train_e/atom_mae: 0.019719
2025-02-04 10:33:27.984 INFO: train_e/atom_mae: 0.019719
train_e/atom_rmse: 0.025622
2025-02-04 10:33:27.985 INFO: train_e/atom_rmse: 0.025622
train_f_mae: 0.133805
2025-02-04 10:33:27.988 INFO: train_f_mae: 0.133805
train_f_rmse: 0.205969
2025-02-04 10:33:27.988 INFO: train_f_rmse: 0.205969
val_e/atom_mae: 0.017849
2025-02-04 10:33:27.990 INFO: val_e/atom_mae: 0.017849
val_e/atom_rmse: 0.019472
2025-02-04 10:33:27.991 INFO: val_e/atom_rmse: 0.019472
val_f_mae: 0.119145
2025-02-04 10:33:27.991 INFO: val_f_mae: 0.119145
val_f_rmse: 0.181389
2025-02-04 10:33:27.991 INFO: val_f_rmse: 0.181389
##### Step: 6 Learning rate: 0.01 #####
2025-02-04 10:35:21.803 INFO: ##### Step: 6 Learning rate: 0.01 #####
Epoch 7, Train Loss: 36.4168, Val Loss: 55.6421
2025-02-04 10:35:21.804 INFO: Epoch 7, Train Loss: 36.4168, Val Loss: 55.6421
train_e/atom_mae: 0.018063
2025-02-04 10:35:21.805 INFO: train_e/atom_mae: 0.018063
train_e/atom_rmse: 0.023370
2025-02-04 10:35:21.805 INFO: train_e/atom_rmse: 0.023370
train_f_mae: 0.118099
2025-02-04 10:35:21.808 INFO: train_f_mae: 0.118099
train_f_rmse: 0.185481
2025-02-04 10:35:21.808 INFO: train_f_rmse: 0.185481
val_e/atom_mae: 0.013075
2025-02-04 10:35:21.810 INFO: val_e/atom_mae: 0.013075
val_e/atom_rmse: 0.018706
2025-02-04 10:35:21.811 INFO: val_e/atom_rmse: 0.018706
val_f_mae: 0.169224
2025-02-04 10:35:21.811 INFO: val_f_mae: 0.169224
val_f_rmse: 0.232982
2025-02-04 10:35:21.811 INFO: val_f_rmse: 0.232982
##### Step: 7 Learning rate: 0.01 #####
2025-02-04 10:37:15.498 INFO: ##### Step: 7 Learning rate: 0.01 #####
Epoch 8, Train Loss: 37.0380, Val Loss: 25.8089
2025-02-04 10:37:15.499 INFO: Epoch 8, Train Loss: 37.0380, Val Loss: 25.8089
train_e/atom_mae: 0.017948
2025-02-04 10:37:15.499 INFO: train_e/atom_mae: 0.017948
train_e/atom_rmse: 0.022550
2025-02-04 10:37:15.500 INFO: train_e/atom_rmse: 0.022550
train_f_mae: 0.120964
2025-02-04 10:37:15.503 INFO: train_f_mae: 0.120964
train_f_rmse: 0.187519
2025-02-04 10:37:15.503 INFO: train_f_rmse: 0.187519
val_e/atom_mae: 0.008541
2025-02-04 10:37:15.505 INFO: val_e/atom_mae: 0.008541
val_e/atom_rmse: 0.011958
2025-02-04 10:37:15.505 INFO: val_e/atom_rmse: 0.011958
val_f_mae: 0.101915
2025-02-04 10:37:15.506 INFO: val_f_mae: 0.101915
val_f_rmse: 0.158974
2025-02-04 10:37:15.506 INFO: val_f_rmse: 0.158974
##### Step: 8 Learning rate: 0.01 #####
2025-02-04 10:39:09.223 INFO: ##### Step: 8 Learning rate: 0.01 #####
Epoch 9, Train Loss: 32.4513, Val Loss: 29.8651
2025-02-04 10:39:09.223 INFO: Epoch 9, Train Loss: 32.4513, Val Loss: 29.8651
train_e/atom_mae: 0.017036
2025-02-04 10:39:09.224 INFO: train_e/atom_mae: 0.017036
train_e/atom_rmse: 0.021957
2025-02-04 10:39:09.224 INFO: train_e/atom_rmse: 0.021957
train_f_mae: 0.112298
2025-02-04 10:39:09.231 INFO: train_f_mae: 0.112298
train_f_rmse: 0.175140
2025-02-04 10:39:09.231 INFO: train_f_rmse: 0.175140
val_e/atom_mae: 0.014108
2025-02-04 10:39:09.233 INFO: val_e/atom_mae: 0.014108
val_e/atom_rmse: 0.015607
2025-02-04 10:39:09.234 INFO: val_e/atom_rmse: 0.015607
val_f_mae: 0.118277
2025-02-04 10:39:09.234 INFO: val_f_mae: 0.118277
val_f_rmse: 0.170140
2025-02-04 10:39:09.234 INFO: val_f_rmse: 0.170140
##### Step: 9 Learning rate: 0.01 #####
2025-02-04 10:41:03.292 INFO: ##### Step: 9 Learning rate: 0.01 #####
Epoch 10, Train Loss: 35.1919, Val Loss: 27.5851
2025-02-04 10:41:03.293 INFO: Epoch 10, Train Loss: 35.1919, Val Loss: 27.5851
train_e/atom_mae: 0.015941
2025-02-04 10:41:03.294 INFO: train_e/atom_mae: 0.015941
train_e/atom_rmse: 0.020257
2025-02-04 10:41:03.294 INFO: train_e/atom_rmse: 0.020257
train_f_mae: 0.119425
2025-02-04 10:41:03.297 INFO: train_f_mae: 0.119425
train_f_rmse: 0.183519
2025-02-04 10:41:03.297 INFO: train_f_rmse: 0.183519
val_e/atom_mae: 0.007454
2025-02-04 10:41:03.299 INFO: val_e/atom_mae: 0.007454
val_e/atom_rmse: 0.010616
2025-02-04 10:41:03.300 INFO: val_e/atom_rmse: 0.010616
val_f_mae: 0.109757
2025-02-04 10:41:03.300 INFO: val_f_mae: 0.109757
val_f_rmse: 0.164822
2025-02-04 10:41:03.300 INFO: val_f_rmse: 0.164822
##### Step: 10 Learning rate: 0.01 #####
2025-02-04 10:43:04.854 INFO: ##### Step: 10 Learning rate: 0.01 #####
Epoch 11, Train Loss: 29.1695, Val Loss: 8644.5953
2025-02-04 10:43:04.855 INFO: Epoch 11, Train Loss: 29.1695, Val Loss: 8644.5953
train_e/atom_mae: 0.016715
2025-02-04 10:43:04.856 INFO: train_e/atom_mae: 0.016715
train_e/atom_rmse: 0.021064
2025-02-04 10:43:04.856 INFO: train_e/atom_rmse: 0.021064
train_f_mae: 0.106703
2025-02-04 10:43:04.859 INFO: train_f_mae: 0.106703
train_f_rmse: 0.165933
2025-02-04 10:43:04.859 INFO: train_f_rmse: 0.165933
val_e/atom_mae: 0.252213
2025-02-04 10:43:04.862 INFO: val_e/atom_mae: 0.252213
val_e/atom_rmse: 0.304699
2025-02-04 10:43:04.862 INFO: val_e/atom_rmse: 0.304699
val_f_mae: 1.774118
2025-02-04 10:43:04.862 INFO: val_f_mae: 1.774118
val_f_rmse: 2.879441
2025-02-04 10:43:04.863 INFO: val_f_rmse: 2.879441
##### Step: 11 Learning rate: 0.01 #####
2025-02-04 10:44:59.033 INFO: ##### Step: 11 Learning rate: 0.01 #####
Epoch 12, Train Loss: 29.8527, Val Loss: 19.2759
2025-02-04 10:44:59.033 INFO: Epoch 12, Train Loss: 29.8527, Val Loss: 19.2759
train_e/atom_mae: 0.016099
2025-02-04 10:44:59.034 INFO: train_e/atom_mae: 0.016099
train_e/atom_rmse: 0.020252
2025-02-04 10:44:59.034 INFO: train_e/atom_rmse: 0.020252
train_f_mae: 0.110147
2025-02-04 10:44:59.041 INFO: train_f_mae: 0.110147
train_f_rmse: 0.168347
2025-02-04 10:44:59.041 INFO: train_f_rmse: 0.168347
val_e/atom_mae: 0.007908
2025-02-04 10:44:59.044 INFO: val_e/atom_mae: 0.007908
val_e/atom_rmse: 0.009783
2025-02-04 10:44:59.044 INFO: val_e/atom_rmse: 0.009783
val_f_mae: 0.087276
2025-02-04 10:44:59.044 INFO: val_f_mae: 0.087276
val_f_rmse: 0.137525
2025-02-04 10:44:59.045 INFO: val_f_rmse: 0.137525
##### Step: 12 Learning rate: 0.01 #####
2025-02-04 10:46:53.088 INFO: ##### Step: 12 Learning rate: 0.01 #####
Epoch 13, Train Loss: 27.6119, Val Loss: 18.2602
2025-02-04 10:46:53.089 INFO: Epoch 13, Train Loss: 27.6119, Val Loss: 18.2602
train_e/atom_mae: 0.015460
2025-02-04 10:46:53.091 INFO: train_e/atom_mae: 0.015460
train_e/atom_rmse: 0.019464
2025-02-04 10:46:53.091 INFO: train_e/atom_rmse: 0.019464
train_f_mae: 0.103860
2025-02-04 10:46:53.094 INFO: train_f_mae: 0.103860
train_f_rmse: 0.161911
2025-02-04 10:46:53.094 INFO: train_f_rmse: 0.161911
val_e/atom_mae: 0.006094
2025-02-04 10:46:53.096 INFO: val_e/atom_mae: 0.006094
val_e/atom_rmse: 0.009091
2025-02-04 10:46:53.097 INFO: val_e/atom_rmse: 0.009091
val_f_mae: 0.084420
2025-02-04 10:46:53.097 INFO: val_f_mae: 0.084420
val_f_rmse: 0.133966
2025-02-04 10:46:53.097 INFO: val_f_rmse: 0.133966
##### Step: 13 Learning rate: 0.01 #####
2025-02-04 10:48:47.473 INFO: ##### Step: 13 Learning rate: 0.01 #####
Epoch 14, Train Loss: 26.8615, Val Loss: 18.0787
2025-02-04 10:48:47.474 INFO: Epoch 14, Train Loss: 26.8615, Val Loss: 18.0787
train_e/atom_mae: 0.017018
2025-02-04 10:48:47.475 INFO: train_e/atom_mae: 0.017018
train_e/atom_rmse: 0.021338
2025-02-04 10:48:47.475 INFO: train_e/atom_rmse: 0.021338
train_f_mae: 0.102392
2025-02-04 10:48:47.478 INFO: train_f_mae: 0.102392
train_f_rmse: 0.158691
2025-02-04 10:48:47.478 INFO: train_f_rmse: 0.158691
val_e/atom_mae: 0.006140
2025-02-04 10:48:47.481 INFO: val_e/atom_mae: 0.006140
val_e/atom_rmse: 0.008525
2025-02-04 10:48:47.481 INFO: val_e/atom_rmse: 0.008525
val_f_mae: 0.083530
2025-02-04 10:48:47.482 INFO: val_f_mae: 0.083530
val_f_rmse: 0.133427
2025-02-04 10:48:47.482 INFO: val_f_rmse: 0.133427
##### Step: 14 Learning rate: 0.01 #####
2025-02-04 10:50:41.633 INFO: ##### Step: 14 Learning rate: 0.01 #####
Epoch 15, Train Loss: 24.2646, Val Loss: 17.0028
2025-02-04 10:50:41.633 INFO: Epoch 15, Train Loss: 24.2646, Val Loss: 17.0028
train_e/atom_mae: 0.014932
2025-02-04 10:50:41.634 INFO: train_e/atom_mae: 0.014932
train_e/atom_rmse: 0.018902
2025-02-04 10:50:41.634 INFO: train_e/atom_rmse: 0.018902
train_f_mae: 0.096200
2025-02-04 10:50:41.637 INFO: train_f_mae: 0.096200
train_f_rmse: 0.151485
2025-02-04 10:50:41.637 INFO: train_f_rmse: 0.151485
val_e/atom_mae: 0.005289
2025-02-04 10:50:41.643 INFO: val_e/atom_mae: 0.005289
val_e/atom_rmse: 0.008346
2025-02-04 10:50:41.644 INFO: val_e/atom_rmse: 0.008346
val_f_mae: 0.081372
2025-02-04 10:50:41.644 INFO: val_f_mae: 0.081372
val_f_rmse: 0.129375
2025-02-04 10:50:41.645 INFO: val_f_rmse: 0.129375
##### Step: 15 Learning rate: 0.01 #####
2025-02-04 10:53:01.417 INFO: ##### Step: 15 Learning rate: 0.01 #####
Epoch 16, Train Loss: 25.4982, Val Loss: 16.6498
2025-02-04 10:53:01.418 INFO: Epoch 16, Train Loss: 25.4982, Val Loss: 16.6498
train_e/atom_mae: 0.024947
2025-02-04 10:53:01.678 INFO: train_e/atom_mae: 0.024947
train_e/atom_rmse: 0.029912
2025-02-04 10:53:01.847 INFO: train_e/atom_rmse: 0.029912
train_f_mae: 0.094343
2025-02-04 10:53:01.849 INFO: train_f_mae: 0.094343
train_f_rmse: 0.148996
2025-02-04 10:53:01.850 INFO: train_f_rmse: 0.148996
val_e/atom_mae: 0.005843
2025-02-04 10:53:01.852 INFO: val_e/atom_mae: 0.005843
val_e/atom_rmse: 0.008295
2025-02-04 10:53:01.852 INFO: val_e/atom_rmse: 0.008295
val_f_mae: 0.080614
2025-02-04 10:53:01.853 INFO: val_f_mae: 0.080614
val_f_rmse: 0.128008
2025-02-04 10:53:01.853 INFO: val_f_rmse: 0.128008
##### Step: 16 Learning rate: 0.01 #####
2025-02-04 10:54:56.207 INFO: ##### Step: 16 Learning rate: 0.01 #####
Epoch 17, Train Loss: 24.5737, Val Loss: 16.5148
2025-02-04 10:54:56.208 INFO: Epoch 17, Train Loss: 24.5737, Val Loss: 16.5148
train_e/atom_mae: 0.014583
2025-02-04 10:54:56.208 INFO: train_e/atom_mae: 0.014583
train_e/atom_rmse: 0.018163
2025-02-04 10:54:56.209 INFO: train_e/atom_rmse: 0.018163
train_f_mae: 0.098632
2025-02-04 10:54:56.212 INFO: train_f_mae: 0.098632
train_f_rmse: 0.152832
2025-02-04 10:54:56.212 INFO: train_f_rmse: 0.152832
val_e/atom_mae: 0.005274
2025-02-04 10:54:56.214 INFO: val_e/atom_mae: 0.005274
val_e/atom_rmse: 0.007927
2025-02-04 10:54:56.214 INFO: val_e/atom_rmse: 0.007927
val_f_mae: 0.079662
2025-02-04 10:54:56.215 INFO: val_f_mae: 0.079662
val_f_rmse: 0.127574
2025-02-04 10:54:56.215 INFO: val_f_rmse: 0.127574
##### Step: 17 Learning rate: 0.01 #####
2025-02-04 10:56:49.874 INFO: ##### Step: 17 Learning rate: 0.01 #####
Epoch 18, Train Loss: 24.1567, Val Loss: 16.3688
2025-02-04 10:56:49.874 INFO: Epoch 18, Train Loss: 24.1567, Val Loss: 16.3688
train_e/atom_mae: 0.017107
2025-02-04 10:56:49.875 INFO: train_e/atom_mae: 0.017107
train_e/atom_rmse: 0.021171
2025-02-04 10:56:49.876 INFO: train_e/atom_rmse: 0.021171
train_f_mae: 0.096818
2025-02-04 10:56:49.878 INFO: train_f_mae: 0.096818
train_f_rmse: 0.150015
2025-02-04 10:56:49.879 INFO: train_f_rmse: 0.150015
val_e/atom_mae: 0.005347
2025-02-04 10:56:49.881 INFO: val_e/atom_mae: 0.005347
val_e/atom_rmse: 0.008207
2025-02-04 10:56:49.881 INFO: val_e/atom_rmse: 0.008207
val_f_mae: 0.078803
2025-02-04 10:56:49.882 INFO: val_f_mae: 0.078803
val_f_rmse: 0.126939
2025-02-04 10:56:49.882 INFO: val_f_rmse: 0.126939
##### Step: 18 Learning rate: 0.01 #####
2025-02-04 10:58:43.536 INFO: ##### Step: 18 Learning rate: 0.01 #####
Epoch 19, Train Loss: 24.3302, Val Loss: 15.7405
2025-02-04 10:58:43.537 INFO: Epoch 19, Train Loss: 24.3302, Val Loss: 15.7405
train_e/atom_mae: 0.014661
2025-02-04 10:58:43.538 INFO: train_e/atom_mae: 0.014661
train_e/atom_rmse: 0.018210
2025-02-04 10:58:43.538 INFO: train_e/atom_rmse: 0.018210
train_f_mae: 0.098717
2025-02-04 10:58:43.541 INFO: train_f_mae: 0.098717
train_f_rmse: 0.152012
2025-02-04 10:58:43.541 INFO: train_f_rmse: 0.152012
val_e/atom_mae: 0.006177
2025-02-04 10:58:43.543 INFO: val_e/atom_mae: 0.006177
val_e/atom_rmse: 0.007850
2025-02-04 10:58:43.544 INFO: val_e/atom_rmse: 0.007850
val_f_mae: 0.078282
2025-02-04 10:58:43.544 INFO: val_f_mae: 0.078282
val_f_rmse: 0.124518
2025-02-04 10:58:43.545 INFO: val_f_rmse: 0.124518
##### Step: 19 Learning rate: 0.01 #####
2025-02-04 11:00:36.883 INFO: ##### Step: 19 Learning rate: 0.01 #####
Epoch 20, Train Loss: 24.8536, Val Loss: 15.5384
2025-02-04 11:00:36.884 INFO: Epoch 20, Train Loss: 24.8536, Val Loss: 15.5384
train_e/atom_mae: 0.013341
2025-02-04 11:00:36.885 INFO: train_e/atom_mae: 0.013341
train_e/atom_rmse: 0.016890
2025-02-04 11:00:36.885 INFO: train_e/atom_rmse: 0.016890
train_f_mae: 0.101511
2025-02-04 11:00:36.888 INFO: train_f_mae: 0.101511
train_f_rmse: 0.154279
2025-02-04 11:00:36.888 INFO: train_f_rmse: 0.154279
val_e/atom_mae: 0.005962
2025-02-04 11:00:36.894 INFO: val_e/atom_mae: 0.005962
val_e/atom_rmse: 0.007778
2025-02-04 11:00:36.894 INFO: val_e/atom_rmse: 0.007778
val_f_mae: 0.077556
2025-02-04 11:00:36.895 INFO: val_f_mae: 0.077556
val_f_rmse: 0.123715
2025-02-04 11:00:36.895 INFO: val_f_rmse: 0.123715
##### Step: 20 Learning rate: 0.005 #####
2025-02-04 11:02:54.168 INFO: ##### Step: 20 Learning rate: 0.005 #####
Epoch 21, Train Loss: 19.7028, Val Loss: 15.1494
2025-02-04 11:02:54.169 INFO: Epoch 21, Train Loss: 19.7028, Val Loss: 15.1494
train_e/atom_mae: 0.012061
2025-02-04 11:02:54.332 INFO: train_e/atom_mae: 0.012061
train_e/atom_rmse: 0.015269
2025-02-04 11:02:54.536 INFO: train_e/atom_rmse: 0.015269
train_f_mae: 0.086963
2025-02-04 11:02:54.539 INFO: train_f_mae: 0.086963
train_f_rmse: 0.137271
2025-02-04 11:02:54.539 INFO: train_f_rmse: 0.137271
val_e/atom_mae: 0.004983
2025-02-04 11:02:54.541 INFO: val_e/atom_mae: 0.004983
val_e/atom_rmse: 0.006963
2025-02-04 11:02:54.542 INFO: val_e/atom_rmse: 0.006963
val_f_mae: 0.076796
2025-02-04 11:02:54.542 INFO: val_f_mae: 0.076796
val_f_rmse: 0.122317
2025-02-04 11:02:54.543 INFO: val_f_rmse: 0.122317
##### Step: 21 Learning rate: 0.005 #####
2025-02-04 11:04:45.830 INFO: ##### Step: 21 Learning rate: 0.005 #####
Epoch 22, Train Loss: 18.1900, Val Loss: 14.6399
2025-02-04 11:04:45.830 INFO: Epoch 22, Train Loss: 18.1900, Val Loss: 14.6399
train_e/atom_mae: 0.009016
2025-02-04 11:04:45.831 INFO: train_e/atom_mae: 0.009016
train_e/atom_rmse: 0.011831
2025-02-04 11:04:45.831 INFO: train_e/atom_rmse: 0.011831
train_f_mae: 0.083369
2025-02-04 11:04:45.834 INFO: train_f_mae: 0.083369
train_f_rmse: 0.132944
2025-02-04 11:04:45.834 INFO: train_f_rmse: 0.132944
val_e/atom_mae: 0.004916
2025-02-04 11:04:45.836 INFO: val_e/atom_mae: 0.004916
val_e/atom_rmse: 0.006787
2025-02-04 11:04:45.836 INFO: val_e/atom_rmse: 0.006787
val_f_mae: 0.075884
2025-02-04 11:04:45.837 INFO: val_f_mae: 0.075884
val_f_rmse: 0.120258
2025-02-04 11:04:45.837 INFO: val_f_rmse: 0.120258
##### Step: 22 Learning rate: 0.005 #####
2025-02-04 11:06:36.971 INFO: ##### Step: 22 Learning rate: 0.005 #####
Epoch 23, Train Loss: 19.0826, Val Loss: 14.3940
2025-02-04 11:06:36.971 INFO: Epoch 23, Train Loss: 19.0826, Val Loss: 14.3940
train_e/atom_mae: 0.008663
2025-02-04 11:06:36.972 INFO: train_e/atom_mae: 0.008663
train_e/atom_rmse: 0.011683
2025-02-04 11:06:36.972 INFO: train_e/atom_rmse: 0.011683
train_f_mae: 0.087038
2025-02-04 11:06:36.975 INFO: train_f_mae: 0.087038
train_f_rmse: 0.136306
2025-02-04 11:06:36.975 INFO: train_f_rmse: 0.136306
val_e/atom_mae: 0.004581
2025-02-04 11:06:36.977 INFO: val_e/atom_mae: 0.004581
val_e/atom_rmse: 0.006596
2025-02-04 11:06:36.978 INFO: val_e/atom_rmse: 0.006596
val_f_mae: 0.075165
2025-02-04 11:06:36.978 INFO: val_f_mae: 0.075165
val_f_rmse: 0.119275
2025-02-04 11:06:36.978 INFO: val_f_rmse: 0.119275
##### Step: 23 Learning rate: 0.005 #####
2025-02-04 11:08:33.817 INFO: ##### Step: 23 Learning rate: 0.005 #####
Epoch 24, Train Loss: 16.7473, Val Loss: 13.9319
2025-02-04 11:08:33.818 INFO: Epoch 24, Train Loss: 16.7473, Val Loss: 13.9319
train_e/atom_mae: 0.008363
2025-02-04 11:08:33.819 INFO: train_e/atom_mae: 0.008363
train_e/atom_rmse: 0.011204
2025-02-04 11:08:33.819 INFO: train_e/atom_rmse: 0.011204
train_f_mae: 0.078896
2025-02-04 11:08:33.821 INFO: train_f_mae: 0.078896
train_f_rmse: 0.127611
2025-02-04 11:08:33.822 INFO: train_f_rmse: 0.127611
val_e/atom_mae: 0.005050
2025-02-04 11:08:33.824 INFO: val_e/atom_mae: 0.005050
val_e/atom_rmse: 0.006612
2025-02-04 11:08:33.824 INFO: val_e/atom_rmse: 0.006612
val_f_mae: 0.073602
2025-02-04 11:08:33.825 INFO: val_f_mae: 0.073602
val_f_rmse: 0.117323
2025-02-04 11:08:33.825 INFO: val_f_rmse: 0.117323
##### Step: 24 Learning rate: 0.005 #####
2025-02-04 11:10:27.911 INFO: ##### Step: 24 Learning rate: 0.005 #####
Epoch 25, Train Loss: 17.0164, Val Loss: 13.5897
2025-02-04 11:10:27.911 INFO: Epoch 25, Train Loss: 17.0164, Val Loss: 13.5897
train_e/atom_mae: 0.009979
2025-02-04 11:10:27.931 INFO: train_e/atom_mae: 0.009979
train_e/atom_rmse: 0.013328
2025-02-04 11:10:28.013 INFO: train_e/atom_rmse: 0.013328
train_f_mae: 0.079970
2025-02-04 11:10:28.016 INFO: train_f_mae: 0.079970
train_f_rmse: 0.127912
2025-02-04 11:10:28.016 INFO: train_f_rmse: 0.127912
val_e/atom_mae: 0.004921
2025-02-04 11:10:28.018 INFO: val_e/atom_mae: 0.004921
val_e/atom_rmse: 0.006586
2025-02-04 11:10:28.018 INFO: val_e/atom_rmse: 0.006586
val_f_mae: 0.072365
2025-02-04 11:10:28.019 INFO: val_f_mae: 0.072365
val_f_rmse: 0.115855
2025-02-04 11:10:28.019 INFO: val_f_rmse: 0.115855
##### Step: 25 Learning rate: 0.005 #####
2025-02-04 11:12:18.953 INFO: ##### Step: 25 Learning rate: 0.005 #####
Epoch 26, Train Loss: 16.9686, Val Loss: 13.5863
2025-02-04 11:12:18.954 INFO: Epoch 26, Train Loss: 16.9686, Val Loss: 13.5863
train_e/atom_mae: 0.014012
2025-02-04 11:12:18.955 INFO: train_e/atom_mae: 0.014012
train_e/atom_rmse: 0.017717
2025-02-04 11:12:18.955 INFO: train_e/atom_rmse: 0.017717
train_f_mae: 0.077822
2025-02-04 11:12:18.957 INFO: train_f_mae: 0.077822
train_f_rmse: 0.125744
2025-02-04 11:12:18.958 INFO: train_f_rmse: 0.125744
val_e/atom_mae: 0.005195
2025-02-04 11:12:18.960 INFO: val_e/atom_mae: 0.005195
val_e/atom_rmse: 0.006674
2025-02-04 11:12:18.960 INFO: val_e/atom_rmse: 0.006674
val_f_mae: 0.072832
2025-02-04 11:12:18.961 INFO: val_f_mae: 0.072832
val_f_rmse: 0.115832
2025-02-04 11:12:18.961 INFO: val_f_rmse: 0.115832
##### Step: 26 Learning rate: 0.005 #####
2025-02-04 11:14:12.398 INFO: ##### Step: 26 Learning rate: 0.005 #####
Epoch 27, Train Loss: 17.9346, Val Loss: 13.3954
2025-02-04 11:14:12.399 INFO: Epoch 27, Train Loss: 17.9346, Val Loss: 13.3954
train_e/atom_mae: 0.010626
2025-02-04 11:14:12.491 INFO: train_e/atom_mae: 0.010626
train_e/atom_rmse: 0.013784
2025-02-04 11:14:12.499 INFO: train_e/atom_rmse: 0.013784
train_f_mae: 0.083571
2025-02-04 11:14:12.502 INFO: train_f_mae: 0.083571
train_f_rmse: 0.131279
2025-02-04 11:14:12.502 INFO: train_f_rmse: 0.131279
val_e/atom_mae: 0.004061
2025-02-04 11:14:12.505 INFO: val_e/atom_mae: 0.004061
val_e/atom_rmse: 0.006315
2025-02-04 11:14:12.505 INFO: val_e/atom_rmse: 0.006315
val_f_mae: 0.072205
2025-02-04 11:14:12.505 INFO: val_f_mae: 0.072205
val_f_rmse: 0.115075
2025-02-04 11:14:12.506 INFO: val_f_rmse: 0.115075
##### Step: 27 Learning rate: 0.005 #####
2025-02-04 11:16:26.615 INFO: ##### Step: 27 Learning rate: 0.005 #####
Epoch 28, Train Loss: 15.9792, Val Loss: 13.4201
2025-02-04 11:16:26.615 INFO: Epoch 28, Train Loss: 15.9792, Val Loss: 13.4201
train_e/atom_mae: 0.008634
2025-02-04 11:16:26.719 INFO: train_e/atom_mae: 0.008634
train_e/atom_rmse: 0.011177
2025-02-04 11:16:26.865 INFO: train_e/atom_rmse: 0.011177
train_f_mae: 0.078072
2025-02-04 11:16:26.868 INFO: train_f_mae: 0.078072
train_f_rmse: 0.124574
2025-02-04 11:16:26.868 INFO: train_f_rmse: 0.124574
val_e/atom_mae: 0.004750
2025-02-04 11:16:26.870 INFO: val_e/atom_mae: 0.004750
val_e/atom_rmse: 0.006168
2025-02-04 11:16:26.870 INFO: val_e/atom_rmse: 0.006168
val_f_mae: 0.071876
2025-02-04 11:16:26.871 INFO: val_f_mae: 0.071876
val_f_rmse: 0.115215
2025-02-04 11:16:26.871 INFO: val_f_rmse: 0.115215
##### Step: 28 Learning rate: 0.005 #####
2025-02-04 11:18:17.836 INFO: ##### Step: 28 Learning rate: 0.005 #####
Epoch 29, Train Loss: 16.6693, Val Loss: 13.0447
2025-02-04 11:18:17.837 INFO: Epoch 29, Train Loss: 16.6693, Val Loss: 13.0447
train_e/atom_mae: 0.011547
2025-02-04 11:18:17.838 INFO: train_e/atom_mae: 0.011547
train_e/atom_rmse: 0.014861
2025-02-04 11:18:17.838 INFO: train_e/atom_rmse: 0.014861
train_f_mae: 0.079030
2025-02-04 11:18:17.840 INFO: train_f_mae: 0.079030
train_f_rmse: 0.125918
2025-02-04 11:18:17.841 INFO: train_f_rmse: 0.125918
val_e/atom_mae: 0.003819
2025-02-04 11:18:17.843 INFO: val_e/atom_mae: 0.003819
val_e/atom_rmse: 0.006311
2025-02-04 11:18:17.843 INFO: val_e/atom_rmse: 0.006311
val_f_mae: 0.071091
2025-02-04 11:18:17.844 INFO: val_f_mae: 0.071091
val_f_rmse: 0.113549
2025-02-04 11:18:17.844 INFO: val_f_rmse: 0.113549
##### Step: 29 Learning rate: 0.005 #####
2025-02-04 11:20:09.130 INFO: ##### Step: 29 Learning rate: 0.005 #####
Epoch 30, Train Loss: 16.8900, Val Loss: 12.9700
2025-02-04 11:20:09.131 INFO: Epoch 30, Train Loss: 16.8900, Val Loss: 12.9700
train_e/atom_mae: 0.010148
2025-02-04 11:20:09.132 INFO: train_e/atom_mae: 0.010148
train_e/atom_rmse: 0.012805
2025-02-04 11:20:09.132 INFO: train_e/atom_rmse: 0.012805
train_f_mae: 0.081044
2025-02-04 11:20:09.135 INFO: train_f_mae: 0.081044
train_f_rmse: 0.127615
2025-02-04 11:20:09.135 INFO: train_f_rmse: 0.127615
val_e/atom_mae: 0.004979
2025-02-04 11:20:09.137 INFO: val_e/atom_mae: 0.004979
val_e/atom_rmse: 0.006203
2025-02-04 11:20:09.137 INFO: val_e/atom_rmse: 0.006203
val_f_mae: 0.070769
2025-02-04 11:20:09.138 INFO: val_f_mae: 0.070769
val_f_rmse: 0.113234
2025-02-04 11:20:09.138 INFO: val_f_rmse: 0.113234
##### Step: 30 Learning rate: 0.005 #####
2025-02-04 11:22:02.106 INFO: ##### Step: 30 Learning rate: 0.005 #####
Epoch 31, Train Loss: 16.7190, Val Loss: 13.0062
2025-02-04 11:22:02.107 INFO: Epoch 31, Train Loss: 16.7190, Val Loss: 13.0062
train_e/atom_mae: 0.008489
2025-02-04 11:22:02.108 INFO: train_e/atom_mae: 0.008489
train_e/atom_rmse: 0.010883
2025-02-04 11:22:02.108 INFO: train_e/atom_rmse: 0.010883
train_f_mae: 0.080936
2025-02-04 11:22:02.111 INFO: train_f_mae: 0.080936
train_f_rmse: 0.127602
2025-02-04 11:22:02.111 INFO: train_f_rmse: 0.127602
val_e/atom_mae: 0.004233
2025-02-04 11:22:02.113 INFO: val_e/atom_mae: 0.004233
val_e/atom_rmse: 0.006122
2025-02-04 11:22:02.113 INFO: val_e/atom_rmse: 0.006122
val_f_mae: 0.070511
2025-02-04 11:22:02.114 INFO: val_f_mae: 0.070511
val_f_rmse: 0.113413
2025-02-04 11:22:02.114 INFO: val_f_rmse: 0.113413
##### Step: 31 Learning rate: 0.005 #####
2025-02-04 11:23:53.051 INFO: ##### Step: 31 Learning rate: 0.005 #####
Epoch 32, Train Loss: 17.0127, Val Loss: 12.8086
2025-02-04 11:23:53.051 INFO: Epoch 32, Train Loss: 17.0127, Val Loss: 12.8086
train_e/atom_mae: 0.009019
2025-02-04 11:23:53.052 INFO: train_e/atom_mae: 0.009019
train_e/atom_rmse: 0.011690
2025-02-04 11:23:53.052 INFO: train_e/atom_rmse: 0.011690
train_f_mae: 0.081907
2025-02-04 11:23:53.055 INFO: train_f_mae: 0.081907
train_f_rmse: 0.128487
2025-02-04 11:23:53.055 INFO: train_f_rmse: 0.128487
val_e/atom_mae: 0.004100
2025-02-04 11:23:53.057 INFO: val_e/atom_mae: 0.004100
val_e/atom_rmse: 0.005846
2025-02-04 11:23:53.058 INFO: val_e/atom_rmse: 0.005846
val_f_mae: 0.070531
2025-02-04 11:23:53.058 INFO: val_f_mae: 0.070531
val_f_rmse: 0.112593
2025-02-04 11:23:53.058 INFO: val_f_rmse: 0.112593
##### Step: 32 Learning rate: 0.005 #####
2025-02-04 11:25:44.053 INFO: ##### Step: 32 Learning rate: 0.005 #####
Epoch 33, Train Loss: 15.8551, Val Loss: 13.0433
2025-02-04 11:25:44.054 INFO: Epoch 33, Train Loss: 15.8551, Val Loss: 13.0433
train_e/atom_mae: 0.010963
2025-02-04 11:25:44.055 INFO: train_e/atom_mae: 0.010963
train_e/atom_rmse: 0.013636
2025-02-04 11:25:44.055 INFO: train_e/atom_rmse: 0.013636
train_f_mae: 0.076567
2025-02-04 11:25:44.057 INFO: train_f_mae: 0.076567
train_f_rmse: 0.123165
2025-02-04 11:25:44.058 INFO: train_f_rmse: 0.123165
val_e/atom_mae: 0.005211
2025-02-04 11:25:44.060 INFO: val_e/atom_mae: 0.005211
val_e/atom_rmse: 0.006409
2025-02-04 11:25:44.060 INFO: val_e/atom_rmse: 0.006409
val_f_mae: 0.070618
2025-02-04 11:25:44.061 INFO: val_f_mae: 0.070618
val_f_rmse: 0.113524
2025-02-04 11:25:44.061 INFO: val_f_rmse: 0.113524
##### Step: 33 Learning rate: 0.005 #####
2025-02-04 11:27:34.960 INFO: ##### Step: 33 Learning rate: 0.005 #####
Epoch 34, Train Loss: 17.0103, Val Loss: 12.4712
2025-02-04 11:27:34.960 INFO: Epoch 34, Train Loss: 17.0103, Val Loss: 12.4712
train_e/atom_mae: 0.008038
2025-02-04 11:27:34.961 INFO: train_e/atom_mae: 0.008038
train_e/atom_rmse: 0.010435
2025-02-04 11:27:34.961 INFO: train_e/atom_rmse: 0.010435
train_f_mae: 0.082925
2025-02-04 11:27:34.964 INFO: train_f_mae: 0.082925
train_f_rmse: 0.128876
2025-02-04 11:27:34.964 INFO: train_f_rmse: 0.128876
val_e/atom_mae: 0.003770
2025-02-04 11:27:34.966 INFO: val_e/atom_mae: 0.003770
val_e/atom_rmse: 0.005865
2025-02-04 11:27:34.967 INFO: val_e/atom_rmse: 0.005865
val_f_mae: 0.069806
2025-02-04 11:27:34.967 INFO: val_f_mae: 0.069806
val_f_rmse: 0.111075
2025-02-04 11:27:34.967 INFO: val_f_rmse: 0.111075
##### Step: 34 Learning rate: 0.005 #####
2025-02-04 11:29:25.905 INFO: ##### Step: 34 Learning rate: 0.005 #####
Epoch 35, Train Loss: 15.6244, Val Loss: 12.5400
2025-02-04 11:29:25.906 INFO: Epoch 35, Train Loss: 15.6244, Val Loss: 12.5400
train_e/atom_mae: 0.008283
2025-02-04 11:29:25.907 INFO: train_e/atom_mae: 0.008283
train_e/atom_rmse: 0.011056
2025-02-04 11:29:25.907 INFO: train_e/atom_rmse: 0.011056
train_f_mae: 0.077139
2025-02-04 11:29:25.910 INFO: train_f_mae: 0.077139
train_f_rmse: 0.123182
2025-02-04 11:29:25.910 INFO: train_f_rmse: 0.123182
val_e/atom_mae: 0.004394
2025-02-04 11:29:25.912 INFO: val_e/atom_mae: 0.004394
val_e/atom_rmse: 0.005780
2025-02-04 11:29:25.912 INFO: val_e/atom_rmse: 0.005780
val_f_mae: 0.069702
2025-02-04 11:29:25.913 INFO: val_f_mae: 0.069702
val_f_rmse: 0.111407
2025-02-04 11:29:25.913 INFO: val_f_rmse: 0.111407
##### Step: 35 Learning rate: 0.005 #####
2025-02-04 11:31:16.914 INFO: ##### Step: 35 Learning rate: 0.005 #####
Epoch 36, Train Loss: 15.7401, Val Loss: 12.3109
2025-02-04 11:31:16.915 INFO: Epoch 36, Train Loss: 15.7401, Val Loss: 12.3109
train_e/atom_mae: 0.013451
2025-02-04 11:31:16.916 INFO: train_e/atom_mae: 0.013451
train_e/atom_rmse: 0.016809
2025-02-04 11:31:16.916 INFO: train_e/atom_rmse: 0.016809
train_f_mae: 0.075835
2025-02-04 11:31:16.918 INFO: train_f_mae: 0.075835
train_f_rmse: 0.121237
2025-02-04 11:31:16.919 INFO: train_f_rmse: 0.121237
val_e/atom_mae: 0.004199
2025-02-04 11:31:16.921 INFO: val_e/atom_mae: 0.004199
val_e/atom_rmse: 0.005921
2025-02-04 11:31:16.921 INFO: val_e/atom_rmse: 0.005921
val_f_mae: 0.069051
2025-02-04 11:31:16.922 INFO: val_f_mae: 0.069051
val_f_rmse: 0.110342
2025-02-04 11:31:16.922 INFO: val_f_rmse: 0.110342
##### Step: 36 Learning rate: 0.005 #####
2025-02-04 11:33:07.871 INFO: ##### Step: 36 Learning rate: 0.005 #####
Epoch 37, Train Loss: 15.4521, Val Loss: 12.2388
2025-02-04 11:33:07.872 INFO: Epoch 37, Train Loss: 15.4521, Val Loss: 12.2388
train_e/atom_mae: 0.009884
2025-02-04 11:33:07.873 INFO: train_e/atom_mae: 0.009884
train_e/atom_rmse: 0.012726
2025-02-04 11:33:07.873 INFO: train_e/atom_rmse: 0.012726
train_f_mae: 0.076619
2025-02-04 11:33:07.875 INFO: train_f_mae: 0.076619
train_f_rmse: 0.121881
2025-02-04 11:33:07.876 INFO: train_f_rmse: 0.121881
val_e/atom_mae: 0.004926
2025-02-04 11:33:07.878 INFO: val_e/atom_mae: 0.004926
val_e/atom_rmse: 0.005949
2025-02-04 11:33:07.878 INFO: val_e/atom_rmse: 0.005949
val_f_mae: 0.068890
2025-02-04 11:33:07.879 INFO: val_f_mae: 0.068890
val_f_rmse: 0.110009
2025-02-04 11:33:07.879 INFO: val_f_rmse: 0.110009
##### Step: 37 Learning rate: 0.005 #####
2025-02-04 11:34:58.737 INFO: ##### Step: 37 Learning rate: 0.005 #####
Epoch 38, Train Loss: 16.6932, Val Loss: 12.4196
2025-02-04 11:34:58.737 INFO: Epoch 38, Train Loss: 16.6932, Val Loss: 12.4196
train_e/atom_mae: 0.013528
2025-02-04 11:34:58.738 INFO: train_e/atom_mae: 0.013528
train_e/atom_rmse: 0.017468
2025-02-04 11:34:58.739 INFO: train_e/atom_rmse: 0.017468
train_f_mae: 0.079248
2025-02-04 11:34:58.741 INFO: train_f_mae: 0.079248
train_f_rmse: 0.124774
2025-02-04 11:34:58.741 INFO: train_f_rmse: 0.124774
val_e/atom_mae: 0.005498
2025-02-04 11:34:58.744 INFO: val_e/atom_mae: 0.005498
val_e/atom_rmse: 0.006532
2025-02-04 11:34:58.744 INFO: val_e/atom_rmse: 0.006532
val_f_mae: 0.069244
2025-02-04 11:34:58.744 INFO: val_f_mae: 0.069244
val_f_rmse: 0.110711
2025-02-04 11:34:58.745 INFO: val_f_rmse: 0.110711
##### Step: 38 Learning rate: 0.005 #####
2025-02-04 11:36:56.536 INFO: ##### Step: 38 Learning rate: 0.005 #####
Epoch 39, Train Loss: 15.7785, Val Loss: 12.1507
2025-02-04 11:36:56.537 INFO: Epoch 39, Train Loss: 15.7785, Val Loss: 12.1507
train_e/atom_mae: 0.008970
2025-02-04 11:36:57.237 INFO: train_e/atom_mae: 0.008970
train_e/atom_rmse: 0.011447
2025-02-04 11:36:58.267 INFO: train_e/atom_rmse: 0.011447
train_f_mae: 0.078391
2025-02-04 11:36:58.270 INFO: train_f_mae: 0.078391
train_f_rmse: 0.123675
2025-02-04 11:36:58.270 INFO: train_f_rmse: 0.123675
val_e/atom_mae: 0.004210
2025-02-04 11:36:58.272 INFO: val_e/atom_mae: 0.004210
val_e/atom_rmse: 0.005830
2025-02-04 11:36:58.272 INFO: val_e/atom_rmse: 0.005830
val_f_mae: 0.068680
2025-02-04 11:36:58.273 INFO: val_f_mae: 0.068680
val_f_rmse: 0.109632
2025-02-04 11:36:58.273 INFO: val_f_rmse: 0.109632
##### Step: 39 Learning rate: 0.005 #####
2025-02-04 11:38:51.806 INFO: ##### Step: 39 Learning rate: 0.005 #####
Epoch 40, Train Loss: 15.1906, Val Loss: 12.0541
2025-02-04 11:38:51.807 INFO: Epoch 40, Train Loss: 15.1906, Val Loss: 12.0541
train_e/atom_mae: 0.008262
2025-02-04 11:38:51.807 INFO: train_e/atom_mae: 0.008262
train_e/atom_rmse: 0.010778
2025-02-04 11:38:51.808 INFO: train_e/atom_rmse: 0.010778
train_f_mae: 0.076495
2025-02-04 11:38:51.810 INFO: train_f_mae: 0.076495
train_f_rmse: 0.121500
2025-02-04 11:38:51.810 INFO: train_f_rmse: 0.121500
val_e/atom_mae: 0.003878
2025-02-04 11:38:51.813 INFO: val_e/atom_mae: 0.003878
val_e/atom_rmse: 0.005653
2025-02-04 11:38:51.813 INFO: val_e/atom_rmse: 0.005653
val_f_mae: 0.068409
2025-02-04 11:38:51.813 INFO: val_f_mae: 0.068409
val_f_rmse: 0.109224
2025-02-04 11:38:51.814 INFO: val_f_rmse: 0.109224
2025-02-04 11:38:52.068 INFO: training
##### Step: 0 Learning rate: 0.002 #####
2025-02-04 11:40:43.561 INFO: ##### Step: 0 Learning rate: 0.002 #####
Epoch 1, Train Loss: 1015.8486, Val Loss: 74.5319
2025-02-04 11:40:43.562 INFO: Epoch 1, Train Loss: 1015.8486, Val Loss: 74.5319
train_e/atom_mae: 0.086377
2025-02-04 11:40:43.563 INFO: train_e/atom_mae: 0.086377
train_e/atom_rmse: 0.257887
2025-02-04 11:40:43.563 INFO: train_e/atom_rmse: 0.257887
train_f_mae: 0.308035
2025-02-04 11:40:43.566 INFO: train_f_mae: 0.308035
train_f_rmse: 0.877885
2025-02-04 11:40:43.566 INFO: train_f_rmse: 0.877885
val_e/atom_mae: 0.078875
2025-02-04 11:40:43.568 INFO: val_e/atom_mae: 0.078875
val_e/atom_rmse: 0.085216
2025-02-04 11:40:43.568 INFO: val_e/atom_rmse: 0.085216
val_f_mae: 0.160979
2025-02-04 11:40:43.569 INFO: val_f_mae: 0.160979
val_f_rmse: 0.218546
2025-02-04 11:40:43.569 INFO: val_f_rmse: 0.218546
##### Step: 1 Learning rate: 0.004 #####
2025-02-04 11:42:34.074 INFO: ##### Step: 1 Learning rate: 0.004 #####
Epoch 2, Train Loss: 48.8997, Val Loss: 62.0321
2025-02-04 11:42:34.074 INFO: Epoch 2, Train Loss: 48.8997, Val Loss: 62.0321
train_e/atom_mae: 0.027576
2025-02-04 11:42:34.075 INFO: train_e/atom_mae: 0.027576
train_e/atom_rmse: 0.036186
2025-02-04 11:42:34.075 INFO: train_e/atom_rmse: 0.036186
train_f_mae: 0.120838
2025-02-04 11:42:34.078 INFO: train_f_mae: 0.120838
train_f_rmse: 0.209935
2025-02-04 11:42:34.078 INFO: train_f_rmse: 0.209935
val_e/atom_mae: 0.060327
2025-02-04 11:42:34.080 INFO: val_e/atom_mae: 0.060327
val_e/atom_rmse: 0.061062
2025-02-04 11:42:34.081 INFO: val_e/atom_rmse: 0.061062
val_f_mae: 0.172202
2025-02-04 11:42:34.081 INFO: val_f_mae: 0.172202
val_f_rmse: 0.219664
2025-02-04 11:42:34.081 INFO: val_f_rmse: 0.219664
##### Step: 2 Learning rate: 0.006 #####
2025-02-04 11:44:24.611 INFO: ##### Step: 2 Learning rate: 0.006 #####
Epoch 3, Train Loss: 33.6592, Val Loss: 21.7861
2025-02-04 11:44:24.612 INFO: Epoch 3, Train Loss: 33.6592, Val Loss: 21.7861
train_e/atom_mae: 0.025384
2025-02-04 11:44:24.613 INFO: train_e/atom_mae: 0.025384
train_e/atom_rmse: 0.033253
2025-02-04 11:44:24.613 INFO: train_e/atom_rmse: 0.033253
train_f_mae: 0.109136
2025-02-04 11:44:24.616 INFO: train_f_mae: 0.109136
train_f_rmse: 0.171997
2025-02-04 11:44:24.616 INFO: train_f_rmse: 0.171997
val_e/atom_mae: 0.005683
2025-02-04 11:44:24.618 INFO: val_e/atom_mae: 0.005683
val_e/atom_rmse: 0.006564
2025-02-04 11:44:24.619 INFO: val_e/atom_rmse: 0.006564
val_f_mae: 0.095161
2025-02-04 11:44:24.619 INFO: val_f_mae: 0.095161
val_f_rmse: 0.147044
2025-02-04 11:44:24.619 INFO: val_f_rmse: 0.147044
##### Step: 3 Learning rate: 0.008 #####
2025-02-04 11:46:15.335 INFO: ##### Step: 3 Learning rate: 0.008 #####
Epoch 4, Train Loss: 24.6857, Val Loss: 15.9733
2025-02-04 11:46:15.336 INFO: Epoch 4, Train Loss: 24.6857, Val Loss: 15.9733
train_e/atom_mae: 0.020902
2025-02-04 11:46:15.337 INFO: train_e/atom_mae: 0.020902
train_e/atom_rmse: 0.027104
2025-02-04 11:46:15.337 INFO: train_e/atom_rmse: 0.027104
train_f_mae: 0.096662
2025-02-04 11:46:15.339 INFO: train_f_mae: 0.096662
train_f_rmse: 0.148248
2025-02-04 11:46:15.340 INFO: train_f_rmse: 0.148248
val_e/atom_mae: 0.022204
2025-02-04 11:46:15.342 INFO: val_e/atom_mae: 0.022204
val_e/atom_rmse: 0.022599
2025-02-04 11:46:15.342 INFO: val_e/atom_rmse: 0.022599
val_f_mae: 0.080521
2025-02-04 11:46:15.343 INFO: val_f_mae: 0.080521
val_f_rmse: 0.118666
2025-02-04 11:46:15.343 INFO: val_f_rmse: 0.118666
##### Step: 4 Learning rate: 0.01 #####
2025-02-04 11:48:05.985 INFO: ##### Step: 4 Learning rate: 0.01 #####
Epoch 5, Train Loss: 19.2787, Val Loss: 23.5070
2025-02-04 11:48:05.985 INFO: Epoch 5, Train Loss: 19.2787, Val Loss: 23.5070
train_e/atom_mae: 0.015098
2025-02-04 11:48:05.986 INFO: train_e/atom_mae: 0.015098
train_e/atom_rmse: 0.018709
2025-02-04 11:48:05.986 INFO: train_e/atom_rmse: 0.018709
train_f_mae: 0.085252
2025-02-04 11:48:05.989 INFO: train_f_mae: 0.085252
train_f_rmse: 0.134120
2025-02-04 11:48:05.989 INFO: train_f_rmse: 0.134120
val_e/atom_mae: 0.003750
2025-02-04 11:48:05.991 INFO: val_e/atom_mae: 0.003750
val_e/atom_rmse: 0.004846
2025-02-04 11:48:05.992 INFO: val_e/atom_rmse: 0.004846
val_f_mae: 0.104150
2025-02-04 11:48:05.992 INFO: val_f_mae: 0.104150
val_f_rmse: 0.152920
2025-02-04 11:48:05.992 INFO: val_f_rmse: 0.152920
##### Step: 5 Learning rate: 0.01 #####
2025-02-04 11:49:56.525 INFO: ##### Step: 5 Learning rate: 0.01 #####
Epoch 6, Train Loss: 18.7784, Val Loss: 16.9729
2025-02-04 11:49:56.526 INFO: Epoch 6, Train Loss: 18.7784, Val Loss: 16.9729
train_e/atom_mae: 0.017016
2025-02-04 11:49:56.527 INFO: train_e/atom_mae: 0.017016
train_e/atom_rmse: 0.021624
2025-02-04 11:49:56.527 INFO: train_e/atom_rmse: 0.021624
train_f_mae: 0.085499
2025-02-04 11:49:56.530 INFO: train_f_mae: 0.085499
train_f_rmse: 0.130593
2025-02-04 11:49:56.530 INFO: train_f_rmse: 0.130593
val_e/atom_mae: 0.010621
2025-02-04 11:49:56.532 INFO: val_e/atom_mae: 0.010621
val_e/atom_rmse: 0.012225
2025-02-04 11:49:56.532 INFO: val_e/atom_rmse: 0.012225
val_f_mae: 0.084359
2025-02-04 11:49:56.533 INFO: val_f_mae: 0.084359
val_f_rmse: 0.128145
2025-02-04 11:49:56.533 INFO: val_f_rmse: 0.128145
##### Step: 6 Learning rate: 0.01 #####
2025-02-04 11:51:47.097 INFO: ##### Step: 6 Learning rate: 0.01 #####
Epoch 7, Train Loss: 18.1623, Val Loss: 13.9925
2025-02-04 11:51:47.098 INFO: Epoch 7, Train Loss: 18.1623, Val Loss: 13.9925
train_e/atom_mae: 0.019382
2025-02-04 11:51:47.099 INFO: train_e/atom_mae: 0.019382
train_e/atom_rmse: 0.023678
2025-02-04 11:51:47.099 INFO: train_e/atom_rmse: 0.023678
train_f_mae: 0.082528
2025-02-04 11:51:47.101 INFO: train_f_mae: 0.082528
train_f_rmse: 0.126868
2025-02-04 11:51:47.102 INFO: train_f_rmse: 0.126868
val_e/atom_mae: 0.024347
2025-02-04 11:51:47.104 INFO: val_e/atom_mae: 0.024347
val_e/atom_rmse: 0.025273
2025-02-04 11:51:47.104 INFO: val_e/atom_rmse: 0.025273
val_f_mae: 0.066579
2025-02-04 11:51:47.105 INFO: val_f_mae: 0.066579
val_f_rmse: 0.107932
2025-02-04 11:51:47.105 INFO: val_f_rmse: 0.107932
##### Step: 7 Learning rate: 0.01 #####
2025-02-04 11:53:37.788 INFO: ##### Step: 7 Learning rate: 0.01 #####
Epoch 8, Train Loss: 17.0657, Val Loss: 12.7459
2025-02-04 11:53:37.789 INFO: Epoch 8, Train Loss: 17.0657, Val Loss: 12.7459
train_e/atom_mae: 0.014879
2025-02-04 11:53:37.790 INFO: train_e/atom_mae: 0.014879
train_e/atom_rmse: 0.018864
2025-02-04 11:53:37.790 INFO: train_e/atom_rmse: 0.018864
train_f_mae: 0.080471
2025-02-04 11:53:37.793 INFO: train_f_mae: 0.080471
train_f_rmse: 0.125515
2025-02-04 11:53:37.793 INFO: train_f_rmse: 0.125515
val_e/atom_mae: 0.007733
2025-02-04 11:53:37.795 INFO: val_e/atom_mae: 0.007733
val_e/atom_rmse: 0.008980
2025-02-04 11:53:37.796 INFO: val_e/atom_rmse: 0.008980
val_f_mae: 0.076678
2025-02-04 11:53:37.796 INFO: val_f_mae: 0.076678
val_f_rmse: 0.111560
2025-02-04 11:53:37.797 INFO: val_f_rmse: 0.111560
##### Step: 8 Learning rate: 0.01 #####
2025-02-04 11:55:28.458 INFO: ##### Step: 8 Learning rate: 0.01 #####
Epoch 9, Train Loss: 16.7262, Val Loss: 15.3256
2025-02-04 11:55:28.459 INFO: Epoch 9, Train Loss: 16.7262, Val Loss: 15.3256
train_e/atom_mae: 0.012307
2025-02-04 11:55:28.460 INFO: train_e/atom_mae: 0.012307
train_e/atom_rmse: 0.015413
2025-02-04 11:55:28.460 INFO: train_e/atom_rmse: 0.015413
train_f_mae: 0.081167
2025-02-04 11:55:28.463 INFO: train_f_mae: 0.081167
train_f_rmse: 0.125899
2025-02-04 11:55:28.463 INFO: train_f_rmse: 0.125899
val_e/atom_mae: 0.003427
2025-02-04 11:55:28.465 INFO: val_e/atom_mae: 0.003427
val_e/atom_rmse: 0.004909
2025-02-04 11:55:28.465 INFO: val_e/atom_rmse: 0.004909
val_f_mae: 0.083117
2025-02-04 11:55:28.466 INFO: val_f_mae: 0.083117
val_f_rmse: 0.123442
2025-02-04 11:55:28.466 INFO: val_f_rmse: 0.123442
##### Step: 9 Learning rate: 0.01 #####
2025-02-04 11:57:19.124 INFO: ##### Step: 9 Learning rate: 0.01 #####
Epoch 10, Train Loss: 12.6687, Val Loss: 20.7931
2025-02-04 11:57:19.124 INFO: Epoch 10, Train Loss: 12.6687, Val Loss: 20.7931
train_e/atom_mae: 0.012176
2025-02-04 11:57:19.125 INFO: train_e/atom_mae: 0.012176
train_e/atom_rmse: 0.016244
2025-02-04 11:57:19.126 INFO: train_e/atom_rmse: 0.016244
train_f_mae: 0.071808
2025-02-04 11:57:19.128 INFO: train_f_mae: 0.071808
train_f_rmse: 0.108148
2025-02-04 11:57:19.128 INFO: train_f_rmse: 0.108148
val_e/atom_mae: 0.004863
2025-02-04 11:57:19.130 INFO: val_e/atom_mae: 0.004863
val_e/atom_rmse: 0.006337
2025-02-04 11:57:19.131 INFO: val_e/atom_rmse: 0.006337
val_f_mae: 0.098887
2025-02-04 11:57:19.131 INFO: val_f_mae: 0.098887
val_f_rmse: 0.143648
2025-02-04 11:57:19.132 INFO: val_f_rmse: 0.143648
##### Step: 10 Learning rate: 0.01 #####
2025-02-04 11:59:42.973 INFO: ##### Step: 10 Learning rate: 0.01 #####
Epoch 11, Train Loss: 13.7556, Val Loss: 5.3714
2025-02-04 11:59:42.974 INFO: Epoch 11, Train Loss: 13.7556, Val Loss: 5.3714
train_e/atom_mae: 0.012000
2025-02-04 11:59:43.195 INFO: train_e/atom_mae: 0.012000
train_e/atom_rmse: 0.015073
2025-02-04 11:59:43.402 INFO: train_e/atom_rmse: 0.015073
train_f_mae: 0.074298
2025-02-04 11:59:43.404 INFO: train_f_mae: 0.074298
train_f_rmse: 0.113657
2025-02-04 11:59:43.405 INFO: train_f_rmse: 0.113657
val_e/atom_mae: 0.004693
2025-02-04 11:59:43.407 INFO: val_e/atom_mae: 0.004693
val_e/atom_rmse: 0.005355
2025-02-04 11:59:43.407 INFO: val_e/atom_rmse: 0.005355
val_f_mae: 0.046539
2025-02-04 11:59:43.408 INFO: val_f_mae: 0.046539
val_f_rmse: 0.072590
2025-02-04 11:59:43.408 INFO: val_f_rmse: 0.072590
##### Step: 11 Learning rate: 0.01 #####
2025-02-04 12:01:35.412 INFO: ##### Step: 11 Learning rate: 0.01 #####
Epoch 12, Train Loss: 14.2828, Val Loss: 5.3150
2025-02-04 12:01:35.413 INFO: Epoch 12, Train Loss: 14.2828, Val Loss: 5.3150
train_e/atom_mae: 0.012433
2025-02-04 12:01:35.414 INFO: train_e/atom_mae: 0.012433
train_e/atom_rmse: 0.016403
2025-02-04 12:01:35.414 INFO: train_e/atom_rmse: 0.016403
train_f_mae: 0.075420
2025-02-04 12:01:35.417 INFO: train_f_mae: 0.075420
train_f_rmse: 0.115286
2025-02-04 12:01:35.417 INFO: train_f_rmse: 0.115286
val_e/atom_mae: 0.005825
2025-02-04 12:01:35.419 INFO: val_e/atom_mae: 0.005825
val_e/atom_rmse: 0.006434
2025-02-04 12:01:35.419 INFO: val_e/atom_rmse: 0.006434
val_f_mae: 0.047190
2025-02-04 12:01:35.420 INFO: val_f_mae: 0.047190
val_f_rmse: 0.071876
2025-02-04 12:01:35.420 INFO: val_f_rmse: 0.071876
##### Step: 12 Learning rate: 0.01 #####
2025-02-04 12:03:26.652 INFO: ##### Step: 12 Learning rate: 0.01 #####
Epoch 13, Train Loss: 12.1581, Val Loss: 4.8248
2025-02-04 12:03:26.653 INFO: Epoch 13, Train Loss: 12.1581, Val Loss: 4.8248
train_e/atom_mae: 0.016167
2025-02-04 12:03:26.654 INFO: train_e/atom_mae: 0.016167
train_e/atom_rmse: 0.019369
2025-02-04 12:03:26.654 INFO: train_e/atom_rmse: 0.019369
train_f_mae: 0.068631
2025-02-04 12:03:26.657 INFO: train_f_mae: 0.068631
train_f_rmse: 0.103803
2025-02-04 12:03:26.657 INFO: train_f_rmse: 0.103803
val_e/atom_mae: 0.001823
2025-02-04 12:03:26.660 INFO: val_e/atom_mae: 0.001823
val_e/atom_rmse: 0.002506
2025-02-04 12:03:26.660 INFO: val_e/atom_rmse: 0.002506
val_f_mae: 0.044680
2025-02-04 12:03:26.660 INFO: val_f_mae: 0.044680
val_f_rmse: 0.069329
2025-02-04 12:03:26.661 INFO: val_f_rmse: 0.069329
##### Step: 13 Learning rate: 0.01 #####
2025-02-04 12:05:17.890 INFO: ##### Step: 13 Learning rate: 0.01 #####
Epoch 14, Train Loss: 12.8336, Val Loss: 5.3265
2025-02-04 12:05:17.891 INFO: Epoch 14, Train Loss: 12.8336, Val Loss: 5.3265
train_e/atom_mae: 0.014139
2025-02-04 12:05:17.891 INFO: train_e/atom_mae: 0.014139
train_e/atom_rmse: 0.017427
2025-02-04 12:05:17.892 INFO: train_e/atom_rmse: 0.017427
train_f_mae: 0.068677
2025-02-04 12:05:17.894 INFO: train_f_mae: 0.068677
train_f_rmse: 0.108231
2025-02-04 12:05:17.894 INFO: train_f_rmse: 0.108231
val_e/atom_mae: 0.005808
2025-02-04 12:05:17.897 INFO: val_e/atom_mae: 0.005808
val_e/atom_rmse: 0.006326
2025-02-04 12:05:17.897 INFO: val_e/atom_rmse: 0.006326
val_f_mae: 0.046839
2025-02-04 12:05:17.897 INFO: val_f_mae: 0.046839
val_f_rmse: 0.071996
2025-02-04 12:05:17.898 INFO: val_f_rmse: 0.071996
##### Step: 14 Learning rate: 0.01 #####
2025-02-04 12:07:31.344 INFO: ##### Step: 14 Learning rate: 0.01 #####
Epoch 15, Train Loss: 14.7408, Val Loss: 5.8671
2025-02-04 12:07:31.344 INFO: Epoch 15, Train Loss: 14.7408, Val Loss: 5.8671
train_e/atom_mae: 0.014336
2025-02-04 12:07:31.601 INFO: train_e/atom_mae: 0.014336
train_e/atom_rmse: 0.017921
2025-02-04 12:07:31.845 INFO: train_e/atom_rmse: 0.017921
train_f_mae: 0.076343
2025-02-04 12:07:31.847 INFO: train_f_mae: 0.076343
train_f_rmse: 0.116434
2025-02-04 12:07:31.848 INFO: train_f_rmse: 0.116434
val_e/atom_mae: 0.006325
2025-02-04 12:07:31.850 INFO: val_e/atom_mae: 0.006325
val_e/atom_rmse: 0.006990
2025-02-04 12:07:31.850 INFO: val_e/atom_rmse: 0.006990
val_f_mae: 0.048277
2025-02-04 12:07:31.851 INFO: val_f_mae: 0.048277
val_f_rmse: 0.075441
2025-02-04 12:07:31.851 INFO: val_f_rmse: 0.075441
##### Step: 15 Learning rate: 0.01 #####
2025-02-04 12:09:47.026 INFO: ##### Step: 15 Learning rate: 0.01 #####
Epoch 16, Train Loss: 11.9806, Val Loss: 4.8064
2025-02-04 12:09:47.027 INFO: Epoch 16, Train Loss: 11.9806, Val Loss: 4.8064
train_e/atom_mae: 0.009443
2025-02-04 12:09:47.158 INFO: train_e/atom_mae: 0.009443
train_e/atom_rmse: 0.012727
2025-02-04 12:09:47.253 INFO: train_e/atom_rmse: 0.012727
train_f_mae: 0.068935
2025-02-04 12:09:47.256 INFO: train_f_mae: 0.068935
train_f_rmse: 0.106694
2025-02-04 12:09:47.256 INFO: train_f_rmse: 0.106694
val_e/atom_mae: 0.002940
2025-02-04 12:09:47.259 INFO: val_e/atom_mae: 0.002940
val_e/atom_rmse: 0.003555
2025-02-04 12:09:47.259 INFO: val_e/atom_rmse: 0.003555
val_f_mae: 0.044134
2025-02-04 12:09:47.259 INFO: val_f_mae: 0.044134
val_f_rmse: 0.069015
2025-02-04 12:09:47.260 INFO: val_f_rmse: 0.069015
##### Step: 16 Learning rate: 0.01 #####
2025-02-04 12:11:46.448 INFO: ##### Step: 16 Learning rate: 0.01 #####
Epoch 17, Train Loss: 12.7373, Val Loss: 5.1112
2025-02-04 12:11:46.448 INFO: Epoch 17, Train Loss: 12.7373, Val Loss: 5.1112
train_e/atom_mae: 0.010479
2025-02-04 12:11:46.457 INFO: train_e/atom_mae: 0.010479
train_e/atom_rmse: 0.013052
2025-02-04 12:11:46.459 INFO: train_e/atom_rmse: 0.013052
train_f_mae: 0.072305
2025-02-04 12:11:46.462 INFO: train_f_mae: 0.072305
train_f_rmse: 0.110042
2025-02-04 12:11:46.462 INFO: train_f_rmse: 0.110042
val_e/atom_mae: 0.008506
2025-02-04 12:11:46.464 INFO: val_e/atom_mae: 0.008506
val_e/atom_rmse: 0.008847
2025-02-04 12:11:46.464 INFO: val_e/atom_rmse: 0.008847
val_f_mae: 0.045255
2025-02-04 12:11:46.465 INFO: val_f_mae: 0.045255
val_f_rmse: 0.069481
2025-02-04 12:11:46.465 INFO: val_f_rmse: 0.069481
##### Step: 17 Learning rate: 0.01 #####
2025-02-04 12:13:37.648 INFO: ##### Step: 17 Learning rate: 0.01 #####
Epoch 18, Train Loss: 11.8735, Val Loss: 4.9799
2025-02-04 12:13:37.648 INFO: Epoch 18, Train Loss: 11.8735, Val Loss: 4.9799
train_e/atom_mae: 0.009748
2025-02-04 12:13:37.649 INFO: train_e/atom_mae: 0.009748
train_e/atom_rmse: 0.012293
2025-02-04 12:13:37.649 INFO: train_e/atom_rmse: 0.012293
train_f_mae: 0.068686
2025-02-04 12:13:37.652 INFO: train_f_mae: 0.068686
train_f_rmse: 0.106379
2025-02-04 12:13:37.652 INFO: train_f_rmse: 0.106379
val_e/atom_mae: 0.005204
2025-02-04 12:13:37.654 INFO: val_e/atom_mae: 0.005204
val_e/atom_rmse: 0.005673
2025-02-04 12:13:37.654 INFO: val_e/atom_rmse: 0.005673
val_f_mae: 0.045178
2025-02-04 12:13:37.655 INFO: val_f_mae: 0.045178
val_f_rmse: 0.069748
2025-02-04 12:13:37.655 INFO: val_f_rmse: 0.069748
##### Step: 18 Learning rate: 0.01 #####
2025-02-04 12:15:28.835 INFO: ##### Step: 18 Learning rate: 0.01 #####
Epoch 19, Train Loss: 14.6976, Val Loss: 5.1813
2025-02-04 12:15:28.835 INFO: Epoch 19, Train Loss: 14.6976, Val Loss: 5.1813
train_e/atom_mae: 0.015581
2025-02-04 12:15:28.836 INFO: train_e/atom_mae: 0.015581
train_e/atom_rmse: 0.019718
2025-02-04 12:15:28.836 INFO: train_e/atom_rmse: 0.019718
train_f_mae: 0.076865
2025-02-04 12:15:28.839 INFO: train_f_mae: 0.076865
train_f_rmse: 0.115171
2025-02-04 12:15:28.839 INFO: train_f_rmse: 0.115171
val_e/atom_mae: 0.006566
2025-02-04 12:15:28.841 INFO: val_e/atom_mae: 0.006566
val_e/atom_rmse: 0.007318
2025-02-04 12:15:28.842 INFO: val_e/atom_rmse: 0.007318
val_f_mae: 0.046031
2025-02-04 12:15:28.842 INFO: val_f_mae: 0.046031
val_f_rmse: 0.070631
2025-02-04 12:15:28.842 INFO: val_f_rmse: 0.070631
##### Step: 19 Learning rate: 0.01 #####
2025-02-04 12:17:20.042 INFO: ##### Step: 19 Learning rate: 0.01 #####
Epoch 20, Train Loss: 12.0704, Val Loss: 4.7685
2025-02-04 12:17:20.043 INFO: Epoch 20, Train Loss: 12.0704, Val Loss: 4.7685
train_e/atom_mae: 0.009452
2025-02-04 12:17:20.044 INFO: train_e/atom_mae: 0.009452
train_e/atom_rmse: 0.012064
2025-02-04 12:17:20.044 INFO: train_e/atom_rmse: 0.012064
train_f_mae: 0.071838
2025-02-04 12:17:20.047 INFO: train_f_mae: 0.071838
train_f_rmse: 0.107396
2025-02-04 12:17:20.047 INFO: train_f_rmse: 0.107396
val_e/atom_mae: 0.007368
2025-02-04 12:17:20.049 INFO: val_e/atom_mae: 0.007368
val_e/atom_rmse: 0.007690
2025-02-04 12:17:20.050 INFO: val_e/atom_rmse: 0.007690
val_f_mae: 0.043931
2025-02-04 12:17:20.050 INFO: val_f_mae: 0.043931
val_f_rmse: 0.067495
2025-02-04 12:17:20.050 INFO: val_f_rmse: 0.067495
##### Step: 20 Learning rate: 0.005 #####
2025-02-04 12:19:30.744 INFO: ##### Step: 20 Learning rate: 0.005 #####
Epoch 21, Train Loss: 7.0120, Val Loss: 3.9440
2025-02-04 12:19:30.745 INFO: Epoch 21, Train Loss: 7.0120, Val Loss: 3.9440
train_e/atom_mae: 0.005294
2025-02-04 12:19:30.746 INFO: train_e/atom_mae: 0.005294
train_e/atom_rmse: 0.007105
2025-02-04 12:19:30.746 INFO: train_e/atom_rmse: 0.007105
train_f_mae: 0.053617
2025-02-04 12:19:30.748 INFO: train_f_mae: 0.053617
train_f_rmse: 0.082619
2025-02-04 12:19:30.749 INFO: train_f_rmse: 0.082619
val_e/atom_mae: 0.002592
2025-02-04 12:19:30.751 INFO: val_e/atom_mae: 0.002592
val_e/atom_rmse: 0.003120
2025-02-04 12:19:30.751 INFO: val_e/atom_rmse: 0.003120
val_f_mae: 0.040480
2025-02-04 12:19:30.752 INFO: val_f_mae: 0.040480
val_f_rmse: 0.062544
2025-02-04 12:19:30.752 INFO: val_f_rmse: 0.062544
##### Step: 21 Learning rate: 0.005 #####
2025-02-04 12:21:22.029 INFO: ##### Step: 21 Learning rate: 0.005 #####
Epoch 22, Train Loss: 6.4218, Val Loss: 3.8356
2025-02-04 12:21:22.030 INFO: Epoch 22, Train Loss: 6.4218, Val Loss: 3.8356
train_e/atom_mae: 0.007350
2025-02-04 12:21:22.031 INFO: train_e/atom_mae: 0.007350
train_e/atom_rmse: 0.009059
2025-02-04 12:21:22.031 INFO: train_e/atom_rmse: 0.009059
train_f_mae: 0.051371
2025-02-04 12:21:22.034 INFO: train_f_mae: 0.051371
train_f_rmse: 0.078226
2025-02-04 12:21:22.034 INFO: train_f_rmse: 0.078226
val_e/atom_mae: 0.002001
2025-02-04 12:21:22.036 INFO: val_e/atom_mae: 0.002001
val_e/atom_rmse: 0.002492
2025-02-04 12:21:22.037 INFO: val_e/atom_rmse: 0.002492
val_f_mae: 0.039532
2025-02-04 12:21:22.037 INFO: val_f_mae: 0.039532
val_f_rmse: 0.061780
2025-02-04 12:21:22.037 INFO: val_f_rmse: 0.061780
##### Step: 22 Learning rate: 0.005 #####
2025-02-04 12:23:13.317 INFO: ##### Step: 22 Learning rate: 0.005 #####
Epoch 23, Train Loss: 5.7977, Val Loss: 3.7331
2025-02-04 12:23:13.317 INFO: Epoch 23, Train Loss: 5.7977, Val Loss: 3.7331
train_e/atom_mae: 0.006520
2025-02-04 12:23:13.318 INFO: train_e/atom_mae: 0.006520
train_e/atom_rmse: 0.008190
2025-02-04 12:23:13.318 INFO: train_e/atom_rmse: 0.008190
train_f_mae: 0.048461
2025-02-04 12:23:13.321 INFO: train_f_mae: 0.048461
train_f_rmse: 0.074501
2025-02-04 12:23:13.321 INFO: train_f_rmse: 0.074501
val_e/atom_mae: 0.003652
2025-02-04 12:23:13.323 INFO: val_e/atom_mae: 0.003652
val_e/atom_rmse: 0.004014
2025-02-04 12:23:13.324 INFO: val_e/atom_rmse: 0.004014
val_f_mae: 0.038565
2025-02-04 12:23:13.324 INFO: val_f_mae: 0.038565
val_f_rmse: 0.060647
2025-02-04 12:23:13.324 INFO: val_f_rmse: 0.060647
##### Step: 23 Learning rate: 0.005 #####
2025-02-04 12:25:04.634 INFO: ##### Step: 23 Learning rate: 0.005 #####
Epoch 24, Train Loss: 5.9247, Val Loss: 3.5130
2025-02-04 12:25:04.635 INFO: Epoch 24, Train Loss: 5.9247, Val Loss: 3.5130
train_e/atom_mae: 0.008837
2025-02-04 12:25:04.636 INFO: train_e/atom_mae: 0.008837
train_e/atom_rmse: 0.010753
2025-02-04 12:25:04.636 INFO: train_e/atom_rmse: 0.010753
train_f_mae: 0.048077
2025-02-04 12:25:04.639 INFO: train_f_mae: 0.048077
train_f_rmse: 0.074152
2025-02-04 12:25:04.639 INFO: train_f_rmse: 0.074152
val_e/atom_mae: 0.003561
2025-02-04 12:25:04.641 INFO: val_e/atom_mae: 0.003561
val_e/atom_rmse: 0.004079
2025-02-04 12:25:04.642 INFO: val_e/atom_rmse: 0.004079
val_f_mae: 0.037820
2025-02-04 12:25:04.642 INFO: val_f_mae: 0.037820
val_f_rmse: 0.058776
2025-02-04 12:25:04.642 INFO: val_f_rmse: 0.058776
##### Step: 24 Learning rate: 0.005 #####
2025-02-04 12:26:55.831 INFO: ##### Step: 24 Learning rate: 0.005 #####
Epoch 25, Train Loss: 5.6913, Val Loss: 3.4392
2025-02-04 12:26:55.832 INFO: Epoch 25, Train Loss: 5.6913, Val Loss: 3.4392
train_e/atom_mae: 0.007665
2025-02-04 12:26:55.833 INFO: train_e/atom_mae: 0.007665
train_e/atom_rmse: 0.009769
2025-02-04 12:26:55.833 INFO: train_e/atom_rmse: 0.009769
train_f_mae: 0.046986
2025-02-04 12:26:55.836 INFO: train_f_mae: 0.046986
train_f_rmse: 0.073072
2025-02-04 12:26:55.836 INFO: train_f_rmse: 0.073072
val_e/atom_mae: 0.001142
2025-02-04 12:26:55.838 INFO: val_e/atom_mae: 0.001142
val_e/atom_rmse: 0.001643
2025-02-04 12:26:55.838 INFO: val_e/atom_rmse: 0.001643
val_f_mae: 0.037298
2025-02-04 12:26:55.839 INFO: val_f_mae: 0.037298
val_f_rmse: 0.058583
2025-02-04 12:26:55.839 INFO: val_f_rmse: 0.058583
##### Step: 25 Learning rate: 0.005 #####
2025-02-04 12:28:46.855 INFO: ##### Step: 25 Learning rate: 0.005 #####
Epoch 26, Train Loss: 5.1749, Val Loss: 3.3843
2025-02-04 12:28:46.855 INFO: Epoch 26, Train Loss: 5.1749, Val Loss: 3.3843
train_e/atom_mae: 0.006272
2025-02-04 12:28:46.856 INFO: train_e/atom_mae: 0.006272
train_e/atom_rmse: 0.007665
2025-02-04 12:28:46.856 INFO: train_e/atom_rmse: 0.007665
train_f_mae: 0.045216
2025-02-04 12:28:46.859 INFO: train_f_mae: 0.045216
train_f_rmse: 0.070416
2025-02-04 12:28:46.859 INFO: train_f_rmse: 0.070416
val_e/atom_mae: 0.004119
2025-02-04 12:28:46.861 INFO: val_e/atom_mae: 0.004119
val_e/atom_rmse: 0.004402
2025-02-04 12:28:46.862 INFO: val_e/atom_rmse: 0.004402
val_f_mae: 0.036806
2025-02-04 12:28:46.862 INFO: val_f_mae: 0.036806
val_f_rmse: 0.057576
2025-02-04 12:28:46.862 INFO: val_f_rmse: 0.057576
##### Step: 26 Learning rate: 0.005 #####
2025-02-04 12:30:38.059 INFO: ##### Step: 26 Learning rate: 0.005 #####
Epoch 27, Train Loss: 5.8413, Val Loss: 3.2323
2025-02-04 12:30:38.059 INFO: Epoch 27, Train Loss: 5.8413, Val Loss: 3.2323
train_e/atom_mae: 0.008001
2025-02-04 12:30:38.060 INFO: train_e/atom_mae: 0.008001
train_e/atom_rmse: 0.009859
2025-02-04 12:30:38.061 INFO: train_e/atom_rmse: 0.009859
train_f_mae: 0.047202
2025-02-04 12:30:38.063 INFO: train_f_mae: 0.047202
train_f_rmse: 0.074047
2025-02-04 12:30:38.063 INFO: train_f_rmse: 0.074047
val_e/atom_mae: 0.001091
2025-02-04 12:30:38.066 INFO: val_e/atom_mae: 0.001091
val_e/atom_rmse: 0.001696
2025-02-04 12:30:38.066 INFO: val_e/atom_rmse: 0.001696
val_f_mae: 0.036348
2025-02-04 12:30:38.066 INFO: val_f_mae: 0.036348
val_f_rmse: 0.056779
2025-02-04 12:30:38.067 INFO: val_f_rmse: 0.056779
##### Step: 27 Learning rate: 0.005 #####
2025-02-04 12:32:29.271 INFO: ##### Step: 27 Learning rate: 0.005 #####
Epoch 28, Train Loss: 5.5485, Val Loss: 3.4677
2025-02-04 12:32:29.272 INFO: Epoch 28, Train Loss: 5.5485, Val Loss: 3.4677
train_e/atom_mae: 0.008497
2025-02-04 12:32:29.273 INFO: train_e/atom_mae: 0.008497
train_e/atom_rmse: 0.010445
2025-02-04 12:32:29.273 INFO: train_e/atom_rmse: 0.010445
train_f_mae: 0.046447
2025-02-04 12:32:29.276 INFO: train_f_mae: 0.046447
train_f_rmse: 0.071738
2025-02-04 12:32:29.276 INFO: train_f_rmse: 0.071738
val_e/atom_mae: 0.003815
2025-02-04 12:32:29.278 INFO: val_e/atom_mae: 0.003815
val_e/atom_rmse: 0.004242
2025-02-04 12:32:29.278 INFO: val_e/atom_rmse: 0.004242
val_f_mae: 0.037307
2025-02-04 12:32:29.279 INFO: val_f_mae: 0.037307
val_f_rmse: 0.058337
2025-02-04 12:32:29.279 INFO: val_f_rmse: 0.058337
##### Step: 28 Learning rate: 0.005 #####
2025-02-04 12:34:40.418 INFO: ##### Step: 28 Learning rate: 0.005 #####
Epoch 29, Train Loss: 5.5326, Val Loss: 3.2056
2025-02-04 12:34:40.419 INFO: Epoch 29, Train Loss: 5.5326, Val Loss: 3.2056
train_e/atom_mae: 0.005307
2025-02-04 12:34:40.510 INFO: train_e/atom_mae: 0.005307
train_e/atom_rmse: 0.006693
2025-02-04 12:34:40.664 INFO: train_e/atom_rmse: 0.006693
train_f_mae: 0.048100
2025-02-04 12:34:40.666 INFO: train_f_mae: 0.048100
train_f_rmse: 0.073263
2025-02-04 12:34:40.667 INFO: train_f_rmse: 0.073263
val_e/atom_mae: 0.002045
2025-02-04 12:34:40.669 INFO: val_e/atom_mae: 0.002045
val_e/atom_rmse: 0.002395
2025-02-04 12:34:40.669 INFO: val_e/atom_rmse: 0.002395
val_f_mae: 0.035847
2025-02-04 12:34:40.669 INFO: val_f_mae: 0.035847
val_f_rmse: 0.056452
2025-02-04 12:34:40.670 INFO: val_f_rmse: 0.056452
##### Step: 29 Learning rate: 0.005 #####
2025-02-04 12:36:32.632 INFO: ##### Step: 29 Learning rate: 0.005 #####
Epoch 30, Train Loss: 5.6627, Val Loss: 3.0994
2025-02-04 12:36:32.632 INFO: Epoch 30, Train Loss: 5.6627, Val Loss: 3.0994
train_e/atom_mae: 0.006064
2025-02-04 12:36:32.633 INFO: train_e/atom_mae: 0.006064
train_e/atom_rmse: 0.007727
2025-02-04 12:36:32.633 INFO: train_e/atom_rmse: 0.007727
train_f_mae: 0.048386
2025-02-04 12:36:32.636 INFO: train_f_mae: 0.048386
train_f_rmse: 0.073774
2025-02-04 12:36:32.636 INFO: train_f_rmse: 0.073774
val_e/atom_mae: 0.001594
2025-02-04 12:36:32.638 INFO: val_e/atom_mae: 0.001594
val_e/atom_rmse: 0.002054
2025-02-04 12:36:32.639 INFO: val_e/atom_rmse: 0.002054
val_f_mae: 0.035503
2025-02-04 12:36:32.639 INFO: val_f_mae: 0.035503
val_f_rmse: 0.055556
2025-02-04 12:36:32.639 INFO: val_f_rmse: 0.055556
##### Step: 30 Learning rate: 0.005 #####
2025-02-04 12:38:23.549 INFO: ##### Step: 30 Learning rate: 0.005 #####
Epoch 31, Train Loss: 5.5897, Val Loss: 3.0556
2025-02-04 12:38:23.550 INFO: Epoch 31, Train Loss: 5.5897, Val Loss: 3.0556
train_e/atom_mae: 0.006939
2025-02-04 12:38:23.551 INFO: train_e/atom_mae: 0.006939
train_e/atom_rmse: 0.008850
2025-02-04 12:38:23.551 INFO: train_e/atom_rmse: 0.008850
train_f_mae: 0.047694
2025-02-04 12:38:23.554 INFO: train_f_mae: 0.047694
train_f_rmse: 0.072807
2025-02-04 12:38:23.554 INFO: train_f_rmse: 0.072807
val_e/atom_mae: 0.002631
2025-02-04 12:38:23.556 INFO: val_e/atom_mae: 0.002631
val_e/atom_rmse: 0.003067
2025-02-04 12:38:23.556 INFO: val_e/atom_rmse: 0.003067
val_f_mae: 0.035381
2025-02-04 12:38:23.557 INFO: val_f_mae: 0.035381
val_f_rmse: 0.054983
2025-02-04 12:38:23.557 INFO: val_f_rmse: 0.054983
##### Step: 31 Learning rate: 0.005 #####
2025-02-04 12:40:30.789 INFO: ##### Step: 31 Learning rate: 0.005 #####
Epoch 32, Train Loss: 4.8849, Val Loss: 2.9623
2025-02-04 12:40:30.791 INFO: Epoch 32, Train Loss: 4.8849, Val Loss: 2.9623
train_e/atom_mae: 0.005324
2025-02-04 12:40:30.870 INFO: train_e/atom_mae: 0.005324
train_e/atom_rmse: 0.006575
2025-02-04 12:40:30.981 INFO: train_e/atom_rmse: 0.006575
train_f_mae: 0.043650
2025-02-04 12:40:30.984 INFO: train_f_mae: 0.043650
train_f_rmse: 0.068743
2025-02-04 12:40:30.984 INFO: train_f_rmse: 0.068743
val_e/atom_mae: 0.001549
2025-02-04 12:40:30.986 INFO: val_e/atom_mae: 0.001549
val_e/atom_rmse: 0.002112
2025-02-04 12:40:30.986 INFO: val_e/atom_rmse: 0.002112
val_f_mae: 0.034912
2025-02-04 12:40:30.987 INFO: val_f_mae: 0.034912
val_f_rmse: 0.054287
2025-02-04 12:40:30.987 INFO: val_f_rmse: 0.054287
##### Step: 32 Learning rate: 0.005 #####
2025-02-04 12:42:22.336 INFO: ##### Step: 32 Learning rate: 0.005 #####
Epoch 33, Train Loss: 4.9546, Val Loss: 3.0467
2025-02-04 12:42:22.336 INFO: Epoch 33, Train Loss: 4.9546, Val Loss: 3.0467
train_e/atom_mae: 0.007162
2025-02-04 12:42:22.337 INFO: train_e/atom_mae: 0.007162
train_e/atom_rmse: 0.009213
2025-02-04 12:42:22.338 INFO: train_e/atom_rmse: 0.009213
train_f_mae: 0.044179
2025-02-04 12:42:22.340 INFO: train_f_mae: 0.044179
train_f_rmse: 0.068130
2025-02-04 12:42:22.340 INFO: train_f_rmse: 0.068130
val_e/atom_mae: 0.003647
2025-02-04 12:42:22.343 INFO: val_e/atom_mae: 0.003647
val_e/atom_rmse: 0.003853
2025-02-04 12:42:22.343 INFO: val_e/atom_rmse: 0.003853
val_f_mae: 0.035188
2025-02-04 12:42:22.343 INFO: val_f_mae: 0.035188
val_f_rmse: 0.054715
2025-02-04 12:42:22.344 INFO: val_f_rmse: 0.054715
##### Step: 33 Learning rate: 0.005 #####
2025-02-04 12:44:13.290 INFO: ##### Step: 33 Learning rate: 0.005 #####
Epoch 34, Train Loss: 5.1063, Val Loss: 2.9797
2025-02-04 12:44:13.291 INFO: Epoch 34, Train Loss: 5.1063, Val Loss: 2.9797
train_e/atom_mae: 0.004419
2025-02-04 12:44:13.292 INFO: train_e/atom_mae: 0.004419
train_e/atom_rmse: 0.005597
2025-02-04 12:44:13.292 INFO: train_e/atom_rmse: 0.005597
train_f_mae: 0.045936
2025-02-04 12:44:13.295 INFO: train_f_mae: 0.045936
train_f_rmse: 0.070645
2025-02-04 12:44:13.295 INFO: train_f_rmse: 0.070645
val_e/atom_mae: 0.002066
2025-02-04 12:44:13.297 INFO: val_e/atom_mae: 0.002066
val_e/atom_rmse: 0.002460
2025-02-04 12:44:13.298 INFO: val_e/atom_rmse: 0.002460
val_f_mae: 0.034674
2025-02-04 12:44:13.298 INFO: val_f_mae: 0.034674
val_f_rmse: 0.054403
2025-02-04 12:44:13.298 INFO: val_f_rmse: 0.054403
##### Step: 34 Learning rate: 0.005 #####
2025-02-04 12:46:04.276 INFO: ##### Step: 34 Learning rate: 0.005 #####
Epoch 35, Train Loss: 5.8376, Val Loss: 2.9752
2025-02-04 12:46:04.276 INFO: Epoch 35, Train Loss: 5.8376, Val Loss: 2.9752
train_e/atom_mae: 0.010339
2025-02-04 12:46:04.277 INFO: train_e/atom_mae: 0.010339
train_e/atom_rmse: 0.012923
2025-02-04 12:46:04.278 INFO: train_e/atom_rmse: 0.012923
train_f_mae: 0.047210
2025-02-04 12:46:04.284 INFO: train_f_mae: 0.047210
train_f_rmse: 0.072263
2025-02-04 12:46:04.284 INFO: train_f_rmse: 0.072263
val_e/atom_mae: 0.001484
2025-02-04 12:46:04.286 INFO: val_e/atom_mae: 0.001484
val_e/atom_rmse: 0.001938
2025-02-04 12:46:04.286 INFO: val_e/atom_rmse: 0.001938
val_f_mae: 0.034838
2025-02-04 12:46:04.287 INFO: val_f_mae: 0.034838
val_f_rmse: 0.054442
2025-02-04 12:46:04.287 INFO: val_f_rmse: 0.054442
##### Step: 35 Learning rate: 0.005 #####
2025-02-04 12:47:59.687 INFO: ##### Step: 35 Learning rate: 0.005 #####
Epoch 36, Train Loss: 4.9677, Val Loss: 2.9878
2025-02-04 12:47:59.688 INFO: Epoch 36, Train Loss: 4.9677, Val Loss: 2.9878
train_e/atom_mae: 0.005907
2025-02-04 12:47:59.784 INFO: train_e/atom_mae: 0.005907
train_e/atom_rmse: 0.007673
2025-02-04 12:47:59.856 INFO: train_e/atom_rmse: 0.007673
train_f_mae: 0.043692
2025-02-04 12:47:59.859 INFO: train_f_mae: 0.043692
train_f_rmse: 0.068925
2025-02-04 12:47:59.859 INFO: train_f_rmse: 0.068925
val_e/atom_mae: 0.001829
2025-02-04 12:47:59.861 INFO: val_e/atom_mae: 0.001829
val_e/atom_rmse: 0.002263
2025-02-04 12:47:59.862 INFO: val_e/atom_rmse: 0.002263
val_f_mae: 0.034428
2025-02-04 12:47:59.862 INFO: val_f_mae: 0.034428
val_f_rmse: 0.054504
2025-02-04 12:47:59.863 INFO: val_f_rmse: 0.054504
##### Step: 36 Learning rate: 0.005 #####
2025-02-04 12:49:51.282 INFO: ##### Step: 36 Learning rate: 0.005 #####
Epoch 37, Train Loss: 5.3543, Val Loss: 2.8699
2025-02-04 12:49:51.283 INFO: Epoch 37, Train Loss: 5.3543, Val Loss: 2.8699
train_e/atom_mae: 0.005618
2025-02-04 12:49:51.284 INFO: train_e/atom_mae: 0.005618
train_e/atom_rmse: 0.006872
2025-02-04 12:49:51.284 INFO: train_e/atom_rmse: 0.006872
train_f_mae: 0.047266
2025-02-04 12:49:51.287 INFO: train_f_mae: 0.047266
train_f_rmse: 0.071973
2025-02-04 12:49:51.287 INFO: train_f_rmse: 0.071973
val_e/atom_mae: 0.001834
2025-02-04 12:49:51.289 INFO: val_e/atom_mae: 0.001834
val_e/atom_rmse: 0.002238
2025-02-04 12:49:51.289 INFO: val_e/atom_rmse: 0.002238
val_f_mae: 0.033994
2025-02-04 12:49:51.290 INFO: val_f_mae: 0.033994
val_f_rmse: 0.053409
2025-02-04 12:49:51.290 INFO: val_f_rmse: 0.053409
##### Step: 37 Learning rate: 0.005 #####
2025-02-04 12:51:42.781 INFO: ##### Step: 37 Learning rate: 0.005 #####
Epoch 38, Train Loss: 5.6027, Val Loss: 2.8420
2025-02-04 12:51:42.782 INFO: Epoch 38, Train Loss: 5.6027, Val Loss: 2.8420
train_e/atom_mae: 0.008077
2025-02-04 12:51:42.783 INFO: train_e/atom_mae: 0.008077
train_e/atom_rmse: 0.010296
2025-02-04 12:51:42.783 INFO: train_e/atom_rmse: 0.010296
train_f_mae: 0.048382
2025-02-04 12:51:42.786 INFO: train_f_mae: 0.048382
train_f_rmse: 0.072193
2025-02-04 12:51:42.786 INFO: train_f_rmse: 0.072193
val_e/atom_mae: 0.002349
2025-02-04 12:51:42.792 INFO: val_e/atom_mae: 0.002349
val_e/atom_rmse: 0.002674
2025-02-04 12:51:42.792 INFO: val_e/atom_rmse: 0.002674
val_f_mae: 0.033999
2025-02-04 12:51:42.792 INFO: val_f_mae: 0.033999
val_f_rmse: 0.053077
2025-02-04 12:51:42.793 INFO: val_f_rmse: 0.053077
##### Step: 38 Learning rate: 0.005 #####
2025-02-04 12:53:34.436 INFO: ##### Step: 38 Learning rate: 0.005 #####
Epoch 39, Train Loss: 5.1836, Val Loss: 2.9199
2025-02-04 12:53:34.437 INFO: Epoch 39, Train Loss: 5.1836, Val Loss: 2.9199
train_e/atom_mae: 0.007929
2025-02-04 12:53:34.438 INFO: train_e/atom_mae: 0.007929
train_e/atom_rmse: 0.009722
2025-02-04 12:53:34.438 INFO: train_e/atom_rmse: 0.009722
train_f_mae: 0.046750
2025-02-04 12:53:34.441 INFO: train_f_mae: 0.046750
train_f_rmse: 0.069535
2025-02-04 12:53:34.441 INFO: train_f_rmse: 0.069535
val_e/atom_mae: 0.002808
2025-02-04 12:53:34.443 INFO: val_e/atom_mae: 0.002808
val_e/atom_rmse: 0.003092
2025-02-04 12:53:34.444 INFO: val_e/atom_rmse: 0.003092
val_f_mae: 0.034207
2025-02-04 12:53:34.444 INFO: val_f_mae: 0.034207
val_f_rmse: 0.053723
2025-02-04 12:53:34.445 INFO: val_f_rmse: 0.053723
##### Step: 39 Learning rate: 0.005 #####
2025-02-04 12:55:25.169 INFO: ##### Step: 39 Learning rate: 0.005 #####
Epoch 40, Train Loss: 6.8354, Val Loss: 2.9872
2025-02-04 12:55:25.170 INFO: Epoch 40, Train Loss: 6.8354, Val Loss: 2.9872
train_e/atom_mae: 0.007362
2025-02-04 12:55:25.170 INFO: train_e/atom_mae: 0.007362
train_e/atom_rmse: 0.009699
2025-02-04 12:55:25.171 INFO: train_e/atom_rmse: 0.009699
train_f_mae: 0.052944
2025-02-04 12:55:25.173 INFO: train_f_mae: 0.052944
train_f_rmse: 0.080552
2025-02-04 12:55:25.174 INFO: train_f_rmse: 0.080552
val_e/atom_mae: 0.002883
2025-02-04 12:55:25.176 INFO: val_e/atom_mae: 0.002883
val_e/atom_rmse: 0.003284
2025-02-04 12:55:25.176 INFO: val_e/atom_rmse: 0.003284
val_f_mae: 0.034614
2025-02-04 12:55:25.177 INFO: val_f_mae: 0.034614
val_f_rmse: 0.054307
2025-02-04 12:55:25.177 INFO: val_f_rmse: 0.054307
2025-02-04 12:55:26.692 INFO: training
##### Step: 0 Learning rate: 0.002 #####
2025-02-04 12:57:19.492 INFO: ##### Step: 0 Learning rate: 0.002 #####
Epoch 1, Train Loss: 4.0998, Val Loss: 3.4839
2025-02-04 12:57:19.493 INFO: Epoch 1, Train Loss: 4.0998, Val Loss: 3.4839
train_e/atom_mae: 0.004296
2025-02-04 12:57:19.519 INFO: train_e/atom_mae: 0.004296
train_e/atom_rmse: 0.005815
2025-02-04 12:57:19.528 INFO: train_e/atom_rmse: 0.005815
train_f_mae: 0.040781
2025-02-04 12:57:19.530 INFO: train_f_mae: 0.040781
train_f_rmse: 0.063049
2025-02-04 12:57:19.530 INFO: train_f_rmse: 0.063049
val_e/atom_mae: 0.005103
2025-02-04 12:57:19.533 INFO: val_e/atom_mae: 0.005103
val_e/atom_rmse: 0.005263
2025-02-04 12:57:19.533 INFO: val_e/atom_rmse: 0.005263
val_f_mae: 0.039106
2025-02-04 12:57:19.533 INFO: val_f_mae: 0.039106
val_f_rmse: 0.058153
2025-02-04 12:57:19.534 INFO: val_f_rmse: 0.058153
##### Step: 1 Learning rate: 0.004 #####
2025-02-04 12:59:13.940 INFO: ##### Step: 1 Learning rate: 0.004 #####
Epoch 2, Train Loss: 4.0297, Val Loss: 4.3233
2025-02-04 12:59:13.942 INFO: Epoch 2, Train Loss: 4.0297, Val Loss: 4.3233
train_e/atom_mae: 0.003940
2025-02-04 12:59:14.018 INFO: train_e/atom_mae: 0.003940
train_e/atom_rmse: 0.005306
2025-02-04 12:59:14.028 INFO: train_e/atom_rmse: 0.005306
train_f_mae: 0.040955
2025-02-04 12:59:14.031 INFO: train_f_mae: 0.040955
train_f_rmse: 0.062657
2025-02-04 12:59:14.031 INFO: train_f_rmse: 0.062657
val_e/atom_mae: 0.015517
2025-02-04 12:59:14.033 INFO: val_e/atom_mae: 0.015517
val_e/atom_rmse: 0.015579
2025-02-04 12:59:14.034 INFO: val_e/atom_rmse: 0.015579
val_f_mae: 0.037589
2025-02-04 12:59:14.034 INFO: val_f_mae: 0.037589
val_f_rmse: 0.058562
2025-02-04 12:59:14.034 INFO: val_f_rmse: 0.058562
##### Step: 2 Learning rate: 0.006 #####
2025-02-04 13:01:04.927 INFO: ##### Step: 2 Learning rate: 0.006 #####
Epoch 3, Train Loss: 7.1058, Val Loss: 5.7229
2025-02-04 13:01:04.927 INFO: Epoch 3, Train Loss: 7.1058, Val Loss: 5.7229
train_e/atom_mae: 0.011563
2025-02-04 13:01:04.928 INFO: train_e/atom_mae: 0.011563
train_e/atom_rmse: 0.014388
2025-02-04 13:01:04.928 INFO: train_e/atom_rmse: 0.014388
train_f_mae: 0.052708
2025-02-04 13:01:04.931 INFO: train_f_mae: 0.052708
train_f_rmse: 0.079641
2025-02-04 13:01:04.931 INFO: train_f_rmse: 0.079641
val_e/atom_mae: 0.002846
2025-02-04 13:01:04.933 INFO: val_e/atom_mae: 0.002846
val_e/atom_rmse: 0.003300
2025-02-04 13:01:04.933 INFO: val_e/atom_rmse: 0.003300
val_f_mae: 0.047968
2025-02-04 13:01:04.934 INFO: val_f_mae: 0.047968
val_f_rmse: 0.075380
2025-02-04 13:01:04.934 INFO: val_f_rmse: 0.075380
##### Step: 3 Learning rate: 0.008 #####
2025-02-04 13:02:55.273 INFO: ##### Step: 3 Learning rate: 0.008 #####
Epoch 4, Train Loss: 6.5772, Val Loss: 8.7630
2025-02-04 13:02:55.274 INFO: Epoch 4, Train Loss: 6.5772, Val Loss: 8.7630
train_e/atom_mae: 0.007001
2025-02-04 13:02:55.275 INFO: train_e/atom_mae: 0.007001
train_e/atom_rmse: 0.009260
2025-02-04 13:02:55.275 INFO: train_e/atom_rmse: 0.009260
train_f_mae: 0.051118
2025-02-04 13:02:55.278 INFO: train_f_mae: 0.051118
train_f_rmse: 0.079127
2025-02-04 13:02:55.278 INFO: train_f_rmse: 0.079127
val_e/atom_mae: 0.007521
2025-02-04 13:02:55.280 INFO: val_e/atom_mae: 0.007521
val_e/atom_rmse: 0.008216
2025-02-04 13:02:55.280 INFO: val_e/atom_rmse: 0.008216
val_f_mae: 0.064935
2025-02-04 13:02:55.281 INFO: val_f_mae: 0.064935
val_f_rmse: 0.092235
2025-02-04 13:02:55.281 INFO: val_f_rmse: 0.092235
##### Step: 4 Learning rate: 0.01 #####
2025-02-04 13:04:49.389 INFO: ##### Step: 4 Learning rate: 0.01 #####
Epoch 5, Train Loss: 9.3711, Val Loss: 5.7125
2025-02-04 13:04:49.390 INFO: Epoch 5, Train Loss: 9.3711, Val Loss: 5.7125
train_e/atom_mae: 0.011444
2025-02-04 13:04:49.411 INFO: train_e/atom_mae: 0.011444
train_e/atom_rmse: 0.016960
2025-02-04 13:04:49.429 INFO: train_e/atom_rmse: 0.016960
train_f_mae: 0.060520
2025-02-04 13:04:49.432 INFO: train_f_mae: 0.060520
train_f_rmse: 0.091163
2025-02-04 13:04:49.432 INFO: train_f_rmse: 0.091163
val_e/atom_mae: 0.002378
2025-02-04 13:04:49.434 INFO: val_e/atom_mae: 0.002378
val_e/atom_rmse: 0.002910
2025-02-04 13:04:49.434 INFO: val_e/atom_rmse: 0.002910
val_f_mae: 0.052521
2025-02-04 13:04:49.435 INFO: val_f_mae: 0.052521
val_f_rmse: 0.075369
2025-02-04 13:04:49.435 INFO: val_f_rmse: 0.075369
##### Step: 5 Learning rate: 0.01 #####
2025-02-04 13:06:45.762 INFO: ##### Step: 5 Learning rate: 0.01 #####
Epoch 6, Train Loss: 10.0932, Val Loss: 12.8077
2025-02-04 13:06:45.762 INFO: Epoch 6, Train Loss: 10.0932, Val Loss: 12.8077
train_e/atom_mae: 0.011462
2025-02-04 13:06:45.811 INFO: train_e/atom_mae: 0.011462
train_e/atom_rmse: 0.014727
2025-02-04 13:06:45.827 INFO: train_e/atom_rmse: 0.014727
train_f_mae: 0.063651
2025-02-04 13:06:45.830 INFO: train_f_mae: 0.063651
train_f_rmse: 0.096404
2025-02-04 13:06:45.830 INFO: train_f_rmse: 0.096404
val_e/atom_mae: 0.018279
2025-02-04 13:06:45.832 INFO: val_e/atom_mae: 0.018279
val_e/atom_rmse: 0.019001
2025-02-04 13:06:45.833 INFO: val_e/atom_rmse: 0.019001
val_f_mae: 0.078509
2025-02-04 13:06:45.833 INFO: val_f_mae: 0.078509
val_f_rmse: 0.107158
2025-02-04 13:06:45.833 INFO: val_f_rmse: 0.107158
##### Step: 6 Learning rate: 0.01 #####
2025-02-04 13:08:46.140 INFO: ##### Step: 6 Learning rate: 0.01 #####
Epoch 7, Train Loss: 6.8730, Val Loss: 11.4693
2025-02-04 13:08:46.152 INFO: Epoch 7, Train Loss: 6.8730, Val Loss: 11.4693
train_e/atom_mae: 0.011825
2025-02-04 13:08:46.203 INFO: train_e/atom_mae: 0.011825
train_e/atom_rmse: 0.014138
2025-02-04 13:08:46.298 INFO: train_e/atom_rmse: 0.014138
train_f_mae: 0.051491
2025-02-04 13:08:46.301 INFO: train_f_mae: 0.051491
train_f_rmse: 0.078334
2025-02-04 13:08:46.301 INFO: train_f_rmse: 0.078334
val_e/atom_mae: 0.020134
2025-02-04 13:08:46.303 INFO: val_e/atom_mae: 0.020134
val_e/atom_rmse: 0.020546
2025-02-04 13:08:46.304 INFO: val_e/atom_rmse: 0.020546
val_f_mae: 0.071774
2025-02-04 13:08:46.304 INFO: val_f_mae: 0.071774
val_f_rmse: 0.099567
2025-02-04 13:08:46.304 INFO: val_f_rmse: 0.099567
##### Step: 7 Learning rate: 0.01 #####
2025-02-04 13:10:39.083 INFO: ##### Step: 7 Learning rate: 0.01 #####
Epoch 8, Train Loss: 7.3677, Val Loss: 3.9821
2025-02-04 13:10:39.084 INFO: Epoch 8, Train Loss: 7.3677, Val Loss: 3.9821
train_e/atom_mae: 0.009496
2025-02-04 13:10:39.086 INFO: train_e/atom_mae: 0.009496
train_e/atom_rmse: 0.012486
2025-02-04 13:10:39.088 INFO: train_e/atom_rmse: 0.012486
train_f_mae: 0.054234
2025-02-04 13:10:39.091 INFO: train_f_mae: 0.054234
train_f_rmse: 0.082419
2025-02-04 13:10:39.091 INFO: train_f_rmse: 0.082419
val_e/atom_mae: 0.002123
2025-02-04 13:10:39.093 INFO: val_e/atom_mae: 0.002123
val_e/atom_rmse: 0.002619
2025-02-04 13:10:39.094 INFO: val_e/atom_rmse: 0.002619
val_f_mae: 0.041994
2025-02-04 13:10:39.094 INFO: val_f_mae: 0.041994
val_f_rmse: 0.062923
2025-02-04 13:10:39.094 INFO: val_f_rmse: 0.062923
##### Step: 8 Learning rate: 0.01 #####
2025-02-04 13:12:30.057 INFO: ##### Step: 8 Learning rate: 0.01 #####
Epoch 9, Train Loss: 6.8468, Val Loss: 4.9523
2025-02-04 13:12:30.057 INFO: Epoch 9, Train Loss: 6.8468, Val Loss: 4.9523
train_e/atom_mae: 0.009150
2025-02-04 13:12:30.058 INFO: train_e/atom_mae: 0.009150
train_e/atom_rmse: 0.011412
2025-02-04 13:12:30.058 INFO: train_e/atom_rmse: 0.011412
train_f_mae: 0.051417
2025-02-04 13:12:30.061 INFO: train_f_mae: 0.051417
train_f_rmse: 0.079791
2025-02-04 13:12:30.061 INFO: train_f_rmse: 0.079791
val_e/atom_mae: 0.008421
2025-02-04 13:12:30.063 INFO: val_e/atom_mae: 0.008421
val_e/atom_rmse: 0.008628
2025-02-04 13:12:30.063 INFO: val_e/atom_rmse: 0.008628
val_f_mae: 0.044324
2025-02-04 13:12:30.064 INFO: val_f_mae: 0.044324
val_f_rmse: 0.068415
2025-02-04 13:12:30.064 INFO: val_f_rmse: 0.068415
##### Step: 9 Learning rate: 0.01 #####
2025-02-04 13:14:20.273 INFO: ##### Step: 9 Learning rate: 0.01 #####
Epoch 10, Train Loss: 8.8618, Val Loss: 10.9107
2025-02-04 13:14:20.274 INFO: Epoch 10, Train Loss: 8.8618, Val Loss: 10.9107
train_e/atom_mae: 0.009736
2025-02-04 13:14:20.275 INFO: train_e/atom_mae: 0.009736
train_e/atom_rmse: 0.013355
2025-02-04 13:14:20.275 INFO: train_e/atom_rmse: 0.013355
train_f_mae: 0.058452
2025-02-04 13:14:20.278 INFO: train_f_mae: 0.058452
train_f_rmse: 0.090578
2025-02-04 13:14:20.278 INFO: train_f_rmse: 0.090578
val_e/atom_mae: 0.004321
2025-02-04 13:14:20.280 INFO: val_e/atom_mae: 0.004321
val_e/atom_rmse: 0.005502
2025-02-04 13:14:20.280 INFO: val_e/atom_rmse: 0.005502
val_f_mae: 0.072824
2025-02-04 13:14:20.281 INFO: val_f_mae: 0.072824
val_f_rmse: 0.103919
2025-02-04 13:14:20.281 INFO: val_f_rmse: 0.103919
##### Step: 10 Learning rate: 0.01 #####
2025-02-04 13:16:12.375 INFO: ##### Step: 10 Learning rate: 0.01 #####
Epoch 11, Train Loss: 8.6617, Val Loss: 3.5069
2025-02-04 13:16:12.375 INFO: Epoch 11, Train Loss: 8.6617, Val Loss: 3.5069
train_e/atom_mae: 0.008571
2025-02-04 13:16:12.376 INFO: train_e/atom_mae: 0.008571
train_e/atom_rmse: 0.010875
2025-02-04 13:16:12.377 INFO: train_e/atom_rmse: 0.010875
train_f_mae: 0.061297
2025-02-04 13:16:12.379 INFO: train_f_mae: 0.061297
train_f_rmse: 0.090696
2025-02-04 13:16:12.379 INFO: train_f_rmse: 0.090696
val_e/atom_mae: 0.004431
2025-02-04 13:16:12.381 INFO: val_e/atom_mae: 0.004431
val_e/atom_rmse: 0.004738
2025-02-04 13:16:12.382 INFO: val_e/atom_rmse: 0.004738
val_f_mae: 0.037546
2025-02-04 13:16:12.382 INFO: val_f_mae: 0.037546
val_f_rmse: 0.058531
2025-02-04 13:16:12.382 INFO: val_f_rmse: 0.058531
##### Step: 11 Learning rate: 0.01 #####
2025-02-04 13:18:03.114 INFO: ##### Step: 11 Learning rate: 0.01 #####
Epoch 12, Train Loss: 13.3912, Val Loss: 4.1137
2025-02-04 13:18:03.115 INFO: Epoch 12, Train Loss: 13.3912, Val Loss: 4.1137
train_e/atom_mae: 0.014463
2025-02-04 13:18:03.116 INFO: train_e/atom_mae: 0.014463
train_e/atom_rmse: 0.018241
2025-02-04 13:18:03.116 INFO: train_e/atom_rmse: 0.018241
train_f_mae: 0.075328
2025-02-04 13:18:03.119 INFO: train_f_mae: 0.075328
train_f_rmse: 0.110293
2025-02-04 13:18:03.119 INFO: train_f_rmse: 0.110293
val_e/atom_mae: 0.008676
2025-02-04 13:18:03.121 INFO: val_e/atom_mae: 0.008676
val_e/atom_rmse: 0.009246
2025-02-04 13:18:03.121 INFO: val_e/atom_rmse: 0.009246
val_f_mae: 0.039928
2025-02-04 13:18:03.122 INFO: val_f_mae: 0.039928
val_f_rmse: 0.061651
2025-02-04 13:18:03.122 INFO: val_f_rmse: 0.061651
##### Step: 12 Learning rate: 0.01 #####
2025-02-04 13:20:00.192 INFO: ##### Step: 12 Learning rate: 0.01 #####
Epoch 13, Train Loss: 9.9457, Val Loss: 4.0234
2025-02-04 13:20:00.193 INFO: Epoch 13, Train Loss: 9.9457, Val Loss: 4.0234
train_e/atom_mae: 0.011794
2025-02-04 13:20:00.220 INFO: train_e/atom_mae: 0.011794
train_e/atom_rmse: 0.014666
2025-02-04 13:20:00.223 INFO: train_e/atom_rmse: 0.014666
train_f_mae: 0.062817
2025-02-04 13:20:00.226 INFO: train_f_mae: 0.062817
train_f_rmse: 0.095670
2025-02-04 13:20:00.226 INFO: train_f_rmse: 0.095670
val_e/atom_mae: 0.008111
2025-02-04 13:20:00.228 INFO: val_e/atom_mae: 0.008111
val_e/atom_rmse: 0.008600
2025-02-04 13:20:00.229 INFO: val_e/atom_rmse: 0.008600
val_f_mae: 0.038991
2025-02-04 13:20:00.229 INFO: val_f_mae: 0.038991
val_f_rmse: 0.061256
2025-02-04 13:20:00.229 INFO: val_f_rmse: 0.061256
##### Step: 13 Learning rate: 0.01 #####
2025-02-04 13:21:50.908 INFO: ##### Step: 13 Learning rate: 0.01 #####
Epoch 14, Train Loss: 14.9913, Val Loss: 4.3925
2025-02-04 13:21:50.909 INFO: Epoch 14, Train Loss: 14.9913, Val Loss: 4.3925
train_e/atom_mae: 0.015028
2025-02-04 13:21:50.910 INFO: train_e/atom_mae: 0.015028
train_e/atom_rmse: 0.019261
2025-02-04 13:21:50.910 INFO: train_e/atom_rmse: 0.019261
train_f_mae: 0.077550
2025-02-04 13:21:50.913 INFO: train_f_mae: 0.077550
train_f_rmse: 0.116721
2025-02-04 13:21:50.913 INFO: train_f_rmse: 0.116721
val_e/atom_mae: 0.008146
2025-02-04 13:21:50.915 INFO: val_e/atom_mae: 0.008146
val_e/atom_rmse: 0.008510
2025-02-04 13:21:50.915 INFO: val_e/atom_rmse: 0.008510
val_f_mae: 0.042498
2025-02-04 13:21:50.916 INFO: val_f_mae: 0.042498
val_f_rmse: 0.064256
2025-02-04 13:21:50.916 INFO: val_f_rmse: 0.064256
##### Step: 14 Learning rate: 0.01 #####
2025-02-04 13:23:41.632 INFO: ##### Step: 14 Learning rate: 0.01 #####
Epoch 15, Train Loss: 16.2165, Val Loss: 4.6722
2025-02-04 13:23:41.633 INFO: Epoch 15, Train Loss: 16.2165, Val Loss: 4.6722
train_e/atom_mae: 0.021585
2025-02-04 13:23:41.634 INFO: train_e/atom_mae: 0.021585
train_e/atom_rmse: 0.028204
2025-02-04 13:23:41.634 INFO: train_e/atom_rmse: 0.028204
train_f_mae: 0.076152
2025-02-04 13:23:41.636 INFO: train_f_mae: 0.076152
train_f_rmse: 0.115256
2025-02-04 13:23:41.637 INFO: train_f_rmse: 0.115256
val_e/atom_mae: 0.008696
2025-02-04 13:23:41.639 INFO: val_e/atom_mae: 0.008696
val_e/atom_rmse: 0.009012
2025-02-04 13:23:41.639 INFO: val_e/atom_rmse: 0.009012
val_f_mae: 0.041974
2025-02-04 13:23:41.640 INFO: val_f_mae: 0.041974
val_f_rmse: 0.066152
2025-02-04 13:23:41.640 INFO: val_f_rmse: 0.066152
##### Step: 15 Learning rate: 0.01 #####
2025-02-04 13:25:32.501 INFO: ##### Step: 15 Learning rate: 0.01 #####
Epoch 16, Train Loss: 8.5820, Val Loss: 3.8756
2025-02-04 13:25:32.502 INFO: Epoch 16, Train Loss: 8.5820, Val Loss: 3.8756
train_e/atom_mae: 0.012131
2025-02-04 13:25:32.503 INFO: train_e/atom_mae: 0.012131
train_e/atom_rmse: 0.014619
2025-02-04 13:25:32.503 INFO: train_e/atom_rmse: 0.014619
train_f_mae: 0.058391
2025-02-04 13:25:32.506 INFO: train_f_mae: 0.058391
train_f_rmse: 0.088284
2025-02-04 13:25:32.506 INFO: train_f_rmse: 0.088284
val_e/atom_mae: 0.005218
2025-02-04 13:25:32.508 INFO: val_e/atom_mae: 0.005218
val_e/atom_rmse: 0.005569
2025-02-04 13:25:32.509 INFO: val_e/atom_rmse: 0.005569
val_f_mae: 0.038702
2025-02-04 13:25:32.509 INFO: val_f_mae: 0.038702
val_f_rmse: 0.061343
2025-02-04 13:25:32.509 INFO: val_f_rmse: 0.061343
##### Step: 16 Learning rate: 0.01 #####
2025-02-04 13:27:26.554 INFO: ##### Step: 16 Learning rate: 0.01 #####
Epoch 17, Train Loss: 8.4987, Val Loss: 3.8089
2025-02-04 13:27:26.554 INFO: Epoch 17, Train Loss: 8.4987, Val Loss: 3.8089
train_e/atom_mae: 0.012229
2025-02-04 13:27:26.576 INFO: train_e/atom_mae: 0.012229
train_e/atom_rmse: 0.015593
2025-02-04 13:27:26.595 INFO: train_e/atom_rmse: 0.015593
train_f_mae: 0.057180
2025-02-04 13:27:26.598 INFO: train_f_mae: 0.057180
train_f_rmse: 0.087192
2025-02-04 13:27:26.598 INFO: train_f_rmse: 0.087192
val_e/atom_mae: 0.006918
2025-02-04 13:27:26.600 INFO: val_e/atom_mae: 0.006918
val_e/atom_rmse: 0.007184
2025-02-04 13:27:26.601 INFO: val_e/atom_rmse: 0.007184
val_f_mae: 0.038781
2025-02-04 13:27:26.601 INFO: val_f_mae: 0.038781
val_f_rmse: 0.060178
2025-02-04 13:27:26.601 INFO: val_f_rmse: 0.060178
##### Step: 17 Learning rate: 0.01 #####
2025-02-04 13:29:17.943 INFO: ##### Step: 17 Learning rate: 0.01 #####
Epoch 18, Train Loss: 8.8190, Val Loss: 3.8695
2025-02-04 13:29:17.943 INFO: Epoch 18, Train Loss: 8.8190, Val Loss: 3.8695
train_e/atom_mae: 0.007776
2025-02-04 13:29:17.944 INFO: train_e/atom_mae: 0.007776
train_e/atom_rmse: 0.010098
2025-02-04 13:29:17.944 INFO: train_e/atom_rmse: 0.010098
train_f_mae: 0.059818
2025-02-04 13:29:17.947 INFO: train_f_mae: 0.059818
train_f_rmse: 0.091886
2025-02-04 13:29:17.947 INFO: train_f_rmse: 0.091886
val_e/atom_mae: 0.008917
2025-02-04 13:29:17.949 INFO: val_e/atom_mae: 0.008917
val_e/atom_rmse: 0.009060
2025-02-04 13:29:17.950 INFO: val_e/atom_rmse: 0.009060
val_f_mae: 0.037999
2025-02-04 13:29:17.950 INFO: val_f_mae: 0.037999
val_f_rmse: 0.059746
2025-02-04 13:29:17.950 INFO: val_f_rmse: 0.059746
##### Step: 18 Learning rate: 0.01 #####
2025-02-04 13:31:09.043 INFO: ##### Step: 18 Learning rate: 0.01 #####
Epoch 19, Train Loss: 7.6233, Val Loss: 3.7837
2025-02-04 13:31:09.044 INFO: Epoch 19, Train Loss: 7.6233, Val Loss: 3.7837
train_e/atom_mae: 0.008986
2025-02-04 13:31:09.045 INFO: train_e/atom_mae: 0.008986
train_e/atom_rmse: 0.011993
2025-02-04 13:31:09.045 INFO: train_e/atom_rmse: 0.011993
train_f_mae: 0.056338
2025-02-04 13:31:09.048 INFO: train_f_mae: 0.056338
train_f_rmse: 0.084220
2025-02-04 13:31:09.048 INFO: train_f_rmse: 0.084220
val_e/atom_mae: 0.008548
2025-02-04 13:31:09.050 INFO: val_e/atom_mae: 0.008548
val_e/atom_rmse: 0.008696
2025-02-04 13:31:09.050 INFO: val_e/atom_rmse: 0.008696
val_f_mae: 0.037988
2025-02-04 13:31:09.051 INFO: val_f_mae: 0.037988
val_f_rmse: 0.059213
2025-02-04 13:31:09.051 INFO: val_f_rmse: 0.059213
##### Step: 19 Learning rate: 0.01 #####
2025-02-04 13:33:26.266 INFO: ##### Step: 19 Learning rate: 0.01 #####
Epoch 20, Train Loss: 8.4553, Val Loss: 3.3704
2025-02-04 13:33:26.267 INFO: Epoch 20, Train Loss: 8.4553, Val Loss: 3.3704
train_e/atom_mae: 0.007912
2025-02-04 13:33:26.383 INFO: train_e/atom_mae: 0.007912
train_e/atom_rmse: 0.010452
2025-02-04 13:33:26.466 INFO: train_e/atom_rmse: 0.010452
train_f_mae: 0.060229
2025-02-04 13:33:26.469 INFO: train_f_mae: 0.060229
train_f_rmse: 0.089736
2025-02-04 13:33:26.469 INFO: train_f_rmse: 0.089736
val_e/atom_mae: 0.002272
2025-02-04 13:33:26.471 INFO: val_e/atom_mae: 0.002272
val_e/atom_rmse: 0.002696
2025-02-04 13:33:26.471 INFO: val_e/atom_rmse: 0.002696
val_f_mae: 0.036968
2025-02-04 13:33:26.472 INFO: val_f_mae: 0.036968
val_f_rmse: 0.057846
2025-02-04 13:33:26.472 INFO: val_f_rmse: 0.057846
##### Step: 20 Learning rate: 0.005 #####
2025-02-04 13:35:18.028 INFO: ##### Step: 20 Learning rate: 0.005 #####
Epoch 21, Train Loss: 4.5923, Val Loss: 2.9891
2025-02-04 13:35:18.029 INFO: Epoch 21, Train Loss: 4.5923, Val Loss: 2.9891
train_e/atom_mae: 0.006338
2025-02-04 13:35:18.030 INFO: train_e/atom_mae: 0.006338
train_e/atom_rmse: 0.008726
2025-02-04 13:35:18.030 INFO: train_e/atom_rmse: 0.008726
train_f_mae: 0.042682
2025-02-04 13:35:18.033 INFO: train_f_mae: 0.042682
train_f_rmse: 0.065663
2025-02-04 13:35:18.033 INFO: train_f_rmse: 0.065663
val_e/atom_mae: 0.001532
2025-02-04 13:35:18.035 INFO: val_e/atom_mae: 0.001532
val_e/atom_rmse: 0.001942
2025-02-04 13:35:18.035 INFO: val_e/atom_rmse: 0.001942
val_f_mae: 0.034987
2025-02-04 13:35:18.036 INFO: val_f_mae: 0.034987
val_f_rmse: 0.054562
2025-02-04 13:35:18.036 INFO: val_f_rmse: 0.054562
##### Step: 21 Learning rate: 0.005 #####
2025-02-04 13:37:08.962 INFO: ##### Step: 21 Learning rate: 0.005 #####
Epoch 22, Train Loss: 5.5052, Val Loss: 3.1012
2025-02-04 13:37:08.962 INFO: Epoch 22, Train Loss: 5.5052, Val Loss: 3.1012
train_e/atom_mae: 0.006008
2025-02-04 13:37:08.963 INFO: train_e/atom_mae: 0.006008
train_e/atom_rmse: 0.007838
2025-02-04 13:37:08.963 INFO: train_e/atom_rmse: 0.007838
train_f_mae: 0.047720
2025-02-04 13:37:08.966 INFO: train_f_mae: 0.047720
train_f_rmse: 0.072655
2025-02-04 13:37:08.966 INFO: train_f_rmse: 0.072655
val_e/atom_mae: 0.002318
2025-02-04 13:37:08.968 INFO: val_e/atom_mae: 0.002318
val_e/atom_rmse: 0.002611
2025-02-04 13:37:08.969 INFO: val_e/atom_rmse: 0.002611
val_f_mae: 0.034916
2025-02-04 13:37:08.969 INFO: val_f_mae: 0.034916
val_f_rmse: 0.055464
2025-02-04 13:37:08.969 INFO: val_f_rmse: 0.055464
##### Step: 22 Learning rate: 0.005 #####
2025-02-04 13:39:02.562 INFO: ##### Step: 22 Learning rate: 0.005 #####
Epoch 23, Train Loss: 8.5244, Val Loss: 2.9193
2025-02-04 13:39:02.563 INFO: Epoch 23, Train Loss: 8.5244, Val Loss: 2.9193
train_e/atom_mae: 0.008846
2025-02-04 13:39:02.563 INFO: train_e/atom_mae: 0.008846
train_e/atom_rmse: 0.011272
2025-02-04 13:39:02.582 INFO: train_e/atom_rmse: 0.011272
train_f_mae: 0.059751
2025-02-04 13:39:02.584 INFO: train_f_mae: 0.059751
train_f_rmse: 0.089756
2025-02-04 13:39:02.584 INFO: train_f_rmse: 0.089756
val_e/atom_mae: 0.001951
2025-02-04 13:39:02.587 INFO: val_e/atom_mae: 0.001951
val_e/atom_rmse: 0.002301
2025-02-04 13:39:02.587 INFO: val_e/atom_rmse: 0.002301
val_f_mae: 0.034654
2025-02-04 13:39:02.587 INFO: val_f_mae: 0.034654
val_f_rmse: 0.053857
2025-02-04 13:39:02.588 INFO: val_f_rmse: 0.053857
##### Step: 23 Learning rate: 0.005 #####
2025-02-04 13:41:06.654 INFO: ##### Step: 23 Learning rate: 0.005 #####
Epoch 24, Train Loss: 3.9938, Val Loss: 2.8171
2025-02-04 13:41:06.655 INFO: Epoch 24, Train Loss: 3.9938, Val Loss: 2.8171
train_e/atom_mae: 0.004082
2025-02-04 13:41:06.745 INFO: train_e/atom_mae: 0.004082
train_e/atom_rmse: 0.005323
2025-02-04 13:41:06.790 INFO: train_e/atom_rmse: 0.005323
train_f_mae: 0.040801
2025-02-04 13:41:06.793 INFO: train_f_mae: 0.040801
train_f_rmse: 0.062365
2025-02-04 13:41:06.793 INFO: train_f_rmse: 0.062365
val_e/atom_mae: 0.002659
2025-02-04 13:41:06.795 INFO: val_e/atom_mae: 0.002659
val_e/atom_rmse: 0.002916
2025-02-04 13:41:06.796 INFO: val_e/atom_rmse: 0.002916
val_f_mae: 0.033724
2025-02-04 13:41:06.796 INFO: val_f_mae: 0.033724
val_f_rmse: 0.052789
2025-02-04 13:41:06.796 INFO: val_f_rmse: 0.052789
##### Step: 24 Learning rate: 0.005 #####
2025-02-04 13:42:58.447 INFO: ##### Step: 24 Learning rate: 0.005 #####
Epoch 25, Train Loss: 4.3450, Val Loss: 2.8208
2025-02-04 13:42:58.447 INFO: Epoch 25, Train Loss: 4.3450, Val Loss: 2.8208
train_e/atom_mae: 0.005941
2025-02-04 13:42:58.448 INFO: train_e/atom_mae: 0.005941
train_e/atom_rmse: 0.007503
2025-02-04 13:42:58.448 INFO: train_e/atom_rmse: 0.007503
train_f_mae: 0.042478
2025-02-04 13:42:58.451 INFO: train_f_mae: 0.042478
train_f_rmse: 0.064323
2025-02-04 13:42:58.451 INFO: train_f_rmse: 0.064323
val_e/atom_mae: 0.001625
2025-02-04 13:42:58.453 INFO: val_e/atom_mae: 0.001625
val_e/atom_rmse: 0.001921
2025-02-04 13:42:58.453 INFO: val_e/atom_rmse: 0.001921
val_f_mae: 0.033707
2025-02-04 13:42:58.454 INFO: val_f_mae: 0.033707
val_f_rmse: 0.052993
2025-02-04 13:42:58.454 INFO: val_f_rmse: 0.052993
##### Step: 25 Learning rate: 0.005 #####
2025-02-04 13:44:49.333 INFO: ##### Step: 25 Learning rate: 0.005 #####
Epoch 26, Train Loss: 4.6710, Val Loss: 2.8549
2025-02-04 13:44:49.333 INFO: Epoch 26, Train Loss: 4.6710, Val Loss: 2.8549
train_e/atom_mae: 0.008664
2025-02-04 13:44:49.334 INFO: train_e/atom_mae: 0.008664
train_e/atom_rmse: 0.011659
2025-02-04 13:44:49.334 INFO: train_e/atom_rmse: 0.011659
train_f_mae: 0.042717
2025-02-04 13:44:49.337 INFO: train_f_mae: 0.042717
train_f_rmse: 0.064575
2025-02-04 13:44:49.337 INFO: train_f_rmse: 0.064575
val_e/atom_mae: 0.002448
2025-02-04 13:44:49.339 INFO: val_e/atom_mae: 0.002448
val_e/atom_rmse: 0.002791
2025-02-04 13:44:49.340 INFO: val_e/atom_rmse: 0.002791
val_f_mae: 0.033784
2025-02-04 13:44:49.340 INFO: val_f_mae: 0.033784
val_f_rmse: 0.053165
2025-02-04 13:44:49.340 INFO: val_f_rmse: 0.053165
##### Step: 26 Learning rate: 0.005 #####
2025-02-04 13:46:40.180 INFO: ##### Step: 26 Learning rate: 0.005 #####
Epoch 27, Train Loss: 6.1310, Val Loss: 2.9492
2025-02-04 13:46:40.181 INFO: Epoch 27, Train Loss: 6.1310, Val Loss: 2.9492
train_e/atom_mae: 0.009559
2025-02-04 13:46:40.182 INFO: train_e/atom_mae: 0.009559
train_e/atom_rmse: 0.012106
2025-02-04 13:46:40.182 INFO: train_e/atom_rmse: 0.012106
train_f_mae: 0.048443
2025-02-04 13:46:40.185 INFO: train_f_mae: 0.048443
train_f_rmse: 0.074771
2025-02-04 13:46:40.185 INFO: train_f_rmse: 0.074771
val_e/atom_mae: 0.002354
2025-02-04 13:46:40.187 INFO: val_e/atom_mae: 0.002354
val_e/atom_rmse: 0.002702
2025-02-04 13:46:40.187 INFO: val_e/atom_rmse: 0.002702
val_f_mae: 0.034033
2025-02-04 13:46:40.188 INFO: val_f_mae: 0.034033
val_f_rmse: 0.054080
2025-02-04 13:46:40.188 INFO: val_f_rmse: 0.054080
##### Step: 27 Learning rate: 0.005 #####
2025-02-04 13:48:30.906 INFO: ##### Step: 27 Learning rate: 0.005 #####
Epoch 28, Train Loss: 3.9355, Val Loss: 2.7044
2025-02-04 13:48:30.906 INFO: Epoch 28, Train Loss: 3.9355, Val Loss: 2.7044
train_e/atom_mae: 0.003747
2025-02-04 13:48:30.907 INFO: train_e/atom_mae: 0.003747
train_e/atom_rmse: 0.004682
2025-02-04 13:48:30.907 INFO: train_e/atom_rmse: 0.004682
train_f_mae: 0.040606
2025-02-04 13:48:30.910 INFO: train_f_mae: 0.040606
train_f_rmse: 0.062086
2025-02-04 13:48:30.910 INFO: train_f_rmse: 0.062086
val_e/atom_mae: 0.003141
2025-02-04 13:48:30.912 INFO: val_e/atom_mae: 0.003141
val_e/atom_rmse: 0.003356
2025-02-04 13:48:30.912 INFO: val_e/atom_rmse: 0.003356
val_f_mae: 0.032961
2025-02-04 13:48:30.913 INFO: val_f_mae: 0.032961
val_f_rmse: 0.051610
2025-02-04 13:48:30.913 INFO: val_f_rmse: 0.051610
##### Step: 28 Learning rate: 0.005 #####
2025-02-04 13:50:22.052 INFO: ##### Step: 28 Learning rate: 0.005 #####
Epoch 29, Train Loss: 4.3809, Val Loss: 2.8829
2025-02-04 13:50:22.052 INFO: Epoch 29, Train Loss: 4.3809, Val Loss: 2.8829
train_e/atom_mae: 0.006425
2025-02-04 13:50:22.053 INFO: train_e/atom_mae: 0.006425
train_e/atom_rmse: 0.008134
2025-02-04 13:50:22.053 INFO: train_e/atom_rmse: 0.008134
train_f_mae: 0.042271
2025-02-04 13:50:22.056 INFO: train_f_mae: 0.042271
train_f_rmse: 0.064319
2025-02-04 13:50:22.056 INFO: train_f_rmse: 0.064319
val_e/atom_mae: 0.002805
2025-02-04 13:50:22.058 INFO: val_e/atom_mae: 0.002805
val_e/atom_rmse: 0.003108
2025-02-04 13:50:22.059 INFO: val_e/atom_rmse: 0.003108
val_f_mae: 0.033025
2025-02-04 13:50:22.059 INFO: val_f_mae: 0.033025
val_f_rmse: 0.053382
2025-02-04 13:50:22.059 INFO: val_f_rmse: 0.053382
##### Step: 29 Learning rate: 0.005 #####
2025-02-04 13:52:13.200 INFO: ##### Step: 29 Learning rate: 0.005 #####
Epoch 30, Train Loss: 4.8330, Val Loss: 2.7074
2025-02-04 13:52:13.200 INFO: Epoch 30, Train Loss: 4.8330, Val Loss: 2.7074
train_e/atom_mae: 0.009559
2025-02-04 13:52:13.201 INFO: train_e/atom_mae: 0.009559
train_e/atom_rmse: 0.011681
2025-02-04 13:52:13.201 INFO: train_e/atom_rmse: 0.011681
train_f_mae: 0.043797
2025-02-04 13:52:13.204 INFO: train_f_mae: 0.043797
train_f_rmse: 0.065803
2025-02-04 13:52:13.204 INFO: train_f_rmse: 0.065803
val_e/atom_mae: 0.001481
2025-02-04 13:52:13.206 INFO: val_e/atom_mae: 0.001481
val_e/atom_rmse: 0.001854
2025-02-04 13:52:13.207 INFO: val_e/atom_rmse: 0.001854
val_f_mae: 0.033217
2025-02-04 13:52:13.207 INFO: val_f_mae: 0.033217
val_f_rmse: 0.051916
2025-02-04 13:52:13.207 INFO: val_f_rmse: 0.051916
##### Step: 30 Learning rate: 0.005 #####
2025-02-04 13:54:29.105 INFO: ##### Step: 30 Learning rate: 0.005 #####
Epoch 31, Train Loss: 4.2905, Val Loss: 2.6691
2025-02-04 13:54:29.106 INFO: Epoch 31, Train Loss: 4.2905, Val Loss: 2.6691
train_e/atom_mae: 0.005442
2025-02-04 13:54:29.278 INFO: train_e/atom_mae: 0.005442
train_e/atom_rmse: 0.006621
2025-02-04 13:54:29.457 INFO: train_e/atom_rmse: 0.006621
train_f_mae: 0.042953
2025-02-04 13:54:29.460 INFO: train_f_mae: 0.042953
train_f_rmse: 0.064256
2025-02-04 13:54:29.460 INFO: train_f_rmse: 0.064256
val_e/atom_mae: 0.001568
2025-02-04 13:54:29.462 INFO: val_e/atom_mae: 0.001568
val_e/atom_rmse: 0.001880
2025-02-04 13:54:29.462 INFO: val_e/atom_rmse: 0.001880
val_f_mae: 0.032921
2025-02-04 13:54:29.463 INFO: val_f_mae: 0.032921
val_f_rmse: 0.051544
2025-02-04 13:54:29.463 INFO: val_f_rmse: 0.051544
##### Step: 31 Learning rate: 0.005 #####
2025-02-04 13:56:20.479 INFO: ##### Step: 31 Learning rate: 0.005 #####
Epoch 32, Train Loss: 4.0601, Val Loss: 2.8247
2025-02-04 13:56:20.479 INFO: Epoch 32, Train Loss: 4.0601, Val Loss: 2.8247
train_e/atom_mae: 0.006286
2025-02-04 13:56:20.480 INFO: train_e/atom_mae: 0.006286
train_e/atom_rmse: 0.008092
2025-02-04 13:56:20.480 INFO: train_e/atom_rmse: 0.008092
train_f_mae: 0.040292
2025-02-04 13:56:20.483 INFO: train_f_mae: 0.040292
train_f_rmse: 0.061796
2025-02-04 13:56:20.483 INFO: train_f_rmse: 0.061796
val_e/atom_mae: 0.003366
2025-02-04 13:56:20.485 INFO: val_e/atom_mae: 0.003366
val_e/atom_rmse: 0.003608
2025-02-04 13:56:20.486 INFO: val_e/atom_rmse: 0.003608
val_f_mae: 0.033177
2025-02-04 13:56:20.486 INFO: val_f_mae: 0.033177
val_f_rmse: 0.052702
2025-02-04 13:56:20.487 INFO: val_f_rmse: 0.052702
##### Step: 32 Learning rate: 0.005 #####
2025-02-04 13:58:11.476 INFO: ##### Step: 32 Learning rate: 0.005 #####
Epoch 33, Train Loss: 4.5263, Val Loss: 2.6908
2025-02-04 13:58:11.483 INFO: Epoch 33, Train Loss: 4.5263, Val Loss: 2.6908
train_e/atom_mae: 0.004057
2025-02-04 13:58:11.485 INFO: train_e/atom_mae: 0.004057
train_e/atom_rmse: 0.005330
2025-02-04 13:58:11.485 INFO: train_e/atom_rmse: 0.005330
train_f_mae: 0.043741
2025-02-04 13:58:11.488 INFO: train_f_mae: 0.043741
train_f_rmse: 0.066495
2025-02-04 13:58:11.488 INFO: train_f_rmse: 0.066495
val_e/atom_mae: 0.002377
2025-02-04 13:58:11.490 INFO: val_e/atom_mae: 0.002377
val_e/atom_rmse: 0.002630
2025-02-04 13:58:11.490 INFO: val_e/atom_rmse: 0.002630
val_f_mae: 0.032650
2025-02-04 13:58:11.491 INFO: val_f_mae: 0.032650
val_f_rmse: 0.051639
2025-02-04 13:58:11.491 INFO: val_f_rmse: 0.051639
##### Step: 33 Learning rate: 0.005 #####
2025-02-04 14:00:24.636 INFO: ##### Step: 33 Learning rate: 0.005 #####
Epoch 34, Train Loss: 4.4059, Val Loss: 2.8587
2025-02-04 14:00:24.637 INFO: Epoch 34, Train Loss: 4.4059, Val Loss: 2.8587
train_e/atom_mae: 0.004355
2025-02-04 14:00:24.826 INFO: train_e/atom_mae: 0.004355
train_e/atom_rmse: 0.005334
2025-02-04 14:00:24.956 INFO: train_e/atom_rmse: 0.005334
train_f_mae: 0.042058
2025-02-04 14:00:24.959 INFO: train_f_mae: 0.042058
train_f_rmse: 0.065582
2025-02-04 14:00:24.959 INFO: train_f_rmse: 0.065582
val_e/atom_mae: 0.003098
2025-02-04 14:00:24.961 INFO: val_e/atom_mae: 0.003098
val_e/atom_rmse: 0.003420
2025-02-04 14:00:24.961 INFO: val_e/atom_rmse: 0.003420
val_f_mae: 0.033429
2025-02-04 14:00:24.962 INFO: val_f_mae: 0.033429
val_f_rmse: 0.053055
2025-02-04 14:00:24.962 INFO: val_f_rmse: 0.053055
##### Step: 34 Learning rate: 0.005 #####
2025-02-04 14:02:16.669 INFO: ##### Step: 34 Learning rate: 0.005 #####
Epoch 35, Train Loss: 6.7134, Val Loss: 2.7902
2025-02-04 14:02:16.669 INFO: Epoch 35, Train Loss: 6.7134, Val Loss: 2.7902
train_e/atom_mae: 0.007514
2025-02-04 14:02:16.670 INFO: train_e/atom_mae: 0.007514
train_e/atom_rmse: 0.009179
2025-02-04 14:02:16.670 INFO: train_e/atom_rmse: 0.009179
train_f_mae: 0.051040
2025-02-04 14:02:16.673 INFO: train_f_mae: 0.051040
train_f_rmse: 0.080017
2025-02-04 14:02:16.673 INFO: train_f_rmse: 0.080017
val_e/atom_mae: 0.004146
2025-02-04 14:02:16.675 INFO: val_e/atom_mae: 0.004146
val_e/atom_rmse: 0.004288
2025-02-04 14:02:16.676 INFO: val_e/atom_rmse: 0.004288
val_f_mae: 0.032804
2025-02-04 14:02:16.676 INFO: val_f_mae: 0.032804
val_f_rmse: 0.052185
2025-02-04 14:02:16.676 INFO: val_f_rmse: 0.052185
##### Step: 35 Learning rate: 0.005 #####
2025-02-04 14:04:08.441 INFO: ##### Step: 35 Learning rate: 0.005 #####
Epoch 36, Train Loss: 4.0398, Val Loss: 2.6073
2025-02-04 14:04:08.442 INFO: Epoch 36, Train Loss: 4.0398, Val Loss: 2.6073
train_e/atom_mae: 0.006704
2025-02-04 14:04:08.443 INFO: train_e/atom_mae: 0.006704
train_e/atom_rmse: 0.008302
2025-02-04 14:04:08.443 INFO: train_e/atom_rmse: 0.008302
train_f_mae: 0.040601
2025-02-04 14:04:08.446 INFO: train_f_mae: 0.040601
train_f_rmse: 0.061528
2025-02-04 14:04:08.446 INFO: train_f_rmse: 0.061528
val_e/atom_mae: 0.001585
2025-02-04 14:04:08.448 INFO: val_e/atom_mae: 0.001585
val_e/atom_rmse: 0.001923
2025-02-04 14:04:08.448 INFO: val_e/atom_rmse: 0.001923
val_f_mae: 0.032239
2025-02-04 14:04:08.449 INFO: val_f_mae: 0.032239
val_f_rmse: 0.050940
2025-02-04 14:04:08.449 INFO: val_f_rmse: 0.050940
##### Step: 36 Learning rate: 0.005 #####
2025-02-04 14:06:04.311 INFO: ##### Step: 36 Learning rate: 0.005 #####
Epoch 37, Train Loss: 8.0232, Val Loss: 2.7127
2025-02-04 14:06:04.312 INFO: Epoch 37, Train Loss: 8.0232, Val Loss: 2.7127
train_e/atom_mae: 0.012095
2025-02-04 14:06:04.329 INFO: train_e/atom_mae: 0.012095
train_e/atom_rmse: 0.014821
2025-02-04 14:06:04.384 INFO: train_e/atom_rmse: 0.014821
train_f_mae: 0.056435
2025-02-04 14:06:04.387 INFO: train_f_mae: 0.056435
train_f_rmse: 0.084932
2025-02-04 14:06:04.387 INFO: train_f_rmse: 0.084932
val_e/atom_mae: 0.003166
2025-02-04 14:06:04.389 INFO: val_e/atom_mae: 0.003166
val_e/atom_rmse: 0.003352
2025-02-04 14:06:04.390 INFO: val_e/atom_rmse: 0.003352
val_f_mae: 0.032794
2025-02-04 14:06:04.390 INFO: val_f_mae: 0.032794
val_f_rmse: 0.051696
2025-02-04 14:06:04.390 INFO: val_f_rmse: 0.051696
##### Step: 37 Learning rate: 0.005 #####
2025-02-04 14:07:56.042 INFO: ##### Step: 37 Learning rate: 0.005 #####
Epoch 38, Train Loss: 4.3111, Val Loss: 2.6669
2025-02-04 14:07:56.042 INFO: Epoch 38, Train Loss: 4.3111, Val Loss: 2.6669
train_e/atom_mae: 0.007537
2025-02-04 14:07:56.043 INFO: train_e/atom_mae: 0.007537
train_e/atom_rmse: 0.009327
2025-02-04 14:07:56.043 INFO: train_e/atom_rmse: 0.009327
train_f_mae: 0.041562
2025-02-04 14:07:56.046 INFO: train_f_mae: 0.041562
train_f_rmse: 0.063170
2025-02-04 14:07:56.046 INFO: train_f_rmse: 0.063170
val_e/atom_mae: 0.003029
2025-02-04 14:07:56.048 INFO: val_e/atom_mae: 0.003029
val_e/atom_rmse: 0.003218
2025-02-04 14:07:56.048 INFO: val_e/atom_rmse: 0.003218
val_f_mae: 0.032389
2025-02-04 14:07:56.049 INFO: val_f_mae: 0.032389
val_f_rmse: 0.051288
2025-02-04 14:07:56.049 INFO: val_f_rmse: 0.051288
##### Step: 38 Learning rate: 0.005 #####
2025-02-04 14:09:59.090 INFO: ##### Step: 38 Learning rate: 0.005 #####
Epoch 39, Train Loss: 4.3766, Val Loss: 2.7659
2025-02-04 14:09:59.090 INFO: Epoch 39, Train Loss: 4.3766, Val Loss: 2.7659
train_e/atom_mae: 0.006220
2025-02-04 14:09:59.116 INFO: train_e/atom_mae: 0.006220
train_e/atom_rmse: 0.008173
2025-02-04 14:09:59.118 INFO: train_e/atom_rmse: 0.008173
train_f_mae: 0.042273
2025-02-04 14:09:59.120 INFO: train_f_mae: 0.042273
train_f_rmse: 0.064268
2025-02-04 14:09:59.121 INFO: train_f_rmse: 0.064268
val_e/atom_mae: 0.003570
2025-02-04 14:09:59.123 INFO: val_e/atom_mae: 0.003570
val_e/atom_rmse: 0.003723
2025-02-04 14:09:59.123 INFO: val_e/atom_rmse: 0.003723
val_f_mae: 0.032617
2025-02-04 14:09:59.124 INFO: val_f_mae: 0.032617
val_f_rmse: 0.052129
2025-02-04 14:09:59.124 INFO: val_f_rmse: 0.052129
##### Step: 39 Learning rate: 0.005 #####
2025-02-04 14:11:50.651 INFO: ##### Step: 39 Learning rate: 0.005 #####
Epoch 40, Train Loss: 3.5008, Val Loss: 2.5620
2025-02-04 14:11:50.652 INFO: Epoch 40, Train Loss: 3.5008, Val Loss: 2.5620
train_e/atom_mae: 0.004374
2025-02-04 14:11:50.653 INFO: train_e/atom_mae: 0.004374
train_e/atom_rmse: 0.005497
2025-02-04 14:11:50.653 INFO: train_e/atom_rmse: 0.005497
train_f_mae: 0.038112
2025-02-04 14:11:50.656 INFO: train_f_mae: 0.038112
train_f_rmse: 0.058219
2025-02-04 14:11:50.656 INFO: train_f_rmse: 0.058219
val_e/atom_mae: 0.002463
2025-02-04 14:11:50.658 INFO: val_e/atom_mae: 0.002463
val_e/atom_rmse: 0.002703
2025-02-04 14:11:50.659 INFO: val_e/atom_rmse: 0.002703
val_f_mae: 0.031721
2025-02-04 14:11:50.659 INFO: val_f_mae: 0.031721
val_f_rmse: 0.050363
2025-02-04 14:11:50.659 INFO: val_f_rmse: 0.050363
2025-02-04 14:11:51.495 INFO: training
##### Step: 0 Learning rate: 0.002 #####
2025-02-04 14:13:45.130 INFO: ##### Step: 0 Learning rate: 0.002 #####
Epoch 1, Train Loss: 2.8965, Val Loss: 2.7351
2025-02-04 14:13:45.131 INFO: Epoch 1, Train Loss: 2.8965, Val Loss: 2.7351
train_e/atom_mae: 0.003144
2025-02-04 14:13:45.147 INFO: train_e/atom_mae: 0.003144
train_e/atom_rmse: 0.003866
2025-02-04 14:13:45.148 INFO: train_e/atom_rmse: 0.003866
train_f_mae: 0.033862
2025-02-04 14:13:45.151 INFO: train_f_mae: 0.033862
train_f_rmse: 0.053305
2025-02-04 14:13:45.151 INFO: train_f_rmse: 0.053305
val_e/atom_mae: 0.002308
2025-02-04 14:13:45.153 INFO: val_e/atom_mae: 0.002308
val_e/atom_rmse: 0.002857
2025-02-04 14:13:45.154 INFO: val_e/atom_rmse: 0.002857
val_f_mae: 0.033042
2025-02-04 14:13:45.154 INFO: val_f_mae: 0.033042
val_f_rmse: 0.052022
2025-02-04 14:13:45.154 INFO: val_f_rmse: 0.052022
##### Step: 1 Learning rate: 0.004 #####
2025-02-04 14:15:36.141 INFO: ##### Step: 1 Learning rate: 0.004 #####
Epoch 2, Train Loss: 3.3997, Val Loss: 4.4350
2025-02-04 14:15:36.141 INFO: Epoch 2, Train Loss: 3.3997, Val Loss: 4.4350
train_e/atom_mae: 0.003626
2025-02-04 14:15:36.142 INFO: train_e/atom_mae: 0.003626
train_e/atom_rmse: 0.004743
2025-02-04 14:15:36.142 INFO: train_e/atom_rmse: 0.004743
train_f_mae: 0.038072
2025-02-04 14:15:36.145 INFO: train_f_mae: 0.038072
train_f_rmse: 0.057592
2025-02-04 14:15:36.145 INFO: train_f_rmse: 0.057592
val_e/atom_mae: 0.002453
2025-02-04 14:15:36.147 INFO: val_e/atom_mae: 0.002453
val_e/atom_rmse: 0.002790
2025-02-04 14:15:36.148 INFO: val_e/atom_rmse: 0.002790
val_f_mae: 0.046811
2025-02-04 14:15:36.148 INFO: val_f_mae: 0.046811
val_f_rmse: 0.066377
2025-02-04 14:15:36.148 INFO: val_f_rmse: 0.066377
##### Step: 2 Learning rate: 0.006 #####
2025-02-04 14:17:27.297 INFO: ##### Step: 2 Learning rate: 0.006 #####
Epoch 3, Train Loss: 3.6596, Val Loss: 4.0244
2025-02-04 14:17:27.297 INFO: Epoch 3, Train Loss: 3.6596, Val Loss: 4.0244
train_e/atom_mae: 0.004910
2025-02-04 14:17:27.298 INFO: train_e/atom_mae: 0.004910
train_e/atom_rmse: 0.006031
2025-02-04 14:17:27.298 INFO: train_e/atom_rmse: 0.006031
train_f_mae: 0.038858
2025-02-04 14:17:27.301 INFO: train_f_mae: 0.038858
train_f_rmse: 0.059376
2025-02-04 14:17:27.301 INFO: train_f_rmse: 0.059376
val_e/atom_mae: 0.001989
2025-02-04 14:17:27.303 INFO: val_e/atom_mae: 0.001989
val_e/atom_rmse: 0.002544
2025-02-04 14:17:27.303 INFO: val_e/atom_rmse: 0.002544
val_f_mae: 0.041370
2025-02-04 14:17:27.304 INFO: val_f_mae: 0.041370
val_f_rmse: 0.063242
2025-02-04 14:17:27.304 INFO: val_f_rmse: 0.063242
##### Step: 3 Learning rate: 0.008 #####
2025-02-04 14:19:18.274 INFO: ##### Step: 3 Learning rate: 0.008 #####
Epoch 4, Train Loss: 5.9220, Val Loss: 4.8952
2025-02-04 14:19:18.274 INFO: Epoch 4, Train Loss: 5.9220, Val Loss: 4.8952
train_e/atom_mae: 0.009028
2025-02-04 14:19:18.275 INFO: train_e/atom_mae: 0.009028
train_e/atom_rmse: 0.011746
2025-02-04 14:19:18.276 INFO: train_e/atom_rmse: 0.011746
train_f_mae: 0.049146
2025-02-04 14:19:18.278 INFO: train_f_mae: 0.049146
train_f_rmse: 0.073576
2025-02-04 14:19:18.279 INFO: train_f_rmse: 0.073576
val_e/atom_mae: 0.002633
2025-02-04 14:19:18.281 INFO: val_e/atom_mae: 0.002633
val_e/atom_rmse: 0.003170
2025-02-04 14:19:18.281 INFO: val_e/atom_rmse: 0.003170
val_f_mae: 0.044381
2025-02-04 14:19:18.282 INFO: val_f_mae: 0.044381
val_f_rmse: 0.069717
2025-02-04 14:19:18.282 INFO: val_f_rmse: 0.069717
##### Step: 4 Learning rate: 0.01 #####
2025-02-04 14:21:09.529 INFO: ##### Step: 4 Learning rate: 0.01 #####
Epoch 5, Train Loss: 5.4495, Val Loss: 16.6750
2025-02-04 14:21:09.530 INFO: Epoch 5, Train Loss: 5.4495, Val Loss: 16.6750
train_e/atom_mae: 0.006565
2025-02-04 14:21:09.530 INFO: train_e/atom_mae: 0.006565
train_e/atom_rmse: 0.008360
2025-02-04 14:21:09.531 INFO: train_e/atom_rmse: 0.008360
train_f_mae: 0.047646
2025-02-04 14:21:09.533 INFO: train_f_mae: 0.047646
train_f_rmse: 0.072055
2025-02-04 14:21:09.534 INFO: train_f_rmse: 0.072055
val_e/atom_mae: 0.007149
2025-02-04 14:21:09.536 INFO: val_e/atom_mae: 0.007149
val_e/atom_rmse: 0.007922
2025-02-04 14:21:09.536 INFO: val_e/atom_rmse: 0.007922
val_f_mae: 0.096005
2025-02-04 14:21:09.536 INFO: val_f_mae: 0.096005
val_f_rmse: 0.128171
2025-02-04 14:21:09.537 INFO: val_f_rmse: 0.128171
##### Step: 5 Learning rate: 0.01 #####
2025-02-04 14:23:00.868 INFO: ##### Step: 5 Learning rate: 0.01 #####
Epoch 6, Train Loss: 14.4432, Val Loss: 10.0966
2025-02-04 14:23:00.868 INFO: Epoch 6, Train Loss: 14.4432, Val Loss: 10.0966
train_e/atom_mae: 0.017518
2025-02-04 14:23:00.869 INFO: train_e/atom_mae: 0.017518
train_e/atom_rmse: 0.022104
2025-02-04 14:23:00.869 INFO: train_e/atom_rmse: 0.022104
train_f_mae: 0.076413
2025-02-04 14:23:00.872 INFO: train_f_mae: 0.076413
train_f_rmse: 0.112437
2025-02-04 14:23:00.872 INFO: train_f_rmse: 0.112437
val_e/atom_mae: 0.016827
2025-02-04 14:23:00.874 INFO: val_e/atom_mae: 0.016827
val_e/atom_rmse: 0.017522
2025-02-04 14:23:00.875 INFO: val_e/atom_rmse: 0.017522
val_f_mae: 0.065402
2025-02-04 14:23:00.875 INFO: val_f_mae: 0.065402
val_f_rmse: 0.094700
2025-02-04 14:23:00.875 INFO: val_f_rmse: 0.094700
##### Step: 6 Learning rate: 0.01 #####
2025-02-04 14:24:52.072 INFO: ##### Step: 6 Learning rate: 0.01 #####
Epoch 7, Train Loss: 12.8547, Val Loss: 7.3085
2025-02-04 14:24:52.073 INFO: Epoch 7, Train Loss: 12.8547, Val Loss: 7.3085
train_e/atom_mae: 0.011385
2025-02-04 14:24:52.074 INFO: train_e/atom_mae: 0.011385
train_e/atom_rmse: 0.014051
2025-02-04 14:24:52.074 INFO: train_e/atom_rmse: 0.014051
train_f_mae: 0.073181
2025-02-04 14:24:52.077 INFO: train_f_mae: 0.073181
train_f_rmse: 0.110122
2025-02-04 14:24:52.077 INFO: train_f_rmse: 0.110122
val_e/atom_mae: 0.011611
2025-02-04 14:24:52.079 INFO: val_e/atom_mae: 0.011611
val_e/atom_rmse: 0.011749
2025-02-04 14:24:52.079 INFO: val_e/atom_rmse: 0.011749
val_f_mae: 0.057893
2025-02-04 14:24:52.080 INFO: val_f_mae: 0.057893
val_f_rmse: 0.082490
2025-02-04 14:24:52.080 INFO: val_f_rmse: 0.082490
##### Step: 7 Learning rate: 0.01 #####
2025-02-04 14:26:43.477 INFO: ##### Step: 7 Learning rate: 0.01 #####
Epoch 8, Train Loss: 7.0600, Val Loss: 11.6168
2025-02-04 14:26:43.478 INFO: Epoch 8, Train Loss: 7.0600, Val Loss: 11.6168
train_e/atom_mae: 0.008429
2025-02-04 14:26:43.479 INFO: train_e/atom_mae: 0.008429
train_e/atom_rmse: 0.010431
2025-02-04 14:26:43.479 INFO: train_e/atom_rmse: 0.010431
train_f_mae: 0.053748
2025-02-04 14:26:43.482 INFO: train_f_mae: 0.053748
train_f_rmse: 0.081602
2025-02-04 14:26:43.482 INFO: train_f_rmse: 0.081602
val_e/atom_mae: 0.004769
2025-02-04 14:26:43.484 INFO: val_e/atom_mae: 0.004769
val_e/atom_rmse: 0.005564
2025-02-04 14:26:43.484 INFO: val_e/atom_rmse: 0.005564
val_f_mae: 0.075578
2025-02-04 14:26:43.485 INFO: val_f_mae: 0.075578
val_f_rmse: 0.107243
2025-02-04 14:26:43.485 INFO: val_f_rmse: 0.107243
##### Step: 8 Learning rate: 0.01 #####
2025-02-04 14:28:34.549 INFO: ##### Step: 8 Learning rate: 0.01 #####
Epoch 9, Train Loss: 7.9140, Val Loss: 12.8929
2025-02-04 14:28:34.550 INFO: Epoch 9, Train Loss: 7.9140, Val Loss: 12.8929
train_e/atom_mae: 0.007904
2025-02-04 14:28:34.551 INFO: train_e/atom_mae: 0.007904
train_e/atom_rmse: 0.010158
2025-02-04 14:28:34.551 INFO: train_e/atom_rmse: 0.010158
train_f_mae: 0.056849
2025-02-04 14:28:34.554 INFO: train_f_mae: 0.056849
train_f_rmse: 0.086797
2025-02-04 14:28:34.554 INFO: train_f_rmse: 0.086797
val_e/atom_mae: 0.009307
2025-02-04 14:28:34.556 INFO: val_e/atom_mae: 0.009307
val_e/atom_rmse: 0.009812
2025-02-04 14:28:34.556 INFO: val_e/atom_rmse: 0.009812
val_f_mae: 0.075099
2025-02-04 14:28:34.557 INFO: val_f_mae: 0.075099
val_f_rmse: 0.111914
2025-02-04 14:28:34.557 INFO: val_f_rmse: 0.111914
##### Step: 9 Learning rate: 0.01 #####
2025-02-04 14:30:25.809 INFO: ##### Step: 9 Learning rate: 0.01 #####
Epoch 10, Train Loss: 8.2626, Val Loss: 5.6331
2025-02-04 14:30:25.810 INFO: Epoch 10, Train Loss: 8.2626, Val Loss: 5.6331
train_e/atom_mae: 0.013274
2025-02-04 14:30:25.811 INFO: train_e/atom_mae: 0.013274
train_e/atom_rmse: 0.016976
2025-02-04 14:30:25.811 INFO: train_e/atom_rmse: 0.016976
train_f_mae: 0.055902
2025-02-04 14:30:25.814 INFO: train_f_mae: 0.055902
train_f_rmse: 0.084854
2025-02-04 14:30:25.814 INFO: train_f_rmse: 0.084854
val_e/atom_mae: 0.006116
2025-02-04 14:30:25.816 INFO: val_e/atom_mae: 0.006116
val_e/atom_rmse: 0.006292
2025-02-04 14:30:25.817 INFO: val_e/atom_rmse: 0.006292
val_f_mae: 0.051042
2025-02-04 14:30:25.817 INFO: val_f_mae: 0.051042
val_f_rmse: 0.074092
2025-02-04 14:30:25.817 INFO: val_f_rmse: 0.074092
##### Step: 10 Learning rate: 0.01 #####
2025-02-04 14:32:17.429 INFO: ##### Step: 10 Learning rate: 0.01 #####
Epoch 11, Train Loss: 6.0598, Val Loss: 3.2308
2025-02-04 14:32:17.430 INFO: Epoch 11, Train Loss: 6.0598, Val Loss: 3.2308
train_e/atom_mae: 0.006823
2025-02-04 14:32:17.431 INFO: train_e/atom_mae: 0.006823
train_e/atom_rmse: 0.009034
2025-02-04 14:32:17.431 INFO: train_e/atom_rmse: 0.009034
train_f_mae: 0.050497
2025-02-04 14:32:17.434 INFO: train_f_mae: 0.050497
train_f_rmse: 0.075888
2025-02-04 14:32:17.434 INFO: train_f_rmse: 0.075888
val_e/atom_mae: 0.004191
2025-02-04 14:32:17.436 INFO: val_e/atom_mae: 0.004191
val_e/atom_rmse: 0.004505
2025-02-04 14:32:17.436 INFO: val_e/atom_rmse: 0.004505
val_f_mae: 0.034791
2025-02-04 14:32:17.437 INFO: val_f_mae: 0.034791
val_f_rmse: 0.056207
2025-02-04 14:32:17.437 INFO: val_f_rmse: 0.056207
##### Step: 11 Learning rate: 0.01 #####
2025-02-04 14:34:09.025 INFO: ##### Step: 11 Learning rate: 0.01 #####
Epoch 12, Train Loss: 6.8716, Val Loss: 3.2883
2025-02-04 14:34:09.026 INFO: Epoch 12, Train Loss: 6.8716, Val Loss: 3.2883
train_e/atom_mae: 0.007355
2025-02-04 14:34:09.026 INFO: train_e/atom_mae: 0.007355
train_e/atom_rmse: 0.009627
2025-02-04 14:34:09.027 INFO: train_e/atom_rmse: 0.009627
train_f_mae: 0.053570
2025-02-04 14:34:09.029 INFO: train_f_mae: 0.053570
train_f_rmse: 0.080808
2025-02-04 14:34:09.030 INFO: train_f_rmse: 0.080808
val_e/atom_mae: 0.005351
2025-02-04 14:34:09.032 INFO: val_e/atom_mae: 0.005351
val_e/atom_rmse: 0.005528
2025-02-04 14:34:09.032 INFO: val_e/atom_rmse: 0.005528
val_f_mae: 0.035252
2025-02-04 14:34:09.033 INFO: val_f_mae: 0.035252
val_f_rmse: 0.056374
2025-02-04 14:34:09.033 INFO: val_f_rmse: 0.056374
##### Step: 12 Learning rate: 0.01 #####
2025-02-04 14:36:00.822 INFO: ##### Step: 12 Learning rate: 0.01 #####
Epoch 13, Train Loss: 8.3043, Val Loss: 3.7264
2025-02-04 14:36:00.823 INFO: Epoch 13, Train Loss: 8.3043, Val Loss: 3.7264
train_e/atom_mae: 0.012432
2025-02-04 14:36:00.824 INFO: train_e/atom_mae: 0.012432
train_e/atom_rmse: 0.015677
2025-02-04 14:36:00.824 INFO: train_e/atom_rmse: 0.015677
train_f_mae: 0.057099
2025-02-04 14:36:00.827 INFO: train_f_mae: 0.057099
train_f_rmse: 0.086013
2025-02-04 14:36:00.827 INFO: train_f_rmse: 0.086013
val_e/atom_mae: 0.009798
2025-02-04 14:36:00.829 INFO: val_e/atom_mae: 0.009798
val_e/atom_rmse: 0.009982
2025-02-04 14:36:00.829 INFO: val_e/atom_rmse: 0.009982
val_f_mae: 0.036946
2025-02-04 14:36:00.830 INFO: val_f_mae: 0.036946
val_f_rmse: 0.057985
2025-02-04 14:36:00.830 INFO: val_f_rmse: 0.057985
##### Step: 13 Learning rate: 0.01 #####
2025-02-04 14:37:52.479 INFO: ##### Step: 13 Learning rate: 0.01 #####
Epoch 14, Train Loss: 7.0419, Val Loss: 3.6263
2025-02-04 14:37:52.480 INFO: Epoch 14, Train Loss: 7.0419, Val Loss: 3.6263
train_e/atom_mae: 0.007120
2025-02-04 14:37:52.481 INFO: train_e/atom_mae: 0.007120
train_e/atom_rmse: 0.009317
2025-02-04 14:37:52.481 INFO: train_e/atom_rmse: 0.009317
train_f_mae: 0.054395
2025-02-04 14:37:52.484 INFO: train_f_mae: 0.054395
train_f_rmse: 0.081987
2025-02-04 14:37:52.484 INFO: train_f_rmse: 0.081987
val_e/atom_mae: 0.010332
2025-02-04 14:37:52.486 INFO: val_e/atom_mae: 0.010332
val_e/atom_rmse: 0.010462
2025-02-04 14:37:52.486 INFO: val_e/atom_rmse: 0.010462
val_f_mae: 0.036794
2025-02-04 14:37:52.487 INFO: val_f_mae: 0.036794
val_f_rmse: 0.056782
2025-02-04 14:37:52.487 INFO: val_f_rmse: 0.056782
##### Step: 14 Learning rate: 0.01 #####
2025-02-04 14:39:44.255 INFO: ##### Step: 14 Learning rate: 0.01 #####
Epoch 15, Train Loss: 6.2406, Val Loss: 3.1232
2025-02-04 14:39:44.256 INFO: Epoch 15, Train Loss: 6.2406, Val Loss: 3.1232
train_e/atom_mae: 0.008569
2025-02-04 14:39:44.257 INFO: train_e/atom_mae: 0.008569
train_e/atom_rmse: 0.010687
2025-02-04 14:39:44.257 INFO: train_e/atom_rmse: 0.010687
train_f_mae: 0.049936
2025-02-04 14:39:44.260 INFO: train_f_mae: 0.049936
train_f_rmse: 0.076286
2025-02-04 14:39:44.260 INFO: train_f_rmse: 0.076286
val_e/atom_mae: 0.006317
2025-02-04 14:39:44.262 INFO: val_e/atom_mae: 0.006317
val_e/atom_rmse: 0.006622
2025-02-04 14:39:44.262 INFO: val_e/atom_rmse: 0.006622
val_f_mae: 0.034319
2025-02-04 14:39:44.263 INFO: val_f_mae: 0.034319
val_f_rmse: 0.054428
2025-02-04 14:39:44.263 INFO: val_f_rmse: 0.054428
##### Step: 15 Learning rate: 0.01 #####
2025-02-04 14:41:35.915 INFO: ##### Step: 15 Learning rate: 0.01 #####
Epoch 16, Train Loss: 6.0152, Val Loss: 2.9522
2025-02-04 14:41:35.916 INFO: Epoch 16, Train Loss: 6.0152, Val Loss: 2.9522
train_e/atom_mae: 0.008416
2025-02-04 14:41:35.917 INFO: train_e/atom_mae: 0.008416
train_e/atom_rmse: 0.010369
2025-02-04 14:41:35.917 INFO: train_e/atom_rmse: 0.010369
train_f_mae: 0.049716
2025-02-04 14:41:35.920 INFO: train_f_mae: 0.049716
train_f_rmse: 0.074959
2025-02-04 14:41:35.920 INFO: train_f_rmse: 0.074959
val_e/atom_mae: 0.003055
2025-02-04 14:41:35.922 INFO: val_e/atom_mae: 0.003055
val_e/atom_rmse: 0.003290
2025-02-04 14:41:35.922 INFO: val_e/atom_rmse: 0.003290
val_f_mae: 0.034045
2025-02-04 14:41:35.923 INFO: val_f_mae: 0.034045
val_f_rmse: 0.053968
2025-02-04 14:41:35.923 INFO: val_f_rmse: 0.053968
##### Step: 16 Learning rate: 0.01 #####
2025-02-04 14:43:27.594 INFO: ##### Step: 16 Learning rate: 0.01 #####
Epoch 17, Train Loss: 6.3436, Val Loss: 2.9863
2025-02-04 14:43:27.594 INFO: Epoch 17, Train Loss: 6.3436, Val Loss: 2.9863
train_e/atom_mae: 0.007954
2025-02-04 14:43:27.595 INFO: train_e/atom_mae: 0.007954
train_e/atom_rmse: 0.009739
2025-02-04 14:43:27.596 INFO: train_e/atom_rmse: 0.009739
train_f_mae: 0.050993
2025-02-04 14:43:27.602 INFO: train_f_mae: 0.050993
train_f_rmse: 0.077420
2025-02-04 14:43:27.602 INFO: train_f_rmse: 0.077420
val_e/atom_mae: 0.003106
2025-02-04 14:43:27.604 INFO: val_e/atom_mae: 0.003106
val_e/atom_rmse: 0.003271
2025-02-04 14:43:27.604 INFO: val_e/atom_rmse: 0.003271
val_f_mae: 0.034158
2025-02-04 14:43:27.605 INFO: val_f_mae: 0.034158
val_f_rmse: 0.054293
2025-02-04 14:43:27.605 INFO: val_f_rmse: 0.054293
##### Step: 17 Learning rate: 0.01 #####
2025-02-04 14:45:19.105 INFO: ##### Step: 17 Learning rate: 0.01 #####
Epoch 18, Train Loss: 7.7361, Val Loss: 2.9163
2025-02-04 14:45:19.105 INFO: Epoch 18, Train Loss: 7.7361, Val Loss: 2.9163
train_e/atom_mae: 0.008289
2025-02-04 14:45:19.106 INFO: train_e/atom_mae: 0.008289
train_e/atom_rmse: 0.011312
2025-02-04 14:45:19.107 INFO: train_e/atom_rmse: 0.011312
train_f_mae: 0.055194
2025-02-04 14:45:19.109 INFO: train_f_mae: 0.055194
train_f_rmse: 0.085231
2025-02-04 14:45:19.109 INFO: train_f_rmse: 0.085231
val_e/atom_mae: 0.004580
2025-02-04 14:45:19.112 INFO: val_e/atom_mae: 0.004580
val_e/atom_rmse: 0.004824
2025-02-04 14:45:19.112 INFO: val_e/atom_rmse: 0.004824
val_f_mae: 0.033909
2025-02-04 14:45:19.112 INFO: val_f_mae: 0.033909
val_f_rmse: 0.053222
2025-02-04 14:45:19.113 INFO: val_f_rmse: 0.053222
##### Step: 18 Learning rate: 0.01 #####
2025-02-04 14:47:10.672 INFO: ##### Step: 18 Learning rate: 0.01 #####
Epoch 19, Train Loss: 6.4115, Val Loss: 3.1215
2025-02-04 14:47:10.673 INFO: Epoch 19, Train Loss: 6.4115, Val Loss: 3.1215
train_e/atom_mae: 0.008975
2025-02-04 14:47:10.673 INFO: train_e/atom_mae: 0.008975
train_e/atom_rmse: 0.011789
2025-02-04 14:47:10.674 INFO: train_e/atom_rmse: 0.011789
train_f_mae: 0.051571
2025-02-04 14:47:10.676 INFO: train_f_mae: 0.051571
train_f_rmse: 0.076806
2025-02-04 14:47:10.677 INFO: train_f_rmse: 0.076806
val_e/atom_mae: 0.004952
2025-02-04 14:47:10.679 INFO: val_e/atom_mae: 0.004952
val_e/atom_rmse: 0.005094
2025-02-04 14:47:10.679 INFO: val_e/atom_rmse: 0.005094
val_f_mae: 0.034231
2025-02-04 14:47:10.680 INFO: val_f_mae: 0.034231
val_f_rmse: 0.055016
2025-02-04 14:47:10.680 INFO: val_f_rmse: 0.055016
##### Step: 19 Learning rate: 0.01 #####
2025-02-04 14:49:02.449 INFO: ##### Step: 19 Learning rate: 0.01 #####
Epoch 20, Train Loss: 11.7809, Val Loss: 3.3457
2025-02-04 14:49:02.450 INFO: Epoch 20, Train Loss: 11.7809, Val Loss: 3.3457
train_e/atom_mae: 0.011693
2025-02-04 14:49:02.451 INFO: train_e/atom_mae: 0.011693
train_e/atom_rmse: 0.014407
2025-02-04 14:49:02.451 INFO: train_e/atom_rmse: 0.014407
train_f_mae: 0.068668
2025-02-04 14:49:02.454 INFO: train_f_mae: 0.068668
train_f_rmse: 0.104956
2025-02-04 14:49:02.454 INFO: train_f_rmse: 0.104956
val_e/atom_mae: 0.005806
2025-02-04 14:49:02.456 INFO: val_e/atom_mae: 0.005806
val_e/atom_rmse: 0.005957
2025-02-04 14:49:02.457 INFO: val_e/atom_rmse: 0.005957
val_f_mae: 0.035493
2025-02-04 14:49:02.457 INFO: val_f_mae: 0.035493
val_f_rmse: 0.056700
2025-02-04 14:49:02.457 INFO: val_f_rmse: 0.056700
##### Step: 20 Learning rate: 0.005 #####
2025-02-04 14:50:54.009 INFO: ##### Step: 20 Learning rate: 0.005 #####
Epoch 21, Train Loss: 4.2667, Val Loss: 2.8377
2025-02-04 14:50:54.009 INFO: Epoch 21, Train Loss: 4.2667, Val Loss: 2.8377
train_e/atom_mae: 0.005525
2025-02-04 14:50:54.010 INFO: train_e/atom_mae: 0.005525
train_e/atom_rmse: 0.007027
2025-02-04 14:50:54.010 INFO: train_e/atom_rmse: 0.007027
train_f_mae: 0.041608
2025-02-04 14:50:54.013 INFO: train_f_mae: 0.041608
train_f_rmse: 0.063911
2025-02-04 14:50:54.013 INFO: train_f_rmse: 0.063911
val_e/atom_mae: 0.003391
2025-02-04 14:50:54.015 INFO: val_e/atom_mae: 0.003391
val_e/atom_rmse: 0.003628
2025-02-04 14:50:54.016 INFO: val_e/atom_rmse: 0.003628
val_f_mae: 0.033272
2025-02-04 14:50:54.016 INFO: val_f_mae: 0.033272
val_f_rmse: 0.052828
2025-02-04 14:50:54.017 INFO: val_f_rmse: 0.052828
##### Step: 21 Learning rate: 0.005 #####
2025-02-04 14:52:45.538 INFO: ##### Step: 21 Learning rate: 0.005 #####
Epoch 22, Train Loss: 3.5391, Val Loss: 2.7861
2025-02-04 14:52:45.539 INFO: Epoch 22, Train Loss: 3.5391, Val Loss: 2.7861
train_e/atom_mae: 0.004864
2025-02-04 14:52:45.540 INFO: train_e/atom_mae: 0.004864
train_e/atom_rmse: 0.006117
2025-02-04 14:52:45.540 INFO: train_e/atom_rmse: 0.006117
train_f_mae: 0.038021
2025-02-04 14:52:45.542 INFO: train_f_mae: 0.038021
train_f_rmse: 0.058320
2025-02-04 14:52:45.543 INFO: train_f_rmse: 0.058320
val_e/atom_mae: 0.001944
2025-02-04 14:52:45.545 INFO: val_e/atom_mae: 0.001944
val_e/atom_rmse: 0.002318
2025-02-04 14:52:45.545 INFO: val_e/atom_rmse: 0.002318
val_f_mae: 0.032393
2025-02-04 14:52:45.546 INFO: val_f_mae: 0.032393
val_f_rmse: 0.052598
2025-02-04 14:52:45.546 INFO: val_f_rmse: 0.052598
##### Step: 22 Learning rate: 0.005 #####
2025-02-04 14:54:37.112 INFO: ##### Step: 22 Learning rate: 0.005 #####
Epoch 23, Train Loss: 3.8359, Val Loss: 2.5933
2025-02-04 14:54:37.113 INFO: Epoch 23, Train Loss: 3.8359, Val Loss: 2.5933
train_e/atom_mae: 0.005311
2025-02-04 14:54:37.114 INFO: train_e/atom_mae: 0.005311
train_e/atom_rmse: 0.006748
2025-02-04 14:54:37.114 INFO: train_e/atom_rmse: 0.006748
train_f_mae: 0.040851
2025-02-04 14:54:37.117 INFO: train_f_mae: 0.040851
train_f_rmse: 0.060565
2025-02-04 14:54:37.117 INFO: train_f_rmse: 0.060565
val_e/atom_mae: 0.001236
2025-02-04 14:54:37.123 INFO: val_e/atom_mae: 0.001236
val_e/atom_rmse: 0.001569
2025-02-04 14:54:37.123 INFO: val_e/atom_rmse: 0.001569
val_f_mae: 0.031882
2025-02-04 14:54:37.123 INFO: val_f_mae: 0.031882
val_f_rmse: 0.050846
2025-02-04 14:54:37.124 INFO: val_f_rmse: 0.050846
##### Step: 23 Learning rate: 0.005 #####
2025-02-04 14:56:28.633 INFO: ##### Step: 23 Learning rate: 0.005 #####
Epoch 24, Train Loss: 3.3532, Val Loss: 2.5840
2025-02-04 14:56:28.633 INFO: Epoch 24, Train Loss: 3.3532, Val Loss: 2.5840
train_e/atom_mae: 0.004825
2025-02-04 14:56:28.634 INFO: train_e/atom_mae: 0.004825
train_e/atom_rmse: 0.006444
2025-02-04 14:56:28.635 INFO: train_e/atom_rmse: 0.006444
train_f_mae: 0.037410
2025-02-04 14:56:28.637 INFO: train_f_mae: 0.037410
train_f_rmse: 0.056570
2025-02-04 14:56:28.638 INFO: train_f_rmse: 0.056570
val_e/atom_mae: 0.001216
2025-02-04 14:56:28.640 INFO: val_e/atom_mae: 0.001216
val_e/atom_rmse: 0.001482
2025-02-04 14:56:28.640 INFO: val_e/atom_rmse: 0.001482
val_f_mae: 0.031553
2025-02-04 14:56:28.641 INFO: val_f_mae: 0.031553
val_f_rmse: 0.050751
2025-02-04 14:56:28.641 INFO: val_f_rmse: 0.050751
##### Step: 24 Learning rate: 0.005 #####
2025-02-04 14:58:20.427 INFO: ##### Step: 24 Learning rate: 0.005 #####
Epoch 25, Train Loss: 4.3295, Val Loss: 2.6195
2025-02-04 14:58:20.427 INFO: Epoch 25, Train Loss: 4.3295, Val Loss: 2.6195
train_e/atom_mae: 0.008475
2025-02-04 14:58:20.428 INFO: train_e/atom_mae: 0.008475
train_e/atom_rmse: 0.010730
2025-02-04 14:58:20.428 INFO: train_e/atom_rmse: 0.010730
train_f_mae: 0.041312
2025-02-04 14:58:20.431 INFO: train_f_mae: 0.041312
train_f_rmse: 0.062491
2025-02-04 14:58:20.431 INFO: train_f_rmse: 0.062491
val_e/atom_mae: 0.001401
2025-02-04 14:58:20.433 INFO: val_e/atom_mae: 0.001401
val_e/atom_rmse: 0.001696
2025-02-04 14:58:20.434 INFO: val_e/atom_rmse: 0.001696
val_f_mae: 0.031796
2025-02-04 14:58:20.434 INFO: val_f_mae: 0.031796
val_f_rmse: 0.051080
2025-02-04 14:58:20.434 INFO: val_f_rmse: 0.051080
##### Step: 25 Learning rate: 0.005 #####
2025-02-04 15:00:20.780 INFO: ##### Step: 25 Learning rate: 0.005 #####
Epoch 26, Train Loss: 3.9687, Val Loss: 2.5695
2025-02-04 15:00:20.780 INFO: Epoch 26, Train Loss: 3.9687, Val Loss: 2.5695
train_e/atom_mae: 0.003815
2025-02-04 15:00:20.781 INFO: train_e/atom_mae: 0.003815
train_e/atom_rmse: 0.004981
2025-02-04 15:00:20.942 INFO: train_e/atom_rmse: 0.004981
train_f_mae: 0.041186
2025-02-04 15:00:20.945 INFO: train_f_mae: 0.041186
train_f_rmse: 0.062267
2025-02-04 15:00:20.945 INFO: train_f_rmse: 0.062267
val_e/atom_mae: 0.001967
2025-02-04 15:00:20.948 INFO: val_e/atom_mae: 0.001967
val_e/atom_rmse: 0.002220
2025-02-04 15:00:20.948 INFO: val_e/atom_rmse: 0.002220
val_f_mae: 0.031801
2025-02-04 15:00:20.948 INFO: val_f_mae: 0.031801
val_f_rmse: 0.050525
2025-02-04 15:00:20.949 INFO: val_f_rmse: 0.050525
##### Step: 26 Learning rate: 0.005 #####
2025-02-04 15:02:18.675 INFO: ##### Step: 26 Learning rate: 0.005 #####
Epoch 27, Train Loss: 4.0756, Val Loss: 2.5956
2025-02-04 15:02:18.676 INFO: Epoch 27, Train Loss: 4.0756, Val Loss: 2.5956
train_e/atom_mae: 0.004682
2025-02-04 15:02:18.683 INFO: train_e/atom_mae: 0.004682
train_e/atom_rmse: 0.005803
2025-02-04 15:02:18.705 INFO: train_e/atom_rmse: 0.005803
train_f_mae: 0.041001
2025-02-04 15:02:18.708 INFO: train_f_mae: 0.041001
train_f_rmse: 0.062861
2025-02-04 15:02:18.708 INFO: train_f_rmse: 0.062861
val_e/atom_mae: 0.003194
2025-02-04 15:02:18.710 INFO: val_e/atom_mae: 0.003194
val_e/atom_rmse: 0.003452
2025-02-04 15:02:18.710 INFO: val_e/atom_rmse: 0.003452
val_f_mae: 0.031860
2025-02-04 15:02:18.711 INFO: val_f_mae: 0.031860
val_f_rmse: 0.050526
2025-02-04 15:02:18.711 INFO: val_f_rmse: 0.050526
##### Step: 27 Learning rate: 0.005 #####
2025-02-04 15:04:16.509 INFO: ##### Step: 27 Learning rate: 0.005 #####
Epoch 28, Train Loss: 3.7289, Val Loss: 2.5894
2025-02-04 15:04:16.509 INFO: Epoch 28, Train Loss: 3.7289, Val Loss: 2.5894
train_e/atom_mae: 0.005105
2025-02-04 15:04:16.599 INFO: train_e/atom_mae: 0.005105
train_e/atom_rmse: 0.006260
2025-02-04 15:04:16.616 INFO: train_e/atom_rmse: 0.006260
train_f_mae: 0.040420
2025-02-04 15:04:16.619 INFO: train_f_mae: 0.040420
train_f_rmse: 0.059870
2025-02-04 15:04:16.619 INFO: train_f_rmse: 0.059870
val_e/atom_mae: 0.003494
2025-02-04 15:04:16.621 INFO: val_e/atom_mae: 0.003494
val_e/atom_rmse: 0.003640
2025-02-04 15:04:16.621 INFO: val_e/atom_rmse: 0.003640
val_f_mae: 0.031423
2025-02-04 15:04:16.622 INFO: val_f_mae: 0.031423
val_f_rmse: 0.050410
2025-02-04 15:04:16.622 INFO: val_f_rmse: 0.050410
##### Step: 28 Learning rate: 0.005 #####
2025-02-04 15:06:08.421 INFO: ##### Step: 28 Learning rate: 0.005 #####
Epoch 29, Train Loss: 4.6043, Val Loss: 2.7083
2025-02-04 15:06:08.421 INFO: Epoch 29, Train Loss: 4.6043, Val Loss: 2.7083
train_e/atom_mae: 0.006656
2025-02-04 15:06:08.422 INFO: train_e/atom_mae: 0.006656
train_e/atom_rmse: 0.008942
2025-02-04 15:06:08.422 INFO: train_e/atom_rmse: 0.008942
train_f_mae: 0.043841
2025-02-04 15:06:08.425 INFO: train_f_mae: 0.043841
train_f_rmse: 0.065647
2025-02-04 15:06:08.425 INFO: train_f_rmse: 0.065647
val_e/atom_mae: 0.006619
2025-02-04 15:06:08.427 INFO: val_e/atom_mae: 0.006619
val_e/atom_rmse: 0.006722
2025-02-04 15:06:08.427 INFO: val_e/atom_rmse: 0.006722
val_f_mae: 0.031787
2025-02-04 15:06:08.428 INFO: val_f_mae: 0.031787
val_f_rmse: 0.050430
2025-02-04 15:06:08.428 INFO: val_f_rmse: 0.050430
##### Step: 29 Learning rate: 0.005 #####
2025-02-04 15:08:00.246 INFO: ##### Step: 29 Learning rate: 0.005 #####
Epoch 30, Train Loss: 3.2959, Val Loss: 2.5417
2025-02-04 15:08:00.247 INFO: Epoch 30, Train Loss: 3.2959, Val Loss: 2.5417
train_e/atom_mae: 0.004729
2025-02-04 15:08:00.247 INFO: train_e/atom_mae: 0.004729
train_e/atom_rmse: 0.005981
2025-02-04 15:08:00.248 INFO: train_e/atom_rmse: 0.005981
train_f_mae: 0.037218
2025-02-04 15:08:00.250 INFO: train_f_mae: 0.037218
train_f_rmse: 0.056250
2025-02-04 15:08:00.250 INFO: train_f_rmse: 0.056250
val_e/atom_mae: 0.002560
2025-02-04 15:08:00.253 INFO: val_e/atom_mae: 0.002560
val_e/atom_rmse: 0.002796
2025-02-04 15:08:00.253 INFO: val_e/atom_rmse: 0.002796
val_f_mae: 0.031474
2025-02-04 15:08:00.253 INFO: val_f_mae: 0.031474
val_f_rmse: 0.050132
2025-02-04 15:08:00.254 INFO: val_f_rmse: 0.050132
##### Step: 30 Learning rate: 0.005 #####
2025-02-04 15:10:13.531 INFO: ##### Step: 30 Learning rate: 0.005 #####
Epoch 31, Train Loss: 4.0437, Val Loss: 2.5362
2025-02-04 15:10:13.532 INFO: Epoch 31, Train Loss: 4.0437, Val Loss: 2.5362
train_e/atom_mae: 0.005763
2025-02-04 15:10:13.700 INFO: train_e/atom_mae: 0.005763
train_e/atom_rmse: 0.007680
2025-02-04 15:10:13.844 INFO: train_e/atom_rmse: 0.007680
train_f_mae: 0.041058
2025-02-04 15:10:13.847 INFO: train_f_mae: 0.041058
train_f_rmse: 0.061857
2025-02-04 15:10:13.847 INFO: train_f_rmse: 0.061857
val_e/atom_mae: 0.003567
2025-02-04 15:10:13.849 INFO: val_e/atom_mae: 0.003567
val_e/atom_rmse: 0.003740
2025-02-04 15:10:13.850 INFO: val_e/atom_rmse: 0.003740
val_f_mae: 0.031179
2025-02-04 15:10:13.850 INFO: val_f_mae: 0.031179
val_f_rmse: 0.049864
2025-02-04 15:10:13.851 INFO: val_f_rmse: 0.049864
##### Step: 31 Learning rate: 0.005 #####
2025-02-04 15:12:05.560 INFO: ##### Step: 31 Learning rate: 0.005 #####
Epoch 32, Train Loss: 3.8326, Val Loss: 2.7038
2025-02-04 15:12:05.560 INFO: Epoch 32, Train Loss: 3.8326, Val Loss: 2.7038
train_e/atom_mae: 0.004004
2025-02-04 15:12:05.561 INFO: train_e/atom_mae: 0.004004
train_e/atom_rmse: 0.005378
2025-02-04 15:12:05.561 INFO: train_e/atom_rmse: 0.005378
train_f_mae: 0.040069
2025-02-04 15:12:05.564 INFO: train_f_mae: 0.040069
train_f_rmse: 0.061041
2025-02-04 15:12:05.564 INFO: train_f_rmse: 0.061041
val_e/atom_mae: 0.002767
2025-02-04 15:12:05.566 INFO: val_e/atom_mae: 0.002767
val_e/atom_rmse: 0.003039
2025-02-04 15:12:05.566 INFO: val_e/atom_rmse: 0.003039
val_f_mae: 0.032498
2025-02-04 15:12:05.567 INFO: val_f_mae: 0.032498
val_f_rmse: 0.051680
2025-02-04 15:12:05.567 INFO: val_f_rmse: 0.051680
##### Step: 32 Learning rate: 0.005 #####
2025-02-04 15:13:57.079 INFO: ##### Step: 32 Learning rate: 0.005 #####
Epoch 33, Train Loss: 3.9556, Val Loss: 2.5777
2025-02-04 15:13:57.080 INFO: Epoch 33, Train Loss: 3.9556, Val Loss: 2.5777
train_e/atom_mae: 0.005706
2025-02-04 15:13:57.081 INFO: train_e/atom_mae: 0.005706
train_e/atom_rmse: 0.007536
2025-02-04 15:13:57.081 INFO: train_e/atom_rmse: 0.007536
train_f_mae: 0.040526
2025-02-04 15:13:57.084 INFO: train_f_mae: 0.040526
train_f_rmse: 0.061207
2025-02-04 15:13:57.084 INFO: train_f_rmse: 0.061207
val_e/atom_mae: 0.003107
2025-02-04 15:13:57.086 INFO: val_e/atom_mae: 0.003107
val_e/atom_rmse: 0.003335
2025-02-04 15:13:57.086 INFO: val_e/atom_rmse: 0.003335
val_f_mae: 0.031188
2025-02-04 15:13:57.087 INFO: val_f_mae: 0.031188
val_f_rmse: 0.050377
2025-02-04 15:13:57.087 INFO: val_f_rmse: 0.050377
##### Step: 33 Learning rate: 0.005 #####
2025-02-04 15:15:48.919 INFO: ##### Step: 33 Learning rate: 0.005 #####
Epoch 34, Train Loss: 3.3694, Val Loss: 2.5064
2025-02-04 15:15:48.919 INFO: Epoch 34, Train Loss: 3.3694, Val Loss: 2.5064
train_e/atom_mae: 0.006222
2025-02-04 15:15:48.920 INFO: train_e/atom_mae: 0.006222
train_e/atom_rmse: 0.007627
2025-02-04 15:15:48.920 INFO: train_e/atom_rmse: 0.007627
train_f_mae: 0.036590
2025-02-04 15:15:48.923 INFO: train_f_mae: 0.036590
train_f_rmse: 0.056170
2025-02-04 15:15:48.923 INFO: train_f_rmse: 0.056170
val_e/atom_mae: 0.002473
2025-02-04 15:15:48.925 INFO: val_e/atom_mae: 0.002473
val_e/atom_rmse: 0.002735
2025-02-04 15:15:48.925 INFO: val_e/atom_rmse: 0.002735
val_f_mae: 0.031077
2025-02-04 15:15:48.926 INFO: val_f_mae: 0.031077
val_f_rmse: 0.049788
2025-02-04 15:15:48.926 INFO: val_f_rmse: 0.049788
##### Step: 34 Learning rate: 0.005 #####
2025-02-04 15:17:40.695 INFO: ##### Step: 34 Learning rate: 0.005 #####
Epoch 35, Train Loss: 3.1873, Val Loss: 2.4933
2025-02-04 15:17:40.695 INFO: Epoch 35, Train Loss: 3.1873, Val Loss: 2.4933
train_e/atom_mae: 0.004395
2025-02-04 15:17:40.696 INFO: train_e/atom_mae: 0.004395
train_e/atom_rmse: 0.005737
2025-02-04 15:17:40.697 INFO: train_e/atom_rmse: 0.005737
train_f_mae: 0.036643
2025-02-04 15:17:40.699 INFO: train_f_mae: 0.036643
train_f_rmse: 0.055371
2025-02-04 15:17:40.699 INFO: train_f_rmse: 0.055371
val_e/atom_mae: 0.001761
2025-02-04 15:17:40.702 INFO: val_e/atom_mae: 0.001761
val_e/atom_rmse: 0.002070
2025-02-04 15:17:40.702 INFO: val_e/atom_rmse: 0.002070
val_f_mae: 0.030716
2025-02-04 15:17:40.702 INFO: val_f_mae: 0.030716
val_f_rmse: 0.049781
2025-02-04 15:17:40.703 INFO: val_f_rmse: 0.049781
##### Step: 35 Learning rate: 0.005 #####
2025-02-04 15:19:32.584 INFO: ##### Step: 35 Learning rate: 0.005 #####
Epoch 36, Train Loss: 2.9691, Val Loss: 2.4557
2025-02-04 15:19:32.584 INFO: Epoch 36, Train Loss: 2.9691, Val Loss: 2.4557
train_e/atom_mae: 0.003567
2025-02-04 15:19:32.585 INFO: train_e/atom_mae: 0.003567
train_e/atom_rmse: 0.004587
2025-02-04 15:19:32.585 INFO: train_e/atom_rmse: 0.004587
train_f_mae: 0.035270
2025-02-04 15:19:32.588 INFO: train_f_mae: 0.035270
train_f_rmse: 0.053773
2025-02-04 15:19:32.588 INFO: train_f_rmse: 0.053773
val_e/atom_mae: 0.001793
2025-02-04 15:19:32.590 INFO: val_e/atom_mae: 0.001793
val_e/atom_rmse: 0.002064
2025-02-04 15:19:32.591 INFO: val_e/atom_rmse: 0.002064
val_f_mae: 0.030541
2025-02-04 15:19:32.591 INFO: val_f_mae: 0.030541
val_f_rmse: 0.049401
2025-02-04 15:19:32.592 INFO: val_f_rmse: 0.049401
##### Step: 36 Learning rate: 0.005 #####
2025-02-04 15:21:24.179 INFO: ##### Step: 36 Learning rate: 0.005 #####
Epoch 37, Train Loss: 3.3749, Val Loss: 2.4107
2025-02-04 15:21:24.179 INFO: Epoch 37, Train Loss: 3.3749, Val Loss: 2.4107
train_e/atom_mae: 0.003519
2025-02-04 15:21:24.180 INFO: train_e/atom_mae: 0.003519
train_e/atom_rmse: 0.004823
2025-02-04 15:21:24.180 INFO: train_e/atom_rmse: 0.004823
train_f_mae: 0.037994
2025-02-04 15:21:24.183 INFO: train_f_mae: 0.037994
train_f_rmse: 0.057351
2025-02-04 15:21:24.183 INFO: train_f_rmse: 0.057351
val_e/atom_mae: 0.000736
2025-02-04 15:21:24.185 INFO: val_e/atom_mae: 0.000736
val_e/atom_rmse: 0.001048
2025-02-04 15:21:24.185 INFO: val_e/atom_rmse: 0.001048
val_f_mae: 0.030402
2025-02-04 15:21:24.186 INFO: val_f_mae: 0.030402
val_f_rmse: 0.049069
2025-02-04 15:21:24.186 INFO: val_f_rmse: 0.049069
##### Step: 37 Learning rate: 0.005 #####
2025-02-04 15:23:15.653 INFO: ##### Step: 37 Learning rate: 0.005 #####
Epoch 38, Train Loss: 3.7621, Val Loss: 2.4782
2025-02-04 15:23:15.653 INFO: Epoch 38, Train Loss: 3.7621, Val Loss: 2.4782
train_e/atom_mae: 0.005356
2025-02-04 15:23:15.654 INFO: train_e/atom_mae: 0.005356
train_e/atom_rmse: 0.007112
2025-02-04 15:23:15.654 INFO: train_e/atom_rmse: 0.007112
train_f_mae: 0.039887
2025-02-04 15:23:15.657 INFO: train_f_mae: 0.039887
train_f_rmse: 0.059796
2025-02-04 15:23:15.657 INFO: train_f_rmse: 0.059796
val_e/atom_mae: 0.001977
2025-02-04 15:23:15.659 INFO: val_e/atom_mae: 0.001977
val_e/atom_rmse: 0.002303
2025-02-04 15:23:15.659 INFO: val_e/atom_rmse: 0.002303
val_f_mae: 0.030842
2025-02-04 15:23:15.660 INFO: val_f_mae: 0.030842
val_f_rmse: 0.049598
2025-02-04 15:23:15.660 INFO: val_f_rmse: 0.049598
##### Step: 38 Learning rate: 0.005 #####
2025-02-04 15:25:07.111 INFO: ##### Step: 38 Learning rate: 0.005 #####
Epoch 39, Train Loss: 3.8815, Val Loss: 2.5774
2025-02-04 15:25:07.112 INFO: Epoch 39, Train Loss: 3.8815, Val Loss: 2.5774
train_e/atom_mae: 0.006267
2025-02-04 15:25:07.113 INFO: train_e/atom_mae: 0.006267
train_e/atom_rmse: 0.007847
2025-02-04 15:25:07.113 INFO: train_e/atom_rmse: 0.007847
train_f_mae: 0.040443
2025-02-04 15:25:07.115 INFO: train_f_mae: 0.040443
train_f_rmse: 0.060452
2025-02-04 15:25:07.116 INFO: train_f_rmse: 0.060452
val_e/atom_mae: 0.003653
2025-02-04 15:25:07.118 INFO: val_e/atom_mae: 0.003653
val_e/atom_rmse: 0.003822
2025-02-04 15:25:07.118 INFO: val_e/atom_rmse: 0.003822
val_f_mae: 0.030910
2025-02-04 15:25:07.119 INFO: val_f_mae: 0.030910
val_f_rmse: 0.050239
2025-02-04 15:25:07.119 INFO: val_f_rmse: 0.050239
##### Step: 39 Learning rate: 0.005 #####
2025-02-04 15:26:58.903 INFO: ##### Step: 39 Learning rate: 0.005 #####
Epoch 40, Train Loss: 3.8039, Val Loss: 2.6581
2025-02-04 15:26:58.904 INFO: Epoch 40, Train Loss: 3.8039, Val Loss: 2.6581
train_e/atom_mae: 0.007055
2025-02-04 15:26:58.905 INFO: train_e/atom_mae: 0.007055
train_e/atom_rmse: 0.009202
2025-02-04 15:26:58.905 INFO: train_e/atom_rmse: 0.009202
train_f_mae: 0.038945
2025-02-04 15:26:58.908 INFO: train_f_mae: 0.038945
train_f_rmse: 0.059091
2025-02-04 15:26:58.908 INFO: train_f_rmse: 0.059091
val_e/atom_mae: 0.004693
2025-02-04 15:26:58.910 INFO: val_e/atom_mae: 0.004693
val_e/atom_rmse: 0.004916
2025-02-04 15:26:58.910 INFO: val_e/atom_rmse: 0.004916
val_f_mae: 0.030971
2025-02-04 15:26:58.911 INFO: val_f_mae: 0.030971
val_f_rmse: 0.050699
2025-02-04 15:26:58.911 INFO: val_f_rmse: 0.050699
2025-02-04 15:26:59.132 INFO: training
##### Step: 0 Learning rate: 0.002 #####
2025-02-04 15:28:50.291 INFO: ##### Step: 0 Learning rate: 0.002 #####
Epoch 1, Train Loss: 2.7286, Val Loss: 2.7680
2025-02-04 15:28:50.292 INFO: Epoch 1, Train Loss: 2.7286, Val Loss: 2.7680
train_e/atom_mae: 0.003708
2025-02-04 15:28:50.293 INFO: train_e/atom_mae: 0.003708
train_e/atom_rmse: 0.004957
2025-02-04 15:28:50.293 INFO: train_e/atom_rmse: 0.004957
train_f_mae: 0.033643
2025-02-04 15:28:50.296 INFO: train_f_mae: 0.033643
train_f_rmse: 0.051361
2025-02-04 15:28:50.296 INFO: train_f_rmse: 0.051361
val_e/atom_mae: 0.001723
2025-02-04 15:28:50.298 INFO: val_e/atom_mae: 0.001723
val_e/atom_rmse: 0.001952
2025-02-04 15:28:50.299 INFO: val_e/atom_rmse: 0.001952
val_f_mae: 0.033856
2025-02-04 15:28:50.299 INFO: val_f_mae: 0.033856
val_f_rmse: 0.052473
2025-02-04 15:28:50.300 INFO: val_f_rmse: 0.052473
##### Step: 1 Learning rate: 0.004 #####
2025-02-04 15:30:41.606 INFO: ##### Step: 1 Learning rate: 0.004 #####
Epoch 2, Train Loss: 2.8196, Val Loss: 3.9614
2025-02-04 15:30:41.607 INFO: Epoch 2, Train Loss: 2.8196, Val Loss: 3.9614
train_e/atom_mae: 0.003722
2025-02-04 15:30:41.608 INFO: train_e/atom_mae: 0.003722
train_e/atom_rmse: 0.004872
2025-02-04 15:30:41.608 INFO: train_e/atom_rmse: 0.004872
train_f_mae: 0.034835
2025-02-04 15:30:41.611 INFO: train_f_mae: 0.034835
train_f_rmse: 0.052270
2025-02-04 15:30:41.611 INFO: train_f_rmse: 0.052270
val_e/atom_mae: 0.009571
2025-02-04 15:30:41.613 INFO: val_e/atom_mae: 0.009571
val_e/atom_rmse: 0.009733
2025-02-04 15:30:41.614 INFO: val_e/atom_rmse: 0.009733
val_f_mae: 0.040843
2025-02-04 15:30:41.614 INFO: val_f_mae: 0.040843
val_f_rmse: 0.060106
2025-02-04 15:30:41.614 INFO: val_f_rmse: 0.060106
##### Step: 2 Learning rate: 0.006 #####
2025-02-04 15:32:32.855 INFO: ##### Step: 2 Learning rate: 0.006 #####
Epoch 3, Train Loss: 4.2400, Val Loss: 4.1005
2025-02-04 15:32:32.856 INFO: Epoch 3, Train Loss: 4.2400, Val Loss: 4.1005
train_e/atom_mae: 0.005457
2025-02-04 15:32:32.857 INFO: train_e/atom_mae: 0.005457
train_e/atom_rmse: 0.006797
2025-02-04 15:32:32.857 INFO: train_e/atom_rmse: 0.006797
train_f_mae: 0.042532
2025-02-04 15:32:32.860 INFO: train_f_mae: 0.042532
train_f_rmse: 0.063794
2025-02-04 15:32:32.860 INFO: train_f_rmse: 0.063794
val_e/atom_mae: 0.011037
2025-02-04 15:32:32.862 INFO: val_e/atom_mae: 0.011037
val_e/atom_rmse: 0.011180
2025-02-04 15:32:32.862 INFO: val_e/atom_rmse: 0.011180
val_f_mae: 0.041064
2025-02-04 15:32:32.863 INFO: val_f_mae: 0.041064
val_f_rmse: 0.060304
2025-02-04 15:32:32.863 INFO: val_f_rmse: 0.060304
##### Step: 3 Learning rate: 0.008 #####
2025-02-04 15:34:24.049 INFO: ##### Step: 3 Learning rate: 0.008 #####
Epoch 4, Train Loss: 4.9346, Val Loss: 4.2566
2025-02-04 15:34:24.050 INFO: Epoch 4, Train Loss: 4.9346, Val Loss: 4.2566
train_e/atom_mae: 0.007125
2025-02-04 15:34:24.051 INFO: train_e/atom_mae: 0.007125
train_e/atom_rmse: 0.009168
2025-02-04 15:34:24.051 INFO: train_e/atom_rmse: 0.009168
train_f_mae: 0.044692
2025-02-04 15:34:24.054 INFO: train_f_mae: 0.044692
train_f_rmse: 0.068006
2025-02-04 15:34:24.054 INFO: train_f_rmse: 0.068006
val_e/atom_mae: 0.010511
2025-02-04 15:34:24.056 INFO: val_e/atom_mae: 0.010511
val_e/atom_rmse: 0.010619
2025-02-04 15:34:24.056 INFO: val_e/atom_rmse: 0.010619
val_f_mae: 0.039667
2025-02-04 15:34:24.057 INFO: val_f_mae: 0.039667
val_f_rmse: 0.061999
2025-02-04 15:34:24.057 INFO: val_f_rmse: 0.061999
##### Step: 4 Learning rate: 0.01 #####
2025-02-04 15:36:15.318 INFO: ##### Step: 4 Learning rate: 0.01 #####
Epoch 5, Train Loss: 5.7365, Val Loss: 6.0261
2025-02-04 15:36:15.319 INFO: Epoch 5, Train Loss: 5.7365, Val Loss: 6.0261
train_e/atom_mae: 0.008394
2025-02-04 15:36:15.320 INFO: train_e/atom_mae: 0.008394
train_e/atom_rmse: 0.010921
2025-02-04 15:36:15.320 INFO: train_e/atom_rmse: 0.010921
train_f_mae: 0.047572
2025-02-04 15:36:15.323 INFO: train_f_mae: 0.047572
train_f_rmse: 0.072779
2025-02-04 15:36:15.323 INFO: train_f_rmse: 0.072779
val_e/atom_mae: 0.012802
2025-02-04 15:36:15.325 INFO: val_e/atom_mae: 0.012802
val_e/atom_rmse: 0.012951
2025-02-04 15:36:15.325 INFO: val_e/atom_rmse: 0.012951
val_f_mae: 0.046180
2025-02-04 15:36:15.326 INFO: val_f_mae: 0.046180
val_f_rmse: 0.073565
2025-02-04 15:36:15.326 INFO: val_f_rmse: 0.073565
##### Step: 5 Learning rate: 0.01 #####
2025-02-04 15:38:06.473 INFO: ##### Step: 5 Learning rate: 0.01 #####
Epoch 6, Train Loss: 6.2926, Val Loss: 5.9321
2025-02-04 15:38:06.474 INFO: Epoch 6, Train Loss: 6.2926, Val Loss: 5.9321
train_e/atom_mae: 0.007462
2025-02-04 15:38:06.475 INFO: train_e/atom_mae: 0.007462
train_e/atom_rmse: 0.009808
2025-02-04 15:38:06.475 INFO: train_e/atom_rmse: 0.009808
train_f_mae: 0.050857
2025-02-04 15:38:06.478 INFO: train_f_mae: 0.050857
train_f_rmse: 0.077058
2025-02-04 15:38:06.478 INFO: train_f_rmse: 0.077058
val_e/atom_mae: 0.002847
2025-02-04 15:38:06.480 INFO: val_e/atom_mae: 0.002847
val_e/atom_rmse: 0.003546
2025-02-04 15:38:06.480 INFO: val_e/atom_rmse: 0.003546
val_f_mae: 0.056014
2025-02-04 15:38:06.481 INFO: val_f_mae: 0.056014
val_f_rmse: 0.076743
2025-02-04 15:38:06.481 INFO: val_f_rmse: 0.076743
##### Step: 6 Learning rate: 0.01 #####
2025-02-04 15:39:57.543 INFO: ##### Step: 6 Learning rate: 0.01 #####
Epoch 7, Train Loss: 6.5971, Val Loss: 5.4470
2025-02-04 15:39:57.543 INFO: Epoch 7, Train Loss: 6.5971, Val Loss: 5.4470
train_e/atom_mae: 0.006243
2025-02-04 15:39:57.544 INFO: train_e/atom_mae: 0.006243
train_e/atom_rmse: 0.008211
2025-02-04 15:39:57.544 INFO: train_e/atom_rmse: 0.008211
train_f_mae: 0.053078
2025-02-04 15:39:57.547 INFO: train_f_mae: 0.053078
train_f_rmse: 0.079678
2025-02-04 15:39:57.547 INFO: train_f_rmse: 0.079678
val_e/atom_mae: 0.001748
2025-02-04 15:39:57.549 INFO: val_e/atom_mae: 0.001748
val_e/atom_rmse: 0.002130
2025-02-04 15:39:57.550 INFO: val_e/atom_rmse: 0.002130
val_f_mae: 0.049829
2025-02-04 15:39:57.550 INFO: val_f_mae: 0.049829
val_f_rmse: 0.073692
2025-02-04 15:39:57.550 INFO: val_f_rmse: 0.073692
##### Step: 7 Learning rate: 0.01 #####
2025-02-04 15:41:48.653 INFO: ##### Step: 7 Learning rate: 0.01 #####
Epoch 8, Train Loss: 6.0595, Val Loss: 9.9791
2025-02-04 15:41:48.654 INFO: Epoch 8, Train Loss: 6.0595, Val Loss: 9.9791
train_e/atom_mae: 0.007018
2025-02-04 15:41:48.655 INFO: train_e/atom_mae: 0.007018
train_e/atom_rmse: 0.009432
2025-02-04 15:41:48.655 INFO: train_e/atom_rmse: 0.009432
train_f_mae: 0.050029
2025-02-04 15:41:48.658 INFO: train_f_mae: 0.050029
train_f_rmse: 0.075707
2025-02-04 15:41:48.658 INFO: train_f_rmse: 0.075707
val_e/atom_mae: 0.003044
2025-02-04 15:41:48.660 INFO: val_e/atom_mae: 0.003044
val_e/atom_rmse: 0.003918
2025-02-04 15:41:48.661 INFO: val_e/atom_rmse: 0.003918
val_f_mae: 0.067106
2025-02-04 15:41:48.661 INFO: val_f_mae: 0.067106
val_f_rmse: 0.099581
2025-02-04 15:41:48.661 INFO: val_f_rmse: 0.099581
##### Step: 8 Learning rate: 0.01 #####
2025-02-04 15:44:11.264 INFO: ##### Step: 8 Learning rate: 0.01 #####
Epoch 9, Train Loss: 6.2163, Val Loss: 6.3450
2025-02-04 15:44:11.286 INFO: Epoch 9, Train Loss: 6.2163, Val Loss: 6.3450
train_e/atom_mae: 0.008462
2025-02-04 15:44:11.536 INFO: train_e/atom_mae: 0.008462
train_e/atom_rmse: 0.010677
2025-02-04 15:44:11.747 INFO: train_e/atom_rmse: 0.010677
train_f_mae: 0.050467
2025-02-04 15:44:11.753 INFO: train_f_mae: 0.050467
train_f_rmse: 0.076132
2025-02-04 15:44:11.754 INFO: train_f_rmse: 0.076132
val_e/atom_mae: 0.009773
2025-02-04 15:44:11.756 INFO: val_e/atom_mae: 0.009773
val_e/atom_rmse: 0.009870
2025-02-04 15:44:11.756 INFO: val_e/atom_rmse: 0.009870
val_f_mae: 0.052652
2025-02-04 15:44:11.757 INFO: val_f_mae: 0.052652
val_f_rmse: 0.077365
2025-02-04 15:44:11.757 INFO: val_f_rmse: 0.077365
##### Step: 9 Learning rate: 0.01 #####
2025-02-04 15:46:03.047 INFO: ##### Step: 9 Learning rate: 0.01 #####
Epoch 10, Train Loss: 6.0370, Val Loss: 5.6490
2025-02-04 15:46:03.047 INFO: Epoch 10, Train Loss: 6.0370, Val Loss: 5.6490
train_e/atom_mae: 0.007932
2025-02-04 15:46:03.048 INFO: train_e/atom_mae: 0.007932
train_e/atom_rmse: 0.010293
2025-02-04 15:46:03.048 INFO: train_e/atom_rmse: 0.010293
train_f_mae: 0.049672
2025-02-04 15:46:03.051 INFO: train_f_mae: 0.049672
train_f_rmse: 0.075142
2025-02-04 15:46:03.051 INFO: train_f_rmse: 0.075142
val_e/atom_mae: 0.002957
2025-02-04 15:46:03.053 INFO: val_e/atom_mae: 0.002957
val_e/atom_rmse: 0.003673
2025-02-04 15:46:03.054 INFO: val_e/atom_rmse: 0.003673
val_f_mae: 0.048587
2025-02-04 15:46:03.054 INFO: val_f_mae: 0.048587
val_f_rmse: 0.074807
2025-02-04 15:46:03.054 INFO: val_f_rmse: 0.074807
##### Step: 10 Learning rate: 0.01 #####
2025-02-04 15:47:56.073 INFO: ##### Step: 10 Learning rate: 0.01 #####
Epoch 11, Train Loss: 5.3079, Val Loss: 3.0765
2025-02-04 15:47:56.074 INFO: Epoch 11, Train Loss: 5.3079, Val Loss: 3.0765
train_e/atom_mae: 0.008874
2025-02-04 15:47:56.075 INFO: train_e/atom_mae: 0.008874
train_e/atom_rmse: 0.011222
2025-02-04 15:47:56.075 INFO: train_e/atom_rmse: 0.011222
train_f_mae: 0.044846
2025-02-04 15:47:56.078 INFO: train_f_mae: 0.044846
train_f_rmse: 0.069596
2025-02-04 15:47:56.078 INFO: train_f_rmse: 0.069596
val_e/atom_mae: 0.006856
2025-02-04 15:47:56.080 INFO: val_e/atom_mae: 0.006856
val_e/atom_rmse: 0.007004
2025-02-04 15:47:56.080 INFO: val_e/atom_rmse: 0.007004
val_f_mae: 0.033209
2025-02-04 15:47:56.081 INFO: val_f_mae: 0.033209
val_f_rmse: 0.053821
2025-02-04 15:47:56.081 INFO: val_f_rmse: 0.053821
##### Step: 11 Learning rate: 0.01 #####
2025-02-04 15:49:47.809 INFO: ##### Step: 11 Learning rate: 0.01 #####
Epoch 12, Train Loss: 5.6046, Val Loss: 2.8707
2025-02-04 15:49:47.809 INFO: Epoch 12, Train Loss: 5.6046, Val Loss: 2.8707
train_e/atom_mae: 0.008272
2025-02-04 15:49:47.810 INFO: train_e/atom_mae: 0.008272
train_e/atom_rmse: 0.010346
2025-02-04 15:49:47.811 INFO: train_e/atom_rmse: 0.010346
train_f_mae: 0.047142
2025-02-04 15:49:47.813 INFO: train_f_mae: 0.047142
train_f_rmse: 0.072181
2025-02-04 15:49:47.813 INFO: train_f_rmse: 0.072181
val_e/atom_mae: 0.003551
2025-02-04 15:49:47.815 INFO: val_e/atom_mae: 0.003551
val_e/atom_rmse: 0.003686
2025-02-04 15:49:47.816 INFO: val_e/atom_rmse: 0.003686
val_f_mae: 0.032253
2025-02-04 15:49:47.816 INFO: val_f_mae: 0.032253
val_f_rmse: 0.053110
2025-02-04 15:49:47.817 INFO: val_f_rmse: 0.053110
##### Step: 12 Learning rate: 0.01 #####
2025-02-04 15:51:39.557 INFO: ##### Step: 12 Learning rate: 0.01 #####
Epoch 13, Train Loss: 5.5168, Val Loss: 2.8083
2025-02-04 15:51:39.557 INFO: Epoch 13, Train Loss: 5.5168, Val Loss: 2.8083
train_e/atom_mae: 0.008159
2025-02-04 15:51:39.558 INFO: train_e/atom_mae: 0.008159
train_e/atom_rmse: 0.010009
2025-02-04 15:51:39.558 INFO: train_e/atom_rmse: 0.010009
train_f_mae: 0.046943
2025-02-04 15:51:39.561 INFO: train_f_mae: 0.046943
train_f_rmse: 0.071746
2025-02-04 15:51:39.561 INFO: train_f_rmse: 0.071746
val_e/atom_mae: 0.004799
2025-02-04 15:51:39.563 INFO: val_e/atom_mae: 0.004799
val_e/atom_rmse: 0.004962
2025-02-04 15:51:39.564 INFO: val_e/atom_rmse: 0.004962
val_f_mae: 0.032963
2025-02-04 15:51:39.564 INFO: val_f_mae: 0.032963
val_f_rmse: 0.052122
2025-02-04 15:51:39.564 INFO: val_f_rmse: 0.052122
##### Step: 13 Learning rate: 0.01 #####
2025-02-04 15:53:31.465 INFO: ##### Step: 13 Learning rate: 0.01 #####
Epoch 14, Train Loss: 6.2998, Val Loss: 3.1972
2025-02-04 15:53:31.465 INFO: Epoch 14, Train Loss: 6.2998, Val Loss: 3.1972
train_e/atom_mae: 0.008893
2025-02-04 15:53:31.466 INFO: train_e/atom_mae: 0.008893
train_e/atom_rmse: 0.011600
2025-02-04 15:53:31.466 INFO: train_e/atom_rmse: 0.011600
train_f_mae: 0.050281
2025-02-04 15:53:31.469 INFO: train_f_mae: 0.050281
train_f_rmse: 0.076183
2025-02-04 15:53:31.469 INFO: train_f_rmse: 0.076183
val_e/atom_mae: 0.009024
2025-02-04 15:53:31.471 INFO: val_e/atom_mae: 0.009024
val_e/atom_rmse: 0.009105
2025-02-04 15:53:31.471 INFO: val_e/atom_rmse: 0.009105
val_f_mae: 0.034379
2025-02-04 15:53:31.472 INFO: val_f_mae: 0.034379
val_f_rmse: 0.053789
2025-02-04 15:53:31.472 INFO: val_f_rmse: 0.053789
##### Step: 14 Learning rate: 0.01 #####
2025-02-04 15:55:23.395 INFO: ##### Step: 14 Learning rate: 0.01 #####
Epoch 15, Train Loss: 7.5067, Val Loss: 3.2472
2025-02-04 15:55:23.395 INFO: Epoch 15, Train Loss: 7.5067, Val Loss: 3.2472
train_e/atom_mae: 0.011800
2025-02-04 15:55:23.396 INFO: train_e/atom_mae: 0.011800
train_e/atom_rmse: 0.015348
2025-02-04 15:55:23.397 INFO: train_e/atom_rmse: 0.015348
train_f_mae: 0.054645
2025-02-04 15:55:23.399 INFO: train_f_mae: 0.054645
train_f_rmse: 0.081476
2025-02-04 15:55:23.399 INFO: train_f_rmse: 0.081476
val_e/atom_mae: 0.008642
2025-02-04 15:55:23.401 INFO: val_e/atom_mae: 0.008642
val_e/atom_rmse: 0.008854
2025-02-04 15:55:23.402 INFO: val_e/atom_rmse: 0.008854
val_f_mae: 0.034433
2025-02-04 15:55:23.402 INFO: val_f_mae: 0.034433
val_f_rmse: 0.054415
2025-02-04 15:55:23.403 INFO: val_f_rmse: 0.054415
##### Step: 15 Learning rate: 0.01 #####
2025-02-04 15:57:15.088 INFO: ##### Step: 15 Learning rate: 0.01 #####
Epoch 16, Train Loss: 6.9509, Val Loss: 3.1053
2025-02-04 15:57:15.089 INFO: Epoch 16, Train Loss: 6.9509, Val Loss: 3.1053
train_e/atom_mae: 0.008813
2025-02-04 15:57:15.090 INFO: train_e/atom_mae: 0.008813
train_e/atom_rmse: 0.011342
2025-02-04 15:57:15.090 INFO: train_e/atom_rmse: 0.011342
train_f_mae: 0.053715
2025-02-04 15:57:15.093 INFO: train_f_mae: 0.053715
train_f_rmse: 0.080478
2025-02-04 15:57:15.093 INFO: train_f_rmse: 0.080478
val_e/atom_mae: 0.004807
2025-02-04 15:57:15.095 INFO: val_e/atom_mae: 0.004807
val_e/atom_rmse: 0.004967
2025-02-04 15:57:15.095 INFO: val_e/atom_rmse: 0.004967
val_f_mae: 0.034592
2025-02-04 15:57:15.096 INFO: val_f_mae: 0.034592
val_f_rmse: 0.054918
2025-02-04 15:57:15.096 INFO: val_f_rmse: 0.054918
##### Step: 16 Learning rate: 0.01 #####
2025-02-04 15:59:06.923 INFO: ##### Step: 16 Learning rate: 0.01 #####
Epoch 17, Train Loss: 5.6062, Val Loss: 2.9649
2025-02-04 15:59:06.924 INFO: Epoch 17, Train Loss: 5.6062, Val Loss: 2.9649
train_e/atom_mae: 0.006345
2025-02-04 15:59:06.925 INFO: train_e/atom_mae: 0.006345
train_e/atom_rmse: 0.008138
2025-02-04 15:59:06.925 INFO: train_e/atom_rmse: 0.008138
train_f_mae: 0.048277
2025-02-04 15:59:06.928 INFO: train_f_mae: 0.048277
train_f_rmse: 0.073226
2025-02-04 15:59:06.928 INFO: train_f_rmse: 0.073226
val_e/atom_mae: 0.006649
2025-02-04 15:59:06.930 INFO: val_e/atom_mae: 0.006649
val_e/atom_rmse: 0.006750
2025-02-04 15:59:06.931 INFO: val_e/atom_rmse: 0.006750
val_f_mae: 0.033571
2025-02-04 15:59:06.931 INFO: val_f_mae: 0.033571
val_f_rmse: 0.052894
2025-02-04 15:59:06.931 INFO: val_f_rmse: 0.052894
##### Step: 17 Learning rate: 0.01 #####
2025-02-04 16:00:58.709 INFO: ##### Step: 17 Learning rate: 0.01 #####
Epoch 18, Train Loss: 4.8815, Val Loss: 2.7433
2025-02-04 16:00:58.710 INFO: Epoch 18, Train Loss: 4.8815, Val Loss: 2.7433
train_e/atom_mae: 0.006891
2025-02-04 16:00:58.711 INFO: train_e/atom_mae: 0.006891
train_e/atom_rmse: 0.008705
2025-02-04 16:00:58.711 INFO: train_e/atom_rmse: 0.008705
train_f_mae: 0.044213
2025-02-04 16:00:58.714 INFO: train_f_mae: 0.044213
train_f_rmse: 0.067839
2025-02-04 16:00:58.714 INFO: train_f_rmse: 0.067839
val_e/atom_mae: 0.004257
2025-02-04 16:00:58.716 INFO: val_e/atom_mae: 0.004257
val_e/atom_rmse: 0.004439
2025-02-04 16:00:58.717 INFO: val_e/atom_rmse: 0.004439
val_f_mae: 0.032406
2025-02-04 16:00:58.717 INFO: val_f_mae: 0.032406
val_f_rmse: 0.051686
2025-02-04 16:00:58.717 INFO: val_f_rmse: 0.051686
##### Step: 18 Learning rate: 0.01 #####
2025-02-04 16:02:50.466 INFO: ##### Step: 18 Learning rate: 0.01 #####
Epoch 19, Train Loss: 5.1553, Val Loss: 2.7966
2025-02-04 16:02:50.466 INFO: Epoch 19, Train Loss: 5.1553, Val Loss: 2.7966
train_e/atom_mae: 0.006193
2025-02-04 16:02:50.467 INFO: train_e/atom_mae: 0.006193
train_e/atom_rmse: 0.007769
2025-02-04 16:02:50.467 INFO: train_e/atom_rmse: 0.007769
train_f_mae: 0.046695
2025-02-04 16:02:50.470 INFO: train_f_mae: 0.046695
train_f_rmse: 0.070234
2025-02-04 16:02:50.470 INFO: train_f_rmse: 0.070234
val_e/atom_mae: 0.006162
2025-02-04 16:02:50.473 INFO: val_e/atom_mae: 0.006162
val_e/atom_rmse: 0.006279
2025-02-04 16:02:50.473 INFO: val_e/atom_rmse: 0.006279
val_f_mae: 0.032510
2025-02-04 16:02:50.473 INFO: val_f_mae: 0.032510
val_f_rmse: 0.051489
2025-02-04 16:02:50.474 INFO: val_f_rmse: 0.051489
##### Step: 19 Learning rate: 0.01 #####
2025-02-04 16:04:42.286 INFO: ##### Step: 19 Learning rate: 0.01 #####
Epoch 20, Train Loss: 5.2391, Val Loss: 2.8167
2025-02-04 16:04:42.286 INFO: Epoch 20, Train Loss: 5.2391, Val Loss: 2.8167
train_e/atom_mae: 0.006534
2025-02-04 16:04:42.287 INFO: train_e/atom_mae: 0.006534
train_e/atom_rmse: 0.008482
2025-02-04 16:04:42.288 INFO: train_e/atom_rmse: 0.008482
train_f_mae: 0.047122
2025-02-04 16:04:42.290 INFO: train_f_mae: 0.047122
train_f_rmse: 0.070526
2025-02-04 16:04:42.290 INFO: train_f_rmse: 0.070526
val_e/atom_mae: 0.004267
2025-02-04 16:04:42.293 INFO: val_e/atom_mae: 0.004267
val_e/atom_rmse: 0.004441
2025-02-04 16:04:42.293 INFO: val_e/atom_rmse: 0.004441
val_f_mae: 0.032611
2025-02-04 16:04:42.293 INFO: val_f_mae: 0.032611
val_f_rmse: 0.052401
2025-02-04 16:04:42.294 INFO: val_f_rmse: 0.052401
##### Step: 20 Learning rate: 0.005 #####
2025-02-04 16:06:33.934 INFO: ##### Step: 20 Learning rate: 0.005 #####
Epoch 21, Train Loss: 4.3418, Val Loss: 2.6539
2025-02-04 16:06:33.935 INFO: Epoch 21, Train Loss: 4.3418, Val Loss: 2.6539
train_e/atom_mae: 0.007519
2025-02-04 16:06:33.936 INFO: train_e/atom_mae: 0.007519
train_e/atom_rmse: 0.009544
2025-02-04 16:06:33.936 INFO: train_e/atom_rmse: 0.009544
train_f_mae: 0.041099
2025-02-04 16:06:33.939 INFO: train_f_mae: 0.041099
train_f_rmse: 0.063294
2025-02-04 16:06:33.939 INFO: train_f_rmse: 0.063294
val_e/atom_mae: 0.003937
2025-02-04 16:06:33.941 INFO: val_e/atom_mae: 0.003937
val_e/atom_rmse: 0.004127
2025-02-04 16:06:33.942 INFO: val_e/atom_rmse: 0.004127
val_f_mae: 0.031938
2025-02-04 16:06:33.942 INFO: val_f_mae: 0.031938
val_f_rmse: 0.050910
2025-02-04 16:06:33.942 INFO: val_f_rmse: 0.050910
##### Step: 21 Learning rate: 0.005 #####
2025-02-04 16:08:25.909 INFO: ##### Step: 21 Learning rate: 0.005 #####
Epoch 22, Train Loss: 3.6734, Val Loss: 2.6243
2025-02-04 16:08:25.910 INFO: Epoch 22, Train Loss: 3.6734, Val Loss: 2.6243
train_e/atom_mae: 0.005471
2025-02-04 16:08:25.911 INFO: train_e/atom_mae: 0.005471
train_e/atom_rmse: 0.006888
2025-02-04 16:08:25.911 INFO: train_e/atom_rmse: 0.006888
train_f_mae: 0.038976
2025-02-04 16:08:25.914 INFO: train_f_mae: 0.038976
train_f_rmse: 0.059148
2025-02-04 16:08:25.914 INFO: train_f_rmse: 0.059148
val_e/atom_mae: 0.004238
2025-02-04 16:08:25.916 INFO: val_e/atom_mae: 0.004238
val_e/atom_rmse: 0.004370
2025-02-04 16:08:25.917 INFO: val_e/atom_rmse: 0.004370
val_f_mae: 0.031099
2025-02-04 16:08:25.917 INFO: val_f_mae: 0.031099
val_f_rmse: 0.050549
2025-02-04 16:08:25.917 INFO: val_f_rmse: 0.050549
##### Step: 22 Learning rate: 0.005 #####
2025-02-04 16:10:17.796 INFO: ##### Step: 22 Learning rate: 0.005 #####
Epoch 23, Train Loss: 3.2241, Val Loss: 2.5262
2025-02-04 16:10:17.796 INFO: Epoch 23, Train Loss: 3.2241, Val Loss: 2.5262
train_e/atom_mae: 0.005358
2025-02-04 16:10:17.797 INFO: train_e/atom_mae: 0.005358
train_e/atom_rmse: 0.006865
2025-02-04 16:10:17.797 INFO: train_e/atom_rmse: 0.006865
train_f_mae: 0.036583
2025-02-04 16:10:17.800 INFO: train_f_mae: 0.036583
train_f_rmse: 0.055230
2025-02-04 16:10:17.800 INFO: train_f_rmse: 0.055230
val_e/atom_mae: 0.002128
2025-02-04 16:10:17.806 INFO: val_e/atom_mae: 0.002128
val_e/atom_rmse: 0.002396
2025-02-04 16:10:17.806 INFO: val_e/atom_rmse: 0.002396
val_f_mae: 0.030961
2025-02-04 16:10:17.806 INFO: val_f_mae: 0.030961
val_f_rmse: 0.050059
2025-02-04 16:10:17.807 INFO: val_f_rmse: 0.050059
##### Step: 23 Learning rate: 0.005 #####
2025-02-04 16:12:09.757 INFO: ##### Step: 23 Learning rate: 0.005 #####
Epoch 24, Train Loss: 3.3374, Val Loss: 2.4791
2025-02-04 16:12:09.758 INFO: Epoch 24, Train Loss: 3.3374, Val Loss: 2.4791
train_e/atom_mae: 0.003827
2025-02-04 16:12:09.759 INFO: train_e/atom_mae: 0.003827
train_e/atom_rmse: 0.004982
2025-02-04 16:12:09.759 INFO: train_e/atom_rmse: 0.004982
train_f_mae: 0.037913
2025-02-04 16:12:09.761 INFO: train_f_mae: 0.037913
train_f_rmse: 0.056973
2025-02-04 16:12:09.762 INFO: train_f_rmse: 0.056973
val_e/atom_mae: 0.001713
2025-02-04 16:12:09.764 INFO: val_e/atom_mae: 0.001713
val_e/atom_rmse: 0.001958
2025-02-04 16:12:09.764 INFO: val_e/atom_rmse: 0.001958
val_f_mae: 0.030834
2025-02-04 16:12:09.765 INFO: val_f_mae: 0.030834
val_f_rmse: 0.049653
2025-02-04 16:12:09.765 INFO: val_f_rmse: 0.049653
##### Step: 24 Learning rate: 0.005 #####
2025-02-04 16:14:01.447 INFO: ##### Step: 24 Learning rate: 0.005 #####
Epoch 25, Train Loss: 2.8203, Val Loss: 2.4048
2025-02-04 16:14:01.448 INFO: Epoch 25, Train Loss: 2.8203, Val Loss: 2.4048
train_e/atom_mae: 0.002615
2025-02-04 16:14:01.449 INFO: train_e/atom_mae: 0.002615
train_e/atom_rmse: 0.003545
2025-02-04 16:14:01.449 INFO: train_e/atom_rmse: 0.003545
train_f_mae: 0.034769
2025-02-04 16:14:01.452 INFO: train_f_mae: 0.034769
train_f_rmse: 0.052669
2025-02-04 16:14:01.452 INFO: train_f_rmse: 0.052669
val_e/atom_mae: 0.001920
2025-02-04 16:14:01.454 INFO: val_e/atom_mae: 0.001920
val_e/atom_rmse: 0.002156
2025-02-04 16:14:01.454 INFO: val_e/atom_rmse: 0.002156
val_f_mae: 0.030268
2025-02-04 16:14:01.455 INFO: val_f_mae: 0.030268
val_f_rmse: 0.048864
2025-02-04 16:14:01.455 INFO: val_f_rmse: 0.048864
##### Step: 25 Learning rate: 0.005 #####
2025-02-04 16:15:53.136 INFO: ##### Step: 25 Learning rate: 0.005 #####
Epoch 26, Train Loss: 3.7332, Val Loss: 2.5509
2025-02-04 16:15:53.137 INFO: Epoch 26, Train Loss: 3.7332, Val Loss: 2.5509
train_e/atom_mae: 0.005540
2025-02-04 16:15:53.138 INFO: train_e/atom_mae: 0.005540
train_e/atom_rmse: 0.007062
2025-02-04 16:15:53.138 INFO: train_e/atom_rmse: 0.007062
train_f_mae: 0.039150
2025-02-04 16:15:53.140 INFO: train_f_mae: 0.039150
train_f_rmse: 0.059577
2025-02-04 16:15:53.141 INFO: train_f_rmse: 0.059577
val_e/atom_mae: 0.004473
2025-02-04 16:15:53.143 INFO: val_e/atom_mae: 0.004473
val_e/atom_rmse: 0.004588
2025-02-04 16:15:53.143 INFO: val_e/atom_rmse: 0.004588
val_f_mae: 0.030641
2025-02-04 16:15:53.144 INFO: val_f_mae: 0.030641
val_f_rmse: 0.049735
2025-02-04 16:15:53.144 INFO: val_f_rmse: 0.049735
##### Step: 26 Learning rate: 0.005 #####
2025-02-04 16:17:44.591 INFO: ##### Step: 26 Learning rate: 0.005 #####
Epoch 27, Train Loss: 2.9293, Val Loss: 2.4725
2025-02-04 16:17:44.592 INFO: Epoch 27, Train Loss: 2.9293, Val Loss: 2.4725
train_e/atom_mae: 0.003451
2025-02-04 16:17:44.593 INFO: train_e/atom_mae: 0.003451
train_e/atom_rmse: 0.004297
2025-02-04 16:17:44.593 INFO: train_e/atom_rmse: 0.004297
train_f_mae: 0.035579
2025-02-04 16:17:44.596 INFO: train_f_mae: 0.035579
train_f_rmse: 0.053490
2025-02-04 16:17:44.596 INFO: train_f_rmse: 0.053490
val_e/atom_mae: 0.002090
2025-02-04 16:17:44.598 INFO: val_e/atom_mae: 0.002090
val_e/atom_rmse: 0.002269
2025-02-04 16:17:44.598 INFO: val_e/atom_rmse: 0.002269
val_f_mae: 0.030380
2025-02-04 16:17:44.599 INFO: val_f_mae: 0.030380
val_f_rmse: 0.049544
2025-02-04 16:17:44.599 INFO: val_f_rmse: 0.049544
##### Step: 27 Learning rate: 0.005 #####
2025-02-04 16:19:36.233 INFO: ##### Step: 27 Learning rate: 0.005 #####
Epoch 28, Train Loss: 3.1420, Val Loss: 2.5368
2025-02-04 16:19:36.234 INFO: Epoch 28, Train Loss: 3.1420, Val Loss: 2.5368
train_e/atom_mae: 0.004386
2025-02-04 16:19:36.235 INFO: train_e/atom_mae: 0.004386
train_e/atom_rmse: 0.006016
2025-02-04 16:19:36.235 INFO: train_e/atom_rmse: 0.006016
train_f_mae: 0.036179
2025-02-04 16:19:36.238 INFO: train_f_mae: 0.036179
train_f_rmse: 0.054851
2025-02-04 16:19:36.238 INFO: train_f_rmse: 0.054851
val_e/atom_mae: 0.002636
2025-02-04 16:19:36.240 INFO: val_e/atom_mae: 0.002636
val_e/atom_rmse: 0.002898
2025-02-04 16:19:36.241 INFO: val_e/atom_rmse: 0.002898
val_f_mae: 0.030985
2025-02-04 16:19:36.241 INFO: val_f_mae: 0.030985
val_f_rmse: 0.050057
2025-02-04 16:19:36.241 INFO: val_f_rmse: 0.050057
##### Step: 28 Learning rate: 0.005 #####
2025-02-04 16:21:27.927 INFO: ##### Step: 28 Learning rate: 0.005 #####
Epoch 29, Train Loss: 3.2289, Val Loss: 2.3516
2025-02-04 16:21:27.928 INFO: Epoch 29, Train Loss: 3.2289, Val Loss: 2.3516
train_e/atom_mae: 0.005027
2025-02-04 16:21:27.928 INFO: train_e/atom_mae: 0.005027
train_e/atom_rmse: 0.006533
2025-02-04 16:21:27.929 INFO: train_e/atom_rmse: 0.006533
train_f_mae: 0.036949
2025-02-04 16:21:27.931 INFO: train_f_mae: 0.036949
train_f_rmse: 0.055421
2025-02-04 16:21:27.932 INFO: train_f_rmse: 0.055421
val_e/atom_mae: 0.000937
2025-02-04 16:21:27.934 INFO: val_e/atom_mae: 0.000937
val_e/atom_rmse: 0.001227
2025-02-04 16:21:27.934 INFO: val_e/atom_rmse: 0.001227
val_f_mae: 0.029994
2025-02-04 16:21:27.935 INFO: val_f_mae: 0.029994
val_f_rmse: 0.048442
2025-02-04 16:21:27.935 INFO: val_f_rmse: 0.048442
##### Step: 29 Learning rate: 0.005 #####
2025-02-04 16:23:19.792 INFO: ##### Step: 29 Learning rate: 0.005 #####
Epoch 30, Train Loss: 3.0968, Val Loss: 2.4071
2025-02-04 16:23:19.793 INFO: Epoch 30, Train Loss: 3.0968, Val Loss: 2.4071
train_e/atom_mae: 0.005234
2025-02-04 16:23:19.794 INFO: train_e/atom_mae: 0.005234
train_e/atom_rmse: 0.006956
2025-02-04 16:23:19.794 INFO: train_e/atom_rmse: 0.006956
train_f_mae: 0.035892
2025-02-04 16:23:19.797 INFO: train_f_mae: 0.035892
train_f_rmse: 0.054023
2025-02-04 16:23:19.797 INFO: train_f_rmse: 0.054023
val_e/atom_mae: 0.001274
2025-02-04 16:23:19.799 INFO: val_e/atom_mae: 0.001274
val_e/atom_rmse: 0.001554
2025-02-04 16:23:19.799 INFO: val_e/atom_rmse: 0.001554
val_f_mae: 0.030004
2025-02-04 16:23:19.800 INFO: val_f_mae: 0.030004
val_f_rmse: 0.048984
2025-02-04 16:23:19.800 INFO: val_f_rmse: 0.048984
##### Step: 30 Learning rate: 0.005 #####
2025-02-04 16:25:11.511 INFO: ##### Step: 30 Learning rate: 0.005 #####
Epoch 31, Train Loss: 3.1195, Val Loss: 2.5236
2025-02-04 16:25:11.512 INFO: Epoch 31, Train Loss: 3.1195, Val Loss: 2.5236
train_e/atom_mae: 0.004537
2025-02-04 16:25:11.513 INFO: train_e/atom_mae: 0.004537
train_e/atom_rmse: 0.005963
2025-02-04 16:25:11.513 INFO: train_e/atom_rmse: 0.005963
train_f_mae: 0.035973
2025-02-04 16:25:11.519 INFO: train_f_mae: 0.035973
train_f_rmse: 0.054667
2025-02-04 16:25:11.520 INFO: train_f_rmse: 0.054667
val_e/atom_mae: 0.001593
2025-02-04 16:25:11.522 INFO: val_e/atom_mae: 0.001593
val_e/atom_rmse: 0.001880
2025-02-04 16:25:11.522 INFO: val_e/atom_rmse: 0.001880
val_f_mae: 0.030081
2025-02-04 16:25:11.523 INFO: val_f_mae: 0.030081
val_f_rmse: 0.050123
2025-02-04 16:25:11.523 INFO: val_f_rmse: 0.050123
##### Step: 31 Learning rate: 0.005 #####
2025-02-04 16:27:02.755 INFO: ##### Step: 31 Learning rate: 0.005 #####
Epoch 32, Train Loss: 3.1851, Val Loss: 2.5514
2025-02-04 16:27:02.756 INFO: Epoch 32, Train Loss: 3.1851, Val Loss: 2.5514
train_e/atom_mae: 0.004271
2025-02-04 16:27:02.757 INFO: train_e/atom_mae: 0.004271
train_e/atom_rmse: 0.006121
2025-02-04 16:27:02.757 INFO: train_e/atom_rmse: 0.006121
train_f_mae: 0.037153
2025-02-04 16:27:02.760 INFO: train_f_mae: 0.037153
train_f_rmse: 0.055200
2025-02-04 16:27:02.760 INFO: train_f_rmse: 0.055200
val_e/atom_mae: 0.005466
2025-02-04 16:27:02.762 INFO: val_e/atom_mae: 0.005466
val_e/atom_rmse: 0.005565
2025-02-04 16:27:02.762 INFO: val_e/atom_rmse: 0.005565
val_f_mae: 0.030443
2025-02-04 16:27:02.763 INFO: val_f_mae: 0.030443
val_f_rmse: 0.049360
2025-02-04 16:27:02.763 INFO: val_f_rmse: 0.049360
##### Step: 32 Learning rate: 0.005 #####
2025-02-04 16:28:54.647 INFO: ##### Step: 32 Learning rate: 0.005 #####
Epoch 33, Train Loss: 2.9165, Val Loss: 2.4299
2025-02-04 16:28:54.648 INFO: Epoch 33, Train Loss: 2.9165, Val Loss: 2.4299
train_e/atom_mae: 0.004058
2025-02-04 16:28:54.648 INFO: train_e/atom_mae: 0.004058
train_e/atom_rmse: 0.005724
2025-02-04 16:28:54.649 INFO: train_e/atom_rmse: 0.005724
train_f_mae: 0.034888
2025-02-04 16:28:54.651 INFO: train_f_mae: 0.034888
train_f_rmse: 0.052874
2025-02-04 16:28:54.652 INFO: train_f_rmse: 0.052874
val_e/atom_mae: 0.001103
2025-02-04 16:28:54.654 INFO: val_e/atom_mae: 0.001103
val_e/atom_rmse: 0.001395
2025-02-04 16:28:54.654 INFO: val_e/atom_rmse: 0.001395
val_f_mae: 0.029894
2025-02-04 16:28:54.654 INFO: val_f_mae: 0.029894
val_f_rmse: 0.049235
2025-02-04 16:28:54.655 INFO: val_f_rmse: 0.049235
##### Step: 33 Learning rate: 0.005 #####
2025-02-04 16:30:46.252 INFO: ##### Step: 33 Learning rate: 0.005 #####
Epoch 34, Train Loss: 2.9621, Val Loss: 2.4567
2025-02-04 16:30:46.253 INFO: Epoch 34, Train Loss: 2.9621, Val Loss: 2.4567
train_e/atom_mae: 0.005287
2025-02-04 16:30:46.254 INFO: train_e/atom_mae: 0.005287
train_e/atom_rmse: 0.006580
2025-02-04 16:30:46.254 INFO: train_e/atom_rmse: 0.006580
train_f_mae: 0.035439
2025-02-04 16:30:46.257 INFO: train_f_mae: 0.035439
train_f_rmse: 0.052938
2025-02-04 16:30:46.257 INFO: train_f_rmse: 0.052938
val_e/atom_mae: 0.002795
2025-02-04 16:30:46.259 INFO: val_e/atom_mae: 0.002795
val_e/atom_rmse: 0.003070
2025-02-04 16:30:46.260 INFO: val_e/atom_rmse: 0.003070
val_f_mae: 0.030283
2025-02-04 16:30:46.260 INFO: val_f_mae: 0.030283
val_f_rmse: 0.049218
2025-02-04 16:30:46.260 INFO: val_f_rmse: 0.049218
##### Step: 34 Learning rate: 0.005 #####
2025-02-04 16:32:37.757 INFO: ##### Step: 34 Learning rate: 0.005 #####
Epoch 35, Train Loss: 3.2871, Val Loss: 2.4438
2025-02-04 16:32:37.758 INFO: Epoch 35, Train Loss: 3.2871, Val Loss: 2.4438
train_e/atom_mae: 0.006126
2025-02-04 16:32:37.759 INFO: train_e/atom_mae: 0.006126
train_e/atom_rmse: 0.007730
2025-02-04 16:32:37.759 INFO: train_e/atom_rmse: 0.007730
train_f_mae: 0.036837
2025-02-04 16:32:37.762 INFO: train_f_mae: 0.036837
train_f_rmse: 0.055379
2025-02-04 16:32:37.762 INFO: train_f_rmse: 0.055379
val_e/atom_mae: 0.002130
2025-02-04 16:32:37.764 INFO: val_e/atom_mae: 0.002130
val_e/atom_rmse: 0.002420
2025-02-04 16:32:37.764 INFO: val_e/atom_rmse: 0.002420
val_f_mae: 0.029799
2025-02-04 16:32:37.765 INFO: val_f_mae: 0.029799
val_f_rmse: 0.049230
2025-02-04 16:32:37.765 INFO: val_f_rmse: 0.049230
##### Step: 35 Learning rate: 0.005 #####
2025-02-04 16:34:29.458 INFO: ##### Step: 35 Learning rate: 0.005 #####
Epoch 36, Train Loss: 2.7972, Val Loss: 2.3423
2025-02-04 16:34:29.459 INFO: Epoch 36, Train Loss: 2.7972, Val Loss: 2.3423
train_e/atom_mae: 0.004326
2025-02-04 16:34:29.460 INFO: train_e/atom_mae: 0.004326
train_e/atom_rmse: 0.005347
2025-02-04 16:34:29.460 INFO: train_e/atom_rmse: 0.005347
train_f_mae: 0.034213
2025-02-04 16:34:29.462 INFO: train_f_mae: 0.034213
train_f_rmse: 0.051883
2025-02-04 16:34:29.463 INFO: train_f_rmse: 0.051883
val_e/atom_mae: 0.001929
2025-02-04 16:34:29.465 INFO: val_e/atom_mae: 0.001929
val_e/atom_rmse: 0.002151
2025-02-04 16:34:29.465 INFO: val_e/atom_rmse: 0.002151
val_f_mae: 0.029633
2025-02-04 16:34:29.466 INFO: val_f_mae: 0.029633
val_f_rmse: 0.048227
2025-02-04 16:34:29.466 INFO: val_f_rmse: 0.048227
##### Step: 36 Learning rate: 0.005 #####
2025-02-04 16:36:21.052 INFO: ##### Step: 36 Learning rate: 0.005 #####
Epoch 37, Train Loss: 3.0839, Val Loss: 2.4466
2025-02-04 16:36:21.052 INFO: Epoch 37, Train Loss: 3.0839, Val Loss: 2.4466
train_e/atom_mae: 0.005560
2025-02-04 16:36:21.053 INFO: train_e/atom_mae: 0.005560
train_e/atom_rmse: 0.006836
2025-02-04 16:36:21.053 INFO: train_e/atom_rmse: 0.006836
train_f_mae: 0.035732
2025-02-04 16:36:21.056 INFO: train_f_mae: 0.035732
train_f_rmse: 0.053960
2025-02-04 16:36:21.056 INFO: train_f_rmse: 0.053960
val_e/atom_mae: 0.002354
2025-02-04 16:36:21.058 INFO: val_e/atom_mae: 0.002354
val_e/atom_rmse: 0.002587
2025-02-04 16:36:21.059 INFO: val_e/atom_rmse: 0.002587
val_f_mae: 0.029982
2025-02-04 16:36:21.059 INFO: val_f_mae: 0.029982
val_f_rmse: 0.049227
2025-02-04 16:36:21.059 INFO: val_f_rmse: 0.049227
##### Step: 37 Learning rate: 0.005 #####
2025-02-04 16:38:12.829 INFO: ##### Step: 37 Learning rate: 0.005 #####
Epoch 38, Train Loss: 2.8933, Val Loss: 2.3742
2025-02-04 16:38:12.830 INFO: Epoch 38, Train Loss: 2.8933, Val Loss: 2.3742
train_e/atom_mae: 0.004795
2025-02-04 16:38:12.831 INFO: train_e/atom_mae: 0.004795
train_e/atom_rmse: 0.006260
2025-02-04 16:38:12.831 INFO: train_e/atom_rmse: 0.006260
train_f_mae: 0.034799
2025-02-04 16:38:12.833 INFO: train_f_mae: 0.034799
train_f_rmse: 0.052429
2025-02-04 16:38:12.834 INFO: train_f_rmse: 0.052429
val_e/atom_mae: 0.001226
2025-02-04 16:38:12.836 INFO: val_e/atom_mae: 0.001226
val_e/atom_rmse: 0.001472
2025-02-04 16:38:12.836 INFO: val_e/atom_rmse: 0.001472
val_f_mae: 0.029530
2025-02-04 16:38:12.837 INFO: val_f_mae: 0.029530
val_f_rmse: 0.048656
2025-02-04 16:38:12.837 INFO: val_f_rmse: 0.048656
##### Step: 38 Learning rate: 0.005 #####
2025-02-04 16:40:04.686 INFO: ##### Step: 38 Learning rate: 0.005 #####
Epoch 39, Train Loss: 4.0695, Val Loss: 2.8332
2025-02-04 16:40:04.687 INFO: Epoch 39, Train Loss: 4.0695, Val Loss: 2.8332
train_e/atom_mae: 0.006947
2025-02-04 16:40:04.688 INFO: train_e/atom_mae: 0.006947
train_e/atom_rmse: 0.009611
2025-02-04 16:40:04.688 INFO: train_e/atom_rmse: 0.009611
train_f_mae: 0.040992
2025-02-04 16:40:04.691 INFO: train_f_mae: 0.040992
train_f_rmse: 0.061066
2025-02-04 16:40:04.691 INFO: train_f_rmse: 0.061066
val_e/atom_mae: 0.007700
2025-02-04 16:40:04.693 INFO: val_e/atom_mae: 0.007700
val_e/atom_rmse: 0.007777
2025-02-04 16:40:04.693 INFO: val_e/atom_rmse: 0.007777
val_f_mae: 0.031465
2025-02-04 16:40:04.694 INFO: val_f_mae: 0.031465
val_f_rmse: 0.051091
2025-02-04 16:40:04.694 INFO: val_f_rmse: 0.051091
##### Step: 39 Learning rate: 0.005 #####
2025-02-04 16:41:56.405 INFO: ##### Step: 39 Learning rate: 0.005 #####
Epoch 40, Train Loss: 3.6376, Val Loss: 2.4730
2025-02-04 16:41:56.405 INFO: Epoch 40, Train Loss: 3.6376, Val Loss: 2.4730
train_e/atom_mae: 0.005665
2025-02-04 16:41:56.406 INFO: train_e/atom_mae: 0.005665
train_e/atom_rmse: 0.007279
2025-02-04 16:41:56.406 INFO: train_e/atom_rmse: 0.007279
train_f_mae: 0.039483
2025-02-04 16:41:56.409 INFO: train_f_mae: 0.039483
train_f_rmse: 0.058671
2025-02-04 16:41:56.409 INFO: train_f_rmse: 0.058671
val_e/atom_mae: 0.001123
2025-02-04 16:41:56.411 INFO: val_e/atom_mae: 0.001123
val_e/atom_rmse: 0.001383
2025-02-04 16:41:56.412 INFO: val_e/atom_rmse: 0.001383
val_f_mae: 0.029866
2025-02-04 16:41:56.412 INFO: val_f_mae: 0.029866
val_f_rmse: 0.049671
2025-02-04 16:41:56.412 INFO: val_f_rmse: 0.049671
2025-02-04 16:41:57.958 INFO: Second train loop:
2025-02-04 16:41:57.958 INFO: training
##### Step: 40 Learning rate: 0.0025 #####
2025-02-04 16:43:55.208 INFO: ##### Step: 40 Learning rate: 0.0025 #####
Epoch 1, Train Loss: 2.8089, Val Loss: 2.5543
2025-02-04 16:43:55.209 INFO: Epoch 1, Train Loss: 2.8089, Val Loss: 2.5543
train_e/atom_mae: 0.002755
2025-02-04 16:43:55.229 INFO: train_e/atom_mae: 0.002755
train_e/atom_rmse: 0.003562
2025-02-04 16:43:55.302 INFO: train_e/atom_rmse: 0.003562
train_f_mae: 0.031522
2025-02-04 16:43:55.305 INFO: train_f_mae: 0.031522
train_f_rmse: 0.048385
2025-02-04 16:43:55.305 INFO: train_f_rmse: 0.048385
val_e/atom_mae: 0.001564
2025-02-04 16:43:55.307 INFO: val_e/atom_mae: 0.001564
val_e/atom_rmse: 0.001773
2025-02-04 16:43:55.308 INFO: val_e/atom_rmse: 0.001773
val_f_mae: 0.029524
2025-02-04 16:43:55.308 INFO: val_f_mae: 0.029524
val_f_rmse: 0.049397
2025-02-04 16:43:55.308 INFO: val_f_rmse: 0.049397
##### Step: 41 Learning rate: 0.0025 #####
2025-02-04 16:45:47.013 INFO: ##### Step: 41 Learning rate: 0.0025 #####
Epoch 2, Train Loss: 2.3175, Val Loss: 2.3463
2025-02-04 16:45:47.014 INFO: Epoch 2, Train Loss: 2.3175, Val Loss: 2.3463
train_e/atom_mae: 0.001479
2025-02-04 16:45:47.015 INFO: train_e/atom_mae: 0.001479
train_e/atom_rmse: 0.002111
2025-02-04 16:45:47.015 INFO: train_e/atom_rmse: 0.002111
train_f_mae: 0.030779
2025-02-04 16:45:47.017 INFO: train_f_mae: 0.030779
train_f_rmse: 0.046404
2025-02-04 16:45:47.018 INFO: train_f_rmse: 0.046404
val_e/atom_mae: 0.001062
2025-02-04 16:45:47.020 INFO: val_e/atom_mae: 0.001062
val_e/atom_rmse: 0.001296
2025-02-04 16:45:47.020 INFO: val_e/atom_rmse: 0.001296
val_f_mae: 0.029264
2025-02-04 16:45:47.021 INFO: val_f_mae: 0.029264
val_f_rmse: 0.047801
2025-02-04 16:45:47.021 INFO: val_f_rmse: 0.047801
##### Step: 42 Learning rate: 0.0025 #####
2025-02-04 16:47:38.645 INFO: ##### Step: 42 Learning rate: 0.0025 #####
Epoch 3, Train Loss: 2.9221, Val Loss: 2.3904
2025-02-04 16:47:38.646 INFO: Epoch 3, Train Loss: 2.9221, Val Loss: 2.3904
train_e/atom_mae: 0.003175
2025-02-04 16:47:38.647 INFO: train_e/atom_mae: 0.003175
train_e/atom_rmse: 0.003986
2025-02-04 16:47:38.647 INFO: train_e/atom_rmse: 0.003986
train_f_mae: 0.032562
2025-02-04 16:47:38.650 INFO: train_f_mae: 0.032562
train_f_rmse: 0.048335
2025-02-04 16:47:38.650 INFO: train_f_rmse: 0.048335
val_e/atom_mae: 0.001099
2025-02-04 16:47:38.652 INFO: val_e/atom_mae: 0.001099
val_e/atom_rmse: 0.001314
2025-02-04 16:47:38.653 INFO: val_e/atom_rmse: 0.001314
val_f_mae: 0.029414
2025-02-04 16:47:38.653 INFO: val_f_mae: 0.029414
val_f_rmse: 0.048244
2025-02-04 16:47:38.653 INFO: val_f_rmse: 0.048244
##### Step: 43 Learning rate: 0.0025 #####
2025-02-04 16:49:30.330 INFO: ##### Step: 43 Learning rate: 0.0025 #####
Epoch 4, Train Loss: 2.5811, Val Loss: 2.3675
2025-02-04 16:49:30.330 INFO: Epoch 4, Train Loss: 2.5811, Val Loss: 2.3675
train_e/atom_mae: 0.002366
2025-02-04 16:49:30.331 INFO: train_e/atom_mae: 0.002366
train_e/atom_rmse: 0.003030
2025-02-04 16:49:30.331 INFO: train_e/atom_rmse: 0.003030
train_f_mae: 0.032040
2025-02-04 16:49:30.334 INFO: train_f_mae: 0.032040
train_f_rmse: 0.047356
2025-02-04 16:49:30.334 INFO: train_f_rmse: 0.047356
val_e/atom_mae: 0.000777
2025-02-04 16:49:30.336 INFO: val_e/atom_mae: 0.000777
val_e/atom_rmse: 0.001017
2025-02-04 16:49:30.337 INFO: val_e/atom_rmse: 0.001017
val_f_mae: 0.029313
2025-02-04 16:49:30.337 INFO: val_f_mae: 0.029313
val_f_rmse: 0.048267
2025-02-04 16:49:30.337 INFO: val_f_rmse: 0.048267
##### Step: 44 Learning rate: 0.0025 #####
2025-02-04 16:51:22.109 INFO: ##### Step: 44 Learning rate: 0.0025 #####
Epoch 5, Train Loss: 2.5896, Val Loss: 2.4003
2025-02-04 16:51:22.110 INFO: Epoch 5, Train Loss: 2.5896, Val Loss: 2.4003
train_e/atom_mae: 0.002201
2025-02-04 16:51:22.111 INFO: train_e/atom_mae: 0.002201
train_e/atom_rmse: 0.003120
2025-02-04 16:51:22.111 INFO: train_e/atom_rmse: 0.003120
train_f_mae: 0.031682
2025-02-04 16:51:22.114 INFO: train_f_mae: 0.031682
train_f_rmse: 0.047231
2025-02-04 16:51:22.114 INFO: train_f_rmse: 0.047231
val_e/atom_mae: 0.000946
2025-02-04 16:51:22.116 INFO: val_e/atom_mae: 0.000946
val_e/atom_rmse: 0.001155
2025-02-04 16:51:22.116 INFO: val_e/atom_rmse: 0.001155
val_f_mae: 0.029363
2025-02-04 16:51:22.117 INFO: val_f_mae: 0.029363
val_f_rmse: 0.048496
2025-02-04 16:51:22.117 INFO: val_f_rmse: 0.048496
##### Step: 45 Learning rate: 0.0025 #####
2025-02-04 16:53:13.790 INFO: ##### Step: 45 Learning rate: 0.0025 #####
Epoch 6, Train Loss: 2.9698, Val Loss: 2.4927
2025-02-04 16:53:13.791 INFO: Epoch 6, Train Loss: 2.9698, Val Loss: 2.4927
train_e/atom_mae: 0.003110
2025-02-04 16:53:13.792 INFO: train_e/atom_mae: 0.003110
train_e/atom_rmse: 0.004003
2025-02-04 16:53:13.792 INFO: train_e/atom_rmse: 0.004003
train_f_mae: 0.032299
2025-02-04 16:53:13.795 INFO: train_f_mae: 0.032299
train_f_rmse: 0.048778
2025-02-04 16:53:13.795 INFO: train_f_rmse: 0.048778
val_e/atom_mae: 0.001949
2025-02-04 16:53:13.797 INFO: val_e/atom_mae: 0.001949
val_e/atom_rmse: 0.002121
2025-02-04 16:53:13.798 INFO: val_e/atom_rmse: 0.002121
val_f_mae: 0.029414
2025-02-04 16:53:13.798 INFO: val_f_mae: 0.029414
val_f_rmse: 0.048250
2025-02-04 16:53:13.798 INFO: val_f_rmse: 0.048250
##### Step: 46 Learning rate: 0.0025 #####
2025-02-04 16:55:05.589 INFO: ##### Step: 46 Learning rate: 0.0025 #####
Epoch 7, Train Loss: 2.9342, Val Loss: 2.5594
2025-02-04 16:55:05.590 INFO: Epoch 7, Train Loss: 2.9342, Val Loss: 2.5594
train_e/atom_mae: 0.002871
2025-02-04 16:55:05.591 INFO: train_e/atom_mae: 0.002871
train_e/atom_rmse: 0.004003
2025-02-04 16:55:05.591 INFO: train_e/atom_rmse: 0.004003
train_f_mae: 0.032396
2025-02-04 16:55:05.594 INFO: train_f_mae: 0.032396
train_f_rmse: 0.048411
2025-02-04 16:55:05.594 INFO: train_f_rmse: 0.048411
val_e/atom_mae: 0.001619
2025-02-04 16:55:05.596 INFO: val_e/atom_mae: 0.001619
val_e/atom_rmse: 0.001836
2025-02-04 16:55:05.596 INFO: val_e/atom_rmse: 0.001836
val_f_mae: 0.029817
2025-02-04 16:55:05.597 INFO: val_f_mae: 0.029817
val_f_rmse: 0.049360
2025-02-04 16:55:05.597 INFO: val_f_rmse: 0.049360
##### Step: 47 Learning rate: 0.0025 #####
2025-02-04 16:56:57.257 INFO: ##### Step: 47 Learning rate: 0.0025 #####
Epoch 8, Train Loss: 2.6332, Val Loss: 2.3945
2025-02-04 16:56:57.258 INFO: Epoch 8, Train Loss: 2.6332, Val Loss: 2.3945
train_e/atom_mae: 0.002162
2025-02-04 16:56:57.258 INFO: train_e/atom_mae: 0.002162
train_e/atom_rmse: 0.002946
2025-02-04 16:56:57.259 INFO: train_e/atom_rmse: 0.002946
train_f_mae: 0.032379
2025-02-04 16:56:57.261 INFO: train_f_mae: 0.032379
train_f_rmse: 0.048097
2025-02-04 16:56:57.262 INFO: train_f_rmse: 0.048097
val_e/atom_mae: 0.001044
2025-02-04 16:56:57.264 INFO: val_e/atom_mae: 0.001044
val_e/atom_rmse: 0.001292
2025-02-04 16:56:57.264 INFO: val_e/atom_rmse: 0.001292
val_f_mae: 0.029243
2025-02-04 16:56:57.264 INFO: val_f_mae: 0.029243
val_f_rmse: 0.048307
2025-02-04 16:56:57.265 INFO: val_f_rmse: 0.048307
##### Step: 48 Learning rate: 0.0025 #####
2025-02-04 16:58:48.849 INFO: ##### Step: 48 Learning rate: 0.0025 #####
Epoch 9, Train Loss: 2.3932, Val Loss: 2.3389
2025-02-04 16:58:48.850 INFO: Epoch 9, Train Loss: 2.3932, Val Loss: 2.3389
train_e/atom_mae: 0.002135
2025-02-04 16:58:48.851 INFO: train_e/atom_mae: 0.002135
train_e/atom_rmse: 0.002878
2025-02-04 16:58:48.851 INFO: train_e/atom_rmse: 0.002878
train_f_mae: 0.030517
2025-02-04 16:58:48.853 INFO: train_f_mae: 0.030517
train_f_rmse: 0.045693
2025-02-04 16:58:48.854 INFO: train_f_rmse: 0.045693
val_e/atom_mae: 0.000745
2025-02-04 16:58:48.856 INFO: val_e/atom_mae: 0.000745
val_e/atom_rmse: 0.000969
2025-02-04 16:58:48.856 INFO: val_e/atom_rmse: 0.000969
val_f_mae: 0.029125
2025-02-04 16:58:48.857 INFO: val_f_mae: 0.029125
val_f_rmse: 0.048015
2025-02-04 16:58:48.857 INFO: val_f_rmse: 0.048015
##### Step: 49 Learning rate: 0.0025 #####
2025-02-04 17:00:40.521 INFO: ##### Step: 49 Learning rate: 0.0025 #####
Epoch 10, Train Loss: 2.7600, Val Loss: 2.3823
2025-02-04 17:00:40.522 INFO: Epoch 10, Train Loss: 2.7600, Val Loss: 2.3823
train_e/atom_mae: 0.003103
2025-02-04 17:00:40.523 INFO: train_e/atom_mae: 0.003103
train_e/atom_rmse: 0.003781
2025-02-04 17:00:40.523 INFO: train_e/atom_rmse: 0.003781
train_f_mae: 0.031382
2025-02-04 17:00:40.526 INFO: train_f_mae: 0.031382
train_f_rmse: 0.047256
2025-02-04 17:00:40.526 INFO: train_f_rmse: 0.047256
val_e/atom_mae: 0.000916
2025-02-04 17:00:40.528 INFO: val_e/atom_mae: 0.000916
val_e/atom_rmse: 0.001140
2025-02-04 17:00:40.528 INFO: val_e/atom_rmse: 0.001140
val_f_mae: 0.029228
2025-02-04 17:00:40.529 INFO: val_f_mae: 0.029228
val_f_rmse: 0.048328
2025-02-04 17:00:40.529 INFO: val_f_rmse: 0.048328
##### Step: 50 Learning rate: 0.0025 #####
2025-02-04 17:02:37.639 INFO: ##### Step: 50 Learning rate: 0.0025 #####
Epoch 11, Train Loss: 2.5467, Val Loss: 2.3680
2025-02-04 17:02:37.639 INFO: Epoch 11, Train Loss: 2.5467, Val Loss: 2.3680
train_e/atom_mae: 0.002245
2025-02-04 17:02:37.678 INFO: train_e/atom_mae: 0.002245
train_e/atom_rmse: 0.003142
2025-02-04 17:02:37.714 INFO: train_e/atom_rmse: 0.003142
train_f_mae: 0.031215
2025-02-04 17:02:37.716 INFO: train_f_mae: 0.031215
train_f_rmse: 0.046720
2025-02-04 17:02:37.717 INFO: train_f_rmse: 0.046720
val_e/atom_mae: 0.000649
2025-02-04 17:02:37.719 INFO: val_e/atom_mae: 0.000649
val_e/atom_rmse: 0.000855
2025-02-04 17:02:37.719 INFO: val_e/atom_rmse: 0.000855
val_f_mae: 0.029157
2025-02-04 17:02:37.720 INFO: val_f_mae: 0.029157
val_f_rmse: 0.048391
2025-02-04 17:02:37.720 INFO: val_f_rmse: 0.048391
##### Step: 51 Learning rate: 0.0025 #####
2025-02-04 17:04:29.431 INFO: ##### Step: 51 Learning rate: 0.0025 #####
Epoch 12, Train Loss: 3.0545, Val Loss: 2.6151
2025-02-04 17:04:29.432 INFO: Epoch 12, Train Loss: 3.0545, Val Loss: 2.6151
train_e/atom_mae: 0.002943
2025-02-04 17:04:29.433 INFO: train_e/atom_mae: 0.002943
train_e/atom_rmse: 0.003654
2025-02-04 17:04:29.433 INFO: train_e/atom_rmse: 0.003654
train_f_mae: 0.033806
2025-02-04 17:04:29.436 INFO: train_f_mae: 0.033806
train_f_rmse: 0.050618
2025-02-04 17:04:29.436 INFO: train_f_rmse: 0.050618
val_e/atom_mae: 0.002348
2025-02-04 17:04:29.438 INFO: val_e/atom_mae: 0.002348
val_e/atom_rmse: 0.002478
2025-02-04 17:04:29.438 INFO: val_e/atom_rmse: 0.002478
val_f_mae: 0.029325
2025-02-04 17:04:29.439 INFO: val_f_mae: 0.029325
val_f_rmse: 0.048890
2025-02-04 17:04:29.439 INFO: val_f_rmse: 0.048890
##### Step: 52 Learning rate: 0.0025 #####
2025-02-04 17:06:21.167 INFO: ##### Step: 52 Learning rate: 0.0025 #####
Epoch 13, Train Loss: 2.6898, Val Loss: 2.4737
2025-02-04 17:06:21.167 INFO: Epoch 13, Train Loss: 2.6898, Val Loss: 2.4737
train_e/atom_mae: 0.002620
2025-02-04 17:06:21.168 INFO: train_e/atom_mae: 0.002620
train_e/atom_rmse: 0.003444
2025-02-04 17:06:21.168 INFO: train_e/atom_rmse: 0.003444
train_f_mae: 0.031866
2025-02-04 17:06:21.171 INFO: train_f_mae: 0.031866
train_f_rmse: 0.047462
2025-02-04 17:06:21.171 INFO: train_f_rmse: 0.047462
val_e/atom_mae: 0.001375
2025-02-04 17:06:21.173 INFO: val_e/atom_mae: 0.001375
val_e/atom_rmse: 0.001600
2025-02-04 17:06:21.174 INFO: val_e/atom_rmse: 0.001600
val_f_mae: 0.029184
2025-02-04 17:06:21.174 INFO: val_f_mae: 0.029184
val_f_rmse: 0.048789
2025-02-04 17:06:21.174 INFO: val_f_rmse: 0.048789
##### Step: 53 Learning rate: 0.0025 #####
2025-02-04 17:08:16.691 INFO: ##### Step: 53 Learning rate: 0.0025 #####
Epoch 14, Train Loss: 2.4511, Val Loss: 2.4141
2025-02-04 17:08:16.692 INFO: Epoch 14, Train Loss: 2.4511, Val Loss: 2.4141
train_e/atom_mae: 0.001895
2025-02-04 17:08:16.725 INFO: train_e/atom_mae: 0.001895
train_e/atom_rmse: 0.002509
2025-02-04 17:08:16.728 INFO: train_e/atom_rmse: 0.002509
train_f_mae: 0.031302
2025-02-04 17:08:16.730 INFO: train_f_mae: 0.031302
train_f_rmse: 0.047107
2025-02-04 17:08:16.731 INFO: train_f_rmse: 0.047107
val_e/atom_mae: 0.001270
2025-02-04 17:08:16.733 INFO: val_e/atom_mae: 0.001270
val_e/atom_rmse: 0.001445
2025-02-04 17:08:16.733 INFO: val_e/atom_rmse: 0.001445
val_f_mae: 0.029193
2025-02-04 17:08:16.734 INFO: val_f_mae: 0.029193
val_f_rmse: 0.048354
2025-02-04 17:08:16.734 INFO: val_f_rmse: 0.048354
##### Step: 54 Learning rate: 0.0025 #####
2025-02-04 17:10:13.836 INFO: ##### Step: 54 Learning rate: 0.0025 #####
Epoch 15, Train Loss: 2.3857, Val Loss: 2.4549
2025-02-04 17:10:13.837 INFO: Epoch 15, Train Loss: 2.3857, Val Loss: 2.4549
train_e/atom_mae: 0.002132
2025-02-04 17:10:13.908 INFO: train_e/atom_mae: 0.002132
train_e/atom_rmse: 0.002804
2025-02-04 17:10:13.923 INFO: train_e/atom_rmse: 0.002804
train_f_mae: 0.030390
2025-02-04 17:10:13.926 INFO: train_f_mae: 0.030390
train_f_rmse: 0.045780
2025-02-04 17:10:13.926 INFO: train_f_rmse: 0.045780
val_e/atom_mae: 0.001894
2025-02-04 17:10:13.928 INFO: val_e/atom_mae: 0.001894
val_e/atom_rmse: 0.002070
2025-02-04 17:10:13.929 INFO: val_e/atom_rmse: 0.002070
val_f_mae: 0.029082
2025-02-04 17:10:13.929 INFO: val_f_mae: 0.029082
val_f_rmse: 0.047941
2025-02-04 17:10:13.929 INFO: val_f_rmse: 0.047941
##### Step: 55 Learning rate: 0.0025 #####
2025-02-04 17:12:05.905 INFO: ##### Step: 55 Learning rate: 0.0025 #####
Epoch 16, Train Loss: 2.8333, Val Loss: 2.3745
2025-02-04 17:12:05.905 INFO: Epoch 16, Train Loss: 2.8333, Val Loss: 2.3745
train_e/atom_mae: 0.002290
2025-02-04 17:12:05.906 INFO: train_e/atom_mae: 0.002290
train_e/atom_rmse: 0.002979
2025-02-04 17:12:05.907 INFO: train_e/atom_rmse: 0.002979
train_f_mae: 0.033431
2025-02-04 17:12:05.909 INFO: train_f_mae: 0.033431
train_f_rmse: 0.050062
2025-02-04 17:12:05.909 INFO: train_f_rmse: 0.050062
val_e/atom_mae: 0.000916
2025-02-04 17:12:05.911 INFO: val_e/atom_mae: 0.000916
val_e/atom_rmse: 0.001169
2025-02-04 17:12:05.912 INFO: val_e/atom_rmse: 0.001169
val_f_mae: 0.029229
2025-02-04 17:12:05.912 INFO: val_f_mae: 0.029229
val_f_rmse: 0.048225
2025-02-04 17:12:05.913 INFO: val_f_rmse: 0.048225
##### Step: 56 Learning rate: 0.0025 #####
2025-02-04 17:14:01.282 INFO: ##### Step: 56 Learning rate: 0.0025 #####
Epoch 17, Train Loss: 2.4456, Val Loss: 2.4012
2025-02-04 17:14:01.283 INFO: Epoch 17, Train Loss: 2.4456, Val Loss: 2.4012
train_e/atom_mae: 0.002255
2025-02-04 17:14:01.293 INFO: train_e/atom_mae: 0.002255
train_e/atom_rmse: 0.002883
2025-02-04 17:14:01.388 INFO: train_e/atom_rmse: 0.002883
train_f_mae: 0.030659
2025-02-04 17:14:01.391 INFO: train_f_mae: 0.030659
train_f_rmse: 0.046252
2025-02-04 17:14:01.391 INFO: train_f_rmse: 0.046252
val_e/atom_mae: 0.001519
2025-02-04 17:14:01.393 INFO: val_e/atom_mae: 0.001519
val_e/atom_rmse: 0.001734
2025-02-04 17:14:01.393 INFO: val_e/atom_rmse: 0.001734
val_f_mae: 0.029001
2025-02-04 17:14:01.394 INFO: val_f_mae: 0.029001
val_f_rmse: 0.047870
2025-02-04 17:14:01.394 INFO: val_f_rmse: 0.047870
##### Step: 57 Learning rate: 0.0025 #####
2025-02-04 17:15:53.008 INFO: ##### Step: 57 Learning rate: 0.0025 #####
Epoch 18, Train Loss: 2.2904, Val Loss: 2.3747
2025-02-04 17:15:53.009 INFO: Epoch 18, Train Loss: 2.2904, Val Loss: 2.3747
train_e/atom_mae: 0.001699
2025-02-04 17:15:53.010 INFO: train_e/atom_mae: 0.001699
train_e/atom_rmse: 0.002286
2025-02-04 17:15:53.010 INFO: train_e/atom_rmse: 0.002286
train_f_mae: 0.030390
2025-02-04 17:15:53.013 INFO: train_f_mae: 0.030390
train_f_rmse: 0.045800
2025-02-04 17:15:53.013 INFO: train_f_rmse: 0.045800
val_e/atom_mae: 0.000865
2025-02-04 17:15:53.015 INFO: val_e/atom_mae: 0.000865
val_e/atom_rmse: 0.001099
2025-02-04 17:15:53.016 INFO: val_e/atom_rmse: 0.001099
val_f_mae: 0.028989
2025-02-04 17:15:53.016 INFO: val_f_mae: 0.028989
val_f_rmse: 0.048278
2025-02-04 17:15:53.016 INFO: val_f_rmse: 0.048278
##### Step: 58 Learning rate: 0.0025 #####
2025-02-04 17:17:44.764 INFO: ##### Step: 58 Learning rate: 0.0025 #####
Epoch 19, Train Loss: 2.6146, Val Loss: 2.2893
2025-02-04 17:17:44.765 INFO: Epoch 19, Train Loss: 2.6146, Val Loss: 2.2893
train_e/atom_mae: 0.003103
2025-02-04 17:17:44.765 INFO: train_e/atom_mae: 0.003103
train_e/atom_rmse: 0.003758
2025-02-04 17:17:44.766 INFO: train_e/atom_rmse: 0.003758
train_f_mae: 0.030473
2025-02-04 17:17:44.768 INFO: train_f_mae: 0.030473
train_f_rmse: 0.045761
2025-02-04 17:17:44.769 INFO: train_f_rmse: 0.045761
val_e/atom_mae: 0.000775
2025-02-04 17:17:44.774 INFO: val_e/atom_mae: 0.000775
val_e/atom_rmse: 0.001003
2025-02-04 17:17:44.775 INFO: val_e/atom_rmse: 0.001003
val_f_mae: 0.029035
2025-02-04 17:17:44.775 INFO: val_f_mae: 0.029035
val_f_rmse: 0.047467
2025-02-04 17:17:44.775 INFO: val_f_rmse: 0.047467
##### Step: 59 Learning rate: 0.0025 #####
2025-02-04 17:19:51.177 INFO: ##### Step: 59 Learning rate: 0.0025 #####
Epoch 20, Train Loss: 2.8589, Val Loss: 2.9873
2025-02-04 17:19:51.177 INFO: Epoch 20, Train Loss: 2.8589, Val Loss: 2.9873
train_e/atom_mae: 0.003185
2025-02-04 17:19:51.229 INFO: train_e/atom_mae: 0.003185
train_e/atom_rmse: 0.003894
2025-02-04 17:19:51.259 INFO: train_e/atom_rmse: 0.003894
train_f_mae: 0.031735
2025-02-04 17:19:51.262 INFO: train_f_mae: 0.031735
train_f_rmse: 0.047957
2025-02-04 17:19:51.262 INFO: train_f_rmse: 0.047957
val_e/atom_mae: 0.004049
2025-02-04 17:19:51.264 INFO: val_e/atom_mae: 0.004049
val_e/atom_rmse: 0.004136
2025-02-04 17:19:51.265 INFO: val_e/atom_rmse: 0.004136
val_f_mae: 0.029265
2025-02-04 17:19:51.265 INFO: val_f_mae: 0.029265
val_f_rmse: 0.048566
2025-02-04 17:19:51.265 INFO: val_f_rmse: 0.048566
##### Step: 60 Learning rate: 0.00125 #####
2025-02-04 17:21:44.690 INFO: ##### Step: 60 Learning rate: 0.00125 #####
Epoch 21, Train Loss: 2.1791, Val Loss: 2.3537
2025-02-04 17:21:44.691 INFO: Epoch 21, Train Loss: 2.1791, Val Loss: 2.3537
train_e/atom_mae: 0.001797
2025-02-04 17:21:44.692 INFO: train_e/atom_mae: 0.001797
train_e/atom_rmse: 0.002399
2025-02-04 17:21:44.772 INFO: train_e/atom_rmse: 0.002399
train_f_mae: 0.029471
2025-02-04 17:21:44.775 INFO: train_f_mae: 0.029471
train_f_rmse: 0.044350
2025-02-04 17:21:44.775 INFO: train_f_rmse: 0.044350
val_e/atom_mae: 0.000699
2025-02-04 17:21:44.777 INFO: val_e/atom_mae: 0.000699
val_e/atom_rmse: 0.000897
2025-02-04 17:21:44.778 INFO: val_e/atom_rmse: 0.000897
val_f_mae: 0.028904
2025-02-04 17:21:44.778 INFO: val_f_mae: 0.028904
val_f_rmse: 0.048218
2025-02-04 17:21:44.778 INFO: val_f_rmse: 0.048218
##### Step: 61 Learning rate: 0.00125 #####
2025-02-04 17:23:37.162 INFO: ##### Step: 61 Learning rate: 0.00125 #####
Epoch 22, Train Loss: 2.0035, Val Loss: 2.3380
2025-02-04 17:23:37.162 INFO: Epoch 22, Train Loss: 2.0035, Val Loss: 2.3380
train_e/atom_mae: 0.001406
2025-02-04 17:23:37.164 INFO: train_e/atom_mae: 0.001406
train_e/atom_rmse: 0.001955
2025-02-04 17:23:37.164 INFO: train_e/atom_rmse: 0.001955
train_f_mae: 0.028573
2025-02-04 17:23:37.166 INFO: train_f_mae: 0.028573
train_f_rmse: 0.043158
2025-02-04 17:23:37.167 INFO: train_f_rmse: 0.043158
val_e/atom_mae: 0.000722
2025-02-04 17:23:37.169 INFO: val_e/atom_mae: 0.000722
val_e/atom_rmse: 0.000945
2025-02-04 17:23:37.169 INFO: val_e/atom_rmse: 0.000945
val_f_mae: 0.028874
2025-02-04 17:23:37.170 INFO: val_f_mae: 0.028874
val_f_rmse: 0.048016
2025-02-04 17:23:37.170 INFO: val_f_rmse: 0.048016
##### Step: 62 Learning rate: 0.00125 #####
2025-02-04 17:25:35.405 INFO: ##### Step: 62 Learning rate: 0.00125 #####
Epoch 23, Train Loss: 2.0108, Val Loss: 2.3282
2025-02-04 17:25:35.406 INFO: Epoch 23, Train Loss: 2.0108, Val Loss: 2.3282
train_e/atom_mae: 0.001458
2025-02-04 17:25:35.443 INFO: train_e/atom_mae: 0.001458
train_e/atom_rmse: 0.001951
2025-02-04 17:25:35.462 INFO: train_e/atom_rmse: 0.001951
train_f_mae: 0.028700
2025-02-04 17:25:35.465 INFO: train_f_mae: 0.028700
train_f_rmse: 0.043249
2025-02-04 17:25:35.465 INFO: train_f_rmse: 0.043249
val_e/atom_mae: 0.000684
2025-02-04 17:25:35.467 INFO: val_e/atom_mae: 0.000684
val_e/atom_rmse: 0.000901
2025-02-04 17:25:35.467 INFO: val_e/atom_rmse: 0.000901
val_f_mae: 0.028744
2025-02-04 17:25:35.468 INFO: val_f_mae: 0.028744
val_f_rmse: 0.047951
2025-02-04 17:25:35.468 INFO: val_f_rmse: 0.047951
##### Step: 63 Learning rate: 0.00125 #####
2025-02-04 17:27:27.619 INFO: ##### Step: 63 Learning rate: 0.00125 #####
Epoch 24, Train Loss: 1.9525, Val Loss: 2.3209
2025-02-04 17:27:27.620 INFO: Epoch 24, Train Loss: 1.9525, Val Loss: 2.3209
train_e/atom_mae: 0.001365
2025-02-04 17:27:27.621 INFO: train_e/atom_mae: 0.001365
train_e/atom_rmse: 0.001884
2025-02-04 17:27:27.621 INFO: train_e/atom_rmse: 0.001884
train_f_mae: 0.028353
2025-02-04 17:27:27.624 INFO: train_f_mae: 0.028353
train_f_rmse: 0.042682
2025-02-04 17:27:27.624 INFO: train_f_rmse: 0.042682
val_e/atom_mae: 0.000768
2025-02-04 17:27:27.626 INFO: val_e/atom_mae: 0.000768
val_e/atom_rmse: 0.000993
2025-02-04 17:27:27.627 INFO: val_e/atom_rmse: 0.000993
val_f_mae: 0.028654
2025-02-04 17:27:27.627 INFO: val_f_mae: 0.028654
val_f_rmse: 0.047807
2025-02-04 17:27:27.627 INFO: val_f_rmse: 0.047807
##### Step: 64 Learning rate: 0.00125 #####
2025-02-04 17:29:19.361 INFO: ##### Step: 64 Learning rate: 0.00125 #####
Epoch 25, Train Loss: 2.0946, Val Loss: 2.3384
2025-02-04 17:29:19.361 INFO: Epoch 25, Train Loss: 2.0946, Val Loss: 2.3384
train_e/atom_mae: 0.001850
2025-02-04 17:29:19.362 INFO: train_e/atom_mae: 0.001850
train_e/atom_rmse: 0.002426
2025-02-04 17:29:19.362 INFO: train_e/atom_rmse: 0.002426
train_f_mae: 0.028708
2025-02-04 17:29:19.365 INFO: train_f_mae: 0.028708
train_f_rmse: 0.043333
2025-02-04 17:29:19.365 INFO: train_f_rmse: 0.043333
val_e/atom_mae: 0.000744
2025-02-04 17:29:19.367 INFO: val_e/atom_mae: 0.000744
val_e/atom_rmse: 0.000930
2025-02-04 17:29:19.367 INFO: val_e/atom_rmse: 0.000930
val_f_mae: 0.028699
2025-02-04 17:29:19.368 INFO: val_f_mae: 0.028699
val_f_rmse: 0.048038
2025-02-04 17:29:19.368 INFO: val_f_rmse: 0.048038
##### Step: 65 Learning rate: 0.00125 #####
2025-02-04 17:31:11.020 INFO: ##### Step: 65 Learning rate: 0.00125 #####
Epoch 26, Train Loss: 1.9933, Val Loss: 2.3024
2025-02-04 17:31:11.021 INFO: Epoch 26, Train Loss: 1.9933, Val Loss: 2.3024
train_e/atom_mae: 0.001482
2025-02-04 17:31:11.022 INFO: train_e/atom_mae: 0.001482
train_e/atom_rmse: 0.001965
2025-02-04 17:31:11.022 INFO: train_e/atom_rmse: 0.001965
train_f_mae: 0.028566
2025-02-04 17:31:11.025 INFO: train_f_mae: 0.028566
train_f_rmse: 0.043023
2025-02-04 17:31:11.025 INFO: train_f_rmse: 0.043023
val_e/atom_mae: 0.000742
2025-02-04 17:31:11.027 INFO: val_e/atom_mae: 0.000742
val_e/atom_rmse: 0.000955
2025-02-04 17:31:11.028 INFO: val_e/atom_rmse: 0.000955
val_f_mae: 0.028623
2025-02-04 17:31:11.028 INFO: val_f_mae: 0.028623
val_f_rmse: 0.047644
2025-02-04 17:31:11.028 INFO: val_f_rmse: 0.047644
##### Step: 66 Learning rate: 0.00125 #####
2025-02-04 17:33:02.875 INFO: ##### Step: 66 Learning rate: 0.00125 #####
Epoch 27, Train Loss: 2.0603, Val Loss: 2.3362
2025-02-04 17:33:02.875 INFO: Epoch 27, Train Loss: 2.0603, Val Loss: 2.3362
train_e/atom_mae: 0.001807
2025-02-04 17:33:02.876 INFO: train_e/atom_mae: 0.001807
train_e/atom_rmse: 0.002358
2025-02-04 17:33:02.877 INFO: train_e/atom_rmse: 0.002358
train_f_mae: 0.028590
2025-02-04 17:33:02.879 INFO: train_f_mae: 0.028590
train_f_rmse: 0.043074
2025-02-04 17:33:02.879 INFO: train_f_rmse: 0.043074
val_e/atom_mae: 0.000662
2025-02-04 17:33:02.882 INFO: val_e/atom_mae: 0.000662
val_e/atom_rmse: 0.000852
2025-02-04 17:33:02.882 INFO: val_e/atom_rmse: 0.000852
val_f_mae: 0.028571
2025-02-04 17:33:02.882 INFO: val_f_mae: 0.028571
val_f_rmse: 0.048066
2025-02-04 17:33:02.883 INFO: val_f_rmse: 0.048066
##### Step: 67 Learning rate: 0.00125 #####
2025-02-04 17:34:54.653 INFO: ##### Step: 67 Learning rate: 0.00125 #####
Epoch 28, Train Loss: 1.9806, Val Loss: 2.3261
2025-02-04 17:34:54.654 INFO: Epoch 28, Train Loss: 1.9806, Val Loss: 2.3261
train_e/atom_mae: 0.001443
2025-02-04 17:34:54.655 INFO: train_e/atom_mae: 0.001443
train_e/atom_rmse: 0.001976
2025-02-04 17:34:54.655 INFO: train_e/atom_rmse: 0.001976
train_f_mae: 0.028556
2025-02-04 17:34:54.657 INFO: train_f_mae: 0.028556
train_f_rmse: 0.042857
2025-02-04 17:34:54.658 INFO: train_f_rmse: 0.042857
val_e/atom_mae: 0.000657
2025-02-04 17:34:54.660 INFO: val_e/atom_mae: 0.000657
val_e/atom_rmse: 0.000882
2025-02-04 17:34:54.660 INFO: val_e/atom_rmse: 0.000882
val_f_mae: 0.028503
2025-02-04 17:34:54.661 INFO: val_f_mae: 0.028503
val_f_rmse: 0.047946
2025-02-04 17:34:54.661 INFO: val_f_rmse: 0.047946
##### Step: 68 Learning rate: 0.00125 #####
2025-02-04 17:36:46.452 INFO: ##### Step: 68 Learning rate: 0.00125 #####
Epoch 29, Train Loss: 1.9006, Val Loss: 2.3167
2025-02-04 17:36:46.453 INFO: Epoch 29, Train Loss: 1.9006, Val Loss: 2.3167
train_e/atom_mae: 0.001269
2025-02-04 17:36:46.454 INFO: train_e/atom_mae: 0.001269
train_e/atom_rmse: 0.001761
2025-02-04 17:36:46.454 INFO: train_e/atom_rmse: 0.001761
train_f_mae: 0.027841
2025-02-04 17:36:46.457 INFO: train_f_mae: 0.027841
train_f_rmse: 0.042265
2025-02-04 17:36:46.457 INFO: train_f_rmse: 0.042265
val_e/atom_mae: 0.000577
2025-02-04 17:36:46.459 INFO: val_e/atom_mae: 0.000577
val_e/atom_rmse: 0.000795
2025-02-04 17:36:46.460 INFO: val_e/atom_rmse: 0.000795
val_f_mae: 0.028500
2025-02-04 17:36:46.460 INFO: val_f_mae: 0.028500
val_f_rmse: 0.047897
2025-02-04 17:36:46.460 INFO: val_f_rmse: 0.047897
##### Step: 69 Learning rate: 0.00125 #####
2025-02-04 17:38:38.323 INFO: ##### Step: 69 Learning rate: 0.00125 #####
Epoch 30, Train Loss: 2.0762, Val Loss: 2.3387
2025-02-04 17:38:38.324 INFO: Epoch 30, Train Loss: 2.0762, Val Loss: 2.3387
train_e/atom_mae: 0.001458
2025-02-04 17:38:38.324 INFO: train_e/atom_mae: 0.001458
train_e/atom_rmse: 0.001938
2025-02-04 17:38:38.325 INFO: train_e/atom_rmse: 0.001938
train_f_mae: 0.029344
2025-02-04 17:38:38.327 INFO: train_f_mae: 0.029344
train_f_rmse: 0.044020
2025-02-04 17:38:38.327 INFO: train_f_rmse: 0.044020
val_e/atom_mae: 0.000653
2025-02-04 17:38:38.330 INFO: val_e/atom_mae: 0.000653
val_e/atom_rmse: 0.000859
2025-02-04 17:38:38.330 INFO: val_e/atom_rmse: 0.000859
val_f_mae: 0.028555
2025-02-04 17:38:38.330 INFO: val_f_mae: 0.028555
val_f_rmse: 0.048089
2025-02-04 17:38:38.331 INFO: val_f_rmse: 0.048089
##### Step: 70 Learning rate: 0.00125 #####
2025-02-04 17:41:01.161 INFO: ##### Step: 70 Learning rate: 0.00125 #####
Epoch 31, Train Loss: 2.0724, Val Loss: 2.2945
2025-02-04 17:41:01.162 INFO: Epoch 31, Train Loss: 2.0724, Val Loss: 2.2945
train_e/atom_mae: 0.001436
2025-02-04 17:41:01.270 INFO: train_e/atom_mae: 0.001436
train_e/atom_rmse: 0.001952
2025-02-04 17:41:01.445 INFO: train_e/atom_rmse: 0.001952
train_f_mae: 0.029287
2025-02-04 17:41:01.447 INFO: train_f_mae: 0.029287
train_f_rmse: 0.043953
2025-02-04 17:41:01.448 INFO: train_f_rmse: 0.043953
val_e/atom_mae: 0.000598
2025-02-04 17:41:01.453 INFO: val_e/atom_mae: 0.000598
val_e/atom_rmse: 0.000833
2025-02-04 17:41:01.454 INFO: val_e/atom_rmse: 0.000833
val_f_mae: 0.028467
2025-02-04 17:41:01.454 INFO: val_f_mae: 0.028467
val_f_rmse: 0.047650
2025-02-04 17:41:01.454 INFO: val_f_rmse: 0.047650
##### Step: 71 Learning rate: 0.00125 #####
2025-02-04 17:42:53.349 INFO: ##### Step: 71 Learning rate: 0.00125 #####
Epoch 32, Train Loss: 2.1804, Val Loss: 2.2910
2025-02-04 17:42:53.349 INFO: Epoch 32, Train Loss: 2.1804, Val Loss: 2.2910
train_e/atom_mae: 0.002283
2025-02-04 17:42:53.350 INFO: train_e/atom_mae: 0.002283
train_e/atom_rmse: 0.002887
2025-02-04 17:42:53.350 INFO: train_e/atom_rmse: 0.002887
train_f_mae: 0.028936
2025-02-04 17:42:53.353 INFO: train_f_mae: 0.028936
train_f_rmse: 0.043280
2025-02-04 17:42:53.353 INFO: train_f_rmse: 0.043280
val_e/atom_mae: 0.000617
2025-02-04 17:42:53.355 INFO: val_e/atom_mae: 0.000617
val_e/atom_rmse: 0.000845
2025-02-04 17:42:53.355 INFO: val_e/atom_rmse: 0.000845
val_f_mae: 0.028474
2025-02-04 17:42:53.356 INFO: val_f_mae: 0.028474
val_f_rmse: 0.047599
2025-02-04 17:42:53.356 INFO: val_f_rmse: 0.047599
##### Step: 72 Learning rate: 0.00125 #####
2025-02-04 17:44:45.010 INFO: ##### Step: 72 Learning rate: 0.00125 #####
Epoch 33, Train Loss: 1.9944, Val Loss: 2.2848
2025-02-04 17:44:45.010 INFO: Epoch 33, Train Loss: 1.9944, Val Loss: 2.2848
train_e/atom_mae: 0.001563
2025-02-04 17:44:45.011 INFO: train_e/atom_mae: 0.001563
train_e/atom_rmse: 0.002103
2025-02-04 17:44:45.012 INFO: train_e/atom_rmse: 0.002103
train_f_mae: 0.028406
2025-02-04 17:44:45.014 INFO: train_f_mae: 0.028406
train_f_rmse: 0.042795
2025-02-04 17:44:45.014 INFO: train_f_rmse: 0.042795
val_e/atom_mae: 0.000588
2025-02-04 17:44:45.017 INFO: val_e/atom_mae: 0.000588
val_e/atom_rmse: 0.000810
2025-02-04 17:44:45.017 INFO: val_e/atom_rmse: 0.000810
val_f_mae: 0.028412
2025-02-04 17:44:45.017 INFO: val_f_mae: 0.028412
val_f_rmse: 0.047560
2025-02-04 17:44:45.018 INFO: val_f_rmse: 0.047560
##### Step: 73 Learning rate: 0.00125 #####
2025-02-04 17:46:45.479 INFO: ##### Step: 73 Learning rate: 0.00125 #####
Epoch 34, Train Loss: 2.2114, Val Loss: 2.3443
2025-02-04 17:46:45.480 INFO: Epoch 34, Train Loss: 2.2114, Val Loss: 2.3443
train_e/atom_mae: 0.002286
2025-02-04 17:46:45.501 INFO: train_e/atom_mae: 0.002286
train_e/atom_rmse: 0.002955
2025-02-04 17:46:45.514 INFO: train_e/atom_rmse: 0.002955
train_f_mae: 0.028891
2025-02-04 17:46:45.517 INFO: train_f_mae: 0.028891
train_f_rmse: 0.043467
2025-02-04 17:46:45.517 INFO: train_f_rmse: 0.043467
val_e/atom_mae: 0.000705
2025-02-04 17:46:45.519 INFO: val_e/atom_mae: 0.000705
val_e/atom_rmse: 0.000938
2025-02-04 17:46:45.519 INFO: val_e/atom_rmse: 0.000938
val_f_mae: 0.028547
2025-02-04 17:46:45.520 INFO: val_f_mae: 0.028547
val_f_rmse: 0.048092
2025-02-04 17:46:45.520 INFO: val_f_rmse: 0.048092
##### Step: 74 Learning rate: 0.00125 #####
2025-02-04 17:48:37.175 INFO: ##### Step: 74 Learning rate: 0.00125 #####
Epoch 35, Train Loss: 2.1593, Val Loss: 2.3373
2025-02-04 17:48:37.176 INFO: Epoch 35, Train Loss: 2.1593, Val Loss: 2.3373
train_e/atom_mae: 0.002123
2025-02-04 17:48:37.177 INFO: train_e/atom_mae: 0.002123
train_e/atom_rmse: 0.002914
2025-02-04 17:48:37.177 INFO: train_e/atom_rmse: 0.002914
train_f_mae: 0.028579
2025-02-04 17:48:37.179 INFO: train_f_mae: 0.028579
train_f_rmse: 0.042968
2025-02-04 17:48:37.180 INFO: train_f_rmse: 0.042968
val_e/atom_mae: 0.000623
2025-02-04 17:48:37.182 INFO: val_e/atom_mae: 0.000623
val_e/atom_rmse: 0.000829
2025-02-04 17:48:37.182 INFO: val_e/atom_rmse: 0.000829
val_f_mae: 0.028431
2025-02-04 17:48:37.183 INFO: val_f_mae: 0.028431
val_f_rmse: 0.048097
2025-02-04 17:48:37.183 INFO: val_f_rmse: 0.048097
##### Step: 75 Learning rate: 0.00125 #####
2025-02-04 17:50:28.776 INFO: ##### Step: 75 Learning rate: 0.00125 #####
Epoch 36, Train Loss: 2.2119, Val Loss: 2.2938
2025-02-04 17:50:28.776 INFO: Epoch 36, Train Loss: 2.2119, Val Loss: 2.2938
train_e/atom_mae: 0.002215
2025-02-04 17:50:28.777 INFO: train_e/atom_mae: 0.002215
train_e/atom_rmse: 0.002746
2025-02-04 17:50:28.778 INFO: train_e/atom_rmse: 0.002746
train_f_mae: 0.029414
2025-02-04 17:50:28.780 INFO: train_f_mae: 0.029414
train_f_rmse: 0.043977
2025-02-04 17:50:28.781 INFO: train_f_rmse: 0.043977
val_e/atom_mae: 0.000626
2025-02-04 17:50:28.783 INFO: val_e/atom_mae: 0.000626
val_e/atom_rmse: 0.000836
2025-02-04 17:50:28.783 INFO: val_e/atom_rmse: 0.000836
val_f_mae: 0.028364
2025-02-04 17:50:28.783 INFO: val_f_mae: 0.028364
val_f_rmse: 0.047641
2025-02-04 17:50:28.784 INFO: val_f_rmse: 0.047641
##### Step: 76 Learning rate: 0.00125 #####
2025-02-04 17:52:20.386 INFO: ##### Step: 76 Learning rate: 0.00125 #####
Epoch 37, Train Loss: 1.9954, Val Loss: 2.3074
2025-02-04 17:52:20.387 INFO: Epoch 37, Train Loss: 1.9954, Val Loss: 2.3074
train_e/atom_mae: 0.001554
2025-02-04 17:52:20.388 INFO: train_e/atom_mae: 0.001554
train_e/atom_rmse: 0.002092
2025-02-04 17:52:20.389 INFO: train_e/atom_rmse: 0.002092
train_f_mae: 0.028487
2025-02-04 17:52:20.395 INFO: train_f_mae: 0.028487
train_f_rmse: 0.042826
2025-02-04 17:52:20.395 INFO: train_f_rmse: 0.042826
val_e/atom_mae: 0.000623
2025-02-04 17:52:20.398 INFO: val_e/atom_mae: 0.000623
val_e/atom_rmse: 0.000834
2025-02-04 17:52:20.398 INFO: val_e/atom_rmse: 0.000834
val_f_mae: 0.028367
2025-02-04 17:52:20.399 INFO: val_f_mae: 0.028367
val_f_rmse: 0.047779
2025-02-04 17:52:20.399 INFO: val_f_rmse: 0.047779
##### Step: 77 Learning rate: 0.00125 #####
2025-02-04 17:54:12.248 INFO: ##### Step: 77 Learning rate: 0.00125 #####
Epoch 38, Train Loss: 1.8569, Val Loss: 2.3179
2025-02-04 17:54:12.249 INFO: Epoch 38, Train Loss: 1.8569, Val Loss: 2.3179
train_e/atom_mae: 0.001084
2025-02-04 17:54:12.250 INFO: train_e/atom_mae: 0.001084
train_e/atom_rmse: 0.001487
2025-02-04 17:54:12.250 INFO: train_e/atom_rmse: 0.001487
train_f_mae: 0.027824
2025-02-04 17:54:12.253 INFO: train_f_mae: 0.027824
train_f_rmse: 0.042135
2025-02-04 17:54:12.253 INFO: train_f_rmse: 0.042135
val_e/atom_mae: 0.000652
2025-02-04 17:54:12.255 INFO: val_e/atom_mae: 0.000652
val_e/atom_rmse: 0.000866
2025-02-04 17:54:12.255 INFO: val_e/atom_rmse: 0.000866
val_f_mae: 0.028320
2025-02-04 17:54:12.256 INFO: val_f_mae: 0.028320
val_f_rmse: 0.047870
2025-02-04 17:54:12.256 INFO: val_f_rmse: 0.047870
##### Step: 78 Learning rate: 0.00125 #####
2025-02-04 17:56:03.929 INFO: ##### Step: 78 Learning rate: 0.00125 #####
Epoch 39, Train Loss: 1.9086, Val Loss: 2.2906
2025-02-04 17:56:03.929 INFO: Epoch 39, Train Loss: 1.9086, Val Loss: 2.2906
train_e/atom_mae: 0.001139
2025-02-04 17:56:03.930 INFO: train_e/atom_mae: 0.001139
train_e/atom_rmse: 0.001599
2025-02-04 17:56:03.930 INFO: train_e/atom_rmse: 0.001599
train_f_mae: 0.028373
2025-02-04 17:56:03.933 INFO: train_f_mae: 0.028373
train_f_rmse: 0.042595
2025-02-04 17:56:03.933 INFO: train_f_rmse: 0.042595
val_e/atom_mae: 0.000595
2025-02-04 17:56:03.935 INFO: val_e/atom_mae: 0.000595
val_e/atom_rmse: 0.000808
2025-02-04 17:56:03.936 INFO: val_e/atom_rmse: 0.000808
val_f_mae: 0.028333
2025-02-04 17:56:03.936 INFO: val_f_mae: 0.028333
val_f_rmse: 0.047622
2025-02-04 17:56:03.937 INFO: val_f_rmse: 0.047622
##### Step: 79 Learning rate: 0.00125 #####
2025-02-04 17:57:55.597 INFO: ##### Step: 79 Learning rate: 0.00125 #####
Epoch 40, Train Loss: 2.0238, Val Loss: 2.2802
2025-02-04 17:57:55.598 INFO: Epoch 40, Train Loss: 2.0238, Val Loss: 2.2802
train_e/atom_mae: 0.001661
2025-02-04 17:57:55.599 INFO: train_e/atom_mae: 0.001661
train_e/atom_rmse: 0.002233
2025-02-04 17:57:55.599 INFO: train_e/atom_rmse: 0.002233
train_f_mae: 0.028617
2025-02-04 17:57:55.601 INFO: train_f_mae: 0.028617
train_f_rmse: 0.042895
2025-02-04 17:57:55.602 INFO: train_f_rmse: 0.042895
val_e/atom_mae: 0.000618
2025-02-04 17:57:55.604 INFO: val_e/atom_mae: 0.000618
val_e/atom_rmse: 0.000853
2025-02-04 17:57:55.604 INFO: val_e/atom_rmse: 0.000853
val_f_mae: 0.028315
2025-02-04 17:57:55.605 INFO: val_f_mae: 0.028315
val_f_rmse: 0.047484
2025-02-04 17:57:55.605 INFO: val_f_rmse: 0.047484
##### Step: 80 Learning rate: 0.000625 #####
2025-02-04 18:00:20.087 INFO: ##### Step: 80 Learning rate: 0.000625 #####
Epoch 41, Train Loss: 1.8560, Val Loss: 2.2656
2025-02-04 18:00:20.088 INFO: Epoch 41, Train Loss: 1.8560, Val Loss: 2.2656
train_e/atom_mae: 0.001427
2025-02-04 18:00:20.297 INFO: train_e/atom_mae: 0.001427
train_e/atom_rmse: 0.001974
2025-02-04 18:00:20.483 INFO: train_e/atom_rmse: 0.001974
train_f_mae: 0.027392
2025-02-04 18:00:20.486 INFO: train_f_mae: 0.027392
train_f_rmse: 0.041381
2025-02-04 18:00:20.487 INFO: train_f_rmse: 0.041381
val_e/atom_mae: 0.000565
2025-02-04 18:00:20.489 INFO: val_e/atom_mae: 0.000565
val_e/atom_rmse: 0.000793
2025-02-04 18:00:20.489 INFO: val_e/atom_rmse: 0.000793
val_f_mae: 0.028286
2025-02-04 18:00:20.490 INFO: val_f_mae: 0.028286
val_f_rmse: 0.047364
2025-02-04 18:00:20.490 INFO: val_f_rmse: 0.047364
##### Step: 81 Learning rate: 0.000625 #####
2025-02-04 18:02:12.450 INFO: ##### Step: 81 Learning rate: 0.000625 #####
Epoch 42, Train Loss: 1.7875, Val Loss: 2.2626
2025-02-04 18:02:12.451 INFO: Epoch 42, Train Loss: 1.7875, Val Loss: 2.2626
train_e/atom_mae: 0.001119
2025-02-04 18:02:12.452 INFO: train_e/atom_mae: 0.001119
train_e/atom_rmse: 0.001571
2025-02-04 18:02:12.452 INFO: train_e/atom_rmse: 0.001571
train_f_mae: 0.027225
2025-02-04 18:02:12.455 INFO: train_f_mae: 0.027225
train_f_rmse: 0.041189
2025-02-04 18:02:12.455 INFO: train_f_rmse: 0.041189
val_e/atom_mae: 0.000579
2025-02-04 18:02:12.457 INFO: val_e/atom_mae: 0.000579
val_e/atom_rmse: 0.000796
2025-02-04 18:02:12.458 INFO: val_e/atom_rmse: 0.000796
val_f_mae: 0.028188
2025-02-04 18:02:12.458 INFO: val_f_mae: 0.028188
val_f_rmse: 0.047334
2025-02-04 18:02:12.458 INFO: val_f_rmse: 0.047334
##### Step: 82 Learning rate: 0.000625 #####
2025-02-04 18:04:04.190 INFO: ##### Step: 82 Learning rate: 0.000625 #####
Epoch 43, Train Loss: 1.7725, Val Loss: 2.2708
2025-02-04 18:04:04.190 INFO: Epoch 43, Train Loss: 1.7725, Val Loss: 2.2708
train_e/atom_mae: 0.001156
2025-02-04 18:04:04.191 INFO: train_e/atom_mae: 0.001156
train_e/atom_rmse: 0.001581
2025-02-04 18:04:04.191 INFO: train_e/atom_rmse: 0.001581
train_f_mae: 0.027029
2025-02-04 18:04:04.194 INFO: train_f_mae: 0.027029
train_f_rmse: 0.040991
2025-02-04 18:04:04.194 INFO: train_f_rmse: 0.040991
val_e/atom_mae: 0.000543
2025-02-04 18:04:04.196 INFO: val_e/atom_mae: 0.000543
val_e/atom_rmse: 0.000769
2025-02-04 18:04:04.197 INFO: val_e/atom_rmse: 0.000769
val_f_mae: 0.028181
2025-02-04 18:04:04.197 INFO: val_f_mae: 0.028181
val_f_rmse: 0.047437
2025-02-04 18:04:04.197 INFO: val_f_rmse: 0.047437
##### Step: 83 Learning rate: 0.000625 #####
2025-02-04 18:06:08.326 INFO: ##### Step: 83 Learning rate: 0.000625 #####
Epoch 44, Train Loss: 1.8219, Val Loss: 2.2841
2025-02-04 18:06:08.326 INFO: Epoch 44, Train Loss: 1.8219, Val Loss: 2.2841
train_e/atom_mae: 0.001295
2025-02-04 18:06:08.444 INFO: train_e/atom_mae: 0.001295
train_e/atom_rmse: 0.001723
2025-02-04 18:06:08.560 INFO: train_e/atom_rmse: 0.001723
train_f_mae: 0.027584
2025-02-04 18:06:08.562 INFO: train_f_mae: 0.027584
train_f_rmse: 0.041382
2025-02-04 18:06:08.563 INFO: train_f_rmse: 0.041382
val_e/atom_mae: 0.000569
2025-02-04 18:06:08.565 INFO: val_e/atom_mae: 0.000569
val_e/atom_rmse: 0.000791
2025-02-04 18:06:08.565 INFO: val_e/atom_rmse: 0.000791
val_f_mae: 0.028190
2025-02-04 18:06:08.566 INFO: val_f_mae: 0.028190
val_f_rmse: 0.047562
2025-02-04 18:06:08.566 INFO: val_f_rmse: 0.047562
##### Step: 84 Learning rate: 0.000625 #####
2025-02-04 18:08:00.268 INFO: ##### Step: 84 Learning rate: 0.000625 #####
Epoch 45, Train Loss: 1.7955, Val Loss: 2.2809
2025-02-04 18:08:00.268 INFO: Epoch 45, Train Loss: 1.7955, Val Loss: 2.2809
train_e/atom_mae: 0.001045
2025-02-04 18:08:00.269 INFO: train_e/atom_mae: 0.001045
train_e/atom_rmse: 0.001480
2025-02-04 18:08:00.270 INFO: train_e/atom_rmse: 0.001480
train_f_mae: 0.027409
2025-02-04 18:08:00.272 INFO: train_f_mae: 0.027409
train_f_rmse: 0.041410
2025-02-04 18:08:00.272 INFO: train_f_rmse: 0.041410
val_e/atom_mae: 0.000572
2025-02-04 18:08:00.275 INFO: val_e/atom_mae: 0.000572
val_e/atom_rmse: 0.000775
2025-02-04 18:08:00.275 INFO: val_e/atom_rmse: 0.000775
val_f_mae: 0.028182
2025-02-04 18:08:00.275 INFO: val_f_mae: 0.028182
val_f_rmse: 0.047537
2025-02-04 18:08:00.276 INFO: val_f_rmse: 0.047537
##### Step: 85 Learning rate: 0.000625 #####
2025-02-04 18:09:52.154 INFO: ##### Step: 85 Learning rate: 0.000625 #####
Epoch 46, Train Loss: 1.8264, Val Loss: 2.2768
2025-02-04 18:09:52.155 INFO: Epoch 46, Train Loss: 1.8264, Val Loss: 2.2768
train_e/atom_mae: 0.001260
2025-02-04 18:09:52.156 INFO: train_e/atom_mae: 0.001260
train_e/atom_rmse: 0.001736
2025-02-04 18:09:52.156 INFO: train_e/atom_rmse: 0.001736
train_f_mae: 0.027460
2025-02-04 18:09:52.158 INFO: train_f_mae: 0.027460
train_f_rmse: 0.041417
2025-02-04 18:09:52.159 INFO: train_f_rmse: 0.041417
val_e/atom_mae: 0.000558
2025-02-04 18:09:52.161 INFO: val_e/atom_mae: 0.000558
val_e/atom_rmse: 0.000780
2025-02-04 18:09:52.161 INFO: val_e/atom_rmse: 0.000780
val_f_mae: 0.028144
2025-02-04 18:09:52.162 INFO: val_f_mae: 0.028144
val_f_rmse: 0.047494
2025-02-04 18:09:52.162 INFO: val_f_rmse: 0.047494
##### Step: 86 Learning rate: 0.000625 #####
2025-02-04 18:11:44.383 INFO: ##### Step: 86 Learning rate: 0.000625 #####
Epoch 47, Train Loss: 1.8286, Val Loss: 2.2704
2025-02-04 18:11:44.383 INFO: Epoch 47, Train Loss: 1.8286, Val Loss: 2.2704
train_e/atom_mae: 0.001414
2025-02-04 18:11:44.384 INFO: train_e/atom_mae: 0.001414
train_e/atom_rmse: 0.001943
2025-02-04 18:11:44.384 INFO: train_e/atom_rmse: 0.001943
train_f_mae: 0.027232
2025-02-04 18:11:44.387 INFO: train_f_mae: 0.027232
train_f_rmse: 0.041102
2025-02-04 18:11:44.387 INFO: train_f_rmse: 0.041102
val_e/atom_mae: 0.000570
2025-02-04 18:11:44.389 INFO: val_e/atom_mae: 0.000570
val_e/atom_rmse: 0.000779
2025-02-04 18:11:44.390 INFO: val_e/atom_rmse: 0.000779
val_f_mae: 0.028124
2025-02-04 18:11:44.390 INFO: val_f_mae: 0.028124
val_f_rmse: 0.047424
2025-02-04 18:11:44.390 INFO: val_f_rmse: 0.047424
##### Step: 87 Learning rate: 0.000625 #####
2025-02-04 18:13:36.522 INFO: ##### Step: 87 Learning rate: 0.000625 #####
Epoch 48, Train Loss: 1.7696, Val Loss: 2.2693
2025-02-04 18:13:36.523 INFO: Epoch 48, Train Loss: 1.7696, Val Loss: 2.2693
train_e/atom_mae: 0.001142
2025-02-04 18:13:36.524 INFO: train_e/atom_mae: 0.001142
train_e/atom_rmse: 0.001567
2025-02-04 18:13:36.524 INFO: train_e/atom_rmse: 0.001567
train_f_mae: 0.027075
2025-02-04 18:13:36.527 INFO: train_f_mae: 0.027075
train_f_rmse: 0.040976
2025-02-04 18:13:36.527 INFO: train_f_rmse: 0.040976
val_e/atom_mae: 0.000589
2025-02-04 18:13:36.529 INFO: val_e/atom_mae: 0.000589
val_e/atom_rmse: 0.000809
2025-02-04 18:13:36.529 INFO: val_e/atom_rmse: 0.000809
val_f_mae: 0.028088
2025-02-04 18:13:36.530 INFO: val_f_mae: 0.028088
val_f_rmse: 0.047398
2025-02-04 18:13:36.530 INFO: val_f_rmse: 0.047398
##### Step: 88 Learning rate: 0.000625 #####
2025-02-04 18:15:28.486 INFO: ##### Step: 88 Learning rate: 0.000625 #####
Epoch 49, Train Loss: 1.7527, Val Loss: 2.2632
2025-02-04 18:15:28.487 INFO: Epoch 49, Train Loss: 1.7527, Val Loss: 2.2632
train_e/atom_mae: 0.001086
2025-02-04 18:15:28.488 INFO: train_e/atom_mae: 0.001086
train_e/atom_rmse: 0.001503
2025-02-04 18:15:28.488 INFO: train_e/atom_rmse: 0.001503
train_f_mae: 0.027000
2025-02-04 18:15:28.491 INFO: train_f_mae: 0.027000
train_f_rmse: 0.040860
2025-02-04 18:15:28.491 INFO: train_f_rmse: 0.040860
val_e/atom_mae: 0.000574
2025-02-04 18:15:28.493 INFO: val_e/atom_mae: 0.000574
val_e/atom_rmse: 0.000776
2025-02-04 18:15:28.494 INFO: val_e/atom_rmse: 0.000776
val_f_mae: 0.028069
2025-02-04 18:15:28.494 INFO: val_f_mae: 0.028069
val_f_rmse: 0.047350
2025-02-04 18:15:28.494 INFO: val_f_rmse: 0.047350
##### Step: 89 Learning rate: 0.000625 #####
2025-02-04 18:17:20.445 INFO: ##### Step: 89 Learning rate: 0.000625 #####
Epoch 50, Train Loss: 1.7942, Val Loss: 2.2763
2025-02-04 18:17:20.446 INFO: Epoch 50, Train Loss: 1.7942, Val Loss: 2.2763
train_e/atom_mae: 0.000978
2025-02-04 18:17:20.447 INFO: train_e/atom_mae: 0.000978
train_e/atom_rmse: 0.001417
2025-02-04 18:17:20.447 INFO: train_e/atom_rmse: 0.001417
train_f_mae: 0.027471
2025-02-04 18:17:20.450 INFO: train_f_mae: 0.027471
train_f_rmse: 0.041475
2025-02-04 18:17:20.450 INFO: train_f_rmse: 0.041475
val_e/atom_mae: 0.000568
2025-02-04 18:17:20.452 INFO: val_e/atom_mae: 0.000568
val_e/atom_rmse: 0.000783
2025-02-04 18:17:20.453 INFO: val_e/atom_rmse: 0.000783
val_f_mae: 0.028072
2025-02-04 18:17:20.453 INFO: val_f_mae: 0.028072
val_f_rmse: 0.047486
2025-02-04 18:17:20.453 INFO: val_f_rmse: 0.047486
##### Step: 90 Learning rate: 0.000625 #####
2025-02-04 18:19:33.536 INFO: ##### Step: 90 Learning rate: 0.000625 #####
Epoch 51, Train Loss: 1.7997, Val Loss: 2.2796
2025-02-04 18:19:33.537 INFO: Epoch 51, Train Loss: 1.7997, Val Loss: 2.2796
train_e/atom_mae: 0.001318
2025-02-04 18:19:33.656 INFO: train_e/atom_mae: 0.001318
train_e/atom_rmse: 0.001772
2025-02-04 18:19:33.780 INFO: train_e/atom_rmse: 0.001772
train_f_mae: 0.027207
2025-02-04 18:19:33.783 INFO: train_f_mae: 0.027207
train_f_rmse: 0.041036
2025-02-04 18:19:33.783 INFO: train_f_rmse: 0.041036
val_e/atom_mae: 0.000569
2025-02-04 18:19:33.785 INFO: val_e/atom_mae: 0.000569
val_e/atom_rmse: 0.000797
2025-02-04 18:19:33.786 INFO: val_e/atom_rmse: 0.000797
val_f_mae: 0.028062
2025-02-04 18:19:33.786 INFO: val_f_mae: 0.028062
val_f_rmse: 0.047512
2025-02-04 18:19:33.786 INFO: val_f_rmse: 0.047512
##### Step: 91 Learning rate: 0.000625 #####
2025-02-04 18:21:25.993 INFO: ##### Step: 91 Learning rate: 0.000625 #####
Epoch 52, Train Loss: 1.7489, Val Loss: 2.2549
2025-02-04 18:21:25.994 INFO: Epoch 52, Train Loss: 1.7489, Val Loss: 2.2549
train_e/atom_mae: 0.000889
2025-02-04 18:21:25.995 INFO: train_e/atom_mae: 0.000889
train_e/atom_rmse: 0.001316
2025-02-04 18:21:25.995 INFO: train_e/atom_rmse: 0.001316
train_f_mae: 0.027160
2025-02-04 18:21:25.998 INFO: train_f_mae: 0.027160
train_f_rmse: 0.041049
2025-02-04 18:21:25.998 INFO: train_f_rmse: 0.041049
val_e/atom_mae: 0.000569
2025-02-04 18:21:26.000 INFO: val_e/atom_mae: 0.000569
val_e/atom_rmse: 0.000780
2025-02-04 18:21:26.000 INFO: val_e/atom_rmse: 0.000780
val_f_mae: 0.028030
2025-02-04 18:21:26.001 INFO: val_f_mae: 0.028030
val_f_rmse: 0.047264
2025-02-04 18:21:26.001 INFO: val_f_rmse: 0.047264
##### Step: 92 Learning rate: 0.000625 #####
2025-02-04 18:23:18.397 INFO: ##### Step: 92 Learning rate: 0.000625 #####
Epoch 53, Train Loss: 1.7801, Val Loss: 2.2593
2025-02-04 18:23:18.399 INFO: Epoch 53, Train Loss: 1.7801, Val Loss: 2.2593
train_e/atom_mae: 0.001199
2025-02-04 18:23:18.400 INFO: train_e/atom_mae: 0.001199
train_e/atom_rmse: 0.001652
2025-02-04 18:23:18.400 INFO: train_e/atom_rmse: 0.001652
train_f_mae: 0.027154
2025-02-04 18:23:18.403 INFO: train_f_mae: 0.027154
train_f_rmse: 0.040981
2025-02-04 18:23:18.403 INFO: train_f_rmse: 0.040981
val_e/atom_mae: 0.000558
2025-02-04 18:23:18.405 INFO: val_e/atom_mae: 0.000558
val_e/atom_rmse: 0.000782
2025-02-04 18:23:18.406 INFO: val_e/atom_rmse: 0.000782
val_f_mae: 0.028020
2025-02-04 18:23:18.409 INFO: val_f_mae: 0.028020
val_f_rmse: 0.047305
2025-02-04 18:23:18.410 INFO: val_f_rmse: 0.047305
##### Step: 93 Learning rate: 0.000625 #####
2025-02-04 18:25:10.288 INFO: ##### Step: 93 Learning rate: 0.000625 #####
Epoch 54, Train Loss: 1.8355, Val Loss: 2.2699
2025-02-04 18:25:10.289 INFO: Epoch 54, Train Loss: 1.8355, Val Loss: 2.2699
train_e/atom_mae: 0.001616
2025-02-04 18:25:10.290 INFO: train_e/atom_mae: 0.001616
train_e/atom_rmse: 0.002055
2025-02-04 18:25:10.290 INFO: train_e/atom_rmse: 0.002055
train_f_mae: 0.027185
2025-02-04 18:25:10.292 INFO: train_f_mae: 0.027185
train_f_rmse: 0.040986
2025-02-04 18:25:10.293 INFO: train_f_rmse: 0.040986
val_e/atom_mae: 0.000559
2025-02-04 18:25:10.295 INFO: val_e/atom_mae: 0.000559
val_e/atom_rmse: 0.000780
2025-02-04 18:25:10.295 INFO: val_e/atom_rmse: 0.000780
val_f_mae: 0.028016
2025-02-04 18:25:10.296 INFO: val_f_mae: 0.028016
val_f_rmse: 0.047421
2025-02-04 18:25:10.296 INFO: val_f_rmse: 0.047421
##### Step: 94 Learning rate: 0.000625 #####
2025-02-04 18:27:02.550 INFO: ##### Step: 94 Learning rate: 0.000625 #####
Epoch 55, Train Loss: 1.7738, Val Loss: 2.2456
2025-02-04 18:27:02.551 INFO: Epoch 55, Train Loss: 1.7738, Val Loss: 2.2456
train_e/atom_mae: 0.001103
2025-02-04 18:27:02.552 INFO: train_e/atom_mae: 0.001103
train_e/atom_rmse: 0.001616
2025-02-04 18:27:02.552 INFO: train_e/atom_rmse: 0.001616
train_f_mae: 0.026958
2025-02-04 18:27:02.555 INFO: train_f_mae: 0.026958
train_f_rmse: 0.040957
2025-02-04 18:27:02.555 INFO: train_f_rmse: 0.040957
val_e/atom_mae: 0.000616
2025-02-04 18:27:02.557 INFO: val_e/atom_mae: 0.000616
val_e/atom_rmse: 0.000847
2025-02-04 18:27:02.557 INFO: val_e/atom_rmse: 0.000847
val_f_mae: 0.028054
2025-02-04 18:27:02.558 INFO: val_f_mae: 0.028054
val_f_rmse: 0.047122
2025-02-04 18:27:02.558 INFO: val_f_rmse: 0.047122
##### Step: 95 Learning rate: 0.000625 #####
2025-02-04 18:28:54.192 INFO: ##### Step: 95 Learning rate: 0.000625 #####
Epoch 56, Train Loss: 1.8067, Val Loss: 2.2562
2025-02-04 18:28:54.193 INFO: Epoch 56, Train Loss: 1.8067, Val Loss: 2.2562
train_e/atom_mae: 0.001097
2025-02-04 18:28:54.194 INFO: train_e/atom_mae: 0.001097
train_e/atom_rmse: 0.001549
2025-02-04 18:28:54.194 INFO: train_e/atom_rmse: 0.001549
train_f_mae: 0.027543
2025-02-04 18:28:54.197 INFO: train_f_mae: 0.027543
train_f_rmse: 0.041451
2025-02-04 18:28:54.197 INFO: train_f_rmse: 0.041451
val_e/atom_mae: 0.000591
2025-02-04 18:28:54.199 INFO: val_e/atom_mae: 0.000591
val_e/atom_rmse: 0.000804
2025-02-04 18:28:54.200 INFO: val_e/atom_rmse: 0.000804
val_f_mae: 0.027983
2025-02-04 18:28:54.200 INFO: val_f_mae: 0.027983
val_f_rmse: 0.047261
2025-02-04 18:28:54.200 INFO: val_f_rmse: 0.047261
##### Step: 96 Learning rate: 0.000625 #####
2025-02-04 18:30:46.235 INFO: ##### Step: 96 Learning rate: 0.000625 #####
Epoch 57, Train Loss: 1.7718, Val Loss: 2.2573
2025-02-04 18:30:46.236 INFO: Epoch 57, Train Loss: 1.7718, Val Loss: 2.2573
train_e/atom_mae: 0.001101
2025-02-04 18:30:46.237 INFO: train_e/atom_mae: 0.001101
train_e/atom_rmse: 0.001531
2025-02-04 18:30:46.237 INFO: train_e/atom_rmse: 0.001531
train_f_mae: 0.027187
2025-02-04 18:30:46.240 INFO: train_f_mae: 0.027187
train_f_rmse: 0.041053
2025-02-04 18:30:46.240 INFO: train_f_rmse: 0.041053
val_e/atom_mae: 0.000550
2025-02-04 18:30:46.242 INFO: val_e/atom_mae: 0.000550
val_e/atom_rmse: 0.000769
2025-02-04 18:30:46.242 INFO: val_e/atom_rmse: 0.000769
val_f_mae: 0.027967
2025-02-04 18:30:46.243 INFO: val_f_mae: 0.027967
val_f_rmse: 0.047296
2025-02-04 18:30:46.243 INFO: val_f_rmse: 0.047296
##### Step: 97 Learning rate: 0.000625 #####
2025-02-04 18:32:38.240 INFO: ##### Step: 97 Learning rate: 0.000625 #####
Epoch 58, Train Loss: 1.7736, Val Loss: 2.2674
2025-02-04 18:32:38.240 INFO: Epoch 58, Train Loss: 1.7736, Val Loss: 2.2674
train_e/atom_mae: 0.001261
2025-02-04 18:32:38.241 INFO: train_e/atom_mae: 0.001261
train_e/atom_rmse: 0.001719
2025-02-04 18:32:38.242 INFO: train_e/atom_rmse: 0.001719
train_f_mae: 0.027015
2025-02-04 18:32:38.244 INFO: train_f_mae: 0.027015
train_f_rmse: 0.040800
2025-02-04 18:32:38.244 INFO: train_f_rmse: 0.040800
val_e/atom_mae: 0.000580
2025-02-04 18:32:38.247 INFO: val_e/atom_mae: 0.000580
val_e/atom_rmse: 0.000838
2025-02-04 18:32:38.247 INFO: val_e/atom_rmse: 0.000838
val_f_mae: 0.027970
2025-02-04 18:32:38.247 INFO: val_f_mae: 0.027970
val_f_rmse: 0.047359
2025-02-04 18:32:38.248 INFO: val_f_rmse: 0.047359
##### Step: 98 Learning rate: 0.000625 #####
2025-02-04 18:34:29.945 INFO: ##### Step: 98 Learning rate: 0.000625 #####
Epoch 59, Train Loss: 1.7603, Val Loss: 2.2602
2025-02-04 18:34:29.946 INFO: Epoch 59, Train Loss: 1.7603, Val Loss: 2.2602
train_e/atom_mae: 0.000977
2025-02-04 18:34:29.947 INFO: train_e/atom_mae: 0.000977
train_e/atom_rmse: 0.001396
2025-02-04 18:34:29.947 INFO: train_e/atom_rmse: 0.001396
train_f_mae: 0.027199
2025-02-04 18:34:29.950 INFO: train_f_mae: 0.027199
train_f_rmse: 0.041091
2025-02-04 18:34:29.950 INFO: train_f_rmse: 0.041091
val_e/atom_mae: 0.000588
2025-02-04 18:34:29.952 INFO: val_e/atom_mae: 0.000588
val_e/atom_rmse: 0.000810
2025-02-04 18:34:29.953 INFO: val_e/atom_rmse: 0.000810
val_f_mae: 0.027968
2025-02-04 18:34:29.953 INFO: val_f_mae: 0.027968
val_f_rmse: 0.047299
2025-02-04 18:34:29.953 INFO: val_f_rmse: 0.047299
##### Step: 99 Learning rate: 0.000625 #####
2025-02-04 18:36:21.443 INFO: ##### Step: 99 Learning rate: 0.000625 #####
Epoch 60, Train Loss: 1.7492, Val Loss: 2.2788
2025-02-04 18:36:21.443 INFO: Epoch 60, Train Loss: 1.7492, Val Loss: 2.2788
train_e/atom_mae: 0.001006
2025-02-04 18:36:21.444 INFO: train_e/atom_mae: 0.001006
train_e/atom_rmse: 0.001442
2025-02-04 18:36:21.444 INFO: train_e/atom_rmse: 0.001442
train_f_mae: 0.027053
2025-02-04 18:36:21.447 INFO: train_f_mae: 0.027053
train_f_rmse: 0.040897
2025-02-04 18:36:21.447 INFO: train_f_rmse: 0.040897
val_e/atom_mae: 0.000551
2025-02-04 18:36:21.449 INFO: val_e/atom_mae: 0.000551
val_e/atom_rmse: 0.000765
2025-02-04 18:36:21.450 INFO: val_e/atom_rmse: 0.000765
val_f_mae: 0.027953
2025-02-04 18:36:21.450 INFO: val_f_mae: 0.027953
val_f_rmse: 0.047524
2025-02-04 18:36:21.450 INFO: val_f_rmse: 0.047524
##### Step: 100 Learning rate: 0.0003125 #####
2025-02-04 18:38:13.059 INFO: ##### Step: 100 Learning rate: 0.0003125 #####
Epoch 61, Train Loss: 1.7253, Val Loss: 2.2718
2025-02-04 18:38:13.060 INFO: Epoch 61, Train Loss: 1.7253, Val Loss: 2.2718
train_e/atom_mae: 0.001039
2025-02-04 18:38:13.061 INFO: train_e/atom_mae: 0.001039
train_e/atom_rmse: 0.001439
2025-02-04 18:38:13.061 INFO: train_e/atom_rmse: 0.001439
train_f_mae: 0.026816
2025-02-04 18:38:13.064 INFO: train_f_mae: 0.026816
train_f_rmse: 0.040607
2025-02-04 18:38:13.064 INFO: train_f_rmse: 0.040607
val_e/atom_mae: 0.000559
2025-02-04 18:38:13.066 INFO: val_e/atom_mae: 0.000559
val_e/atom_rmse: 0.000774
2025-02-04 18:38:13.067 INFO: val_e/atom_rmse: 0.000774
val_f_mae: 0.027921
2025-02-04 18:38:13.067 INFO: val_f_mae: 0.027921
val_f_rmse: 0.047443
2025-02-04 18:38:13.067 INFO: val_f_rmse: 0.047443
##### Step: 101 Learning rate: 0.0003125 #####
2025-02-04 18:40:04.670 INFO: ##### Step: 101 Learning rate: 0.0003125 #####
Epoch 62, Train Loss: 1.6944, Val Loss: 2.2641
2025-02-04 18:40:04.671 INFO: Epoch 62, Train Loss: 1.6944, Val Loss: 2.2641
train_e/atom_mae: 0.001021
2025-02-04 18:40:04.672 INFO: train_e/atom_mae: 0.001021
train_e/atom_rmse: 0.001401
2025-02-04 18:40:04.672 INFO: train_e/atom_rmse: 0.001401
train_f_mae: 0.026566
2025-02-04 18:40:04.675 INFO: train_f_mae: 0.026566
train_f_rmse: 0.040275
2025-02-04 18:40:04.675 INFO: train_f_rmse: 0.040275
val_e/atom_mae: 0.000575
2025-02-04 18:40:04.677 INFO: val_e/atom_mae: 0.000575
val_e/atom_rmse: 0.000777
2025-02-04 18:40:04.678 INFO: val_e/atom_rmse: 0.000777
val_f_mae: 0.027899
2025-02-04 18:40:04.678 INFO: val_f_mae: 0.027899
val_f_rmse: 0.047360
2025-02-04 18:40:04.678 INFO: val_f_rmse: 0.047360
##### Step: 102 Learning rate: 0.0003125 #####
2025-02-04 18:41:56.461 INFO: ##### Step: 102 Learning rate: 0.0003125 #####
Epoch 63, Train Loss: 1.6682, Val Loss: 2.2715
2025-02-04 18:41:56.461 INFO: Epoch 63, Train Loss: 1.6682, Val Loss: 2.2715
train_e/atom_mae: 0.000717
2025-02-04 18:41:56.462 INFO: train_e/atom_mae: 0.000717
train_e/atom_rmse: 0.001149
2025-02-04 18:41:56.462 INFO: train_e/atom_rmse: 0.001149
train_f_mae: 0.026506
2025-02-04 18:41:56.465 INFO: train_f_mae: 0.026506
train_f_rmse: 0.040244
2025-02-04 18:41:56.465 INFO: train_f_rmse: 0.040244
val_e/atom_mae: 0.000560
2025-02-04 18:41:56.467 INFO: val_e/atom_mae: 0.000560
val_e/atom_rmse: 0.000776
2025-02-04 18:41:56.468 INFO: val_e/atom_rmse: 0.000776
val_f_mae: 0.027902
2025-02-04 18:41:56.468 INFO: val_f_mae: 0.027902
val_f_rmse: 0.047440
2025-02-04 18:41:56.468 INFO: val_f_rmse: 0.047440
##### Step: 103 Learning rate: 0.0003125 #####
2025-02-04 18:43:48.135 INFO: ##### Step: 103 Learning rate: 0.0003125 #####
Epoch 64, Train Loss: 1.7217, Val Loss: 2.2619
2025-02-04 18:43:48.136 INFO: Epoch 64, Train Loss: 1.7217, Val Loss: 2.2619
train_e/atom_mae: 0.001216
2025-02-04 18:43:48.136 INFO: train_e/atom_mae: 0.001216
train_e/atom_rmse: 0.001652
2025-02-04 18:43:48.137 INFO: train_e/atom_rmse: 0.001652
train_f_mae: 0.026539
2025-02-04 18:43:48.139 INFO: train_f_mae: 0.026539
train_f_rmse: 0.040264
2025-02-04 18:43:48.139 INFO: train_f_rmse: 0.040264
val_e/atom_mae: 0.000552
2025-02-04 18:43:48.142 INFO: val_e/atom_mae: 0.000552
val_e/atom_rmse: 0.000781
2025-02-04 18:43:48.142 INFO: val_e/atom_rmse: 0.000781
val_f_mae: 0.027931
2025-02-04 18:43:48.146 INFO: val_f_mae: 0.027931
val_f_rmse: 0.047335
2025-02-04 18:43:48.146 INFO: val_f_rmse: 0.047335
##### Step: 104 Learning rate: 0.0003125 #####
2025-02-04 18:45:39.957 INFO: ##### Step: 104 Learning rate: 0.0003125 #####
Epoch 65, Train Loss: 1.6858, Val Loss: 2.2558
2025-02-04 18:45:39.958 INFO: Epoch 65, Train Loss: 1.6858, Val Loss: 2.2558
train_e/atom_mae: 0.000942
2025-02-04 18:45:39.959 INFO: train_e/atom_mae: 0.000942
train_e/atom_rmse: 0.001333
2025-02-04 18:45:39.959 INFO: train_e/atom_rmse: 0.001333
train_f_mae: 0.026498
2025-02-04 18:45:39.962 INFO: train_f_mae: 0.026498
train_f_rmse: 0.040253
2025-02-04 18:45:39.962 INFO: train_f_rmse: 0.040253
val_e/atom_mae: 0.000542
2025-02-04 18:45:39.964 INFO: val_e/atom_mae: 0.000542
val_e/atom_rmse: 0.000770
2025-02-04 18:45:39.964 INFO: val_e/atom_rmse: 0.000770
val_f_mae: 0.027867
2025-02-04 18:45:39.965 INFO: val_f_mae: 0.027867
val_f_rmse: 0.047278
2025-02-04 18:45:39.965 INFO: val_f_rmse: 0.047278
##### Step: 105 Learning rate: 0.0003125 #####
2025-02-04 18:47:31.832 INFO: ##### Step: 105 Learning rate: 0.0003125 #####
Epoch 66, Train Loss: 1.6804, Val Loss: 2.2530
2025-02-04 18:47:31.832 INFO: Epoch 66, Train Loss: 1.6804, Val Loss: 2.2530
train_e/atom_mae: 0.000980
2025-02-04 18:47:31.833 INFO: train_e/atom_mae: 0.000980
train_e/atom_rmse: 0.001352
2025-02-04 18:47:31.833 INFO: train_e/atom_rmse: 0.001352
train_f_mae: 0.026516
2025-02-04 18:47:31.836 INFO: train_f_mae: 0.026516
train_f_rmse: 0.040162
2025-02-04 18:47:31.836 INFO: train_f_rmse: 0.040162
val_e/atom_mae: 0.000549
2025-02-04 18:47:31.838 INFO: val_e/atom_mae: 0.000549
val_e/atom_rmse: 0.000770
2025-02-04 18:47:31.839 INFO: val_e/atom_rmse: 0.000770
val_f_mae: 0.027838
2025-02-04 18:47:31.839 INFO: val_f_mae: 0.027838
val_f_rmse: 0.047247
2025-02-04 18:47:31.839 INFO: val_f_rmse: 0.047247
##### Step: 106 Learning rate: 0.0003125 #####
2025-02-04 18:49:23.646 INFO: ##### Step: 106 Learning rate: 0.0003125 #####
Epoch 67, Train Loss: 1.6896, Val Loss: 2.2555
2025-02-04 18:49:23.646 INFO: Epoch 67, Train Loss: 1.6896, Val Loss: 2.2555
train_e/atom_mae: 0.000901
2025-02-04 18:49:23.647 INFO: train_e/atom_mae: 0.000901
train_e/atom_rmse: 0.001337
2025-02-04 18:49:23.647 INFO: train_e/atom_rmse: 0.001337
train_f_mae: 0.026588
2025-02-04 18:49:23.650 INFO: train_f_mae: 0.026588
train_f_rmse: 0.040296
2025-02-04 18:49:23.650 INFO: train_f_rmse: 0.040296
val_e/atom_mae: 0.000543
2025-02-04 18:49:23.652 INFO: val_e/atom_mae: 0.000543
val_e/atom_rmse: 0.000762
2025-02-04 18:49:23.653 INFO: val_e/atom_rmse: 0.000762
val_f_mae: 0.027852
2025-02-04 18:49:23.653 INFO: val_f_mae: 0.027852
val_f_rmse: 0.047279
2025-02-04 18:49:23.653 INFO: val_f_rmse: 0.047279
##### Step: 107 Learning rate: 0.0003125 #####
2025-02-04 18:51:15.472 INFO: ##### Step: 107 Learning rate: 0.0003125 #####
Epoch 68, Train Loss: 1.7074, Val Loss: 2.2480
2025-02-04 18:51:15.473 INFO: Epoch 68, Train Loss: 1.7074, Val Loss: 2.2480
train_e/atom_mae: 0.000945
2025-02-04 18:51:15.474 INFO: train_e/atom_mae: 0.000945
train_e/atom_rmse: 0.001388
2025-02-04 18:51:15.474 INFO: train_e/atom_rmse: 0.001388
train_f_mae: 0.026653
2025-02-04 18:51:15.476 INFO: train_f_mae: 0.026653
train_f_rmse: 0.040451
2025-02-04 18:51:15.477 INFO: train_f_rmse: 0.040451
val_e/atom_mae: 0.000555
2025-02-04 18:51:15.479 INFO: val_e/atom_mae: 0.000555
val_e/atom_rmse: 0.000773
2025-02-04 18:51:15.479 INFO: val_e/atom_rmse: 0.000773
val_f_mae: 0.027828
2025-02-04 18:51:15.480 INFO: val_f_mae: 0.027828
val_f_rmse: 0.047195
2025-02-04 18:51:15.480 INFO: val_f_rmse: 0.047195
##### Step: 108 Learning rate: 0.0003125 #####
2025-02-04 18:53:07.364 INFO: ##### Step: 108 Learning rate: 0.0003125 #####
Epoch 69, Train Loss: 1.6826, Val Loss: 2.2427
2025-02-04 18:53:07.365 INFO: Epoch 69, Train Loss: 1.6826, Val Loss: 2.2427
train_e/atom_mae: 0.000868
2025-02-04 18:53:07.366 INFO: train_e/atom_mae: 0.000868
train_e/atom_rmse: 0.001280
2025-02-04 18:53:07.366 INFO: train_e/atom_rmse: 0.001280
train_f_mae: 0.026569
2025-02-04 18:53:07.369 INFO: train_f_mae: 0.026569
train_f_rmse: 0.040277
2025-02-04 18:53:07.369 INFO: train_f_rmse: 0.040277
val_e/atom_mae: 0.000560
2025-02-04 18:53:07.371 INFO: val_e/atom_mae: 0.000560
val_e/atom_rmse: 0.000784
2025-02-04 18:53:07.371 INFO: val_e/atom_rmse: 0.000784
val_f_mae: 0.027853
2025-02-04 18:53:07.372 INFO: val_f_mae: 0.027853
val_f_rmse: 0.047131
2025-02-04 18:53:07.372 INFO: val_f_rmse: 0.047131
##### Step: 109 Learning rate: 0.0003125 #####
2025-02-04 18:54:59.122 INFO: ##### Step: 109 Learning rate: 0.0003125 #####
Epoch 70, Train Loss: 1.6903, Val Loss: 2.2490
2025-02-04 18:54:59.123 INFO: Epoch 70, Train Loss: 1.6903, Val Loss: 2.2490
train_e/atom_mae: 0.001002
2025-02-04 18:54:59.124 INFO: train_e/atom_mae: 0.001002
train_e/atom_rmse: 0.001401
2025-02-04 18:54:59.124 INFO: train_e/atom_rmse: 0.001401
train_f_mae: 0.026481
2025-02-04 18:54:59.126 INFO: train_f_mae: 0.026481
train_f_rmse: 0.040223
2025-02-04 18:54:59.127 INFO: train_f_rmse: 0.040223
val_e/atom_mae: 0.000567
2025-02-04 18:54:59.129 INFO: val_e/atom_mae: 0.000567
val_e/atom_rmse: 0.000781
2025-02-04 18:54:59.129 INFO: val_e/atom_rmse: 0.000781
val_f_mae: 0.027830
2025-02-04 18:54:59.130 INFO: val_f_mae: 0.027830
val_f_rmse: 0.047199
2025-02-04 18:54:59.130 INFO: val_f_rmse: 0.047199
##### Step: 110 Learning rate: 0.0003125 #####
2025-02-04 18:56:51.063 INFO: ##### Step: 110 Learning rate: 0.0003125 #####
Epoch 71, Train Loss: 1.7071, Val Loss: 2.2459
2025-02-04 18:56:51.064 INFO: Epoch 71, Train Loss: 1.7071, Val Loss: 2.2459
train_e/atom_mae: 0.001073
2025-02-04 18:56:51.065 INFO: train_e/atom_mae: 0.001073
train_e/atom_rmse: 0.001522
2025-02-04 18:56:51.065 INFO: train_e/atom_rmse: 0.001522
train_f_mae: 0.026594
2025-02-04 18:56:51.068 INFO: train_f_mae: 0.026594
train_f_rmse: 0.040271
2025-02-04 18:56:51.068 INFO: train_f_rmse: 0.040271
val_e/atom_mae: 0.000589
2025-02-04 18:56:51.070 INFO: val_e/atom_mae: 0.000589
val_e/atom_rmse: 0.000802
2025-02-04 18:56:51.070 INFO: val_e/atom_rmse: 0.000802
val_f_mae: 0.027811
2025-02-04 18:56:51.071 INFO: val_f_mae: 0.027811
val_f_rmse: 0.047154
2025-02-04 18:56:51.071 INFO: val_f_rmse: 0.047154
##### Step: 111 Learning rate: 0.0003125 #####
2025-02-04 18:58:42.897 INFO: ##### Step: 111 Learning rate: 0.0003125 #####
Epoch 72, Train Loss: 1.7014, Val Loss: 2.2402
2025-02-04 18:58:42.898 INFO: Epoch 72, Train Loss: 1.7014, Val Loss: 2.2402
train_e/atom_mae: 0.001147
2025-02-04 18:58:42.899 INFO: train_e/atom_mae: 0.001147
train_e/atom_rmse: 0.001517
2025-02-04 18:58:42.899 INFO: train_e/atom_rmse: 0.001517
train_f_mae: 0.026555
2025-02-04 18:58:42.902 INFO: train_f_mae: 0.026555
train_f_rmse: 0.040206
2025-02-04 18:58:42.902 INFO: train_f_rmse: 0.040206
val_e/atom_mae: 0.000579
2025-02-04 18:58:42.904 INFO: val_e/atom_mae: 0.000579
val_e/atom_rmse: 0.000784
2025-02-04 18:58:42.904 INFO: val_e/atom_rmse: 0.000784
val_f_mae: 0.027804
2025-02-04 18:58:42.905 INFO: val_f_mae: 0.027804
val_f_rmse: 0.047104
2025-02-04 18:58:42.905 INFO: val_f_rmse: 0.047104
##### Step: 112 Learning rate: 0.0003125 #####
2025-02-04 19:00:34.658 INFO: ##### Step: 112 Learning rate: 0.0003125 #####
Epoch 73, Train Loss: 1.6760, Val Loss: 2.2542
2025-02-04 19:00:34.658 INFO: Epoch 73, Train Loss: 1.6760, Val Loss: 2.2542
train_e/atom_mae: 0.000895
2025-02-04 19:00:34.659 INFO: train_e/atom_mae: 0.000895
train_e/atom_rmse: 0.001343
2025-02-04 19:00:34.659 INFO: train_e/atom_rmse: 0.001343
train_f_mae: 0.026437
2025-02-04 19:00:34.662 INFO: train_f_mae: 0.026437
train_f_rmse: 0.040118
2025-02-04 19:00:34.662 INFO: train_f_rmse: 0.040118
val_e/atom_mae: 0.000538
2025-02-04 19:00:34.664 INFO: val_e/atom_mae: 0.000538
val_e/atom_rmse: 0.000752
2025-02-04 19:00:34.664 INFO: val_e/atom_rmse: 0.000752
val_f_mae: 0.027815
2025-02-04 19:00:34.665 INFO: val_f_mae: 0.027815
val_f_rmse: 0.047273
2025-02-04 19:00:34.665 INFO: val_f_rmse: 0.047273
##### Step: 113 Learning rate: 0.0003125 #####
2025-02-04 19:02:26.302 INFO: ##### Step: 113 Learning rate: 0.0003125 #####
Epoch 74, Train Loss: 1.6610, Val Loss: 2.2442
2025-02-04 19:02:26.302 INFO: Epoch 74, Train Loss: 1.6610, Val Loss: 2.2442
train_e/atom_mae: 0.000752
2025-02-04 19:02:26.303 INFO: train_e/atom_mae: 0.000752
train_e/atom_rmse: 0.001197
2025-02-04 19:02:26.303 INFO: train_e/atom_rmse: 0.001197
train_f_mae: 0.026429
2025-02-04 19:02:26.306 INFO: train_f_mae: 0.026429
train_f_rmse: 0.040102
2025-02-04 19:02:26.306 INFO: train_f_rmse: 0.040102
val_e/atom_mae: 0.000539
2025-02-04 19:02:26.308 INFO: val_e/atom_mae: 0.000539
val_e/atom_rmse: 0.000750
2025-02-04 19:02:26.309 INFO: val_e/atom_rmse: 0.000750
val_f_mae: 0.027785
2025-02-04 19:02:26.313 INFO: val_f_mae: 0.027785
val_f_rmse: 0.047168
2025-02-04 19:02:26.313 INFO: val_f_rmse: 0.047168
##### Step: 114 Learning rate: 0.0003125 #####
2025-02-04 19:04:17.947 INFO: ##### Step: 114 Learning rate: 0.0003125 #####
Epoch 75, Train Loss: 1.6929, Val Loss: 2.2443
2025-02-04 19:04:17.947 INFO: Epoch 75, Train Loss: 1.6929, Val Loss: 2.2443
train_e/atom_mae: 0.000865
2025-02-04 19:04:17.948 INFO: train_e/atom_mae: 0.000865
train_e/atom_rmse: 0.001259
2025-02-04 19:04:17.948 INFO: train_e/atom_rmse: 0.001259
train_f_mae: 0.026671
2025-02-04 19:04:17.951 INFO: train_f_mae: 0.026671
train_f_rmse: 0.040428
2025-02-04 19:04:17.951 INFO: train_f_rmse: 0.040428
val_e/atom_mae: 0.000571
2025-02-04 19:04:17.953 INFO: val_e/atom_mae: 0.000571
val_e/atom_rmse: 0.000786
2025-02-04 19:04:17.953 INFO: val_e/atom_rmse: 0.000786
val_f_mae: 0.027763
2025-02-04 19:04:17.954 INFO: val_f_mae: 0.027763
val_f_rmse: 0.047149
2025-02-04 19:04:17.954 INFO: val_f_rmse: 0.047149
##### Step: 115 Learning rate: 0.0003125 #####
2025-02-04 19:06:09.602 INFO: ##### Step: 115 Learning rate: 0.0003125 #####
Epoch 76, Train Loss: 1.7257, Val Loss: 2.2544
2025-02-04 19:06:09.603 INFO: Epoch 76, Train Loss: 1.7257, Val Loss: 2.2544
train_e/atom_mae: 0.001171
2025-02-04 19:06:09.604 INFO: train_e/atom_mae: 0.001171
train_e/atom_rmse: 0.001623
2025-02-04 19:06:09.604 INFO: train_e/atom_rmse: 0.001623
train_f_mae: 0.026647
2025-02-04 19:06:09.607 INFO: train_f_mae: 0.026647
train_f_rmse: 0.040356
2025-02-04 19:06:09.607 INFO: train_f_rmse: 0.040356
val_e/atom_mae: 0.000541
2025-02-04 19:06:09.609 INFO: val_e/atom_mae: 0.000541
val_e/atom_rmse: 0.000768
2025-02-04 19:06:09.610 INFO: val_e/atom_rmse: 0.000768
val_f_mae: 0.027788
2025-02-04 19:06:09.610 INFO: val_f_mae: 0.027788
val_f_rmse: 0.047266
2025-02-04 19:06:09.610 INFO: val_f_rmse: 0.047266
##### Step: 116 Learning rate: 0.0003125 #####
2025-02-04 19:08:01.433 INFO: ##### Step: 116 Learning rate: 0.0003125 #####
Epoch 77, Train Loss: 1.6701, Val Loss: 2.2530
2025-02-04 19:08:01.434 INFO: Epoch 77, Train Loss: 1.6701, Val Loss: 2.2530
train_e/atom_mae: 0.000823
2025-02-04 19:08:01.435 INFO: train_e/atom_mae: 0.000823
train_e/atom_rmse: 0.001245
2025-02-04 19:08:01.435 INFO: train_e/atom_rmse: 0.001245
train_f_mae: 0.026554
2025-02-04 19:08:01.441 INFO: train_f_mae: 0.026554
train_f_rmse: 0.040162
2025-02-04 19:08:01.441 INFO: train_f_rmse: 0.040162
val_e/atom_mae: 0.000550
2025-02-04 19:08:01.444 INFO: val_e/atom_mae: 0.000550
val_e/atom_rmse: 0.000780
2025-02-04 19:08:01.444 INFO: val_e/atom_rmse: 0.000780
val_f_mae: 0.027761
2025-02-04 19:08:01.444 INFO: val_f_mae: 0.027761
val_f_rmse: 0.047242
2025-02-04 19:08:01.445 INFO: val_f_rmse: 0.047242
##### Step: 117 Learning rate: 0.0003125 #####
2025-02-04 19:09:53.287 INFO: ##### Step: 117 Learning rate: 0.0003125 #####
Epoch 78, Train Loss: 1.6740, Val Loss: 2.2366
2025-02-04 19:09:53.287 INFO: Epoch 78, Train Loss: 1.6740, Val Loss: 2.2366
train_e/atom_mae: 0.000994
2025-02-04 19:09:53.288 INFO: train_e/atom_mae: 0.000994
train_e/atom_rmse: 0.001414
2025-02-04 19:09:53.288 INFO: train_e/atom_rmse: 0.001414
train_f_mae: 0.026387
2025-02-04 19:09:53.291 INFO: train_f_mae: 0.026387
train_f_rmse: 0.040005
2025-02-04 19:09:53.291 INFO: train_f_rmse: 0.040005
val_e/atom_mae: 0.000557
2025-02-04 19:09:53.293 INFO: val_e/atom_mae: 0.000557
val_e/atom_rmse: 0.000776
2025-02-04 19:09:53.294 INFO: val_e/atom_rmse: 0.000776
val_f_mae: 0.027750
2025-02-04 19:09:53.294 INFO: val_f_mae: 0.027750
val_f_rmse: 0.047074
2025-02-04 19:09:53.295 INFO: val_f_rmse: 0.047074
##### Step: 118 Learning rate: 0.0003125 #####
2025-02-04 19:11:44.862 INFO: ##### Step: 118 Learning rate: 0.0003125 #####
Epoch 79, Train Loss: 1.6719, Val Loss: 2.2498
2025-02-04 19:11:44.863 INFO: Epoch 79, Train Loss: 1.6719, Val Loss: 2.2498
train_e/atom_mae: 0.000847
2025-02-04 19:11:44.864 INFO: train_e/atom_mae: 0.000847
train_e/atom_rmse: 0.001288
2025-02-04 19:11:44.864 INFO: train_e/atom_rmse: 0.001288
train_f_mae: 0.026448
2025-02-04 19:11:44.866 INFO: train_f_mae: 0.026448
train_f_rmse: 0.040134
2025-02-04 19:11:44.867 INFO: train_f_rmse: 0.040134
val_e/atom_mae: 0.000545
2025-02-04 19:11:44.869 INFO: val_e/atom_mae: 0.000545
val_e/atom_rmse: 0.000765
2025-02-04 19:11:44.869 INFO: val_e/atom_rmse: 0.000765
val_f_mae: 0.027770
2025-02-04 19:11:44.870 INFO: val_f_mae: 0.027770
val_f_rmse: 0.047218
2025-02-04 19:11:44.870 INFO: val_f_rmse: 0.047218
##### Step: 119 Learning rate: 0.0003125 #####
2025-02-04 19:13:36.651 INFO: ##### Step: 119 Learning rate: 0.0003125 #####
Epoch 80, Train Loss: 1.6885, Val Loss: 2.2442
2025-02-04 19:13:36.652 INFO: Epoch 80, Train Loss: 1.6885, Val Loss: 2.2442
train_e/atom_mae: 0.001007
2025-02-04 19:13:36.653 INFO: train_e/atom_mae: 0.001007
train_e/atom_rmse: 0.001410
2025-02-04 19:13:36.653 INFO: train_e/atom_rmse: 0.001410
train_f_mae: 0.026549
2025-02-04 19:13:36.656 INFO: train_f_mae: 0.026549
train_f_rmse: 0.040190
2025-02-04 19:13:36.656 INFO: train_f_rmse: 0.040190
val_e/atom_mae: 0.000550
2025-02-04 19:13:36.658 INFO: val_e/atom_mae: 0.000550
val_e/atom_rmse: 0.000761
2025-02-04 19:13:36.658 INFO: val_e/atom_rmse: 0.000761
val_f_mae: 0.027745
2025-02-04 19:13:36.659 INFO: val_f_mae: 0.027745
val_f_rmse: 0.047162
2025-02-04 19:13:36.659 INFO: val_f_rmse: 0.047162
##### Step: 120 Learning rate: 0.00015625 #####
2025-02-04 19:15:28.573 INFO: ##### Step: 120 Learning rate: 0.00015625 #####
Epoch 81, Train Loss: 1.6400, Val Loss: 2.2307
2025-02-04 19:15:28.574 INFO: Epoch 81, Train Loss: 1.6400, Val Loss: 2.2307
train_e/atom_mae: 0.000764
2025-02-04 19:15:28.575 INFO: train_e/atom_mae: 0.000764
train_e/atom_rmse: 0.001192
2025-02-04 19:15:28.575 INFO: train_e/atom_rmse: 0.001192
train_f_mae: 0.026249
2025-02-04 19:15:28.578 INFO: train_f_mae: 0.026249
train_f_rmse: 0.039845
2025-02-04 19:15:28.578 INFO: train_f_rmse: 0.039845
val_e/atom_mae: 0.000531
2025-02-04 19:15:28.580 INFO: val_e/atom_mae: 0.000531
val_e/atom_rmse: 0.000748
2025-02-04 19:15:28.581 INFO: val_e/atom_rmse: 0.000748
val_f_mae: 0.027721
2025-02-04 19:15:28.581 INFO: val_f_mae: 0.027721
val_f_rmse: 0.047024
2025-02-04 19:15:28.581 INFO: val_f_rmse: 0.047024
##### Step: 121 Learning rate: 0.00015625 #####
2025-02-04 19:17:20.263 INFO: ##### Step: 121 Learning rate: 0.00015625 #####
Epoch 82, Train Loss: 1.6574, Val Loss: 2.2456
2025-02-04 19:17:20.264 INFO: Epoch 82, Train Loss: 1.6574, Val Loss: 2.2456
train_e/atom_mae: 0.000877
2025-02-04 19:17:20.265 INFO: train_e/atom_mae: 0.000877
train_e/atom_rmse: 0.001279
2025-02-04 19:17:20.265 INFO: train_e/atom_rmse: 0.001279
train_f_mae: 0.026370
2025-02-04 19:17:20.268 INFO: train_f_mae: 0.026370
train_f_rmse: 0.039964
2025-02-04 19:17:20.268 INFO: train_f_rmse: 0.039964
val_e/atom_mae: 0.000550
2025-02-04 19:17:20.270 INFO: val_e/atom_mae: 0.000550
val_e/atom_rmse: 0.000764
2025-02-04 19:17:20.270 INFO: val_e/atom_rmse: 0.000764
val_f_mae: 0.027757
2025-02-04 19:17:20.271 INFO: val_f_mae: 0.027757
val_f_rmse: 0.047173
2025-02-04 19:17:20.271 INFO: val_f_rmse: 0.047173
##### Step: 122 Learning rate: 0.00015625 #####
2025-02-04 19:19:11.701 INFO: ##### Step: 122 Learning rate: 0.00015625 #####
Epoch 83, Train Loss: 1.6404, Val Loss: 2.2354
2025-02-04 19:19:11.702 INFO: Epoch 83, Train Loss: 1.6404, Val Loss: 2.2354
train_e/atom_mae: 0.000779
2025-02-04 19:19:11.703 INFO: train_e/atom_mae: 0.000779
train_e/atom_rmse: 0.001196
2025-02-04 19:19:11.703 INFO: train_e/atom_rmse: 0.001196
train_f_mae: 0.026213
2025-02-04 19:19:11.706 INFO: train_f_mae: 0.026213
train_f_rmse: 0.039846
2025-02-04 19:19:11.706 INFO: train_f_rmse: 0.039846
val_e/atom_mae: 0.000540
2025-02-04 19:19:11.708 INFO: val_e/atom_mae: 0.000540
val_e/atom_rmse: 0.000751
2025-02-04 19:19:11.708 INFO: val_e/atom_rmse: 0.000751
val_f_mae: 0.027715
2025-02-04 19:19:11.709 INFO: val_f_mae: 0.027715
val_f_rmse: 0.047073
2025-02-04 19:19:11.709 INFO: val_f_rmse: 0.047073
##### Step: 123 Learning rate: 0.00015625 #####
2025-02-04 19:21:03.407 INFO: ##### Step: 123 Learning rate: 0.00015625 #####
Epoch 84, Train Loss: 1.6273, Val Loss: 2.2442
2025-02-04 19:21:03.408 INFO: Epoch 84, Train Loss: 1.6273, Val Loss: 2.2442
train_e/atom_mae: 0.000664
2025-02-04 19:21:03.409 INFO: train_e/atom_mae: 0.000664
train_e/atom_rmse: 0.001111
2025-02-04 19:21:03.409 INFO: train_e/atom_rmse: 0.001111
train_f_mae: 0.026187
2025-02-04 19:21:03.412 INFO: train_f_mae: 0.026187
train_f_rmse: 0.039771
2025-02-04 19:21:03.412 INFO: train_f_rmse: 0.039771
val_e/atom_mae: 0.000550
2025-02-04 19:21:03.414 INFO: val_e/atom_mae: 0.000550
val_e/atom_rmse: 0.000777
2025-02-04 19:21:03.414 INFO: val_e/atom_rmse: 0.000777
val_f_mae: 0.027778
2025-02-04 19:21:03.415 INFO: val_f_mae: 0.027778
val_f_rmse: 0.047151
2025-02-04 19:21:03.415 INFO: val_f_rmse: 0.047151
##### Step: 124 Learning rate: 0.00015625 #####
2025-02-04 19:22:55.207 INFO: ##### Step: 124 Learning rate: 0.00015625 #####
Epoch 85, Train Loss: 1.6355, Val Loss: 2.2350
2025-02-04 19:22:55.208 INFO: Epoch 85, Train Loss: 1.6355, Val Loss: 2.2350
train_e/atom_mae: 0.000666
2025-02-04 19:22:55.209 INFO: train_e/atom_mae: 0.000666
train_e/atom_rmse: 0.001100
2025-02-04 19:22:55.209 INFO: train_e/atom_rmse: 0.001100
train_f_mae: 0.026194
2025-02-04 19:22:55.212 INFO: train_f_mae: 0.026194
train_f_rmse: 0.039886
2025-02-04 19:22:55.212 INFO: train_f_rmse: 0.039886
val_e/atom_mae: 0.000536
2025-02-04 19:22:55.214 INFO: val_e/atom_mae: 0.000536
val_e/atom_rmse: 0.000753
2025-02-04 19:22:55.214 INFO: val_e/atom_rmse: 0.000753
val_f_mae: 0.027707
2025-02-04 19:22:55.215 INFO: val_f_mae: 0.027707
val_f_rmse: 0.047068
2025-02-04 19:22:55.215 INFO: val_f_rmse: 0.047068
##### Step: 125 Learning rate: 0.00015625 #####
2025-02-04 19:24:46.833 INFO: ##### Step: 125 Learning rate: 0.00015625 #####
Epoch 86, Train Loss: 1.6326, Val Loss: 2.2423
2025-02-04 19:24:46.834 INFO: Epoch 86, Train Loss: 1.6326, Val Loss: 2.2423
train_e/atom_mae: 0.000761
2025-02-04 19:24:46.835 INFO: train_e/atom_mae: 0.000761
train_e/atom_rmse: 0.001164
2025-02-04 19:24:46.835 INFO: train_e/atom_rmse: 0.001164
train_f_mae: 0.026153
2025-02-04 19:24:46.838 INFO: train_f_mae: 0.026153
train_f_rmse: 0.039783
2025-02-04 19:24:46.838 INFO: train_f_rmse: 0.039783
val_e/atom_mae: 0.000540
2025-02-04 19:24:46.840 INFO: val_e/atom_mae: 0.000540
val_e/atom_rmse: 0.000762
2025-02-04 19:24:46.840 INFO: val_e/atom_rmse: 0.000762
val_f_mae: 0.027712
2025-02-04 19:24:46.841 INFO: val_f_mae: 0.027712
val_f_rmse: 0.047140
2025-02-04 19:24:46.841 INFO: val_f_rmse: 0.047140
##### Step: 126 Learning rate: 0.00015625 #####
2025-02-04 19:26:38.727 INFO: ##### Step: 126 Learning rate: 0.00015625 #####
Epoch 87, Train Loss: 1.6390, Val Loss: 2.2362
2025-02-04 19:26:38.728 INFO: Epoch 87, Train Loss: 1.6390, Val Loss: 2.2362
train_e/atom_mae: 0.000732
2025-02-04 19:26:38.728 INFO: train_e/atom_mae: 0.000732
train_e/atom_rmse: 0.001154
2025-02-04 19:26:38.729 INFO: train_e/atom_rmse: 0.001154
train_f_mae: 0.026187
2025-02-04 19:26:38.731 INFO: train_f_mae: 0.026187
train_f_rmse: 0.039874
2025-02-04 19:26:38.732 INFO: train_f_rmse: 0.039874
val_e/atom_mae: 0.000540
2025-02-04 19:26:38.734 INFO: val_e/atom_mae: 0.000540
val_e/atom_rmse: 0.000755
2025-02-04 19:26:38.734 INFO: val_e/atom_rmse: 0.000755
val_f_mae: 0.027699
2025-02-04 19:26:38.734 INFO: val_f_mae: 0.027699
val_f_rmse: 0.047081
2025-02-04 19:26:38.735 INFO: val_f_rmse: 0.047081
##### Step: 127 Learning rate: 0.00015625 #####
2025-02-04 19:28:30.713 INFO: ##### Step: 127 Learning rate: 0.00015625 #####
Epoch 88, Train Loss: 1.6404, Val Loss: 2.2403
2025-02-04 19:28:30.713 INFO: Epoch 88, Train Loss: 1.6404, Val Loss: 2.2403
train_e/atom_mae: 0.000800
2025-02-04 19:28:30.714 INFO: train_e/atom_mae: 0.000800
train_e/atom_rmse: 0.001224
2025-02-04 19:28:30.714 INFO: train_e/atom_rmse: 0.001224
train_f_mae: 0.026188
2025-02-04 19:28:30.717 INFO: train_f_mae: 0.026188
train_f_rmse: 0.039813
2025-02-04 19:28:30.717 INFO: train_f_rmse: 0.039813
val_e/atom_mae: 0.000542
2025-02-04 19:28:30.719 INFO: val_e/atom_mae: 0.000542
val_e/atom_rmse: 0.000764
2025-02-04 19:28:30.720 INFO: val_e/atom_rmse: 0.000764
val_f_mae: 0.027687
2025-02-04 19:28:30.720 INFO: val_f_mae: 0.027687
val_f_rmse: 0.047119
2025-02-04 19:28:30.720 INFO: val_f_rmse: 0.047119
##### Step: 128 Learning rate: 0.00015625 #####
2025-02-04 19:30:22.555 INFO: ##### Step: 128 Learning rate: 0.00015625 #####
Epoch 89, Train Loss: 1.6451, Val Loss: 2.2401
2025-02-04 19:30:22.556 INFO: Epoch 89, Train Loss: 1.6451, Val Loss: 2.2401
train_e/atom_mae: 0.000887
2025-02-04 19:30:22.557 INFO: train_e/atom_mae: 0.000887
train_e/atom_rmse: 0.001275
2025-02-04 19:30:22.557 INFO: train_e/atom_rmse: 0.001275
train_f_mae: 0.026204
2025-02-04 19:30:22.560 INFO: train_f_mae: 0.026204
train_f_rmse: 0.039815
2025-02-04 19:30:22.560 INFO: train_f_rmse: 0.039815
val_e/atom_mae: 0.000549
2025-02-04 19:30:22.562 INFO: val_e/atom_mae: 0.000549
val_e/atom_rmse: 0.000760
2025-02-04 19:30:22.563 INFO: val_e/atom_rmse: 0.000760
val_f_mae: 0.027707
2025-02-04 19:30:22.563 INFO: val_f_mae: 0.027707
val_f_rmse: 0.047117
2025-02-04 19:30:22.563 INFO: val_f_rmse: 0.047117
##### Step: 129 Learning rate: 0.00015625 #####
2025-02-04 19:32:14.628 INFO: ##### Step: 129 Learning rate: 0.00015625 #####
Epoch 90, Train Loss: 1.6348, Val Loss: 2.2480
2025-02-04 19:32:14.628 INFO: Epoch 90, Train Loss: 1.6348, Val Loss: 2.2480
train_e/atom_mae: 0.000763
2025-02-04 19:32:14.629 INFO: train_e/atom_mae: 0.000763
train_e/atom_rmse: 0.001164
2025-02-04 19:32:14.629 INFO: train_e/atom_rmse: 0.001164
train_f_mae: 0.026158
2025-02-04 19:32:14.632 INFO: train_f_mae: 0.026158
train_f_rmse: 0.039810
2025-02-04 19:32:14.632 INFO: train_f_rmse: 0.039810
val_e/atom_mae: 0.000539
2025-02-04 19:32:14.634 INFO: val_e/atom_mae: 0.000539
val_e/atom_rmse: 0.000769
2025-02-04 19:32:14.635 INFO: val_e/atom_rmse: 0.000769
val_f_mae: 0.027688
2025-02-04 19:32:14.635 INFO: val_f_mae: 0.027688
val_f_rmse: 0.047197
2025-02-04 19:32:14.636 INFO: val_f_rmse: 0.047197
##### Step: 130 Learning rate: 0.00015625 #####
2025-02-04 19:34:06.572 INFO: ##### Step: 130 Learning rate: 0.00015625 #####
Epoch 91, Train Loss: 1.6422, Val Loss: 2.2398
2025-02-04 19:34:06.573 INFO: Epoch 91, Train Loss: 1.6422, Val Loss: 2.2398
train_e/atom_mae: 0.000843
2025-02-04 19:34:06.574 INFO: train_e/atom_mae: 0.000843
train_e/atom_rmse: 0.001229
2025-02-04 19:34:06.574 INFO: train_e/atom_rmse: 0.001229
train_f_mae: 0.026228
2025-02-04 19:34:06.577 INFO: train_f_mae: 0.026228
train_f_rmse: 0.039831
2025-02-04 19:34:06.577 INFO: train_f_rmse: 0.039831
val_e/atom_mae: 0.000558
2025-02-04 19:34:06.579 INFO: val_e/atom_mae: 0.000558
val_e/atom_rmse: 0.000782
2025-02-04 19:34:06.579 INFO: val_e/atom_rmse: 0.000782
val_f_mae: 0.027679
2025-02-04 19:34:06.580 INFO: val_f_mae: 0.027679
val_f_rmse: 0.047102
2025-02-04 19:34:06.580 INFO: val_f_rmse: 0.047102
##### Step: 131 Learning rate: 0.00015625 #####
2025-02-04 19:35:58.724 INFO: ##### Step: 131 Learning rate: 0.00015625 #####
Epoch 92, Train Loss: 1.6609, Val Loss: 2.2418
2025-02-04 19:35:58.725 INFO: Epoch 92, Train Loss: 1.6609, Val Loss: 2.2418
train_e/atom_mae: 0.001114
2025-02-04 19:35:58.726 INFO: train_e/atom_mae: 0.001114
train_e/atom_rmse: 0.001479
2025-02-04 19:35:58.726 INFO: train_e/atom_rmse: 0.001479
train_f_mae: 0.026185
2025-02-04 19:35:58.729 INFO: train_f_mae: 0.026185
train_f_rmse: 0.039752
2025-02-04 19:35:58.729 INFO: train_f_rmse: 0.039752
val_e/atom_mae: 0.000555
2025-02-04 19:35:58.731 INFO: val_e/atom_mae: 0.000555
val_e/atom_rmse: 0.000773
2025-02-04 19:35:58.732 INFO: val_e/atom_rmse: 0.000773
val_f_mae: 0.027679
2025-02-04 19:35:58.732 INFO: val_f_mae: 0.027679
val_f_rmse: 0.047129
2025-02-04 19:35:58.732 INFO: val_f_rmse: 0.047129
##### Step: 132 Learning rate: 0.00015625 #####
2025-02-04 19:37:50.514 INFO: ##### Step: 132 Learning rate: 0.00015625 #####
Epoch 93, Train Loss: 1.6507, Val Loss: 2.2371
2025-02-04 19:37:50.515 INFO: Epoch 93, Train Loss: 1.6507, Val Loss: 2.2371
train_e/atom_mae: 0.000905
2025-02-04 19:37:50.516 INFO: train_e/atom_mae: 0.000905
train_e/atom_rmse: 0.001278
2025-02-04 19:37:50.516 INFO: train_e/atom_rmse: 0.001278
train_f_mae: 0.026249
2025-02-04 19:37:50.519 INFO: train_f_mae: 0.026249
train_f_rmse: 0.039881
2025-02-04 19:37:50.519 INFO: train_f_rmse: 0.039881
val_e/atom_mae: 0.000539
2025-02-04 19:37:50.521 INFO: val_e/atom_mae: 0.000539
val_e/atom_rmse: 0.000756
2025-02-04 19:37:50.522 INFO: val_e/atom_rmse: 0.000756
val_f_mae: 0.027666
2025-02-04 19:37:50.522 INFO: val_f_mae: 0.027666
val_f_rmse: 0.047089
2025-02-04 19:37:50.522 INFO: val_f_rmse: 0.047089
##### Step: 133 Learning rate: 0.00015625 #####
2025-02-04 19:39:42.393 INFO: ##### Step: 133 Learning rate: 0.00015625 #####
Epoch 94, Train Loss: 1.6314, Val Loss: 2.2388
2025-02-04 19:39:42.394 INFO: Epoch 94, Train Loss: 1.6314, Val Loss: 2.2388
train_e/atom_mae: 0.000739
2025-02-04 19:39:42.395 INFO: train_e/atom_mae: 0.000739
train_e/atom_rmse: 0.001150
2025-02-04 19:39:42.395 INFO: train_e/atom_rmse: 0.001150
train_f_mae: 0.026128
2025-02-04 19:39:42.398 INFO: train_f_mae: 0.026128
train_f_rmse: 0.039783
2025-02-04 19:39:42.398 INFO: train_f_rmse: 0.039783
val_e/atom_mae: 0.000539
2025-02-04 19:39:42.400 INFO: val_e/atom_mae: 0.000539
val_e/atom_rmse: 0.000758
2025-02-04 19:39:42.401 INFO: val_e/atom_rmse: 0.000758
val_f_mae: 0.027676
2025-02-04 19:39:42.401 INFO: val_f_mae: 0.027676
val_f_rmse: 0.047106
2025-02-04 19:39:42.401 INFO: val_f_rmse: 0.047106
##### Step: 134 Learning rate: 0.00015625 #####
2025-02-04 19:41:34.214 INFO: ##### Step: 134 Learning rate: 0.00015625 #####
Epoch 95, Train Loss: 1.6268, Val Loss: 2.2391
2025-02-04 19:41:34.215 INFO: Epoch 95, Train Loss: 1.6268, Val Loss: 2.2391
train_e/atom_mae: 0.000674
2025-02-04 19:41:34.215 INFO: train_e/atom_mae: 0.000674
train_e/atom_rmse: 0.001122
2025-02-04 19:41:34.216 INFO: train_e/atom_rmse: 0.001122
train_f_mae: 0.026150
2025-02-04 19:41:34.218 INFO: train_f_mae: 0.026150
train_f_rmse: 0.039754
2025-02-04 19:41:34.219 INFO: train_f_rmse: 0.039754
val_e/atom_mae: 0.000533
2025-02-04 19:41:34.221 INFO: val_e/atom_mae: 0.000533
val_e/atom_rmse: 0.000750
2025-02-04 19:41:34.221 INFO: val_e/atom_rmse: 0.000750
val_f_mae: 0.027668
2025-02-04 19:41:34.221 INFO: val_f_mae: 0.027668
val_f_rmse: 0.047113
2025-02-04 19:41:34.222 INFO: val_f_rmse: 0.047113
##### Step: 135 Learning rate: 0.00015625 #####
2025-02-04 19:43:26.383 INFO: ##### Step: 135 Learning rate: 0.00015625 #####
Epoch 96, Train Loss: 1.6305, Val Loss: 2.2499
2025-02-04 19:43:26.383 INFO: Epoch 96, Train Loss: 1.6305, Val Loss: 2.2499
train_e/atom_mae: 0.000719
2025-02-04 19:43:26.384 INFO: train_e/atom_mae: 0.000719
train_e/atom_rmse: 0.001140
2025-02-04 19:43:26.385 INFO: train_e/atom_rmse: 0.001140
train_f_mae: 0.026118
2025-02-04 19:43:26.387 INFO: train_f_mae: 0.026118
train_f_rmse: 0.039782
2025-02-04 19:43:26.387 INFO: train_f_rmse: 0.039782
val_e/atom_mae: 0.000545
2025-02-04 19:43:26.390 INFO: val_e/atom_mae: 0.000545
val_e/atom_rmse: 0.000769
2025-02-04 19:43:26.390 INFO: val_e/atom_rmse: 0.000769
val_f_mae: 0.027670
2025-02-04 19:43:26.390 INFO: val_f_mae: 0.027670
val_f_rmse: 0.047218
2025-02-04 19:43:26.391 INFO: val_f_rmse: 0.047218
##### Step: 136 Learning rate: 0.00015625 #####
2025-02-04 19:45:18.430 INFO: ##### Step: 136 Learning rate: 0.00015625 #####
Epoch 97, Train Loss: 1.6343, Val Loss: 2.2455
2025-02-04 19:45:18.430 INFO: Epoch 97, Train Loss: 1.6343, Val Loss: 2.2455
train_e/atom_mae: 0.000797
2025-02-04 19:45:18.431 INFO: train_e/atom_mae: 0.000797
train_e/atom_rmse: 0.001218
2025-02-04 19:45:18.431 INFO: train_e/atom_rmse: 0.001218
train_f_mae: 0.026121
2025-02-04 19:45:18.434 INFO: train_f_mae: 0.026121
train_f_rmse: 0.039744
2025-02-04 19:45:18.434 INFO: train_f_rmse: 0.039744
val_e/atom_mae: 0.000547
2025-02-04 19:45:18.436 INFO: val_e/atom_mae: 0.000547
val_e/atom_rmse: 0.000761
2025-02-04 19:45:18.436 INFO: val_e/atom_rmse: 0.000761
val_f_mae: 0.027658
2025-02-04 19:45:18.437 INFO: val_f_mae: 0.027658
val_f_rmse: 0.047176
2025-02-04 19:45:18.437 INFO: val_f_rmse: 0.047176
##### Step: 137 Learning rate: 0.00015625 #####
2025-02-04 19:47:10.584 INFO: ##### Step: 137 Learning rate: 0.00015625 #####
Epoch 98, Train Loss: 1.6301, Val Loss: 2.2457
2025-02-04 19:47:10.584 INFO: Epoch 98, Train Loss: 1.6301, Val Loss: 2.2457
train_e/atom_mae: 0.000788
2025-02-04 19:47:10.585 INFO: train_e/atom_mae: 0.000788
train_e/atom_rmse: 0.001190
2025-02-04 19:47:10.586 INFO: train_e/atom_rmse: 0.001190
train_f_mae: 0.026096
2025-02-04 19:47:10.588 INFO: train_f_mae: 0.026096
train_f_rmse: 0.039722
2025-02-04 19:47:10.588 INFO: train_f_rmse: 0.039722
val_e/atom_mae: 0.000541
2025-02-04 19:47:10.591 INFO: val_e/atom_mae: 0.000541
val_e/atom_rmse: 0.000762
2025-02-04 19:47:10.591 INFO: val_e/atom_rmse: 0.000762
val_f_mae: 0.027648
2025-02-04 19:47:10.591 INFO: val_f_mae: 0.027648
val_f_rmse: 0.047177
2025-02-04 19:47:10.592 INFO: val_f_rmse: 0.047177
##### Step: 138 Learning rate: 0.00015625 #####
2025-02-04 19:49:02.716 INFO: ##### Step: 138 Learning rate: 0.00015625 #####
Epoch 99, Train Loss: 1.6387, Val Loss: 2.2442
2025-02-04 19:49:02.717 INFO: Epoch 99, Train Loss: 1.6387, Val Loss: 2.2442
train_e/atom_mae: 0.000844
2025-02-04 19:49:02.718 INFO: train_e/atom_mae: 0.000844
train_e/atom_rmse: 0.001236
2025-02-04 19:49:02.718 INFO: train_e/atom_rmse: 0.001236
train_f_mae: 0.026162
2025-02-04 19:49:02.721 INFO: train_f_mae: 0.026162
train_f_rmse: 0.039778
2025-02-04 19:49:02.721 INFO: train_f_rmse: 0.039778
val_e/atom_mae: 0.000540
2025-02-04 19:49:02.723 INFO: val_e/atom_mae: 0.000540
val_e/atom_rmse: 0.000767
2025-02-04 19:49:02.723 INFO: val_e/atom_rmse: 0.000767
val_f_mae: 0.027644
2025-02-04 19:49:02.724 INFO: val_f_mae: 0.027644
val_f_rmse: 0.047158
2025-02-04 19:49:02.724 INFO: val_f_rmse: 0.047158
##### Step: 139 Learning rate: 0.00015625 #####
2025-02-04 19:50:54.742 INFO: ##### Step: 139 Learning rate: 0.00015625 #####
Epoch 100, Train Loss: 1.6360, Val Loss: 2.2386
2025-02-04 19:50:54.743 INFO: Epoch 100, Train Loss: 1.6360, Val Loss: 2.2386
train_e/atom_mae: 0.000830
2025-02-04 19:50:54.743 INFO: train_e/atom_mae: 0.000830
train_e/atom_rmse: 0.001245
2025-02-04 19:50:54.744 INFO: train_e/atom_rmse: 0.001245
train_f_mae: 0.026145
2025-02-04 19:50:54.746 INFO: train_f_mae: 0.026145
train_f_rmse: 0.039734
2025-02-04 19:50:54.747 INFO: train_f_rmse: 0.039734
val_e/atom_mae: 0.000535
2025-02-04 19:50:54.749 INFO: val_e/atom_mae: 0.000535
val_e/atom_rmse: 0.000758
2025-02-04 19:50:54.749 INFO: val_e/atom_rmse: 0.000758
val_f_mae: 0.027644
2025-02-04 19:50:54.749 INFO: val_f_mae: 0.027644
val_f_rmse: 0.047103
2025-02-04 19:50:54.750 INFO: val_f_rmse: 0.047103
2025-02-04 19:50:55.315 INFO: Third train loop:
##### Step: 140 Learning rate: 7.8125e-05 #####
2025-02-04 19:52:51.706 INFO: ##### Step: 140 Learning rate: 7.8125e-05 #####
Epoch 1, Train Loss: 2.0082, Val Loss: 2.4177
2025-02-04 19:52:51.707 INFO: Epoch 1, Train Loss: 2.0082, Val Loss: 2.4177
train_e/atom_mae: 0.000656
2025-02-04 19:52:51.759 INFO: train_e/atom_mae: 0.000656
train_e/atom_rmse: 0.001074
2025-02-04 19:52:51.770 INFO: train_e/atom_rmse: 0.001074
train_f_mae: 0.026163
2025-02-04 19:52:51.773 INFO: train_f_mae: 0.026163
train_f_rmse: 0.039789
2025-02-04 19:52:51.773 INFO: train_f_rmse: 0.039789
val_e/atom_mae: 0.000520
2025-02-04 19:52:51.775 INFO: val_e/atom_mae: 0.000520
val_e/atom_rmse: 0.000728
2025-02-04 19:52:51.775 INFO: val_e/atom_rmse: 0.000728
val_f_mae: 0.027685
2025-02-04 19:52:51.776 INFO: val_f_mae: 0.027685
val_f_rmse: 0.047154
2025-02-04 19:52:51.776 INFO: val_f_rmse: 0.047154
##### Step: 141 Learning rate: 7.8125e-05 #####
2025-02-04 19:54:43.794 INFO: ##### Step: 141 Learning rate: 7.8125e-05 #####
Epoch 2, Train Loss: 1.9714, Val Loss: 2.4209
2025-02-04 19:54:43.794 INFO: Epoch 2, Train Loss: 1.9714, Val Loss: 2.4209
train_e/atom_mae: 0.000611
2025-02-04 19:54:43.795 INFO: train_e/atom_mae: 0.000611
train_e/atom_rmse: 0.001035
2025-02-04 19:54:43.795 INFO: train_e/atom_rmse: 0.001035
train_f_mae: 0.026071
2025-02-04 19:54:43.798 INFO: train_f_mae: 0.026071
train_f_rmse: 0.039700
2025-02-04 19:54:43.798 INFO: train_f_rmse: 0.039700
val_e/atom_mae: 0.000520
2025-02-04 19:54:43.800 INFO: val_e/atom_mae: 0.000520
val_e/atom_rmse: 0.000729
2025-02-04 19:54:43.801 INFO: val_e/atom_rmse: 0.000729
val_f_mae: 0.027704
2025-02-04 19:54:43.801 INFO: val_f_mae: 0.027704
val_f_rmse: 0.047181
2025-02-04 19:54:43.801 INFO: val_f_rmse: 0.047181
##### Step: 142 Learning rate: 7.8125e-05 #####
2025-02-04 19:56:35.651 INFO: ##### Step: 142 Learning rate: 7.8125e-05 #####
Epoch 3, Train Loss: 2.0450, Val Loss: 2.4334
2025-02-04 19:56:35.651 INFO: Epoch 3, Train Loss: 2.0450, Val Loss: 2.4334
train_e/atom_mae: 0.000732
2025-02-04 19:56:35.652 INFO: train_e/atom_mae: 0.000732
train_e/atom_rmse: 0.001124
2025-02-04 19:56:35.652 INFO: train_e/atom_rmse: 0.001124
train_f_mae: 0.026085
2025-02-04 19:56:35.655 INFO: train_f_mae: 0.026085
train_f_rmse: 0.039737
2025-02-04 19:56:35.655 INFO: train_f_rmse: 0.039737
val_e/atom_mae: 0.000538
2025-02-04 19:56:35.657 INFO: val_e/atom_mae: 0.000538
val_e/atom_rmse: 0.000741
2025-02-04 19:56:35.658 INFO: val_e/atom_rmse: 0.000741
val_f_mae: 0.027731
2025-02-04 19:56:35.658 INFO: val_f_mae: 0.027731
val_f_rmse: 0.047244
2025-02-04 19:56:35.658 INFO: val_f_rmse: 0.047244
##### Step: 143 Learning rate: 7.8125e-05 #####
2025-02-04 19:58:36.526 INFO: ##### Step: 143 Learning rate: 7.8125e-05 #####
Epoch 4, Train Loss: 1.9707, Val Loss: 2.4268
2025-02-04 19:58:36.526 INFO: Epoch 4, Train Loss: 1.9707, Val Loss: 2.4268
train_e/atom_mae: 0.000609
2025-02-04 19:58:36.569 INFO: train_e/atom_mae: 0.000609
train_e/atom_rmse: 0.001028
2025-02-04 19:58:36.626 INFO: train_e/atom_rmse: 0.001028
train_f_mae: 0.026133
2025-02-04 19:58:36.629 INFO: train_f_mae: 0.026133
train_f_rmse: 0.039762
2025-02-04 19:58:36.629 INFO: train_f_rmse: 0.039762
val_e/atom_mae: 0.000518
2025-02-04 19:58:36.631 INFO: val_e/atom_mae: 0.000518
val_e/atom_rmse: 0.000735
2025-02-04 19:58:36.632 INFO: val_e/atom_rmse: 0.000735
val_f_mae: 0.027684
2025-02-04 19:58:36.632 INFO: val_f_mae: 0.027684
val_f_rmse: 0.047211
2025-02-04 19:58:36.633 INFO: val_f_rmse: 0.047211
##### Step: 144 Learning rate: 7.8125e-05 #####
2025-02-04 20:00:30.035 INFO: ##### Step: 144 Learning rate: 7.8125e-05 #####
Epoch 5, Train Loss: 1.9922, Val Loss: 2.4462
2025-02-04 20:00:30.036 INFO: Epoch 5, Train Loss: 1.9922, Val Loss: 2.4462
train_e/atom_mae: 0.000654
2025-02-04 20:00:30.038 INFO: train_e/atom_mae: 0.000654
train_e/atom_rmse: 0.001058
2025-02-04 20:00:30.047 INFO: train_e/atom_rmse: 0.001058
train_f_mae: 0.026111
2025-02-04 20:00:30.050 INFO: train_f_mae: 0.026111
train_f_rmse: 0.039744
2025-02-04 20:00:30.050 INFO: train_f_rmse: 0.039744
val_e/atom_mae: 0.000538
2025-02-04 20:00:30.052 INFO: val_e/atom_mae: 0.000538
val_e/atom_rmse: 0.000748
2025-02-04 20:00:30.052 INFO: val_e/atom_rmse: 0.000748
val_f_mae: 0.027723
2025-02-04 20:00:30.053 INFO: val_f_mae: 0.027723
val_f_rmse: 0.047333
2025-02-04 20:00:30.053 INFO: val_f_rmse: 0.047333
##### Step: 145 Learning rate: 7.8125e-05 #####
2025-02-04 20:02:21.677 INFO: ##### Step: 145 Learning rate: 7.8125e-05 #####
Epoch 6, Train Loss: 1.9851, Val Loss: 2.4256
2025-02-04 20:02:21.678 INFO: Epoch 6, Train Loss: 1.9851, Val Loss: 2.4256
train_e/atom_mae: 0.000646
2025-02-04 20:02:21.679 INFO: train_e/atom_mae: 0.000646
train_e/atom_rmse: 0.001045
2025-02-04 20:02:21.679 INFO: train_e/atom_rmse: 0.001045
train_f_mae: 0.026135
2025-02-04 20:02:21.682 INFO: train_f_mae: 0.026135
train_f_rmse: 0.039781
2025-02-04 20:02:21.682 INFO: train_f_rmse: 0.039781
val_e/atom_mae: 0.000515
2025-02-04 20:02:21.684 INFO: val_e/atom_mae: 0.000515
val_e/atom_rmse: 0.000723
2025-02-04 20:02:21.684 INFO: val_e/atom_rmse: 0.000723
val_f_mae: 0.027708
2025-02-04 20:02:21.685 INFO: val_f_mae: 0.027708
val_f_rmse: 0.047262
2025-02-04 20:02:21.685 INFO: val_f_rmse: 0.047262
##### Step: 146 Learning rate: 7.8125e-05 #####
2025-02-04 20:04:13.361 INFO: ##### Step: 146 Learning rate: 7.8125e-05 #####
Epoch 7, Train Loss: 1.9745, Val Loss: 2.4308
2025-02-04 20:04:13.361 INFO: Epoch 7, Train Loss: 1.9745, Val Loss: 2.4308
train_e/atom_mae: 0.000636
2025-02-04 20:04:13.362 INFO: train_e/atom_mae: 0.000636
train_e/atom_rmse: 0.001037
2025-02-04 20:04:13.363 INFO: train_e/atom_rmse: 0.001037
train_f_mae: 0.026086
2025-02-04 20:04:13.369 INFO: train_f_mae: 0.026086
train_f_rmse: 0.039721
2025-02-04 20:04:13.369 INFO: train_f_rmse: 0.039721
val_e/atom_mae: 0.000517
2025-02-04 20:04:13.371 INFO: val_e/atom_mae: 0.000517
val_e/atom_rmse: 0.000725
2025-02-04 20:04:13.371 INFO: val_e/atom_rmse: 0.000725
val_f_mae: 0.027707
2025-02-04 20:04:13.372 INFO: val_f_mae: 0.027707
val_f_rmse: 0.047301
2025-02-04 20:04:13.372 INFO: val_f_rmse: 0.047301
##### Step: 147 Learning rate: 7.8125e-05 #####
2025-02-04 20:06:09.298 INFO: ##### Step: 147 Learning rate: 7.8125e-05 #####
Epoch 8, Train Loss: 1.9438, Val Loss: 2.4399
2025-02-04 20:06:09.299 INFO: Epoch 8, Train Loss: 1.9438, Val Loss: 2.4399
train_e/atom_mae: 0.000613
2025-02-04 20:06:09.328 INFO: train_e/atom_mae: 0.000613
train_e/atom_rmse: 0.000989
2025-02-04 20:06:09.330 INFO: train_e/atom_rmse: 0.000989
train_f_mae: 0.026133
2025-02-04 20:06:09.332 INFO: train_f_mae: 0.026133
train_f_rmse: 0.039785
2025-02-04 20:06:09.332 INFO: train_f_rmse: 0.039785
val_e/atom_mae: 0.000521
2025-02-04 20:06:09.335 INFO: val_e/atom_mae: 0.000521
val_e/atom_rmse: 0.000733
2025-02-04 20:06:09.335 INFO: val_e/atom_rmse: 0.000733
val_f_mae: 0.027730
2025-02-04 20:06:09.336 INFO: val_f_mae: 0.027730
val_f_rmse: 0.047354
2025-02-04 20:06:09.336 INFO: val_f_rmse: 0.047354
##### Step: 148 Learning rate: 7.8125e-05 #####
2025-02-04 20:08:01.152 INFO: ##### Step: 148 Learning rate: 7.8125e-05 #####
Epoch 9, Train Loss: 1.9832, Val Loss: 2.4373
2025-02-04 20:08:01.153 INFO: Epoch 9, Train Loss: 1.9832, Val Loss: 2.4373
train_e/atom_mae: 0.000667
2025-02-04 20:08:01.154 INFO: train_e/atom_mae: 0.000667
train_e/atom_rmse: 0.001041
2025-02-04 20:08:01.154 INFO: train_e/atom_rmse: 0.001041
train_f_mae: 0.026114
2025-02-04 20:08:01.157 INFO: train_f_mae: 0.026114
train_f_rmse: 0.039798
2025-02-04 20:08:01.157 INFO: train_f_rmse: 0.039798
val_e/atom_mae: 0.000535
2025-02-04 20:08:01.159 INFO: val_e/atom_mae: 0.000535
val_e/atom_rmse: 0.000742
2025-02-04 20:08:01.160 INFO: val_e/atom_rmse: 0.000742
val_f_mae: 0.027762
2025-02-04 20:08:01.160 INFO: val_f_mae: 0.027762
val_f_rmse: 0.047278
2025-02-04 20:08:01.160 INFO: val_f_rmse: 0.047278
##### Step: 149 Learning rate: 7.8125e-05 #####
2025-02-04 20:09:52.851 INFO: ##### Step: 149 Learning rate: 7.8125e-05 #####
Epoch 10, Train Loss: 2.0864, Val Loss: 2.4387
2025-02-04 20:09:52.851 INFO: Epoch 10, Train Loss: 2.0864, Val Loss: 2.4387
train_e/atom_mae: 0.000823
2025-02-04 20:09:52.852 INFO: train_e/atom_mae: 0.000823
train_e/atom_rmse: 0.001166
2025-02-04 20:09:52.852 INFO: train_e/atom_rmse: 0.001166
train_f_mae: 0.026152
2025-02-04 20:09:52.855 INFO: train_f_mae: 0.026152
train_f_rmse: 0.039815
2025-02-04 20:09:52.855 INFO: train_f_rmse: 0.039815
val_e/atom_mae: 0.000533
2025-02-04 20:09:52.857 INFO: val_e/atom_mae: 0.000533
val_e/atom_rmse: 0.000748
2025-02-04 20:09:52.857 INFO: val_e/atom_rmse: 0.000748
val_f_mae: 0.027753
2025-02-04 20:09:52.858 INFO: val_f_mae: 0.027753
val_f_rmse: 0.047261
2025-02-04 20:09:52.858 INFO: val_f_rmse: 0.047261
##### Step: 150 Learning rate: 7.8125e-05 #####
2025-02-04 20:11:47.038 INFO: ##### Step: 150 Learning rate: 7.8125e-05 #####
Epoch 11, Train Loss: 2.0006, Val Loss: 2.4300
2025-02-04 20:11:47.039 INFO: Epoch 11, Train Loss: 2.0006, Val Loss: 2.4300
train_e/atom_mae: 0.000672
2025-02-04 20:11:47.057 INFO: train_e/atom_mae: 0.000672
train_e/atom_rmse: 0.001066
2025-02-04 20:11:47.160 INFO: train_e/atom_rmse: 0.001066
train_f_mae: 0.026134
2025-02-04 20:11:47.163 INFO: train_f_mae: 0.026134
train_f_rmse: 0.039765
2025-02-04 20:11:47.163 INFO: train_f_rmse: 0.039765
val_e/atom_mae: 0.000513
2025-02-04 20:11:47.166 INFO: val_e/atom_mae: 0.000513
val_e/atom_rmse: 0.000724
2025-02-04 20:11:47.166 INFO: val_e/atom_rmse: 0.000724
val_f_mae: 0.027716
2025-02-04 20:11:47.167 INFO: val_f_mae: 0.027716
val_f_rmse: 0.047306
2025-02-04 20:11:47.167 INFO: val_f_rmse: 0.047306
##### Step: 151 Learning rate: 7.8125e-05 #####
2025-02-04 20:13:39.363 INFO: ##### Step: 151 Learning rate: 7.8125e-05 #####
Epoch 12, Train Loss: 1.9983, Val Loss: 2.4296
2025-02-04 20:13:39.364 INFO: Epoch 12, Train Loss: 1.9983, Val Loss: 2.4296
train_e/atom_mae: 0.000678
2025-02-04 20:13:39.365 INFO: train_e/atom_mae: 0.000678
train_e/atom_rmse: 0.001063
2025-02-04 20:13:39.365 INFO: train_e/atom_rmse: 0.001063
train_f_mae: 0.026130
2025-02-04 20:13:39.368 INFO: train_f_mae: 0.026130
train_f_rmse: 0.039771
2025-02-04 20:13:39.368 INFO: train_f_rmse: 0.039771
val_e/atom_mae: 0.000520
2025-02-04 20:13:39.370 INFO: val_e/atom_mae: 0.000520
val_e/atom_rmse: 0.000722
2025-02-04 20:13:39.370 INFO: val_e/atom_rmse: 0.000722
val_f_mae: 0.027715
2025-02-04 20:13:39.371 INFO: val_f_mae: 0.027715
val_f_rmse: 0.047304
2025-02-04 20:13:39.371 INFO: val_f_rmse: 0.047304
##### Step: 152 Learning rate: 7.8125e-05 #####
2025-02-04 20:15:30.985 INFO: ##### Step: 152 Learning rate: 7.8125e-05 #####
Epoch 13, Train Loss: 1.9828, Val Loss: 2.4397
2025-02-04 20:15:30.986 INFO: Epoch 13, Train Loss: 1.9828, Val Loss: 2.4397
train_e/atom_mae: 0.000661
2025-02-04 20:15:30.987 INFO: train_e/atom_mae: 0.000661
train_e/atom_rmse: 0.001045
2025-02-04 20:15:30.987 INFO: train_e/atom_rmse: 0.001045
train_f_mae: 0.026091
2025-02-04 20:15:30.990 INFO: train_f_mae: 0.026091
train_f_rmse: 0.039756
2025-02-04 20:15:30.990 INFO: train_f_rmse: 0.039756
val_e/atom_mae: 0.000536
2025-02-04 20:15:30.992 INFO: val_e/atom_mae: 0.000536
val_e/atom_rmse: 0.000736
2025-02-04 20:15:30.992 INFO: val_e/atom_rmse: 0.000736
val_f_mae: 0.027736
2025-02-04 20:15:30.993 INFO: val_f_mae: 0.027736
val_f_rmse: 0.047335
2025-02-04 20:15:30.993 INFO: val_f_rmse: 0.047335
##### Step: 153 Learning rate: 7.8125e-05 #####
2025-02-04 20:17:23.556 INFO: ##### Step: 153 Learning rate: 7.8125e-05 #####
Epoch 14, Train Loss: 1.9449, Val Loss: 2.4397
2025-02-04 20:17:23.556 INFO: Epoch 14, Train Loss: 1.9449, Val Loss: 2.4397
train_e/atom_mae: 0.000617
2025-02-04 20:17:23.568 INFO: train_e/atom_mae: 0.000617
train_e/atom_rmse: 0.000990
2025-02-04 20:17:23.571 INFO: train_e/atom_rmse: 0.000990
train_f_mae: 0.026146
2025-02-04 20:17:23.573 INFO: train_f_mae: 0.026146
train_f_rmse: 0.039793
2025-02-04 20:17:23.574 INFO: train_f_rmse: 0.039793
val_e/atom_mae: 0.000525
2025-02-04 20:17:23.576 INFO: val_e/atom_mae: 0.000525
val_e/atom_rmse: 0.000746
2025-02-04 20:17:23.576 INFO: val_e/atom_rmse: 0.000746
val_f_mae: 0.027712
2025-02-04 20:17:23.577 INFO: val_f_mae: 0.027712
val_f_rmse: 0.047284
2025-02-04 20:17:23.577 INFO: val_f_rmse: 0.047284
##### Step: 154 Learning rate: 7.8125e-05 #####
2025-02-04 20:19:16.692 INFO: ##### Step: 154 Learning rate: 7.8125e-05 #####
Epoch 15, Train Loss: 1.9790, Val Loss: 2.4328
2025-02-04 20:19:16.692 INFO: Epoch 15, Train Loss: 1.9790, Val Loss: 2.4328
train_e/atom_mae: 0.000661
2025-02-04 20:19:16.693 INFO: train_e/atom_mae: 0.000661
train_e/atom_rmse: 0.001039
2025-02-04 20:19:16.693 INFO: train_e/atom_rmse: 0.001039
train_f_mae: 0.026125
2025-02-04 20:19:16.696 INFO: train_f_mae: 0.026125
train_f_rmse: 0.039765
2025-02-04 20:19:16.696 INFO: train_f_rmse: 0.039765
val_e/atom_mae: 0.000514
2025-02-04 20:19:16.698 INFO: val_e/atom_mae: 0.000514
val_e/atom_rmse: 0.000721
2025-02-04 20:19:16.699 INFO: val_e/atom_rmse: 0.000721
val_f_mae: 0.027774
2025-02-04 20:19:16.699 INFO: val_f_mae: 0.027774
val_f_rmse: 0.047345
2025-02-04 20:19:16.699 INFO: val_f_rmse: 0.047345
##### Step: 155 Learning rate: 7.8125e-05 #####
2025-02-04 20:21:08.486 INFO: ##### Step: 155 Learning rate: 7.8125e-05 #####
Epoch 16, Train Loss: 1.9611, Val Loss: 2.4352
2025-02-04 20:21:08.487 INFO: Epoch 16, Train Loss: 1.9611, Val Loss: 2.4352
train_e/atom_mae: 0.000635
2025-02-04 20:21:08.488 INFO: train_e/atom_mae: 0.000635
train_e/atom_rmse: 0.001010
2025-02-04 20:21:08.488 INFO: train_e/atom_rmse: 0.001010
train_f_mae: 0.026147
2025-02-04 20:21:08.491 INFO: train_f_mae: 0.026147
train_f_rmse: 0.039817
2025-02-04 20:21:08.491 INFO: train_f_rmse: 0.039817
val_e/atom_mae: 0.000515
2025-02-04 20:21:08.493 INFO: val_e/atom_mae: 0.000515
val_e/atom_rmse: 0.000722
2025-02-04 20:21:08.493 INFO: val_e/atom_rmse: 0.000722
val_f_mae: 0.027711
2025-02-04 20:21:08.494 INFO: val_f_mae: 0.027711
val_f_rmse: 0.047366
2025-02-04 20:21:08.494 INFO: val_f_rmse: 0.047366
##### Step: 156 Learning rate: 7.8125e-05 #####
2025-02-04 20:23:00.209 INFO: ##### Step: 156 Learning rate: 7.8125e-05 #####
Epoch 17, Train Loss: 1.9512, Val Loss: 2.4249
2025-02-04 20:23:00.210 INFO: Epoch 17, Train Loss: 1.9512, Val Loss: 2.4249
train_e/atom_mae: 0.000613
2025-02-04 20:23:00.211 INFO: train_e/atom_mae: 0.000613
train_e/atom_rmse: 0.001000
2025-02-04 20:23:00.211 INFO: train_e/atom_rmse: 0.001000
train_f_mae: 0.026121
2025-02-04 20:23:00.214 INFO: train_f_mae: 0.026121
train_f_rmse: 0.039781
2025-02-04 20:23:00.214 INFO: train_f_rmse: 0.039781
val_e/atom_mae: 0.000504
2025-02-04 20:23:00.216 INFO: val_e/atom_mae: 0.000504
val_e/atom_rmse: 0.000712
2025-02-04 20:23:00.216 INFO: val_e/atom_rmse: 0.000712
val_f_mae: 0.027721
2025-02-04 20:23:00.217 INFO: val_f_mae: 0.027721
val_f_rmse: 0.047317
2025-02-04 20:23:00.217 INFO: val_f_rmse: 0.047317
##### Step: 157 Learning rate: 7.8125e-05 #####
2025-02-04 20:24:53.902 INFO: ##### Step: 157 Learning rate: 7.8125e-05 #####
Epoch 18, Train Loss: 1.9334, Val Loss: 2.4291
2025-02-04 20:24:53.903 INFO: Epoch 18, Train Loss: 1.9334, Val Loss: 2.4291
train_e/atom_mae: 0.000573
2025-02-04 20:24:53.907 INFO: train_e/atom_mae: 0.000573
train_e/atom_rmse: 0.000972
2025-02-04 20:24:53.915 INFO: train_e/atom_rmse: 0.000972
train_f_mae: 0.026154
2025-02-04 20:24:53.918 INFO: train_f_mae: 0.026154
train_f_rmse: 0.039816
2025-02-04 20:24:53.918 INFO: train_f_rmse: 0.039816
val_e/atom_mae: 0.000517
2025-02-04 20:24:53.920 INFO: val_e/atom_mae: 0.000517
val_e/atom_rmse: 0.000724
2025-02-04 20:24:53.921 INFO: val_e/atom_rmse: 0.000724
val_f_mae: 0.027708
2025-02-04 20:24:53.921 INFO: val_f_mae: 0.027708
val_f_rmse: 0.047288
2025-02-04 20:24:53.921 INFO: val_f_rmse: 0.047288
##### Step: 158 Learning rate: 7.8125e-05 #####
2025-02-04 20:26:45.683 INFO: ##### Step: 158 Learning rate: 7.8125e-05 #####
Epoch 19, Train Loss: 1.9325, Val Loss: 2.4270
2025-02-04 20:26:45.684 INFO: Epoch 19, Train Loss: 1.9325, Val Loss: 2.4270
train_e/atom_mae: 0.000594
2025-02-04 20:26:45.685 INFO: train_e/atom_mae: 0.000594
train_e/atom_rmse: 0.000973
2025-02-04 20:26:45.685 INFO: train_e/atom_rmse: 0.000973
train_f_mae: 0.026140
2025-02-04 20:26:45.688 INFO: train_f_mae: 0.026140
train_f_rmse: 0.039792
2025-02-04 20:26:45.688 INFO: train_f_rmse: 0.039792
val_e/atom_mae: 0.000517
2025-02-04 20:26:45.690 INFO: val_e/atom_mae: 0.000517
val_e/atom_rmse: 0.000728
2025-02-04 20:26:45.690 INFO: val_e/atom_rmse: 0.000728
val_f_mae: 0.027708
2025-02-04 20:26:45.691 INFO: val_f_mae: 0.027708
val_f_rmse: 0.047248
2025-02-04 20:26:45.691 INFO: val_f_rmse: 0.047248
##### Step: 159 Learning rate: 7.8125e-05 #####
2025-02-04 20:28:37.111 INFO: ##### Step: 159 Learning rate: 7.8125e-05 #####
Epoch 20, Train Loss: 2.0098, Val Loss: 2.4385
2025-02-04 20:28:37.112 INFO: Epoch 20, Train Loss: 2.0098, Val Loss: 2.4385
train_e/atom_mae: 0.000723
2025-02-04 20:28:37.113 INFO: train_e/atom_mae: 0.000723
train_e/atom_rmse: 0.001077
2025-02-04 20:28:37.113 INFO: train_e/atom_rmse: 0.001077
train_f_mae: 0.026108
2025-02-04 20:28:37.116 INFO: train_f_mae: 0.026108
train_f_rmse: 0.039775
2025-02-04 20:28:37.116 INFO: train_f_rmse: 0.039775
val_e/atom_mae: 0.000518
2025-02-04 20:28:37.118 INFO: val_e/atom_mae: 0.000518
val_e/atom_rmse: 0.000735
2025-02-04 20:28:37.118 INFO: val_e/atom_rmse: 0.000735
val_f_mae: 0.027717
2025-02-04 20:28:37.119 INFO: val_f_mae: 0.027717
val_f_rmse: 0.047329
2025-02-04 20:28:37.119 INFO: val_f_rmse: 0.047329
##### Step: 160 Learning rate: 3.90625e-05 #####
2025-02-04 20:30:29.436 INFO: ##### Step: 160 Learning rate: 3.90625e-05 #####
Epoch 21, Train Loss: 1.9032, Val Loss: 2.4284
2025-02-04 20:30:29.437 INFO: Epoch 21, Train Loss: 1.9032, Val Loss: 2.4284
train_e/atom_mae: 0.000555
2025-02-04 20:30:29.438 INFO: train_e/atom_mae: 0.000555
train_e/atom_rmse: 0.000938
2025-02-04 20:30:29.439 INFO: train_e/atom_rmse: 0.000938
train_f_mae: 0.026072
2025-02-04 20:30:29.442 INFO: train_f_mae: 0.026072
train_f_rmse: 0.039739
2025-02-04 20:30:29.442 INFO: train_f_rmse: 0.039739
val_e/atom_mae: 0.000513
2025-02-04 20:30:29.444 INFO: val_e/atom_mae: 0.000513
val_e/atom_rmse: 0.000724
2025-02-04 20:30:29.445 INFO: val_e/atom_rmse: 0.000724
val_f_mae: 0.027701
2025-02-04 20:30:29.445 INFO: val_f_mae: 0.027701
val_f_rmse: 0.047283
2025-02-04 20:30:29.445 INFO: val_f_rmse: 0.047283
##### Step: 161 Learning rate: 3.90625e-05 #####
2025-02-04 20:32:21.166 INFO: ##### Step: 161 Learning rate: 3.90625e-05 #####
Epoch 22, Train Loss: 1.8957, Val Loss: 2.4252
2025-02-04 20:32:21.167 INFO: Epoch 22, Train Loss: 1.8957, Val Loss: 2.4252
train_e/atom_mae: 0.000543
2025-02-04 20:32:21.167 INFO: train_e/atom_mae: 0.000543
train_e/atom_rmse: 0.000926
2025-02-04 20:32:21.168 INFO: train_e/atom_rmse: 0.000926
train_f_mae: 0.026061
2025-02-04 20:32:21.170 INFO: train_f_mae: 0.026061
train_f_rmse: 0.039742
2025-02-04 20:32:21.171 INFO: train_f_rmse: 0.039742
val_e/atom_mae: 0.000509
2025-02-04 20:32:21.176 INFO: val_e/atom_mae: 0.000509
val_e/atom_rmse: 0.000723
2025-02-04 20:32:21.176 INFO: val_e/atom_rmse: 0.000723
val_f_mae: 0.027696
2025-02-04 20:32:21.177 INFO: val_f_mae: 0.027696
val_f_rmse: 0.047260
2025-02-04 20:32:21.177 INFO: val_f_rmse: 0.047260
##### Step: 162 Learning rate: 3.90625e-05 #####
2025-02-04 20:34:12.749 INFO: ##### Step: 162 Learning rate: 3.90625e-05 #####
Epoch 23, Train Loss: 1.9092, Val Loss: 2.4238
2025-02-04 20:34:12.749 INFO: Epoch 23, Train Loss: 1.9092, Val Loss: 2.4238
train_e/atom_mae: 0.000571
2025-02-04 20:34:12.750 INFO: train_e/atom_mae: 0.000571
train_e/atom_rmse: 0.000948
2025-02-04 20:34:12.751 INFO: train_e/atom_rmse: 0.000948
train_f_mae: 0.026070
2025-02-04 20:34:12.753 INFO: train_f_mae: 0.026070
train_f_rmse: 0.039720
2025-02-04 20:34:12.753 INFO: train_f_rmse: 0.039720
val_e/atom_mae: 0.000509
2025-02-04 20:34:12.756 INFO: val_e/atom_mae: 0.000509
val_e/atom_rmse: 0.000718
2025-02-04 20:34:12.756 INFO: val_e/atom_rmse: 0.000718
val_f_mae: 0.027708
2025-02-04 20:34:12.756 INFO: val_f_mae: 0.027708
val_f_rmse: 0.047270
2025-02-04 20:34:12.757 INFO: val_f_rmse: 0.047270
##### Step: 163 Learning rate: 3.90625e-05 #####
2025-02-04 20:36:19.277 INFO: ##### Step: 163 Learning rate: 3.90625e-05 #####
Epoch 24, Train Loss: 1.9488, Val Loss: 2.4216
2025-02-04 20:36:19.278 INFO: Epoch 24, Train Loss: 1.9488, Val Loss: 2.4216
train_e/atom_mae: 0.000655
2025-02-04 20:36:19.413 INFO: train_e/atom_mae: 0.000655
train_e/atom_rmse: 0.001006
2025-02-04 20:36:19.606 INFO: train_e/atom_rmse: 0.001006
train_f_mae: 0.026051
2025-02-04 20:36:19.609 INFO: train_f_mae: 0.026051
train_f_rmse: 0.039695
2025-02-04 20:36:19.609 INFO: train_f_rmse: 0.039695
val_e/atom_mae: 0.000507
2025-02-04 20:36:19.611 INFO: val_e/atom_mae: 0.000507
val_e/atom_rmse: 0.000717
2025-02-04 20:36:19.612 INFO: val_e/atom_rmse: 0.000717
val_f_mae: 0.027699
2025-02-04 20:36:19.612 INFO: val_f_mae: 0.027699
val_f_rmse: 0.047249
2025-02-04 20:36:19.613 INFO: val_f_rmse: 0.047249
##### Step: 164 Learning rate: 3.90625e-05 #####
2025-02-04 20:38:11.740 INFO: ##### Step: 164 Learning rate: 3.90625e-05 #####
Epoch 25, Train Loss: 1.9012, Val Loss: 2.4240
2025-02-04 20:38:11.740 INFO: Epoch 25, Train Loss: 1.9012, Val Loss: 2.4240
train_e/atom_mae: 0.000533
2025-02-04 20:38:11.741 INFO: train_e/atom_mae: 0.000533
train_e/atom_rmse: 0.000937
2025-02-04 20:38:11.741 INFO: train_e/atom_rmse: 0.000937
train_f_mae: 0.026061
2025-02-04 20:38:11.744 INFO: train_f_mae: 0.026061
train_f_rmse: 0.039718
2025-02-04 20:38:11.744 INFO: train_f_rmse: 0.039718
val_e/atom_mae: 0.000512
2025-02-04 20:38:11.746 INFO: val_e/atom_mae: 0.000512
val_e/atom_rmse: 0.000718
2025-02-04 20:38:11.747 INFO: val_e/atom_rmse: 0.000718
val_f_mae: 0.027702
2025-02-04 20:38:11.747 INFO: val_f_mae: 0.027702
val_f_rmse: 0.047269
2025-02-04 20:38:11.747 INFO: val_f_rmse: 0.047269
##### Step: 165 Learning rate: 3.90625e-05 #####
2025-02-04 20:40:03.611 INFO: ##### Step: 165 Learning rate: 3.90625e-05 #####
Epoch 26, Train Loss: 1.9353, Val Loss: 2.4274
2025-02-04 20:40:03.613 INFO: Epoch 26, Train Loss: 1.9353, Val Loss: 2.4274
train_e/atom_mae: 0.000600
2025-02-04 20:40:03.614 INFO: train_e/atom_mae: 0.000600
train_e/atom_rmse: 0.000983
2025-02-04 20:40:03.615 INFO: train_e/atom_rmse: 0.000983
train_f_mae: 0.026069
2025-02-04 20:40:03.617 INFO: train_f_mae: 0.026069
train_f_rmse: 0.039742
2025-02-04 20:40:03.617 INFO: train_f_rmse: 0.039742
val_e/atom_mae: 0.000515
2025-02-04 20:40:03.619 INFO: val_e/atom_mae: 0.000515
val_e/atom_rmse: 0.000726
2025-02-04 20:40:03.620 INFO: val_e/atom_rmse: 0.000726
val_f_mae: 0.027700
2025-02-04 20:40:03.620 INFO: val_f_mae: 0.027700
val_f_rmse: 0.047265
2025-02-04 20:40:03.621 INFO: val_f_rmse: 0.047265
##### Step: 166 Learning rate: 3.90625e-05 #####
2025-02-04 20:42:01.124 INFO: ##### Step: 166 Learning rate: 3.90625e-05 #####
Epoch 27, Train Loss: 1.9217, Val Loss: 2.4276
2025-02-04 20:42:01.125 INFO: Epoch 27, Train Loss: 1.9217, Val Loss: 2.4276
train_e/atom_mae: 0.000585
2025-02-04 20:42:01.156 INFO: train_e/atom_mae: 0.000585
train_e/atom_rmse: 0.000965
2025-02-04 20:42:01.156 INFO: train_e/atom_rmse: 0.000965
train_f_mae: 0.026054
2025-02-04 20:42:01.159 INFO: train_f_mae: 0.026054
train_f_rmse: 0.039733
2025-02-04 20:42:01.159 INFO: train_f_rmse: 0.039733
val_e/atom_mae: 0.000516
2025-02-04 20:42:01.161 INFO: val_e/atom_mae: 0.000516
val_e/atom_rmse: 0.000730
2025-02-04 20:42:01.162 INFO: val_e/atom_rmse: 0.000730
val_f_mae: 0.027709
2025-02-04 20:42:01.162 INFO: val_f_mae: 0.027709
val_f_rmse: 0.047243
2025-02-04 20:42:01.162 INFO: val_f_rmse: 0.047243
##### Step: 167 Learning rate: 3.90625e-05 #####
2025-02-04 20:43:54.836 INFO: ##### Step: 167 Learning rate: 3.90625e-05 #####
Epoch 28, Train Loss: 1.8908, Val Loss: 2.4216
2025-02-04 20:43:54.837 INFO: Epoch 28, Train Loss: 1.8908, Val Loss: 2.4216
train_e/atom_mae: 0.000530
2025-02-04 20:43:54.839 INFO: train_e/atom_mae: 0.000530
train_e/atom_rmse: 0.000919
2025-02-04 20:43:54.889 INFO: train_e/atom_rmse: 0.000919
train_f_mae: 0.026074
2025-02-04 20:43:54.891 INFO: train_f_mae: 0.026074
train_f_rmse: 0.039742
2025-02-04 20:43:54.892 INFO: train_f_rmse: 0.039742
val_e/atom_mae: 0.000505
2025-02-04 20:43:54.894 INFO: val_e/atom_mae: 0.000505
val_e/atom_rmse: 0.000718
2025-02-04 20:43:54.894 INFO: val_e/atom_rmse: 0.000718
val_f_mae: 0.027695
2025-02-04 20:43:54.895 INFO: val_f_mae: 0.027695
val_f_rmse: 0.047247
2025-02-04 20:43:54.895 INFO: val_f_rmse: 0.047247
##### Step: 168 Learning rate: 3.90625e-05 #####
2025-02-04 20:45:47.122 INFO: ##### Step: 168 Learning rate: 3.90625e-05 #####
Epoch 29, Train Loss: 1.9243, Val Loss: 2.4274
2025-02-04 20:45:47.123 INFO: Epoch 29, Train Loss: 1.9243, Val Loss: 2.4274
train_e/atom_mae: 0.000603
2025-02-04 20:45:47.124 INFO: train_e/atom_mae: 0.000603
train_e/atom_rmse: 0.000967
2025-02-04 20:45:47.124 INFO: train_e/atom_rmse: 0.000967
train_f_mae: 0.026082
2025-02-04 20:45:47.127 INFO: train_f_mae: 0.026082
train_f_rmse: 0.039744
2025-02-04 20:45:47.127 INFO: train_f_rmse: 0.039744
val_e/atom_mae: 0.000509
2025-02-04 20:45:47.129 INFO: val_e/atom_mae: 0.000509
val_e/atom_rmse: 0.000722
2025-02-04 20:45:47.129 INFO: val_e/atom_rmse: 0.000722
val_f_mae: 0.027723
2025-02-04 20:45:47.130 INFO: val_f_mae: 0.027723
val_f_rmse: 0.047287
2025-02-04 20:45:47.130 INFO: val_f_rmse: 0.047287
##### Step: 169 Learning rate: 3.90625e-05 #####
2025-02-04 20:47:38.897 INFO: ##### Step: 169 Learning rate: 3.90625e-05 #####
Epoch 30, Train Loss: 1.9477, Val Loss: 2.4231
2025-02-04 20:47:38.897 INFO: Epoch 30, Train Loss: 1.9477, Val Loss: 2.4231
train_e/atom_mae: 0.000630
2025-02-04 20:47:38.898 INFO: train_e/atom_mae: 0.000630
train_e/atom_rmse: 0.001003
2025-02-04 20:47:38.899 INFO: train_e/atom_rmse: 0.001003
train_f_mae: 0.026048
2025-02-04 20:47:38.901 INFO: train_f_mae: 0.026048
train_f_rmse: 0.039707
2025-02-04 20:47:38.901 INFO: train_f_rmse: 0.039707
val_e/atom_mae: 0.000509
2025-02-04 20:47:38.904 INFO: val_e/atom_mae: 0.000509
val_e/atom_rmse: 0.000722
2025-02-04 20:47:38.904 INFO: val_e/atom_rmse: 0.000722
val_f_mae: 0.027695
2025-02-04 20:47:38.904 INFO: val_f_mae: 0.027695
val_f_rmse: 0.047241
2025-02-04 20:47:38.905 INFO: val_f_rmse: 0.047241
##### Step: 170 Learning rate: 3.90625e-05 #####
2025-02-04 20:49:30.955 INFO: ##### Step: 170 Learning rate: 3.90625e-05 #####
Epoch 31, Train Loss: 1.8865, Val Loss: 2.4287
2025-02-04 20:49:30.955 INFO: Epoch 31, Train Loss: 1.8865, Val Loss: 2.4287
train_e/atom_mae: 0.000518
2025-02-04 20:49:30.957 INFO: train_e/atom_mae: 0.000518
train_e/atom_rmse: 0.000918
2025-02-04 20:49:30.957 INFO: train_e/atom_rmse: 0.000918
train_f_mae: 0.026034
2025-02-04 20:49:30.960 INFO: train_f_mae: 0.026034
train_f_rmse: 0.039700
2025-02-04 20:49:30.960 INFO: train_f_rmse: 0.039700
val_e/atom_mae: 0.000516
2025-02-04 20:49:30.962 INFO: val_e/atom_mae: 0.000516
val_e/atom_rmse: 0.000725
2025-02-04 20:49:30.963 INFO: val_e/atom_rmse: 0.000725
val_f_mae: 0.027705
2025-02-04 20:49:30.963 INFO: val_f_mae: 0.027705
val_f_rmse: 0.047284
2025-02-04 20:49:30.964 INFO: val_f_rmse: 0.047284
##### Step: 171 Learning rate: 3.90625e-05 #####
2025-02-04 20:51:22.372 INFO: ##### Step: 171 Learning rate: 3.90625e-05 #####
Epoch 32, Train Loss: 1.8994, Val Loss: 2.4183
2025-02-04 20:51:22.373 INFO: Epoch 32, Train Loss: 1.8994, Val Loss: 2.4183
train_e/atom_mae: 0.000541
2025-02-04 20:51:22.373 INFO: train_e/atom_mae: 0.000541
train_e/atom_rmse: 0.000934
2025-02-04 20:51:22.374 INFO: train_e/atom_rmse: 0.000934
train_f_mae: 0.026063
2025-02-04 20:51:22.376 INFO: train_f_mae: 0.026063
train_f_rmse: 0.039725
2025-02-04 20:51:22.376 INFO: train_f_rmse: 0.039725
val_e/atom_mae: 0.000503
2025-02-04 20:51:22.378 INFO: val_e/atom_mae: 0.000503
val_e/atom_rmse: 0.000716
2025-02-04 20:51:22.379 INFO: val_e/atom_rmse: 0.000716
val_f_mae: 0.027696
2025-02-04 20:51:22.379 INFO: val_f_mae: 0.027696
val_f_rmse: 0.047224
2025-02-04 20:51:22.380 INFO: val_f_rmse: 0.047224
##### Step: 172 Learning rate: 3.90625e-05 #####
2025-02-04 20:53:17.510 INFO: ##### Step: 172 Learning rate: 3.90625e-05 #####
Epoch 33, Train Loss: 1.9084, Val Loss: 2.4176
2025-02-04 20:53:17.510 INFO: Epoch 33, Train Loss: 1.9084, Val Loss: 2.4176
train_e/atom_mae: 0.000596
2025-02-04 20:53:17.549 INFO: train_e/atom_mae: 0.000596
train_e/atom_rmse: 0.000947
2025-02-04 20:53:17.550 INFO: train_e/atom_rmse: 0.000947
train_f_mae: 0.026057
2025-02-04 20:53:17.553 INFO: train_f_mae: 0.026057
train_f_rmse: 0.039724
2025-02-04 20:53:17.553 INFO: train_f_rmse: 0.039724
val_e/atom_mae: 0.000505
2025-02-04 20:53:17.555 INFO: val_e/atom_mae: 0.000505
val_e/atom_rmse: 0.000713
2025-02-04 20:53:17.556 INFO: val_e/atom_rmse: 0.000713
val_f_mae: 0.027707
2025-02-04 20:53:17.556 INFO: val_f_mae: 0.027707
val_f_rmse: 0.047234
2025-02-04 20:53:17.557 INFO: val_f_rmse: 0.047234
##### Step: 173 Learning rate: 3.90625e-05 #####
2025-02-04 20:55:09.033 INFO: ##### Step: 173 Learning rate: 3.90625e-05 #####
Epoch 34, Train Loss: 1.9596, Val Loss: 2.4221
2025-02-04 20:55:09.034 INFO: Epoch 34, Train Loss: 1.9596, Val Loss: 2.4221
train_e/atom_mae: 0.000657
2025-02-04 20:55:09.035 INFO: train_e/atom_mae: 0.000657
train_e/atom_rmse: 0.001020
2025-02-04 20:55:09.035 INFO: train_e/atom_rmse: 0.001020
train_f_mae: 0.026065
2025-02-04 20:55:09.038 INFO: train_f_mae: 0.026065
train_f_rmse: 0.039702
2025-02-04 20:55:09.038 INFO: train_f_rmse: 0.039702
val_e/atom_mae: 0.000510
2025-02-04 20:55:09.040 INFO: val_e/atom_mae: 0.000510
val_e/atom_rmse: 0.000715
2025-02-04 20:55:09.040 INFO: val_e/atom_rmse: 0.000715
val_f_mae: 0.027718
2025-02-04 20:55:09.041 INFO: val_f_mae: 0.027718
val_f_rmse: 0.047268
2025-02-04 20:55:09.041 INFO: val_f_rmse: 0.047268
##### Step: 174 Learning rate: 3.90625e-05 #####
2025-02-04 20:57:00.818 INFO: ##### Step: 174 Learning rate: 3.90625e-05 #####
Epoch 35, Train Loss: 1.9060, Val Loss: 2.4226
2025-02-04 20:57:00.818 INFO: Epoch 35, Train Loss: 1.9060, Val Loss: 2.4226
train_e/atom_mae: 0.000563
2025-02-04 20:57:00.819 INFO: train_e/atom_mae: 0.000563
train_e/atom_rmse: 0.000944
2025-02-04 20:57:00.820 INFO: train_e/atom_rmse: 0.000944
train_f_mae: 0.026059
2025-02-04 20:57:00.822 INFO: train_f_mae: 0.026059
train_f_rmse: 0.039720
2025-02-04 20:57:00.822 INFO: train_f_rmse: 0.039720
val_e/atom_mae: 0.000507
2025-02-04 20:57:00.825 INFO: val_e/atom_mae: 0.000507
val_e/atom_rmse: 0.000715
2025-02-04 20:57:00.825 INFO: val_e/atom_rmse: 0.000715
val_f_mae: 0.027710
2025-02-04 20:57:00.825 INFO: val_f_mae: 0.027710
val_f_rmse: 0.047270
2025-02-04 20:57:00.826 INFO: val_f_rmse: 0.047270
##### Step: 175 Learning rate: 3.90625e-05 #####
2025-02-04 20:58:52.318 INFO: ##### Step: 175 Learning rate: 3.90625e-05 #####
Epoch 36, Train Loss: 1.8982, Val Loss: 2.4220
2025-02-04 20:58:52.319 INFO: Epoch 36, Train Loss: 1.8982, Val Loss: 2.4220
train_e/atom_mae: 0.000543
2025-02-04 20:58:52.320 INFO: train_e/atom_mae: 0.000543
train_e/atom_rmse: 0.000930
2025-02-04 20:58:52.320 INFO: train_e/atom_rmse: 0.000930
train_f_mae: 0.026082
2025-02-04 20:58:52.323 INFO: train_f_mae: 0.026082
train_f_rmse: 0.039743
2025-02-04 20:58:52.323 INFO: train_f_rmse: 0.039743
val_e/atom_mae: 0.000501
2025-02-04 20:58:52.325 INFO: val_e/atom_mae: 0.000501
val_e/atom_rmse: 0.000715
2025-02-04 20:58:52.326 INFO: val_e/atom_rmse: 0.000715
val_f_mae: 0.027709
2025-02-04 20:58:52.326 INFO: val_f_mae: 0.027709
val_f_rmse: 0.047270
2025-02-04 20:58:52.327 INFO: val_f_rmse: 0.047270
##### Step: 176 Learning rate: 3.90625e-05 #####
2025-02-04 21:00:46.825 INFO: ##### Step: 176 Learning rate: 3.90625e-05 #####
Epoch 37, Train Loss: 1.9375, Val Loss: 2.4160
2025-02-04 21:00:46.825 INFO: Epoch 37, Train Loss: 1.9375, Val Loss: 2.4160
train_e/atom_mae: 0.000628
2025-02-04 21:00:46.951 INFO: train_e/atom_mae: 0.000628
train_e/atom_rmse: 0.000983
2025-02-04 21:00:47.010 INFO: train_e/atom_rmse: 0.000983
train_f_mae: 0.026096
2025-02-04 21:00:47.012 INFO: train_f_mae: 0.026096
train_f_rmse: 0.039767
2025-02-04 21:00:47.013 INFO: train_f_rmse: 0.039767
val_e/atom_mae: 0.000503
2025-02-04 21:00:47.015 INFO: val_e/atom_mae: 0.000503
val_e/atom_rmse: 0.000713
2025-02-04 21:00:47.015 INFO: val_e/atom_rmse: 0.000713
val_f_mae: 0.027720
2025-02-04 21:00:47.016 INFO: val_f_mae: 0.027720
val_f_rmse: 0.047217
2025-02-04 21:00:47.016 INFO: val_f_rmse: 0.047217
##### Step: 177 Learning rate: 3.90625e-05 #####
2025-02-04 21:02:38.960 INFO: ##### Step: 177 Learning rate: 3.90625e-05 #####
Epoch 38, Train Loss: 1.8743, Val Loss: 2.4230
2025-02-04 21:02:38.961 INFO: Epoch 38, Train Loss: 1.8743, Val Loss: 2.4230
train_e/atom_mae: 0.000517
2025-02-04 21:02:38.962 INFO: train_e/atom_mae: 0.000517
train_e/atom_rmse: 0.000902
2025-02-04 21:02:38.962 INFO: train_e/atom_rmse: 0.000902
train_f_mae: 0.026019
2025-02-04 21:02:38.965 INFO: train_f_mae: 0.026019
train_f_rmse: 0.039676
2025-02-04 21:02:38.965 INFO: train_f_rmse: 0.039676
val_e/atom_mae: 0.000515
2025-02-04 21:02:38.967 INFO: val_e/atom_mae: 0.000515
val_e/atom_rmse: 0.000720
2025-02-04 21:02:38.967 INFO: val_e/atom_rmse: 0.000720
val_f_mae: 0.027717
2025-02-04 21:02:38.968 INFO: val_f_mae: 0.027717
val_f_rmse: 0.047251
2025-02-04 21:02:38.968 INFO: val_f_rmse: 0.047251
##### Step: 178 Learning rate: 3.90625e-05 #####
2025-02-04 21:04:30.406 INFO: ##### Step: 178 Learning rate: 3.90625e-05 #####
Epoch 39, Train Loss: 1.8842, Val Loss: 2.4263
2025-02-04 21:04:30.407 INFO: Epoch 39, Train Loss: 1.8842, Val Loss: 2.4263
train_e/atom_mae: 0.000527
2025-02-04 21:04:30.408 INFO: train_e/atom_mae: 0.000527
train_e/atom_rmse: 0.000910
2025-02-04 21:04:30.408 INFO: train_e/atom_rmse: 0.000910
train_f_mae: 0.026069
2025-02-04 21:04:30.411 INFO: train_f_mae: 0.026069
train_f_rmse: 0.039737
2025-02-04 21:04:30.411 INFO: train_f_rmse: 0.039737
val_e/atom_mae: 0.000515
2025-02-04 21:04:30.413 INFO: val_e/atom_mae: 0.000515
val_e/atom_rmse: 0.000723
2025-02-04 21:04:30.414 INFO: val_e/atom_rmse: 0.000723
val_f_mae: 0.027719
2025-02-04 21:04:30.414 INFO: val_f_mae: 0.027719
val_f_rmse: 0.047263
2025-02-04 21:04:30.414 INFO: val_f_rmse: 0.047263
##### Step: 179 Learning rate: 3.90625e-05 #####
2025-02-04 21:06:22.568 INFO: ##### Step: 179 Learning rate: 3.90625e-05 #####
Epoch 40, Train Loss: 1.8932, Val Loss: 2.4203
2025-02-04 21:06:22.568 INFO: Epoch 40, Train Loss: 1.8932, Val Loss: 2.4203
train_e/atom_mae: 0.000533
2025-02-04 21:06:22.570 INFO: train_e/atom_mae: 0.000533
train_e/atom_rmse: 0.000920
2025-02-04 21:06:22.570 INFO: train_e/atom_rmse: 0.000920
train_f_mae: 0.026104
2025-02-04 21:06:22.573 INFO: train_f_mae: 0.026104
train_f_rmse: 0.039768
2025-02-04 21:06:22.573 INFO: train_f_rmse: 0.039768
val_e/atom_mae: 0.000508
2025-02-04 21:06:22.575 INFO: val_e/atom_mae: 0.000508
val_e/atom_rmse: 0.000717
2025-02-04 21:06:22.576 INFO: val_e/atom_rmse: 0.000717
val_f_mae: 0.027702
2025-02-04 21:06:22.576 INFO: val_f_mae: 0.027702
val_f_rmse: 0.047240
2025-02-04 21:06:22.576 INFO: val_f_rmse: 0.047240
##### Step: 180 Learning rate: 1.953125e-05 #####
2025-02-04 21:08:16.408 INFO: ##### Step: 180 Learning rate: 1.953125e-05 #####
Epoch 41, Train Loss: 1.9204, Val Loss: 2.4249
2025-02-04 21:08:16.408 INFO: Epoch 41, Train Loss: 1.9204, Val Loss: 2.4249
train_e/atom_mae: 0.000605
2025-02-04 21:08:16.410 INFO: train_e/atom_mae: 0.000605
train_e/atom_rmse: 0.000964
2025-02-04 21:08:16.412 INFO: train_e/atom_rmse: 0.000964
train_f_mae: 0.026033
2025-02-04 21:08:16.415 INFO: train_f_mae: 0.026033
train_f_rmse: 0.039720
2025-02-04 21:08:16.415 INFO: train_f_rmse: 0.039720
val_e/atom_mae: 0.000514
2025-02-04 21:08:16.417 INFO: val_e/atom_mae: 0.000514
val_e/atom_rmse: 0.000724
2025-02-04 21:08:16.417 INFO: val_e/atom_rmse: 0.000724
val_f_mae: 0.027695
2025-02-04 21:08:16.418 INFO: val_f_mae: 0.027695
val_f_rmse: 0.047248
2025-02-04 21:08:16.418 INFO: val_f_rmse: 0.047248
##### Step: 181 Learning rate: 1.953125e-05 #####
2025-02-04 21:10:07.724 INFO: ##### Step: 181 Learning rate: 1.953125e-05 #####
Epoch 42, Train Loss: 1.9019, Val Loss: 2.4207
2025-02-04 21:10:07.725 INFO: Epoch 42, Train Loss: 1.9019, Val Loss: 2.4207
train_e/atom_mae: 0.000560
2025-02-04 21:10:07.726 INFO: train_e/atom_mae: 0.000560
train_e/atom_rmse: 0.000940
2025-02-04 21:10:07.726 INFO: train_e/atom_rmse: 0.000940
train_f_mae: 0.026034
2025-02-04 21:10:07.729 INFO: train_f_mae: 0.026034
train_f_rmse: 0.039702
2025-02-04 21:10:07.729 INFO: train_f_rmse: 0.039702
val_e/atom_mae: 0.000512
2025-02-04 21:10:07.731 INFO: val_e/atom_mae: 0.000512
val_e/atom_rmse: 0.000719
2025-02-04 21:10:07.731 INFO: val_e/atom_rmse: 0.000719
val_f_mae: 0.027701
2025-02-04 21:10:07.732 INFO: val_f_mae: 0.027701
val_f_rmse: 0.047233
2025-02-04 21:10:07.732 INFO: val_f_rmse: 0.047233
##### Step: 182 Learning rate: 1.953125e-05 #####
2025-02-04 21:12:00.650 INFO: ##### Step: 182 Learning rate: 1.953125e-05 #####
Epoch 43, Train Loss: 1.8740, Val Loss: 2.4159
2025-02-04 21:12:00.651 INFO: Epoch 43, Train Loss: 1.8740, Val Loss: 2.4159
train_e/atom_mae: 0.000503
2025-02-04 21:12:00.652 INFO: train_e/atom_mae: 0.000503
train_e/atom_rmse: 0.000898
2025-02-04 21:12:00.652 INFO: train_e/atom_rmse: 0.000898
train_f_mae: 0.026021
2025-02-04 21:12:00.655 INFO: train_f_mae: 0.026021
train_f_rmse: 0.039709
2025-02-04 21:12:00.655 INFO: train_f_rmse: 0.039709
val_e/atom_mae: 0.000503
2025-02-04 21:12:00.657 INFO: val_e/atom_mae: 0.000503
val_e/atom_rmse: 0.000712
2025-02-04 21:12:00.657 INFO: val_e/atom_rmse: 0.000712
val_f_mae: 0.027696
2025-02-04 21:12:00.658 INFO: val_f_mae: 0.027696
val_f_rmse: 0.047221
2025-02-04 21:12:00.658 INFO: val_f_rmse: 0.047221
##### Step: 183 Learning rate: 1.953125e-05 #####
2025-02-04 21:13:52.583 INFO: ##### Step: 183 Learning rate: 1.953125e-05 #####
Epoch 44, Train Loss: 1.8771, Val Loss: 2.4203
2025-02-04 21:13:52.584 INFO: Epoch 44, Train Loss: 1.8771, Val Loss: 2.4203
train_e/atom_mae: 0.000519
2025-02-04 21:13:52.586 INFO: train_e/atom_mae: 0.000519
train_e/atom_rmse: 0.000901
2025-02-04 21:13:52.587 INFO: train_e/atom_rmse: 0.000901
train_f_mae: 0.026041
2025-02-04 21:13:52.590 INFO: train_f_mae: 0.026041
train_f_rmse: 0.039721
2025-02-04 21:13:52.590 INFO: train_f_rmse: 0.039721
val_e/atom_mae: 0.000501
2025-02-04 21:13:52.592 INFO: val_e/atom_mae: 0.000501
val_e/atom_rmse: 0.000712
2025-02-04 21:13:52.592 INFO: val_e/atom_rmse: 0.000712
val_f_mae: 0.027704
2025-02-04 21:13:52.593 INFO: val_f_mae: 0.027704
val_f_rmse: 0.047265
2025-02-04 21:13:52.593 INFO: val_f_rmse: 0.047265
##### Step: 184 Learning rate: 1.953125e-05 #####
2025-02-04 21:15:44.014 INFO: ##### Step: 184 Learning rate: 1.953125e-05 #####
Epoch 45, Train Loss: 1.8692, Val Loss: 2.4190
2025-02-04 21:15:44.015 INFO: Epoch 45, Train Loss: 1.8692, Val Loss: 2.4190
train_e/atom_mae: 0.000508
2025-02-04 21:15:44.015 INFO: train_e/atom_mae: 0.000508
train_e/atom_rmse: 0.000895
2025-02-04 21:15:44.016 INFO: train_e/atom_rmse: 0.000895
train_f_mae: 0.026018
2025-02-04 21:15:44.018 INFO: train_f_mae: 0.026018
train_f_rmse: 0.039673
2025-02-04 21:15:44.019 INFO: train_f_rmse: 0.039673
val_e/atom_mae: 0.000505
2025-02-04 21:15:44.021 INFO: val_e/atom_mae: 0.000505
val_e/atom_rmse: 0.000714
2025-02-04 21:15:44.021 INFO: val_e/atom_rmse: 0.000714
val_f_mae: 0.027697
2025-02-04 21:15:44.021 INFO: val_f_mae: 0.027697
val_f_rmse: 0.047239
2025-02-04 21:15:44.022 INFO: val_f_rmse: 0.047239
##### Step: 185 Learning rate: 1.953125e-05 #####
2025-02-04 21:17:39.945 INFO: ##### Step: 185 Learning rate: 1.953125e-05 #####
Epoch 46, Train Loss: 1.8776, Val Loss: 2.4215
2025-02-04 21:17:39.946 INFO: Epoch 46, Train Loss: 1.8776, Val Loss: 2.4215
train_e/atom_mae: 0.000514
2025-02-04 21:17:39.966 INFO: train_e/atom_mae: 0.000514
train_e/atom_rmse: 0.000903
2025-02-04 21:17:40.065 INFO: train_e/atom_rmse: 0.000903
train_f_mae: 0.026033
2025-02-04 21:17:40.067 INFO: train_f_mae: 0.026033
train_f_rmse: 0.039707
2025-02-04 21:17:40.068 INFO: train_f_rmse: 0.039707
val_e/atom_mae: 0.000506
2025-02-04 21:17:40.073 INFO: val_e/atom_mae: 0.000506
val_e/atom_rmse: 0.000716
2025-02-04 21:17:40.073 INFO: val_e/atom_rmse: 0.000716
val_f_mae: 0.027694
2025-02-04 21:17:40.074 INFO: val_f_mae: 0.027694
val_f_rmse: 0.047254
2025-02-04 21:17:40.074 INFO: val_f_rmse: 0.047254
##### Step: 186 Learning rate: 1.953125e-05 #####
2025-02-04 21:19:31.869 INFO: ##### Step: 186 Learning rate: 1.953125e-05 #####
Epoch 47, Train Loss: 1.8838, Val Loss: 2.4182
2025-02-04 21:19:31.870 INFO: Epoch 47, Train Loss: 1.8838, Val Loss: 2.4182
train_e/atom_mae: 0.000520
2025-02-04 21:19:31.871 INFO: train_e/atom_mae: 0.000520
train_e/atom_rmse: 0.000913
2025-02-04 21:19:31.871 INFO: train_e/atom_rmse: 0.000913
train_f_mae: 0.026038
2025-02-04 21:19:31.873 INFO: train_f_mae: 0.026038
train_f_rmse: 0.039707
2025-02-04 21:19:31.874 INFO: train_f_rmse: 0.039707
val_e/atom_mae: 0.000509
2025-02-04 21:19:31.876 INFO: val_e/atom_mae: 0.000509
val_e/atom_rmse: 0.000717
2025-02-04 21:19:31.876 INFO: val_e/atom_rmse: 0.000717
val_f_mae: 0.027685
2025-02-04 21:19:31.877 INFO: val_f_mae: 0.027685
val_f_rmse: 0.047219
2025-02-04 21:19:31.877 INFO: val_f_rmse: 0.047219
##### Step: 187 Learning rate: 1.953125e-05 #####
2025-02-04 21:21:23.515 INFO: ##### Step: 187 Learning rate: 1.953125e-05 #####
Epoch 48, Train Loss: 1.8729, Val Loss: 2.4214
2025-02-04 21:21:23.516 INFO: Epoch 48, Train Loss: 1.8729, Val Loss: 2.4214
train_e/atom_mae: 0.000511
2025-02-04 21:21:23.517 INFO: train_e/atom_mae: 0.000511
train_e/atom_rmse: 0.000904
2025-02-04 21:21:23.517 INFO: train_e/atom_rmse: 0.000904
train_f_mae: 0.026010
2025-02-04 21:21:23.520 INFO: train_f_mae: 0.026010
train_f_rmse: 0.039645
2025-02-04 21:21:23.520 INFO: train_f_rmse: 0.039645
val_e/atom_mae: 0.000509
2025-02-04 21:21:23.522 INFO: val_e/atom_mae: 0.000509
val_e/atom_rmse: 0.000718
2025-02-04 21:21:23.523 INFO: val_e/atom_rmse: 0.000718
val_f_mae: 0.027703
2025-02-04 21:21:23.523 INFO: val_f_mae: 0.027703
val_f_rmse: 0.047242
2025-02-04 21:21:23.523 INFO: val_f_rmse: 0.047242
##### Step: 188 Learning rate: 1.953125e-05 #####
2025-02-04 21:23:16.170 INFO: ##### Step: 188 Learning rate: 1.953125e-05 #####
Epoch 49, Train Loss: 1.8948, Val Loss: 2.4292
2025-02-04 21:23:16.171 INFO: Epoch 49, Train Loss: 1.8948, Val Loss: 2.4292
train_e/atom_mae: 0.000563
2025-02-04 21:23:16.216 INFO: train_e/atom_mae: 0.000563
train_e/atom_rmse: 0.000930
2025-02-04 21:23:16.218 INFO: train_e/atom_rmse: 0.000930
train_f_mae: 0.026021
2025-02-04 21:23:16.221 INFO: train_f_mae: 0.026021
train_f_rmse: 0.039694
2025-02-04 21:23:16.221 INFO: train_f_rmse: 0.039694
val_e/atom_mae: 0.000513
2025-02-04 21:23:16.223 INFO: val_e/atom_mae: 0.000513
val_e/atom_rmse: 0.000726
2025-02-04 21:23:16.223 INFO: val_e/atom_rmse: 0.000726
val_f_mae: 0.027704
2025-02-04 21:23:16.224 INFO: val_f_mae: 0.027704
val_f_rmse: 0.047285
2025-02-04 21:23:16.224 INFO: val_f_rmse: 0.047285
##### Step: 189 Learning rate: 1.953125e-05 #####
2025-02-04 21:25:08.201 INFO: ##### Step: 189 Learning rate: 1.953125e-05 #####
Epoch 50, Train Loss: 1.9112, Val Loss: 2.4211
2025-02-04 21:25:08.202 INFO: Epoch 50, Train Loss: 1.9112, Val Loss: 2.4211
train_e/atom_mae: 0.000602
2025-02-04 21:25:08.203 INFO: train_e/atom_mae: 0.000602
train_e/atom_rmse: 0.000953
2025-02-04 21:25:08.203 INFO: train_e/atom_rmse: 0.000953
train_f_mae: 0.026039
2025-02-04 21:25:08.206 INFO: train_f_mae: 0.026039
train_f_rmse: 0.039706
2025-02-04 21:25:08.206 INFO: train_f_rmse: 0.039706
val_e/atom_mae: 0.000506
2025-02-04 21:25:08.208 INFO: val_e/atom_mae: 0.000506
val_e/atom_rmse: 0.000714
2025-02-04 21:25:08.208 INFO: val_e/atom_rmse: 0.000714
val_f_mae: 0.027701
2025-02-04 21:25:08.209 INFO: val_f_mae: 0.027701
val_f_rmse: 0.047259
2025-02-04 21:25:08.209 INFO: val_f_rmse: 0.047259
##### Step: 190 Learning rate: 1.953125e-05 #####
2025-02-04 21:27:00.214 INFO: ##### Step: 190 Learning rate: 1.953125e-05 #####
Epoch 51, Train Loss: 1.9224, Val Loss: 2.4176
2025-02-04 21:27:00.215 INFO: Epoch 51, Train Loss: 1.9224, Val Loss: 2.4176
train_e/atom_mae: 0.000602
2025-02-04 21:27:00.215 INFO: train_e/atom_mae: 0.000602
train_e/atom_rmse: 0.000974
2025-02-04 21:27:00.216 INFO: train_e/atom_rmse: 0.000974
train_f_mae: 0.025998
2025-02-04 21:27:00.218 INFO: train_f_mae: 0.025998
train_f_rmse: 0.039657
2025-02-04 21:27:00.219 INFO: train_f_rmse: 0.039657
val_e/atom_mae: 0.000505
2025-02-04 21:27:00.224 INFO: val_e/atom_mae: 0.000505
val_e/atom_rmse: 0.000716
2025-02-04 21:27:00.224 INFO: val_e/atom_rmse: 0.000716
val_f_mae: 0.027687
2025-02-04 21:27:00.225 INFO: val_f_mae: 0.027687
val_f_rmse: 0.047219
2025-02-04 21:27:00.225 INFO: val_f_rmse: 0.047219
##### Step: 191 Learning rate: 1.953125e-05 #####
2025-02-04 21:28:53.660 INFO: ##### Step: 191 Learning rate: 1.953125e-05 #####
Epoch 52, Train Loss: 1.8765, Val Loss: 2.4199
2025-02-04 21:28:53.660 INFO: Epoch 52, Train Loss: 1.8765, Val Loss: 2.4199
train_e/atom_mae: 0.000506
2025-02-04 21:28:53.670 INFO: train_e/atom_mae: 0.000506
train_e/atom_rmse: 0.000905
2025-02-04 21:28:53.742 INFO: train_e/atom_rmse: 0.000905
train_f_mae: 0.026019
2025-02-04 21:28:53.745 INFO: train_f_mae: 0.026019
train_f_rmse: 0.039680
2025-02-04 21:28:53.745 INFO: train_f_rmse: 0.039680
val_e/atom_mae: 0.000504
2025-02-04 21:28:53.747 INFO: val_e/atom_mae: 0.000504
val_e/atom_rmse: 0.000716
2025-02-04 21:28:53.747 INFO: val_e/atom_rmse: 0.000716
val_f_mae: 0.027690
2025-02-04 21:28:53.748 INFO: val_f_mae: 0.027690
val_f_rmse: 0.047238
2025-02-04 21:28:53.748 INFO: val_f_rmse: 0.047238
##### Step: 192 Learning rate: 1.953125e-05 #####
2025-02-04 21:30:46.144 INFO: ##### Step: 192 Learning rate: 1.953125e-05 #####
Epoch 53, Train Loss: 1.8761, Val Loss: 2.4173
2025-02-04 21:30:46.144 INFO: Epoch 53, Train Loss: 1.8761, Val Loss: 2.4173
train_e/atom_mae: 0.000500
2025-02-04 21:30:46.145 INFO: train_e/atom_mae: 0.000500
train_e/atom_rmse: 0.000904
2025-02-04 21:30:46.145 INFO: train_e/atom_rmse: 0.000904
train_f_mae: 0.026013
2025-02-04 21:30:46.148 INFO: train_f_mae: 0.026013
train_f_rmse: 0.039686
2025-02-04 21:30:46.148 INFO: train_f_rmse: 0.039686
val_e/atom_mae: 0.000508
2025-02-04 21:30:46.150 INFO: val_e/atom_mae: 0.000508
val_e/atom_rmse: 0.000718
2025-02-04 21:30:46.151 INFO: val_e/atom_rmse: 0.000718
val_f_mae: 0.027676
2025-02-04 21:30:46.151 INFO: val_f_mae: 0.027676
val_f_rmse: 0.047201
2025-02-04 21:30:46.152 INFO: val_f_rmse: 0.047201
##### Step: 193 Learning rate: 1.953125e-05 #####
2025-02-04 21:32:38.168 INFO: ##### Step: 193 Learning rate: 1.953125e-05 #####
Epoch 54, Train Loss: 1.8778, Val Loss: 2.4198
2025-02-04 21:32:38.168 INFO: Epoch 54, Train Loss: 1.8778, Val Loss: 2.4198
train_e/atom_mae: 0.000511
2025-02-04 21:32:38.169 INFO: train_e/atom_mae: 0.000511
train_e/atom_rmse: 0.000907
2025-02-04 21:32:38.170 INFO: train_e/atom_rmse: 0.000907
train_f_mae: 0.026017
2025-02-04 21:32:38.172 INFO: train_f_mae: 0.026017
train_f_rmse: 0.039683
2025-02-04 21:32:38.172 INFO: train_f_rmse: 0.039683
val_e/atom_mae: 0.000506
2025-02-04 21:32:38.174 INFO: val_e/atom_mae: 0.000506
val_e/atom_rmse: 0.000717
2025-02-04 21:32:38.175 INFO: val_e/atom_rmse: 0.000717
val_f_mae: 0.027690
2025-02-04 21:32:38.175 INFO: val_f_mae: 0.027690
val_f_rmse: 0.047234
2025-02-04 21:32:38.176 INFO: val_f_rmse: 0.047234
##### Step: 194 Learning rate: 1.953125e-05 #####
2025-02-04 21:34:29.783 INFO: ##### Step: 194 Learning rate: 1.953125e-05 #####
Epoch 55, Train Loss: 1.8822, Val Loss: 2.4139
2025-02-04 21:34:29.784 INFO: Epoch 55, Train Loss: 1.8822, Val Loss: 2.4139
train_e/atom_mae: 0.000510
2025-02-04 21:34:29.785 INFO: train_e/atom_mae: 0.000510
train_e/atom_rmse: 0.000911
2025-02-04 21:34:29.786 INFO: train_e/atom_rmse: 0.000911
train_f_mae: 0.026024
2025-02-04 21:34:29.788 INFO: train_f_mae: 0.026024
train_f_rmse: 0.039702
2025-02-04 21:34:29.788 INFO: train_f_rmse: 0.039702
val_e/atom_mae: 0.000503
2025-02-04 21:34:29.791 INFO: val_e/atom_mae: 0.000503
val_e/atom_rmse: 0.000713
2025-02-04 21:34:29.791 INFO: val_e/atom_rmse: 0.000713
val_f_mae: 0.027687
2025-02-04 21:34:29.791 INFO: val_f_mae: 0.027687
val_f_rmse: 0.047197
2025-02-04 21:34:29.792 INFO: val_f_rmse: 0.047197
##### Step: 195 Learning rate: 1.953125e-05 #####
2025-02-04 21:36:29.572 INFO: ##### Step: 195 Learning rate: 1.953125e-05 #####
Epoch 56, Train Loss: 1.8859, Val Loss: 2.4149
2025-02-04 21:36:29.573 INFO: Epoch 56, Train Loss: 1.8859, Val Loss: 2.4149
train_e/atom_mae: 0.000528
2025-02-04 21:36:29.605 INFO: train_e/atom_mae: 0.000528
train_e/atom_rmse: 0.000918
2025-02-04 21:36:29.653 INFO: train_e/atom_rmse: 0.000918
train_f_mae: 0.026021
2025-02-04 21:36:29.656 INFO: train_f_mae: 0.026021
train_f_rmse: 0.039689
2025-02-04 21:36:29.656 INFO: train_f_rmse: 0.039689
val_e/atom_mae: 0.000500
2025-02-04 21:36:29.658 INFO: val_e/atom_mae: 0.000500
val_e/atom_rmse: 0.000709
2025-02-04 21:36:29.658 INFO: val_e/atom_rmse: 0.000709
val_f_mae: 0.027689
2025-02-04 21:36:29.659 INFO: val_f_mae: 0.027689
val_f_rmse: 0.047225
2025-02-04 21:36:29.659 INFO: val_f_rmse: 0.047225
##### Step: 196 Learning rate: 1.953125e-05 #####
2025-02-04 21:38:21.062 INFO: ##### Step: 196 Learning rate: 1.953125e-05 #####
Epoch 57, Train Loss: 1.8834, Val Loss: 2.4124
2025-02-04 21:38:21.063 INFO: Epoch 57, Train Loss: 1.8834, Val Loss: 2.4124
train_e/atom_mae: 0.000538
2025-02-04 21:38:21.064 INFO: train_e/atom_mae: 0.000538
train_e/atom_rmse: 0.000916
2025-02-04 21:38:21.064 INFO: train_e/atom_rmse: 0.000916
train_f_mae: 0.026011
2025-02-04 21:38:21.067 INFO: train_f_mae: 0.026011
train_f_rmse: 0.039676
2025-02-04 21:38:21.067 INFO: train_f_rmse: 0.039676
val_e/atom_mae: 0.000500
2025-02-04 21:38:21.069 INFO: val_e/atom_mae: 0.000500
val_e/atom_rmse: 0.000711
2025-02-04 21:38:21.069 INFO: val_e/atom_rmse: 0.000711
val_f_mae: 0.027689
2025-02-04 21:38:21.070 INFO: val_f_mae: 0.027689
val_f_rmse: 0.047191
2025-02-04 21:38:21.070 INFO: val_f_rmse: 0.047191
##### Step: 197 Learning rate: 1.953125e-05 #####
2025-02-04 21:40:12.471 INFO: ##### Step: 197 Learning rate: 1.953125e-05 #####
Epoch 58, Train Loss: 1.8585, Val Loss: 2.4130
2025-02-04 21:40:12.472 INFO: Epoch 58, Train Loss: 1.8585, Val Loss: 2.4130
train_e/atom_mae: 0.000490
2025-02-04 21:40:12.473 INFO: train_e/atom_mae: 0.000490
train_e/atom_rmse: 0.000875
2025-02-04 21:40:12.473 INFO: train_e/atom_rmse: 0.000875
train_f_mae: 0.026023
2025-02-04 21:40:12.476 INFO: train_f_mae: 0.026023
train_f_rmse: 0.039699
2025-02-04 21:40:12.476 INFO: train_f_rmse: 0.039699
val_e/atom_mae: 0.000500
2025-02-04 21:40:12.478 INFO: val_e/atom_mae: 0.000500
val_e/atom_rmse: 0.000711
2025-02-04 21:40:12.479 INFO: val_e/atom_rmse: 0.000711
val_f_mae: 0.027688
2025-02-04 21:40:12.479 INFO: val_f_mae: 0.027688
val_f_rmse: 0.047198
2025-02-04 21:40:12.480 INFO: val_f_rmse: 0.047198
##### Step: 198 Learning rate: 1.953125e-05 #####
2025-02-04 21:42:07.476 INFO: ##### Step: 198 Learning rate: 1.953125e-05 #####
Epoch 59, Train Loss: 1.8610, Val Loss: 2.4190
2025-02-04 21:42:07.477 INFO: Epoch 59, Train Loss: 1.8610, Val Loss: 2.4190
train_e/atom_mae: 0.000492
2025-02-04 21:42:07.503 INFO: train_e/atom_mae: 0.000492
train_e/atom_rmse: 0.000885
2025-02-04 21:42:07.511 INFO: train_e/atom_rmse: 0.000885
train_f_mae: 0.025999
2025-02-04 21:42:07.514 INFO: train_f_mae: 0.025999
train_f_rmse: 0.039649
2025-02-04 21:42:07.514 INFO: train_f_rmse: 0.039649
val_e/atom_mae: 0.000506
2025-02-04 21:42:07.516 INFO: val_e/atom_mae: 0.000506
val_e/atom_rmse: 0.000714
2025-02-04 21:42:07.517 INFO: val_e/atom_rmse: 0.000714
val_f_mae: 0.027689
2025-02-04 21:42:07.517 INFO: val_f_mae: 0.027689
val_f_rmse: 0.047238
2025-02-04 21:42:07.518 INFO: val_f_rmse: 0.047238
##### Step: 199 Learning rate: 1.953125e-05 #####
2025-02-04 21:43:59.418 INFO: ##### Step: 199 Learning rate: 1.953125e-05 #####
Epoch 60, Train Loss: 1.8868, Val Loss: 2.4194
2025-02-04 21:43:59.418 INFO: Epoch 60, Train Loss: 1.8868, Val Loss: 2.4194
train_e/atom_mae: 0.000537
2025-02-04 21:43:59.419 INFO: train_e/atom_mae: 0.000537
train_e/atom_rmse: 0.000919
2025-02-04 21:43:59.419 INFO: train_e/atom_rmse: 0.000919
train_f_mae: 0.026013
2025-02-04 21:43:59.422 INFO: train_f_mae: 0.026013
train_f_rmse: 0.039691
2025-02-04 21:43:59.422 INFO: train_f_rmse: 0.039691
val_e/atom_mae: 0.000506
2025-02-04 21:43:59.424 INFO: val_e/atom_mae: 0.000506
val_e/atom_rmse: 0.000715
2025-02-04 21:43:59.425 INFO: val_e/atom_rmse: 0.000715
val_f_mae: 0.027678
2025-02-04 21:43:59.425 INFO: val_f_mae: 0.027678
val_f_rmse: 0.047237
2025-02-04 21:43:59.425 INFO: val_f_rmse: 0.047237
##### Step: 200 Learning rate: 9.765625e-06 #####
2025-02-04 21:45:51.242 INFO: ##### Step: 200 Learning rate: 9.765625e-06 #####
Epoch 61, Train Loss: 1.8634, Val Loss: 2.4186
2025-02-04 21:45:51.243 INFO: Epoch 61, Train Loss: 1.8634, Val Loss: 2.4186
train_e/atom_mae: 0.000486
2025-02-04 21:45:51.244 INFO: train_e/atom_mae: 0.000486
train_e/atom_rmse: 0.000886
2025-02-04 21:45:51.245 INFO: train_e/atom_rmse: 0.000886
train_f_mae: 0.025998
2025-02-04 21:45:51.247 INFO: train_f_mae: 0.025998
train_f_rmse: 0.039676
2025-02-04 21:45:51.248 INFO: train_f_rmse: 0.039676
val_e/atom_mae: 0.000502
2025-02-04 21:45:51.250 INFO: val_e/atom_mae: 0.000502
val_e/atom_rmse: 0.000714
2025-02-04 21:45:51.250 INFO: val_e/atom_rmse: 0.000714
val_f_mae: 0.027679
2025-02-04 21:45:51.251 INFO: val_f_mae: 0.027679
val_f_rmse: 0.047239
2025-02-04 21:45:51.251 INFO: val_f_rmse: 0.047239
##### Step: 201 Learning rate: 9.765625e-06 #####
2025-02-04 21:47:47.191 INFO: ##### Step: 201 Learning rate: 9.765625e-06 #####
Epoch 62, Train Loss: 1.8685, Val Loss: 2.4155
2025-02-04 21:47:47.192 INFO: Epoch 62, Train Loss: 1.8685, Val Loss: 2.4155
train_e/atom_mae: 0.000503
2025-02-04 21:47:47.208 INFO: train_e/atom_mae: 0.000503
train_e/atom_rmse: 0.000894
2025-02-04 21:47:47.209 INFO: train_e/atom_rmse: 0.000894
train_f_mae: 0.026005
2025-02-04 21:47:47.212 INFO: train_f_mae: 0.026005
train_f_rmse: 0.039670
2025-02-04 21:47:47.212 INFO: train_f_rmse: 0.039670
val_e/atom_mae: 0.000504
2025-02-04 21:47:47.214 INFO: val_e/atom_mae: 0.000504
val_e/atom_rmse: 0.000716
2025-02-04 21:47:47.215 INFO: val_e/atom_rmse: 0.000716
val_f_mae: 0.027681
2025-02-04 21:47:47.215 INFO: val_f_mae: 0.027681
val_f_rmse: 0.047197
2025-02-04 21:47:47.215 INFO: val_f_rmse: 0.047197
##### Step: 202 Learning rate: 9.765625e-06 #####
2025-02-04 21:49:39.015 INFO: ##### Step: 202 Learning rate: 9.765625e-06 #####
Epoch 63, Train Loss: 1.8733, Val Loss: 2.4181
2025-02-04 21:49:39.016 INFO: Epoch 63, Train Loss: 1.8733, Val Loss: 2.4181
train_e/atom_mae: 0.000524
2025-02-04 21:49:39.017 INFO: train_e/atom_mae: 0.000524
train_e/atom_rmse: 0.000902
2025-02-04 21:49:39.017 INFO: train_e/atom_rmse: 0.000902
train_f_mae: 0.025996
2025-02-04 21:49:39.023 INFO: train_f_mae: 0.025996
train_f_rmse: 0.039662
2025-02-04 21:49:39.023 INFO: train_f_rmse: 0.039662
val_e/atom_mae: 0.000504
2025-02-04 21:49:39.025 INFO: val_e/atom_mae: 0.000504
val_e/atom_rmse: 0.000715
2025-02-04 21:49:39.026 INFO: val_e/atom_rmse: 0.000715
val_f_mae: 0.027686
2025-02-04 21:49:39.026 INFO: val_f_mae: 0.027686
val_f_rmse: 0.047226
2025-02-04 21:49:39.026 INFO: val_f_rmse: 0.047226
##### Step: 203 Learning rate: 9.765625e-06 #####
2025-02-04 21:51:31.463 INFO: ##### Step: 203 Learning rate: 9.765625e-06 #####
Epoch 64, Train Loss: 1.8647, Val Loss: 2.4189
2025-02-04 21:51:31.463 INFO: Epoch 64, Train Loss: 1.8647, Val Loss: 2.4189
train_e/atom_mae: 0.000517
2025-02-04 21:51:31.466 INFO: train_e/atom_mae: 0.000517
train_e/atom_rmse: 0.000889
2025-02-04 21:51:31.467 INFO: train_e/atom_rmse: 0.000889
train_f_mae: 0.025993
2025-02-04 21:51:31.469 INFO: train_f_mae: 0.025993
train_f_rmse: 0.039664
2025-02-04 21:51:31.470 INFO: train_f_rmse: 0.039664
val_e/atom_mae: 0.000503
2025-02-04 21:51:31.472 INFO: val_e/atom_mae: 0.000503
val_e/atom_rmse: 0.000714
2025-02-04 21:51:31.472 INFO: val_e/atom_rmse: 0.000714
val_f_mae: 0.027683
2025-02-04 21:51:31.473 INFO: val_f_mae: 0.027683
val_f_rmse: 0.047242
2025-02-04 21:51:31.473 INFO: val_f_rmse: 0.047242
##### Step: 204 Learning rate: 9.765625e-06 #####
2025-02-04 21:53:23.417 INFO: ##### Step: 204 Learning rate: 9.765625e-06 #####
Epoch 65, Train Loss: 1.8684, Val Loss: 2.4184
2025-02-04 21:53:23.418 INFO: Epoch 65, Train Loss: 1.8684, Val Loss: 2.4184
train_e/atom_mae: 0.000498
2025-02-04 21:53:23.419 INFO: train_e/atom_mae: 0.000498
train_e/atom_rmse: 0.000892
2025-02-04 21:53:23.419 INFO: train_e/atom_rmse: 0.000892
train_f_mae: 0.026011
2025-02-04 21:53:23.422 INFO: train_f_mae: 0.026011
train_f_rmse: 0.039690
2025-02-04 21:53:23.422 INFO: train_f_rmse: 0.039690
val_e/atom_mae: 0.000503
2025-02-04 21:53:23.424 INFO: val_e/atom_mae: 0.000503
val_e/atom_rmse: 0.000714
2025-02-04 21:53:23.424 INFO: val_e/atom_rmse: 0.000714
val_f_mae: 0.027689
2025-02-04 21:53:23.425 INFO: val_f_mae: 0.027689
val_f_rmse: 0.047235
2025-02-04 21:53:23.425 INFO: val_f_rmse: 0.047235
##### Step: 205 Learning rate: 9.765625e-06 #####
2025-02-04 21:55:16.362 INFO: ##### Step: 205 Learning rate: 9.765625e-06 #####
Epoch 66, Train Loss: 1.8611, Val Loss: 2.4189
2025-02-04 21:55:16.363 INFO: Epoch 66, Train Loss: 1.8611, Val Loss: 2.4189
train_e/atom_mae: 0.000486
2025-02-04 21:55:16.364 INFO: train_e/atom_mae: 0.000486
train_e/atom_rmse: 0.000882
2025-02-04 21:55:16.365 INFO: train_e/atom_rmse: 0.000882
train_f_mae: 0.026011
2025-02-04 21:55:16.368 INFO: train_f_mae: 0.026011
train_f_rmse: 0.039677
2025-02-04 21:55:16.368 INFO: train_f_rmse: 0.039677
val_e/atom_mae: 0.000502
2025-02-04 21:55:16.370 INFO: val_e/atom_mae: 0.000502
val_e/atom_rmse: 0.000714
2025-02-04 21:55:16.371 INFO: val_e/atom_rmse: 0.000714
val_f_mae: 0.027691
2025-02-04 21:55:16.371 INFO: val_f_mae: 0.027691
val_f_rmse: 0.047240
2025-02-04 21:55:16.372 INFO: val_f_rmse: 0.047240
##### Step: 206 Learning rate: 9.765625e-06 #####
2025-02-04 21:57:08.371 INFO: ##### Step: 206 Learning rate: 9.765625e-06 #####
Epoch 67, Train Loss: 1.8683, Val Loss: 2.4170
2025-02-04 21:57:08.372 INFO: Epoch 67, Train Loss: 1.8683, Val Loss: 2.4170
train_e/atom_mae: 0.000506
2025-02-04 21:57:08.373 INFO: train_e/atom_mae: 0.000506
train_e/atom_rmse: 0.000892
2025-02-04 21:57:08.373 INFO: train_e/atom_rmse: 0.000892
train_f_mae: 0.026012
2025-02-04 21:57:08.376 INFO: train_f_mae: 0.026012
train_f_rmse: 0.039685
2025-02-04 21:57:08.376 INFO: train_f_rmse: 0.039685
val_e/atom_mae: 0.000500
2025-02-04 21:57:08.378 INFO: val_e/atom_mae: 0.000500
val_e/atom_rmse: 0.000710
2025-02-04 21:57:08.379 INFO: val_e/atom_rmse: 0.000710
val_f_mae: 0.027697
2025-02-04 21:57:08.379 INFO: val_f_mae: 0.027697
val_f_rmse: 0.047245
2025-02-04 21:57:08.379 INFO: val_f_rmse: 0.047245
##### Step: 207 Learning rate: 9.765625e-06 #####
2025-02-04 21:59:00.929 INFO: ##### Step: 207 Learning rate: 9.765625e-06 #####
Epoch 68, Train Loss: 1.8716, Val Loss: 2.4193
2025-02-04 21:59:00.930 INFO: Epoch 68, Train Loss: 1.8716, Val Loss: 2.4193
train_e/atom_mae: 0.000505
2025-02-04 21:59:00.933 INFO: train_e/atom_mae: 0.000505
train_e/atom_rmse: 0.000897
2025-02-04 21:59:00.934 INFO: train_e/atom_rmse: 0.000897
train_f_mae: 0.026005
2025-02-04 21:59:00.936 INFO: train_f_mae: 0.026005
train_f_rmse: 0.039685
2025-02-04 21:59:00.937 INFO: train_f_rmse: 0.039685
val_e/atom_mae: 0.000504
2025-02-04 21:59:00.939 INFO: val_e/atom_mae: 0.000504
val_e/atom_rmse: 0.000715
2025-02-04 21:59:00.939 INFO: val_e/atom_rmse: 0.000715
val_f_mae: 0.027682
2025-02-04 21:59:00.940 INFO: val_f_mae: 0.027682
val_f_rmse: 0.047238
2025-02-04 21:59:00.940 INFO: val_f_rmse: 0.047238
##### Step: 208 Learning rate: 9.765625e-06 #####
2025-02-04 22:00:53.366 INFO: ##### Step: 208 Learning rate: 9.765625e-06 #####
Epoch 69, Train Loss: 1.8651, Val Loss: 2.4191
2025-02-04 22:00:53.366 INFO: Epoch 69, Train Loss: 1.8651, Val Loss: 2.4191
train_e/atom_mae: 0.000508
2025-02-04 22:00:53.367 INFO: train_e/atom_mae: 0.000508
train_e/atom_rmse: 0.000888
2025-02-04 22:00:53.367 INFO: train_e/atom_rmse: 0.000888
train_f_mae: 0.026005
2025-02-04 22:00:53.370 INFO: train_f_mae: 0.026005
train_f_rmse: 0.039682
2025-02-04 22:00:53.370 INFO: train_f_rmse: 0.039682
val_e/atom_mae: 0.000504
2025-02-04 22:00:53.372 INFO: val_e/atom_mae: 0.000504
val_e/atom_rmse: 0.000716
2025-02-04 22:00:53.372 INFO: val_e/atom_rmse: 0.000716
val_f_mae: 0.027685
2025-02-04 22:00:53.373 INFO: val_f_mae: 0.027685
val_f_rmse: 0.047231
2025-02-04 22:00:53.373 INFO: val_f_rmse: 0.047231
##### Step: 209 Learning rate: 9.765625e-06 #####
2025-02-04 22:02:45.747 INFO: ##### Step: 209 Learning rate: 9.765625e-06 #####
Epoch 70, Train Loss: 1.8669, Val Loss: 2.4180
2025-02-04 22:02:45.747 INFO: Epoch 70, Train Loss: 1.8669, Val Loss: 2.4180
train_e/atom_mae: 0.000487
2025-02-04 22:02:45.748 INFO: train_e/atom_mae: 0.000487
train_e/atom_rmse: 0.000891
2025-02-04 22:02:45.750 INFO: train_e/atom_rmse: 0.000891
train_f_mae: 0.026010
2025-02-04 22:02:45.753 INFO: train_f_mae: 0.026010
train_f_rmse: 0.039677
2025-02-04 22:02:45.753 INFO: train_f_rmse: 0.039677
val_e/atom_mae: 0.000502
2025-02-04 22:02:45.755 INFO: val_e/atom_mae: 0.000502
val_e/atom_rmse: 0.000713
2025-02-04 22:02:45.756 INFO: val_e/atom_rmse: 0.000713
val_f_mae: 0.027687
2025-02-04 22:02:45.756 INFO: val_f_mae: 0.027687
val_f_rmse: 0.047239
2025-02-04 22:02:45.756 INFO: val_f_rmse: 0.047239
##### Step: 210 Learning rate: 9.765625e-06 #####
2025-02-04 22:04:38.101 INFO: ##### Step: 210 Learning rate: 9.765625e-06 #####
Epoch 71, Train Loss: 1.8639, Val Loss: 2.4233
2025-02-04 22:04:38.102 INFO: Epoch 71, Train Loss: 1.8639, Val Loss: 2.4233
train_e/atom_mae: 0.000510
2025-02-04 22:04:38.103 INFO: train_e/atom_mae: 0.000510
train_e/atom_rmse: 0.000884
2025-02-04 22:04:38.103 INFO: train_e/atom_rmse: 0.000884
train_f_mae: 0.026008
2025-02-04 22:04:38.106 INFO: train_f_mae: 0.026008
train_f_rmse: 0.039693
2025-02-04 22:04:38.106 INFO: train_f_rmse: 0.039693
val_e/atom_mae: 0.000508
2025-02-04 22:04:38.108 INFO: val_e/atom_mae: 0.000508
val_e/atom_rmse: 0.000719
2025-02-04 22:04:38.109 INFO: val_e/atom_rmse: 0.000719
val_f_mae: 0.027694
2025-02-04 22:04:38.109 INFO: val_f_mae: 0.027694
val_f_rmse: 0.047257
2025-02-04 22:04:38.109 INFO: val_f_rmse: 0.047257
##### Step: 211 Learning rate: 9.765625e-06 #####
2025-02-04 22:06:30.007 INFO: ##### Step: 211 Learning rate: 9.765625e-06 #####
Epoch 72, Train Loss: 1.8641, Val Loss: 2.4196
2025-02-04 22:06:30.008 INFO: Epoch 72, Train Loss: 1.8641, Val Loss: 2.4196
train_e/atom_mae: 0.000492
2025-02-04 22:06:30.009 INFO: train_e/atom_mae: 0.000492
train_e/atom_rmse: 0.000889
2025-02-04 22:06:30.010 INFO: train_e/atom_rmse: 0.000889
train_f_mae: 0.026001
2025-02-04 22:06:30.012 INFO: train_f_mae: 0.026001
train_f_rmse: 0.039662
2025-02-04 22:06:30.013 INFO: train_f_rmse: 0.039662
val_e/atom_mae: 0.000503
2025-02-04 22:06:30.015 INFO: val_e/atom_mae: 0.000503
val_e/atom_rmse: 0.000715
2025-02-04 22:06:30.015 INFO: val_e/atom_rmse: 0.000715
val_f_mae: 0.027688
2025-02-04 22:06:30.016 INFO: val_f_mae: 0.027688
val_f_rmse: 0.047244
2025-02-04 22:06:30.016 INFO: val_f_rmse: 0.047244
##### Step: 212 Learning rate: 9.765625e-06 #####
2025-02-04 22:08:22.051 INFO: ##### Step: 212 Learning rate: 9.765625e-06 #####
Epoch 73, Train Loss: 1.8736, Val Loss: 2.4184
2025-02-04 22:08:22.052 INFO: Epoch 73, Train Loss: 1.8736, Val Loss: 2.4184
train_e/atom_mae: 0.000528
2025-02-04 22:08:22.052 INFO: train_e/atom_mae: 0.000528
train_e/atom_rmse: 0.000903
2025-02-04 22:08:22.053 INFO: train_e/atom_rmse: 0.000903
train_f_mae: 0.025995
2025-02-04 22:08:22.055 INFO: train_f_mae: 0.025995
train_f_rmse: 0.039661
2025-02-04 22:08:22.056 INFO: train_f_rmse: 0.039661
val_e/atom_mae: 0.000503
2025-02-04 22:08:22.058 INFO: val_e/atom_mae: 0.000503
val_e/atom_rmse: 0.000715
2025-02-04 22:08:22.058 INFO: val_e/atom_rmse: 0.000715
val_f_mae: 0.027678
2025-02-04 22:08:22.062 INFO: val_f_mae: 0.027678
val_f_rmse: 0.047230
2025-02-04 22:08:22.062 INFO: val_f_rmse: 0.047230
##### Step: 213 Learning rate: 9.765625e-06 #####
2025-02-04 22:10:13.752 INFO: ##### Step: 213 Learning rate: 9.765625e-06 #####
Epoch 74, Train Loss: 1.8621, Val Loss: 2.4167
2025-02-04 22:10:13.753 INFO: Epoch 74, Train Loss: 1.8621, Val Loss: 2.4167
train_e/atom_mae: 0.000500
2025-02-04 22:10:13.754 INFO: train_e/atom_mae: 0.000500
train_e/atom_rmse: 0.000885
2025-02-04 22:10:13.754 INFO: train_e/atom_rmse: 0.000885
train_f_mae: 0.026001
2025-02-04 22:10:13.757 INFO: train_f_mae: 0.026001
train_f_rmse: 0.039663
2025-02-04 22:10:13.757 INFO: train_f_rmse: 0.039663
val_e/atom_mae: 0.000502
2025-02-04 22:10:13.759 INFO: val_e/atom_mae: 0.000502
val_e/atom_rmse: 0.000712
2025-02-04 22:10:13.759 INFO: val_e/atom_rmse: 0.000712
val_f_mae: 0.027685
2025-02-04 22:10:13.760 INFO: val_f_mae: 0.027685
val_f_rmse: 0.047226
2025-02-04 22:10:13.760 INFO: val_f_rmse: 0.047226
##### Step: 214 Learning rate: 9.765625e-06 #####
2025-02-04 22:12:07.017 INFO: ##### Step: 214 Learning rate: 9.765625e-06 #####
Epoch 75, Train Loss: 1.8642, Val Loss: 2.4163
2025-02-04 22:12:07.017 INFO: Epoch 75, Train Loss: 1.8642, Val Loss: 2.4163
train_e/atom_mae: 0.000487
2025-02-04 22:12:07.020 INFO: train_e/atom_mae: 0.000487
train_e/atom_rmse: 0.000887
2025-02-04 22:12:07.055 INFO: train_e/atom_rmse: 0.000887
train_f_mae: 0.026010
2025-02-04 22:12:07.058 INFO: train_f_mae: 0.026010
train_f_rmse: 0.039675
2025-02-04 22:12:07.058 INFO: train_f_rmse: 0.039675
val_e/atom_mae: 0.000502
2025-02-04 22:12:07.060 INFO: val_e/atom_mae: 0.000502
val_e/atom_rmse: 0.000712
2025-02-04 22:12:07.060 INFO: val_e/atom_rmse: 0.000712
val_f_mae: 0.027680
2025-02-04 22:12:07.061 INFO: val_f_mae: 0.027680
val_f_rmse: 0.047221
2025-02-04 22:12:07.061 INFO: val_f_rmse: 0.047221
##### Step: 215 Learning rate: 9.765625e-06 #####
2025-02-04 22:13:58.890 INFO: ##### Step: 215 Learning rate: 9.765625e-06 #####
Epoch 76, Train Loss: 1.8622, Val Loss: 2.4153
2025-02-04 22:13:58.890 INFO: Epoch 76, Train Loss: 1.8622, Val Loss: 2.4153
train_e/atom_mae: 0.000502
2025-02-04 22:13:58.891 INFO: train_e/atom_mae: 0.000502
train_e/atom_rmse: 0.000885
2025-02-04 22:13:58.891 INFO: train_e/atom_rmse: 0.000885
train_f_mae: 0.025998
2025-02-04 22:13:58.894 INFO: train_f_mae: 0.025998
train_f_rmse: 0.039670
2025-02-04 22:13:58.894 INFO: train_f_rmse: 0.039670
val_e/atom_mae: 0.000501
2025-02-04 22:13:58.896 INFO: val_e/atom_mae: 0.000501
val_e/atom_rmse: 0.000711
2025-02-04 22:13:58.897 INFO: val_e/atom_rmse: 0.000711
val_f_mae: 0.027683
2025-02-04 22:13:58.897 INFO: val_f_mae: 0.027683
val_f_rmse: 0.047222
2025-02-04 22:13:58.897 INFO: val_f_rmse: 0.047222
##### Step: 216 Learning rate: 9.765625e-06 #####
2025-02-04 22:15:51.008 INFO: ##### Step: 216 Learning rate: 9.765625e-06 #####
Epoch 77, Train Loss: 1.8658, Val Loss: 2.4183
2025-02-04 22:15:51.009 INFO: Epoch 77, Train Loss: 1.8658, Val Loss: 2.4183
train_e/atom_mae: 0.000507
2025-02-04 22:15:51.011 INFO: train_e/atom_mae: 0.000507
train_e/atom_rmse: 0.000890
2025-02-04 22:15:51.011 INFO: train_e/atom_rmse: 0.000890
train_f_mae: 0.026008
2025-02-04 22:15:51.014 INFO: train_f_mae: 0.026008
train_f_rmse: 0.039670
2025-02-04 22:15:51.014 INFO: train_f_rmse: 0.039670
val_e/atom_mae: 0.000505
2025-02-04 22:15:51.016 INFO: val_e/atom_mae: 0.000505
val_e/atom_rmse: 0.000716
2025-02-04 22:15:51.017 INFO: val_e/atom_rmse: 0.000716
val_f_mae: 0.027680
2025-02-04 22:15:51.017 INFO: val_f_mae: 0.027680
val_f_rmse: 0.047222
2025-02-04 22:15:51.017 INFO: val_f_rmse: 0.047222
##### Step: 217 Learning rate: 9.765625e-06 #####
2025-02-04 22:17:42.720 INFO: ##### Step: 217 Learning rate: 9.765625e-06 #####
Epoch 78, Train Loss: 1.8763, Val Loss: 2.4215
2025-02-04 22:17:42.720 INFO: Epoch 78, Train Loss: 1.8763, Val Loss: 2.4215
train_e/atom_mae: 0.000520
2025-02-04 22:17:42.721 INFO: train_e/atom_mae: 0.000520
train_e/atom_rmse: 0.000905
2025-02-04 22:17:42.721 INFO: train_e/atom_rmse: 0.000905
train_f_mae: 0.026003
2025-02-04 22:17:42.724 INFO: train_f_mae: 0.026003
train_f_rmse: 0.039679
2025-02-04 22:17:42.724 INFO: train_f_rmse: 0.039679
val_e/atom_mae: 0.000506
2025-02-04 22:17:42.726 INFO: val_e/atom_mae: 0.000506
val_e/atom_rmse: 0.000720
2025-02-04 22:17:42.727 INFO: val_e/atom_rmse: 0.000720
val_f_mae: 0.027680
2025-02-04 22:17:42.727 INFO: val_f_mae: 0.027680
val_f_rmse: 0.047236
2025-02-04 22:17:42.727 INFO: val_f_rmse: 0.047236
##### Step: 218 Learning rate: 9.765625e-06 #####
2025-02-04 22:19:36.885 INFO: ##### Step: 218 Learning rate: 9.765625e-06 #####
Epoch 79, Train Loss: 1.8764, Val Loss: 2.4162
2025-02-04 22:19:36.886 INFO: Epoch 79, Train Loss: 1.8764, Val Loss: 2.4162
train_e/atom_mae: 0.000515
2025-02-04 22:19:36.888 INFO: train_e/atom_mae: 0.000515
train_e/atom_rmse: 0.000906
2025-02-04 22:19:36.890 INFO: train_e/atom_rmse: 0.000906
train_f_mae: 0.026000
2025-02-04 22:19:36.892 INFO: train_f_mae: 0.026000
train_f_rmse: 0.039674
2025-02-04 22:19:36.893 INFO: train_f_rmse: 0.039674
val_e/atom_mae: 0.000502
2025-02-04 22:19:36.895 INFO: val_e/atom_mae: 0.000502
val_e/atom_rmse: 0.000713
2025-02-04 22:19:36.895 INFO: val_e/atom_rmse: 0.000713
val_f_mae: 0.027679
2025-02-04 22:19:36.896 INFO: val_f_mae: 0.027679
val_f_rmse: 0.047219
2025-02-04 22:19:36.896 INFO: val_f_rmse: 0.047219
##### Step: 219 Learning rate: 9.765625e-06 #####
2025-02-04 22:21:31.217 INFO: ##### Step: 219 Learning rate: 9.765625e-06 #####
Epoch 80, Train Loss: 1.8678, Val Loss: 2.4177
2025-02-04 22:21:31.217 INFO: Epoch 80, Train Loss: 1.8678, Val Loss: 2.4177
train_e/atom_mae: 0.000507
2025-02-04 22:21:31.218 INFO: train_e/atom_mae: 0.000507
train_e/atom_rmse: 0.000895
2025-02-04 22:21:31.219 INFO: train_e/atom_rmse: 0.000895
train_f_mae: 0.025995
2025-02-04 22:21:31.221 INFO: train_f_mae: 0.025995
train_f_rmse: 0.039655
2025-02-04 22:21:31.221 INFO: train_f_rmse: 0.039655
val_e/atom_mae: 0.000502
2025-02-04 22:21:31.223 INFO: val_e/atom_mae: 0.000502
val_e/atom_rmse: 0.000712
2025-02-04 22:21:31.224 INFO: val_e/atom_rmse: 0.000712
val_f_mae: 0.027681
2025-02-04 22:21:31.224 INFO: val_f_mae: 0.027681
val_f_rmse: 0.047237
2025-02-04 22:21:31.225 INFO: val_f_rmse: 0.047237
##### Step: 220 Learning rate: 4.8828125e-06 #####
2025-02-04 22:23:24.853 INFO: ##### Step: 220 Learning rate: 4.8828125e-06 #####
Epoch 81, Train Loss: 1.8595, Val Loss: 2.4185
2025-02-04 22:23:24.854 INFO: Epoch 81, Train Loss: 1.8595, Val Loss: 2.4185
train_e/atom_mae: 0.000486
2025-02-04 22:23:24.856 INFO: train_e/atom_mae: 0.000486
train_e/atom_rmse: 0.000880
2025-02-04 22:23:24.857 INFO: train_e/atom_rmse: 0.000880
train_f_mae: 0.026003
2025-02-04 22:23:24.863 INFO: train_f_mae: 0.026003
train_f_rmse: 0.039678
2025-02-04 22:23:24.863 INFO: train_f_rmse: 0.039678
val_e/atom_mae: 0.000501
2025-02-04 22:23:24.865 INFO: val_e/atom_mae: 0.000501
val_e/atom_rmse: 0.000712
2025-02-04 22:23:24.866 INFO: val_e/atom_rmse: 0.000712
val_f_mae: 0.027684
2025-02-04 22:23:24.866 INFO: val_f_mae: 0.027684
val_f_rmse: 0.047245
2025-02-04 22:23:24.866 INFO: val_f_rmse: 0.047245
##### Step: 221 Learning rate: 4.8828125e-06 #####
2025-02-04 22:25:17.948 INFO: ##### Step: 221 Learning rate: 4.8828125e-06 #####
Epoch 82, Train Loss: 1.8602, Val Loss: 2.4182
2025-02-04 22:25:17.949 INFO: Epoch 82, Train Loss: 1.8602, Val Loss: 2.4182
train_e/atom_mae: 0.000480
2025-02-04 22:25:17.949 INFO: train_e/atom_mae: 0.000480
train_e/atom_rmse: 0.000882
2025-02-04 22:25:17.950 INFO: train_e/atom_rmse: 0.000882
train_f_mae: 0.025993
2025-02-04 22:25:17.952 INFO: train_f_mae: 0.025993
train_f_rmse: 0.039670
2025-02-04 22:25:17.953 INFO: train_f_rmse: 0.039670
val_e/atom_mae: 0.000501
2025-02-04 22:25:17.955 INFO: val_e/atom_mae: 0.000501
val_e/atom_rmse: 0.000712
2025-02-04 22:25:17.955 INFO: val_e/atom_rmse: 0.000712
val_f_mae: 0.027684
2025-02-04 22:25:17.955 INFO: val_f_mae: 0.027684
val_f_rmse: 0.047247
2025-02-04 22:25:17.956 INFO: val_f_rmse: 0.047247
##### Step: 222 Learning rate: 4.8828125e-06 #####
2025-02-04 22:27:10.036 INFO: ##### Step: 222 Learning rate: 4.8828125e-06 #####
Epoch 83, Train Loss: 1.8581, Val Loss: 2.4202
2025-02-04 22:27:10.036 INFO: Epoch 83, Train Loss: 1.8581, Val Loss: 2.4202
train_e/atom_mae: 0.000481
2025-02-04 22:27:10.037 INFO: train_e/atom_mae: 0.000481
train_e/atom_rmse: 0.000879
2025-02-04 22:27:10.039 INFO: train_e/atom_rmse: 0.000879
train_f_mae: 0.025998
2025-02-04 22:27:10.042 INFO: train_f_mae: 0.025998
train_f_rmse: 0.039666
2025-02-04 22:27:10.042 INFO: train_f_rmse: 0.039666
val_e/atom_mae: 0.000506
2025-02-04 22:27:10.044 INFO: val_e/atom_mae: 0.000506
val_e/atom_rmse: 0.000718
2025-02-04 22:27:10.045 INFO: val_e/atom_rmse: 0.000718
val_f_mae: 0.027678
2025-02-04 22:27:10.045 INFO: val_f_mae: 0.027678
val_f_rmse: 0.047231
2025-02-04 22:27:10.045 INFO: val_f_rmse: 0.047231
##### Step: 223 Learning rate: 4.8828125e-06 #####
2025-02-04 22:29:02.466 INFO: ##### Step: 223 Learning rate: 4.8828125e-06 #####
Epoch 84, Train Loss: 1.8563, Val Loss: 2.4189
2025-02-04 22:29:02.467 INFO: Epoch 84, Train Loss: 1.8563, Val Loss: 2.4189
train_e/atom_mae: 0.000478
2025-02-04 22:29:02.469 INFO: train_e/atom_mae: 0.000478
train_e/atom_rmse: 0.000875
2025-02-04 22:29:02.469 INFO: train_e/atom_rmse: 0.000875
train_f_mae: 0.026001
2025-02-04 22:29:02.472 INFO: train_f_mae: 0.026001
train_f_rmse: 0.039673
2025-02-04 22:29:02.472 INFO: train_f_rmse: 0.039673
val_e/atom_mae: 0.000503
2025-02-04 22:29:02.475 INFO: val_e/atom_mae: 0.000503
val_e/atom_rmse: 0.000714
2025-02-04 22:29:02.475 INFO: val_e/atom_rmse: 0.000714
val_f_mae: 0.027684
2025-02-04 22:29:02.475 INFO: val_f_mae: 0.027684
val_f_rmse: 0.047237
2025-02-04 22:29:02.476 INFO: val_f_rmse: 0.047237
##### Step: 224 Learning rate: 4.8828125e-06 #####
2025-02-04 22:30:54.620 INFO: ##### Step: 224 Learning rate: 4.8828125e-06 #####
Epoch 85, Train Loss: 1.8553, Val Loss: 2.4155
2025-02-04 22:30:54.620 INFO: Epoch 85, Train Loss: 1.8553, Val Loss: 2.4155
train_e/atom_mae: 0.000483
2025-02-04 22:30:54.621 INFO: train_e/atom_mae: 0.000483
train_e/atom_rmse: 0.000881
2025-02-04 22:30:54.622 INFO: train_e/atom_rmse: 0.000881
train_f_mae: 0.025978
2025-02-04 22:30:54.625 INFO: train_f_mae: 0.025978
train_f_rmse: 0.039612
2025-02-04 22:30:54.625 INFO: train_f_rmse: 0.039612
val_e/atom_mae: 0.000501
2025-02-04 22:30:54.627 INFO: val_e/atom_mae: 0.000501
val_e/atom_rmse: 0.000712
2025-02-04 22:30:54.627 INFO: val_e/atom_rmse: 0.000712
val_f_mae: 0.027677
2025-02-04 22:30:54.628 INFO: val_f_mae: 0.027677
val_f_rmse: 0.047218
2025-02-04 22:30:54.628 INFO: val_f_rmse: 0.047218
##### Step: 225 Learning rate: 4.8828125e-06 #####
2025-02-04 22:32:47.068 INFO: ##### Step: 225 Learning rate: 4.8828125e-06 #####
Epoch 86, Train Loss: 1.8550, Val Loss: 2.4169
2025-02-04 22:32:47.069 INFO: Epoch 86, Train Loss: 1.8550, Val Loss: 2.4169
train_e/atom_mae: 0.000483
2025-02-04 22:32:47.070 INFO: train_e/atom_mae: 0.000483
train_e/atom_rmse: 0.000874
2025-02-04 22:32:47.070 INFO: train_e/atom_rmse: 0.000874
train_f_mae: 0.025998
2025-02-04 22:32:47.073 INFO: train_f_mae: 0.025998
train_f_rmse: 0.039662
2025-02-04 22:32:47.073 INFO: train_f_rmse: 0.039662
val_e/atom_mae: 0.000505
2025-02-04 22:32:47.075 INFO: val_e/atom_mae: 0.000505
val_e/atom_rmse: 0.000715
2025-02-04 22:32:47.075 INFO: val_e/atom_rmse: 0.000715
val_f_mae: 0.027677
2025-02-04 22:32:47.076 INFO: val_f_mae: 0.027677
val_f_rmse: 0.047217
2025-02-04 22:32:47.076 INFO: val_f_rmse: 0.047217
##### Step: 226 Learning rate: 4.8828125e-06 #####
2025-02-04 22:34:39.100 INFO: ##### Step: 226 Learning rate: 4.8828125e-06 #####
Epoch 87, Train Loss: 1.8635, Val Loss: 2.4186
2025-02-04 22:34:39.101 INFO: Epoch 87, Train Loss: 1.8635, Val Loss: 2.4186
train_e/atom_mae: 0.000490
2025-02-04 22:34:39.102 INFO: train_e/atom_mae: 0.000490
train_e/atom_rmse: 0.000887
2025-02-04 22:34:39.102 INFO: train_e/atom_rmse: 0.000887
train_f_mae: 0.025993
2025-02-04 22:34:39.104 INFO: train_f_mae: 0.025993
train_f_rmse: 0.039667
2025-02-04 22:34:39.105 INFO: train_f_rmse: 0.039667
val_e/atom_mae: 0.000503
2025-02-04 22:34:39.107 INFO: val_e/atom_mae: 0.000503
val_e/atom_rmse: 0.000715
2025-02-04 22:34:39.107 INFO: val_e/atom_rmse: 0.000715
val_f_mae: 0.027677
2025-02-04 22:34:39.108 INFO: val_f_mae: 0.027677
val_f_rmse: 0.047233
2025-02-04 22:34:39.108 INFO: val_f_rmse: 0.047233
##### Step: 227 Learning rate: 4.8828125e-06 #####
2025-02-04 22:36:32.925 INFO: ##### Step: 227 Learning rate: 4.8828125e-06 #####
Epoch 88, Train Loss: 1.8624, Val Loss: 2.4156
2025-02-04 22:36:32.926 INFO: Epoch 88, Train Loss: 1.8624, Val Loss: 2.4156
train_e/atom_mae: 0.000484
2025-02-04 22:36:32.929 INFO: train_e/atom_mae: 0.000484
train_e/atom_rmse: 0.000887
2025-02-04 22:36:32.950 INFO: train_e/atom_rmse: 0.000887
train_f_mae: 0.025988
2025-02-04 22:36:32.953 INFO: train_f_mae: 0.025988
train_f_rmse: 0.039651
2025-02-04 22:36:32.953 INFO: train_f_rmse: 0.039651
val_e/atom_mae: 0.000501
2025-02-04 22:36:32.956 INFO: val_e/atom_mae: 0.000501
val_e/atom_rmse: 0.000712
2025-02-04 22:36:32.956 INFO: val_e/atom_rmse: 0.000712
val_f_mae: 0.027678
2025-02-04 22:36:32.956 INFO: val_f_mae: 0.027678
val_f_rmse: 0.047218
2025-02-04 22:36:32.957 INFO: val_f_rmse: 0.047218
##### Step: 228 Learning rate: 4.8828125e-06 #####
2025-02-04 22:38:24.913 INFO: ##### Step: 228 Learning rate: 4.8828125e-06 #####
Epoch 89, Train Loss: 1.8610, Val Loss: 2.4173
2025-02-04 22:38:24.914 INFO: Epoch 89, Train Loss: 1.8610, Val Loss: 2.4173
train_e/atom_mae: 0.000491
2025-02-04 22:38:24.915 INFO: train_e/atom_mae: 0.000491
train_e/atom_rmse: 0.000882
2025-02-04 22:38:24.915 INFO: train_e/atom_rmse: 0.000882
train_f_mae: 0.025999
2025-02-04 22:38:24.918 INFO: train_f_mae: 0.025999
train_f_rmse: 0.039677
2025-02-04 22:38:24.918 INFO: train_f_rmse: 0.039677
val_e/atom_mae: 0.000502
2025-02-04 22:38:24.920 INFO: val_e/atom_mae: 0.000502
val_e/atom_rmse: 0.000712
2025-02-04 22:38:24.920 INFO: val_e/atom_rmse: 0.000712
val_f_mae: 0.027678
2025-02-04 22:38:24.921 INFO: val_f_mae: 0.027678
val_f_rmse: 0.047238
2025-02-04 22:38:24.921 INFO: val_f_rmse: 0.047238
##### Step: 229 Learning rate: 4.8828125e-06 #####
2025-02-04 22:40:17.357 INFO: ##### Step: 229 Learning rate: 4.8828125e-06 #####
Epoch 90, Train Loss: 1.8631, Val Loss: 2.4163
2025-02-04 22:40:17.358 INFO: Epoch 90, Train Loss: 1.8631, Val Loss: 2.4163
train_e/atom_mae: 0.000492
2025-02-04 22:40:17.359 INFO: train_e/atom_mae: 0.000492
train_e/atom_rmse: 0.000887
2025-02-04 22:40:17.359 INFO: train_e/atom_rmse: 0.000887
train_f_mae: 0.025992
2025-02-04 22:40:17.362 INFO: train_f_mae: 0.025992
train_f_rmse: 0.039663
2025-02-04 22:40:17.362 INFO: train_f_rmse: 0.039663
val_e/atom_mae: 0.000500
2025-02-04 22:40:17.364 INFO: val_e/atom_mae: 0.000500
val_e/atom_rmse: 0.000713
2025-02-04 22:40:17.365 INFO: val_e/atom_rmse: 0.000713
val_f_mae: 0.027684
2025-02-04 22:40:17.365 INFO: val_f_mae: 0.027684
val_f_rmse: 0.047220
2025-02-04 22:40:17.365 INFO: val_f_rmse: 0.047220
##### Step: 230 Learning rate: 4.8828125e-06 #####
2025-02-04 22:42:09.850 INFO: ##### Step: 230 Learning rate: 4.8828125e-06 #####
Epoch 91, Train Loss: 1.8566, Val Loss: 2.4172
2025-02-04 22:42:09.850 INFO: Epoch 91, Train Loss: 1.8566, Val Loss: 2.4172
train_e/atom_mae: 0.000487
2025-02-04 22:42:09.851 INFO: train_e/atom_mae: 0.000487
train_e/atom_rmse: 0.000880
2025-02-04 22:42:09.852 INFO: train_e/atom_rmse: 0.000880
train_f_mae: 0.025980
2025-02-04 22:42:09.854 INFO: train_f_mae: 0.025980
train_f_rmse: 0.039635
2025-02-04 22:42:09.854 INFO: train_f_rmse: 0.039635
val_e/atom_mae: 0.000503
2025-02-04 22:42:09.860 INFO: val_e/atom_mae: 0.000503
val_e/atom_rmse: 0.000715
2025-02-04 22:42:09.860 INFO: val_e/atom_rmse: 0.000715
val_f_mae: 0.027677
2025-02-04 22:42:09.861 INFO: val_f_mae: 0.027677
val_f_rmse: 0.047215
2025-02-04 22:42:09.861 INFO: val_f_rmse: 0.047215
##### Step: 231 Learning rate: 4.8828125e-06 #####
2025-02-04 22:44:02.460 INFO: ##### Step: 231 Learning rate: 4.8828125e-06 #####
Epoch 92, Train Loss: 1.8650, Val Loss: 2.4180
2025-02-04 22:44:02.461 INFO: Epoch 92, Train Loss: 1.8650, Val Loss: 2.4180
train_e/atom_mae: 0.000508
2025-02-04 22:44:02.462 INFO: train_e/atom_mae: 0.000508
train_e/atom_rmse: 0.000890
2025-02-04 22:44:02.464 INFO: train_e/atom_rmse: 0.000890
train_f_mae: 0.025997
2025-02-04 22:44:02.467 INFO: train_f_mae: 0.025997
train_f_rmse: 0.039657
2025-02-04 22:44:02.467 INFO: train_f_rmse: 0.039657
val_e/atom_mae: 0.000504
2025-02-04 22:44:02.469 INFO: val_e/atom_mae: 0.000504
val_e/atom_rmse: 0.000716
2025-02-04 22:44:02.469 INFO: val_e/atom_rmse: 0.000716
val_f_mae: 0.027678
2025-02-04 22:44:02.470 INFO: val_f_mae: 0.027678
val_f_rmse: 0.047223
2025-02-04 22:44:02.470 INFO: val_f_rmse: 0.047223
##### Step: 232 Learning rate: 4.8828125e-06 #####
2025-02-04 22:45:54.854 INFO: ##### Step: 232 Learning rate: 4.8828125e-06 #####
Epoch 93, Train Loss: 1.8603, Val Loss: 2.4187
2025-02-04 22:45:54.855 INFO: Epoch 93, Train Loss: 1.8603, Val Loss: 2.4187
train_e/atom_mae: 0.000486
2025-02-04 22:45:54.856 INFO: train_e/atom_mae: 0.000486
train_e/atom_rmse: 0.000883
2025-02-04 22:45:54.856 INFO: train_e/atom_rmse: 0.000883
train_f_mae: 0.025989
2025-02-04 22:45:54.859 INFO: train_f_mae: 0.025989
train_f_rmse: 0.039662
2025-02-04 22:45:54.859 INFO: train_f_rmse: 0.039662
val_e/atom_mae: 0.000502
2025-02-04 22:45:54.861 INFO: val_e/atom_mae: 0.000502
val_e/atom_rmse: 0.000715
2025-02-04 22:45:54.862 INFO: val_e/atom_rmse: 0.000715
val_f_mae: 0.027676
2025-02-04 22:45:54.862 INFO: val_f_mae: 0.027676
val_f_rmse: 0.047233
2025-02-04 22:45:54.862 INFO: val_f_rmse: 0.047233
##### Step: 233 Learning rate: 4.8828125e-06 #####
2025-02-04 22:47:46.938 INFO: ##### Step: 233 Learning rate: 4.8828125e-06 #####
Epoch 94, Train Loss: 1.8569, Val Loss: 2.4181
2025-02-04 22:47:46.938 INFO: Epoch 94, Train Loss: 1.8569, Val Loss: 2.4181
train_e/atom_mae: 0.000481
2025-02-04 22:47:46.940 INFO: train_e/atom_mae: 0.000481
train_e/atom_rmse: 0.000877
2025-02-04 22:47:46.941 INFO: train_e/atom_rmse: 0.000877
train_f_mae: 0.025995
2025-02-04 22:47:46.944 INFO: train_f_mae: 0.025995
train_f_rmse: 0.039668
2025-02-04 22:47:46.944 INFO: train_f_rmse: 0.039668
val_e/atom_mae: 0.000502
2025-02-04 22:47:46.946 INFO: val_e/atom_mae: 0.000502
val_e/atom_rmse: 0.000712
2025-02-04 22:47:46.946 INFO: val_e/atom_rmse: 0.000712
val_f_mae: 0.027681
2025-02-04 22:47:46.947 INFO: val_f_mae: 0.027681
val_f_rmse: 0.047241
2025-02-04 22:47:46.947 INFO: val_f_rmse: 0.047241
##### Step: 234 Learning rate: 4.8828125e-06 #####
2025-02-04 22:49:39.409 INFO: ##### Step: 234 Learning rate: 4.8828125e-06 #####
Epoch 95, Train Loss: 1.8589, Val Loss: 2.4172
2025-02-04 22:49:39.410 INFO: Epoch 95, Train Loss: 1.8589, Val Loss: 2.4172
train_e/atom_mae: 0.000482
2025-02-04 22:49:39.411 INFO: train_e/atom_mae: 0.000482
train_e/atom_rmse: 0.000880
2025-02-04 22:49:39.411 INFO: train_e/atom_rmse: 0.000880
train_f_mae: 0.025991
2025-02-04 22:49:39.414 INFO: train_f_mae: 0.025991
train_f_rmse: 0.039667
2025-02-04 22:49:39.414 INFO: train_f_rmse: 0.039667
val_e/atom_mae: 0.000503
2025-02-04 22:49:39.416 INFO: val_e/atom_mae: 0.000503
val_e/atom_rmse: 0.000713
2025-02-04 22:49:39.417 INFO: val_e/atom_rmse: 0.000713
val_f_mae: 0.027677
2025-02-04 22:49:39.417 INFO: val_f_mae: 0.027677
val_f_rmse: 0.047229
2025-02-04 22:49:39.417 INFO: val_f_rmse: 0.047229
##### Step: 235 Learning rate: 4.8828125e-06 #####
2025-02-04 22:51:31.441 INFO: ##### Step: 235 Learning rate: 4.8828125e-06 #####
Epoch 96, Train Loss: 1.8609, Val Loss: 2.4172
2025-02-04 22:51:31.442 INFO: Epoch 96, Train Loss: 1.8609, Val Loss: 2.4172
train_e/atom_mae: 0.000482
2025-02-04 22:51:31.444 INFO: train_e/atom_mae: 0.000482
train_e/atom_rmse: 0.000883
2025-02-04 22:51:31.446 INFO: train_e/atom_rmse: 0.000883
train_f_mae: 0.025993
2025-02-04 22:51:31.449 INFO: train_f_mae: 0.025993
train_f_rmse: 0.039665
2025-02-04 22:51:31.449 INFO: train_f_rmse: 0.039665
val_e/atom_mae: 0.000500
2025-02-04 22:51:31.451 INFO: val_e/atom_mae: 0.000500
val_e/atom_rmse: 0.000712
2025-02-04 22:51:31.451 INFO: val_e/atom_rmse: 0.000712
val_f_mae: 0.027682
2025-02-04 22:51:31.452 INFO: val_f_mae: 0.027682
val_f_rmse: 0.047234
2025-02-04 22:51:31.452 INFO: val_f_rmse: 0.047234
##### Step: 236 Learning rate: 4.8828125e-06 #####
2025-02-04 22:53:23.885 INFO: ##### Step: 236 Learning rate: 4.8828125e-06 #####
Epoch 97, Train Loss: 1.8583, Val Loss: 2.4173
2025-02-04 22:53:23.886 INFO: Epoch 97, Train Loss: 1.8583, Val Loss: 2.4173
train_e/atom_mae: 0.000492
2025-02-04 22:53:23.887 INFO: train_e/atom_mae: 0.000492
train_e/atom_rmse: 0.000881
2025-02-04 22:53:23.887 INFO: train_e/atom_rmse: 0.000881
train_f_mae: 0.025984
2025-02-04 22:53:23.890 INFO: train_f_mae: 0.025984
train_f_rmse: 0.039647
2025-02-04 22:53:23.890 INFO: train_f_rmse: 0.039647
val_e/atom_mae: 0.000503
2025-02-04 22:53:23.892 INFO: val_e/atom_mae: 0.000503
val_e/atom_rmse: 0.000714
2025-02-04 22:53:23.893 INFO: val_e/atom_rmse: 0.000714
val_f_mae: 0.027680
2025-02-04 22:53:23.893 INFO: val_f_mae: 0.027680
val_f_rmse: 0.047225
2025-02-04 22:53:23.893 INFO: val_f_rmse: 0.047225
##### Step: 237 Learning rate: 4.8828125e-06 #####
2025-02-04 22:55:15.924 INFO: ##### Step: 237 Learning rate: 4.8828125e-06 #####
Epoch 98, Train Loss: 1.8601, Val Loss: 2.4172
2025-02-04 22:55:15.925 INFO: Epoch 98, Train Loss: 1.8601, Val Loss: 2.4172
train_e/atom_mae: 0.000482
2025-02-04 22:55:15.926 INFO: train_e/atom_mae: 0.000482
train_e/atom_rmse: 0.000882
2025-02-04 22:55:15.927 INFO: train_e/atom_rmse: 0.000882
train_f_mae: 0.026001
2025-02-04 22:55:15.930 INFO: train_f_mae: 0.026001
train_f_rmse: 0.039666
2025-02-04 22:55:15.930 INFO: train_f_rmse: 0.039666
val_e/atom_mae: 0.000503
2025-02-04 22:55:15.932 INFO: val_e/atom_mae: 0.000503
val_e/atom_rmse: 0.000714
2025-02-04 22:55:15.932 INFO: val_e/atom_rmse: 0.000714
val_f_mae: 0.027683
2025-02-04 22:55:15.933 INFO: val_f_mae: 0.027683
val_f_rmse: 0.047221
2025-02-04 22:55:15.933 INFO: val_f_rmse: 0.047221
##### Step: 238 Learning rate: 4.8828125e-06 #####
2025-02-04 22:57:08.914 INFO: ##### Step: 238 Learning rate: 4.8828125e-06 #####
Epoch 99, Train Loss: 1.8577, Val Loss: 2.4157
2025-02-04 22:57:08.915 INFO: Epoch 99, Train Loss: 1.8577, Val Loss: 2.4157
train_e/atom_mae: 0.000479
2025-02-04 22:57:08.916 INFO: train_e/atom_mae: 0.000479
train_e/atom_rmse: 0.000880
2025-02-04 22:57:08.918 INFO: train_e/atom_rmse: 0.000880
train_f_mae: 0.025995
2025-02-04 22:57:08.920 INFO: train_f_mae: 0.025995
train_f_rmse: 0.039653
2025-02-04 22:57:08.921 INFO: train_f_rmse: 0.039653
val_e/atom_mae: 0.000501
2025-02-04 22:57:08.923 INFO: val_e/atom_mae: 0.000501
val_e/atom_rmse: 0.000713
2025-02-04 22:57:08.923 INFO: val_e/atom_rmse: 0.000713
val_f_mae: 0.027677
2025-02-04 22:57:08.923 INFO: val_f_mae: 0.027677
val_f_rmse: 0.047213
2025-02-04 22:57:08.924 INFO: val_f_rmse: 0.047213
##### Step: 239 Learning rate: 4.8828125e-06 #####
2025-02-04 22:59:00.740 INFO: ##### Step: 239 Learning rate: 4.8828125e-06 #####
Epoch 100, Train Loss: 1.8580, Val Loss: 2.4168
2025-02-04 22:59:00.741 INFO: Epoch 100, Train Loss: 1.8580, Val Loss: 2.4168
train_e/atom_mae: 0.000482
2025-02-04 22:59:00.742 INFO: train_e/atom_mae: 0.000482
train_e/atom_rmse: 0.000882
2025-02-04 22:59:00.742 INFO: train_e/atom_rmse: 0.000882
train_f_mae: 0.025982
2025-02-04 22:59:00.745 INFO: train_f_mae: 0.025982
train_f_rmse: 0.039642
2025-02-04 22:59:00.745 INFO: train_f_rmse: 0.039642
val_e/atom_mae: 0.000500
2025-02-04 22:59:00.747 INFO: val_e/atom_mae: 0.000500
val_e/atom_rmse: 0.000714
2025-02-04 22:59:00.747 INFO: val_e/atom_rmse: 0.000714
val_f_mae: 0.027676
2025-02-04 22:59:00.748 INFO: val_f_mae: 0.027676
val_f_rmse: 0.047222
2025-02-04 22:59:00.748 INFO: val_f_rmse: 0.047222
2025-02-04 22:59:01.061 INFO: Fourth train loop:
##### Step: 240 Learning rate: 2.44140625e-06 #####
2025-02-04 23:00:52.663 INFO: ##### Step: 240 Learning rate: 2.44140625e-06 #####
Epoch 1, Train Loss: 29.5822, Val Loss: 20.8554
2025-02-04 23:00:52.664 INFO: Epoch 1, Train Loss: 29.5822, Val Loss: 20.8554
train_e/atom_mae: 0.000474
2025-02-04 23:00:52.665 INFO: train_e/atom_mae: 0.000474
train_e/atom_rmse: 0.000872
2025-02-04 23:00:52.665 INFO: train_e/atom_rmse: 0.000872
train_f_mae: 0.025998
2025-02-04 23:00:52.667 INFO: train_f_mae: 0.025998
train_f_rmse: 0.039667
2025-02-04 23:00:52.668 INFO: train_f_rmse: 0.039667
val_e/atom_mae: 0.000500
2025-02-04 23:00:52.670 INFO: val_e/atom_mae: 0.000500
val_e/atom_rmse: 0.000710
2025-02-04 23:00:52.670 INFO: val_e/atom_rmse: 0.000710
val_f_mae: 0.027692
2025-02-04 23:00:52.671 INFO: val_f_mae: 0.027692
val_f_rmse: 0.047250
2025-02-04 23:00:52.671 INFO: val_f_rmse: 0.047250
##### Step: 241 Learning rate: 2.44140625e-06 #####
2025-02-04 23:02:44.914 INFO: ##### Step: 241 Learning rate: 2.44140625e-06 #####
Epoch 2, Train Loss: 29.2439, Val Loss: 20.7636
2025-02-04 23:02:44.914 INFO: Epoch 2, Train Loss: 29.2439, Val Loss: 20.7636
train_e/atom_mae: 0.000476
2025-02-04 23:02:44.916 INFO: train_e/atom_mae: 0.000476
train_e/atom_rmse: 0.000866
2025-02-04 23:02:44.917 INFO: train_e/atom_rmse: 0.000866
train_f_mae: 0.026053
2025-02-04 23:02:44.920 INFO: train_f_mae: 0.026053
train_f_rmse: 0.039720
2025-02-04 23:02:44.920 INFO: train_f_rmse: 0.039720
val_e/atom_mae: 0.000499
2025-02-04 23:02:44.922 INFO: val_e/atom_mae: 0.000499
val_e/atom_rmse: 0.000708
2025-02-04 23:02:44.922 INFO: val_e/atom_rmse: 0.000708
val_f_mae: 0.027742
2025-02-04 23:02:44.923 INFO: val_f_mae: 0.027742
val_f_rmse: 0.047303
2025-02-04 23:02:44.923 INFO: val_f_rmse: 0.047303
##### Step: 242 Learning rate: 2.44140625e-06 #####
2025-02-04 23:04:36.498 INFO: ##### Step: 242 Learning rate: 2.44140625e-06 #####
Epoch 3, Train Loss: 28.7596, Val Loss: 21.0556
2025-02-04 23:04:36.498 INFO: Epoch 3, Train Loss: 28.7596, Val Loss: 21.0556
train_e/atom_mae: 0.000471
2025-02-04 23:04:36.499 INFO: train_e/atom_mae: 0.000471
train_e/atom_rmse: 0.000859
2025-02-04 23:04:36.499 INFO: train_e/atom_rmse: 0.000859
train_f_mae: 0.026079
2025-02-04 23:04:36.502 INFO: train_f_mae: 0.026079
train_f_rmse: 0.039745
2025-02-04 23:04:36.502 INFO: train_f_rmse: 0.039745
val_e/atom_mae: 0.000502
2025-02-04 23:04:36.504 INFO: val_e/atom_mae: 0.000502
val_e/atom_rmse: 0.000713
2025-02-04 23:04:36.505 INFO: val_e/atom_rmse: 0.000713
val_f_mae: 0.027787
2025-02-04 23:04:36.505 INFO: val_f_mae: 0.027787
val_f_rmse: 0.047355
2025-02-04 23:04:36.505 INFO: val_f_rmse: 0.047355
##### Step: 243 Learning rate: 2.44140625e-06 #####
2025-02-04 23:06:27.887 INFO: ##### Step: 243 Learning rate: 2.44140625e-06 #####
Epoch 4, Train Loss: 29.0515, Val Loss: 20.5964
2025-02-04 23:06:27.888 INFO: Epoch 4, Train Loss: 29.0515, Val Loss: 20.5964
train_e/atom_mae: 0.000476
2025-02-04 23:06:27.889 INFO: train_e/atom_mae: 0.000476
train_e/atom_rmse: 0.000863
2025-02-04 23:06:27.889 INFO: train_e/atom_rmse: 0.000863
train_f_mae: 0.026138
2025-02-04 23:06:27.892 INFO: train_f_mae: 0.026138
train_f_rmse: 0.039796
2025-02-04 23:06:27.892 INFO: train_f_rmse: 0.039796
val_e/atom_mae: 0.000496
2025-02-04 23:06:27.894 INFO: val_e/atom_mae: 0.000496
val_e/atom_rmse: 0.000705
2025-02-04 23:06:27.894 INFO: val_e/atom_rmse: 0.000705
val_f_mae: 0.027831
2025-02-04 23:06:27.895 INFO: val_f_mae: 0.027831
val_f_rmse: 0.047381
2025-02-04 23:06:27.895 INFO: val_f_rmse: 0.047381
##### Step: 244 Learning rate: 2.44140625e-06 #####
2025-02-04 23:08:19.956 INFO: ##### Step: 244 Learning rate: 2.44140625e-06 #####
Epoch 5, Train Loss: 28.5996, Val Loss: 20.7768
2025-02-04 23:08:19.956 INFO: Epoch 5, Train Loss: 28.5996, Val Loss: 20.7768
train_e/atom_mae: 0.000469
2025-02-04 23:08:19.960 INFO: train_e/atom_mae: 0.000469
train_e/atom_rmse: 0.000856
2025-02-04 23:08:19.962 INFO: train_e/atom_rmse: 0.000856
train_f_mae: 0.026063
2025-02-04 23:08:19.965 INFO: train_f_mae: 0.026063
train_f_rmse: 0.039757
2025-02-04 23:08:19.965 INFO: train_f_rmse: 0.039757
val_e/atom_mae: 0.000499
2025-02-04 23:08:19.967 INFO: val_e/atom_mae: 0.000499
val_e/atom_rmse: 0.000708
2025-02-04 23:08:19.967 INFO: val_e/atom_rmse: 0.000708
val_f_mae: 0.027757
2025-02-04 23:08:19.968 INFO: val_f_mae: 0.027757
val_f_rmse: 0.047343
2025-02-04 23:08:19.968 INFO: val_f_rmse: 0.047343
##### Step: 245 Learning rate: 2.44140625e-06 #####
2025-02-04 23:10:11.179 INFO: ##### Step: 245 Learning rate: 2.44140625e-06 #####
Epoch 6, Train Loss: 28.4033, Val Loss: 20.6446
2025-02-04 23:10:11.179 INFO: Epoch 6, Train Loss: 28.4033, Val Loss: 20.6446
train_e/atom_mae: 0.000465
2025-02-04 23:10:11.180 INFO: train_e/atom_mae: 0.000465
train_e/atom_rmse: 0.000853
2025-02-04 23:10:11.180 INFO: train_e/atom_rmse: 0.000853
train_f_mae: 0.026100
2025-02-04 23:10:11.183 INFO: train_f_mae: 0.026100
train_f_rmse: 0.039787
2025-02-04 23:10:11.183 INFO: train_f_rmse: 0.039787
val_e/atom_mae: 0.000497
2025-02-04 23:10:11.185 INFO: val_e/atom_mae: 0.000497
val_e/atom_rmse: 0.000705
2025-02-04 23:10:11.185 INFO: val_e/atom_rmse: 0.000705
val_f_mae: 0.027770
2025-02-04 23:10:11.186 INFO: val_f_mae: 0.027770
val_f_rmse: 0.047360
2025-02-04 23:10:11.186 INFO: val_f_rmse: 0.047360
##### Step: 246 Learning rate: 2.44140625e-06 #####
2025-02-04 23:12:02.591 INFO: ##### Step: 246 Learning rate: 2.44140625e-06 #####
Epoch 7, Train Loss: 28.2412, Val Loss: 20.6658
2025-02-04 23:12:02.592 INFO: Epoch 7, Train Loss: 28.2412, Val Loss: 20.6658
train_e/atom_mae: 0.000464
2025-02-04 23:12:02.594 INFO: train_e/atom_mae: 0.000464
train_e/atom_rmse: 0.000850
2025-02-04 23:12:02.595 INFO: train_e/atom_rmse: 0.000850
train_f_mae: 0.026104
2025-02-04 23:12:02.598 INFO: train_f_mae: 0.026104
train_f_rmse: 0.039800
2025-02-04 23:12:02.598 INFO: train_f_rmse: 0.039800
val_e/atom_mae: 0.000497
2025-02-04 23:12:02.600 INFO: val_e/atom_mae: 0.000497
val_e/atom_rmse: 0.000706
2025-02-04 23:12:02.600 INFO: val_e/atom_rmse: 0.000706
val_f_mae: 0.027799
2025-02-04 23:12:02.601 INFO: val_f_mae: 0.027799
val_f_rmse: 0.047383
2025-02-04 23:12:02.601 INFO: val_f_rmse: 0.047383
##### Step: 247 Learning rate: 2.44140625e-06 #####
2025-02-04 23:13:53.998 INFO: ##### Step: 247 Learning rate: 2.44140625e-06 #####
Epoch 8, Train Loss: 28.5513, Val Loss: 20.6490
2025-02-04 23:13:53.998 INFO: Epoch 8, Train Loss: 28.5513, Val Loss: 20.6490
train_e/atom_mae: 0.000470
2025-02-04 23:13:53.999 INFO: train_e/atom_mae: 0.000470
train_e/atom_rmse: 0.000855
2025-02-04 23:13:54.000 INFO: train_e/atom_rmse: 0.000855
train_f_mae: 0.026091
2025-02-04 23:13:54.006 INFO: train_f_mae: 0.026091
train_f_rmse: 0.039782
2025-02-04 23:13:54.006 INFO: train_f_rmse: 0.039782
val_e/atom_mae: 0.000495
2025-02-04 23:13:54.008 INFO: val_e/atom_mae: 0.000495
val_e/atom_rmse: 0.000706
2025-02-04 23:13:54.008 INFO: val_e/atom_rmse: 0.000706
val_f_mae: 0.027781
2025-02-04 23:13:54.009 INFO: val_f_mae: 0.027781
val_f_rmse: 0.047372
2025-02-04 23:13:54.009 INFO: val_f_rmse: 0.047372
##### Step: 248 Learning rate: 2.44140625e-06 #####
2025-02-04 23:15:45.724 INFO: ##### Step: 248 Learning rate: 2.44140625e-06 #####
Epoch 9, Train Loss: 28.4045, Val Loss: 20.5928
2025-02-04 23:15:45.725 INFO: Epoch 9, Train Loss: 28.4045, Val Loss: 20.5928
train_e/atom_mae: 0.000468
2025-02-04 23:15:45.728 INFO: train_e/atom_mae: 0.000468
train_e/atom_rmse: 0.000853
2025-02-04 23:15:45.729 INFO: train_e/atom_rmse: 0.000853
train_f_mae: 0.026087
2025-02-04 23:15:45.732 INFO: train_f_mae: 0.026087
train_f_rmse: 0.039790
2025-02-04 23:15:45.732 INFO: train_f_rmse: 0.039790
val_e/atom_mae: 0.000494
2025-02-04 23:15:45.734 INFO: val_e/atom_mae: 0.000494
val_e/atom_rmse: 0.000705
2025-02-04 23:15:45.734 INFO: val_e/atom_rmse: 0.000705
val_f_mae: 0.027772
2025-02-04 23:15:45.735 INFO: val_f_mae: 0.027772
val_f_rmse: 0.047370
2025-02-04 23:15:45.735 INFO: val_f_rmse: 0.047370
##### Step: 249 Learning rate: 2.44140625e-06 #####
2025-02-04 23:17:37.347 INFO: ##### Step: 249 Learning rate: 2.44140625e-06 #####
Epoch 10, Train Loss: 28.1996, Val Loss: 20.5171
2025-02-04 23:17:37.348 INFO: Epoch 10, Train Loss: 28.1996, Val Loss: 20.5171
train_e/atom_mae: 0.000466
2025-02-04 23:17:37.350 INFO: train_e/atom_mae: 0.000466
train_e/atom_rmse: 0.000850
2025-02-04 23:17:37.350 INFO: train_e/atom_rmse: 0.000850
train_f_mae: 0.026096
2025-02-04 23:17:37.353 INFO: train_f_mae: 0.026096
train_f_rmse: 0.039788
2025-02-04 23:17:37.353 INFO: train_f_rmse: 0.039788
val_e/atom_mae: 0.000494
2025-02-04 23:17:37.355 INFO: val_e/atom_mae: 0.000494
val_e/atom_rmse: 0.000703
2025-02-04 23:17:37.356 INFO: val_e/atom_rmse: 0.000703
val_f_mae: 0.027814
2025-02-04 23:17:37.356 INFO: val_f_mae: 0.027814
val_f_rmse: 0.047398
2025-02-04 23:17:37.356 INFO: val_f_rmse: 0.047398
##### Step: 250 Learning rate: 2.44140625e-06 #####
2025-02-04 23:19:28.859 INFO: ##### Step: 250 Learning rate: 2.44140625e-06 #####
Epoch 11, Train Loss: 28.1418, Val Loss: 20.6999
2025-02-04 23:19:28.860 INFO: Epoch 11, Train Loss: 28.1418, Val Loss: 20.6999
train_e/atom_mae: 0.000463
2025-02-04 23:19:28.861 INFO: train_e/atom_mae: 0.000463
train_e/atom_rmse: 0.000849
2025-02-04 23:19:28.861 INFO: train_e/atom_rmse: 0.000849
train_f_mae: 0.026130
2025-02-04 23:19:28.863 INFO: train_f_mae: 0.026130
train_f_rmse: 0.039828
2025-02-04 23:19:28.864 INFO: train_f_rmse: 0.039828
val_e/atom_mae: 0.000495
2025-02-04 23:19:28.866 INFO: val_e/atom_mae: 0.000495
val_e/atom_rmse: 0.000706
2025-02-04 23:19:28.866 INFO: val_e/atom_rmse: 0.000706
val_f_mae: 0.027810
2025-02-04 23:19:28.867 INFO: val_f_mae: 0.027810
val_f_rmse: 0.047414
2025-02-04 23:19:28.867 INFO: val_f_rmse: 0.047414
##### Step: 251 Learning rate: 2.44140625e-06 #####
2025-02-04 23:21:20.403 INFO: ##### Step: 251 Learning rate: 2.44140625e-06 #####
Epoch 12, Train Loss: 28.2395, Val Loss: 20.5401
2025-02-04 23:21:20.404 INFO: Epoch 12, Train Loss: 28.2395, Val Loss: 20.5401
train_e/atom_mae: 0.000463
2025-02-04 23:21:20.405 INFO: train_e/atom_mae: 0.000463
train_e/atom_rmse: 0.000850
2025-02-04 23:21:20.405 INFO: train_e/atom_rmse: 0.000850
train_f_mae: 0.026107
2025-02-04 23:21:20.408 INFO: train_f_mae: 0.026107
train_f_rmse: 0.039815
2025-02-04 23:21:20.408 INFO: train_f_rmse: 0.039815
val_e/atom_mae: 0.000493
2025-02-04 23:21:20.410 INFO: val_e/atom_mae: 0.000493
val_e/atom_rmse: 0.000703
2025-02-04 23:21:20.410 INFO: val_e/atom_rmse: 0.000703
val_f_mae: 0.027824
2025-02-04 23:21:20.411 INFO: val_f_mae: 0.027824
val_f_rmse: 0.047425
2025-02-04 23:21:20.411 INFO: val_f_rmse: 0.047425
##### Step: 252 Learning rate: 2.44140625e-06 #####
2025-02-04 23:23:11.987 INFO: ##### Step: 252 Learning rate: 2.44140625e-06 #####
Epoch 13, Train Loss: 27.7670, Val Loss: 20.5530
2025-02-04 23:23:11.987 INFO: Epoch 13, Train Loss: 27.7670, Val Loss: 20.5530
train_e/atom_mae: 0.000463
2025-02-04 23:23:11.988 INFO: train_e/atom_mae: 0.000463
train_e/atom_rmse: 0.000843
2025-02-04 23:23:11.989 INFO: train_e/atom_rmse: 0.000843
train_f_mae: 0.026145
2025-02-04 23:23:11.991 INFO: train_f_mae: 0.026145
train_f_rmse: 0.039858
2025-02-04 23:23:11.991 INFO: train_f_rmse: 0.039858
val_e/atom_mae: 0.000493
2025-02-04 23:23:11.994 INFO: val_e/atom_mae: 0.000493
val_e/atom_rmse: 0.000703
2025-02-04 23:23:11.994 INFO: val_e/atom_rmse: 0.000703
val_f_mae: 0.027843
2025-02-04 23:23:11.994 INFO: val_f_mae: 0.027843
val_f_rmse: 0.047445
2025-02-04 23:23:11.995 INFO: val_f_rmse: 0.047445
##### Step: 253 Learning rate: 2.44140625e-06 #####
2025-02-04 23:25:25.223 INFO: ##### Step: 253 Learning rate: 2.44140625e-06 #####
Epoch 14, Train Loss: 27.6914, Val Loss: 20.5362
2025-02-04 23:25:25.224 INFO: Epoch 14, Train Loss: 27.6914, Val Loss: 20.5362
train_e/atom_mae: 0.000459
2025-02-04 23:25:25.352 INFO: train_e/atom_mae: 0.000459
train_e/atom_rmse: 0.000841
2025-02-04 23:25:25.492 INFO: train_e/atom_rmse: 0.000841
train_f_mae: 0.026179
2025-02-04 23:25:25.494 INFO: train_f_mae: 0.026179
train_f_rmse: 0.039888
2025-02-04 23:25:25.495 INFO: train_f_rmse: 0.039888
val_e/atom_mae: 0.000494
2025-02-04 23:25:25.497 INFO: val_e/atom_mae: 0.000494
val_e/atom_rmse: 0.000703
2025-02-04 23:25:25.497 INFO: val_e/atom_rmse: 0.000703
val_f_mae: 0.027883
2025-02-04 23:25:25.498 INFO: val_f_mae: 0.027883
val_f_rmse: 0.047481
2025-02-04 23:25:25.498 INFO: val_f_rmse: 0.047481
##### Step: 254 Learning rate: 2.44140625e-06 #####
2025-02-04 23:27:17.127 INFO: ##### Step: 254 Learning rate: 2.44140625e-06 #####
Epoch 15, Train Loss: 28.0392, Val Loss: 20.6690
2025-02-04 23:27:17.128 INFO: Epoch 15, Train Loss: 28.0392, Val Loss: 20.6690
train_e/atom_mae: 0.000462
2025-02-04 23:27:17.129 INFO: train_e/atom_mae: 0.000462
train_e/atom_rmse: 0.000847
2025-02-04 23:27:17.129 INFO: train_e/atom_rmse: 0.000847
train_f_mae: 0.026170
2025-02-04 23:27:17.132 INFO: train_f_mae: 0.026170
train_f_rmse: 0.039888
2025-02-04 23:27:17.132 INFO: train_f_rmse: 0.039888
val_e/atom_mae: 0.000495
2025-02-04 23:27:17.134 INFO: val_e/atom_mae: 0.000495
val_e/atom_rmse: 0.000706
2025-02-04 23:27:17.134 INFO: val_e/atom_rmse: 0.000706
val_f_mae: 0.027864
2025-02-04 23:27:17.135 INFO: val_f_mae: 0.027864
val_f_rmse: 0.047475
2025-02-04 23:27:17.135 INFO: val_f_rmse: 0.047475
##### Step: 255 Learning rate: 2.44140625e-06 #####
2025-02-04 23:29:08.841 INFO: ##### Step: 255 Learning rate: 2.44140625e-06 #####
Epoch 16, Train Loss: 27.7751, Val Loss: 20.5931
2025-02-04 23:29:08.841 INFO: Epoch 16, Train Loss: 27.7751, Val Loss: 20.5931
train_e/atom_mae: 0.000464
2025-02-04 23:29:08.842 INFO: train_e/atom_mae: 0.000464
train_e/atom_rmse: 0.000843
2025-02-04 23:29:08.843 INFO: train_e/atom_rmse: 0.000843
train_f_mae: 0.026147
2025-02-04 23:29:08.845 INFO: train_f_mae: 0.026147
train_f_rmse: 0.039880
2025-02-04 23:29:08.845 INFO: train_f_rmse: 0.039880
val_e/atom_mae: 0.000493
2025-02-04 23:29:08.848 INFO: val_e/atom_mae: 0.000493
val_e/atom_rmse: 0.000704
2025-02-04 23:29:08.848 INFO: val_e/atom_rmse: 0.000704
val_f_mae: 0.027833
2025-02-04 23:29:08.848 INFO: val_f_mae: 0.027833
val_f_rmse: 0.047451
2025-02-04 23:29:08.849 INFO: val_f_rmse: 0.047451
##### Step: 256 Learning rate: 2.44140625e-06 #####
2025-02-04 23:31:00.337 INFO: ##### Step: 256 Learning rate: 2.44140625e-06 #####
Epoch 17, Train Loss: 27.6251, Val Loss: 20.6730
2025-02-04 23:31:00.337 INFO: Epoch 17, Train Loss: 27.6251, Val Loss: 20.6730
train_e/atom_mae: 0.000458
2025-02-04 23:31:00.338 INFO: train_e/atom_mae: 0.000458
train_e/atom_rmse: 0.000840
2025-02-04 23:31:00.338 INFO: train_e/atom_rmse: 0.000840
train_f_mae: 0.026142
2025-02-04 23:31:00.344 INFO: train_f_mae: 0.026142
train_f_rmse: 0.039877
2025-02-04 23:31:00.345 INFO: train_f_rmse: 0.039877
val_e/atom_mae: 0.000494
2025-02-04 23:31:00.347 INFO: val_e/atom_mae: 0.000494
val_e/atom_rmse: 0.000706
2025-02-04 23:31:00.347 INFO: val_e/atom_rmse: 0.000706
val_f_mae: 0.027839
2025-02-04 23:31:00.348 INFO: val_f_mae: 0.027839
val_f_rmse: 0.047469
2025-02-04 23:31:00.348 INFO: val_f_rmse: 0.047469
##### Step: 257 Learning rate: 2.44140625e-06 #####
2025-02-04 23:32:51.921 INFO: ##### Step: 257 Learning rate: 2.44140625e-06 #####
Epoch 18, Train Loss: 27.6819, Val Loss: 20.6186
2025-02-04 23:32:51.921 INFO: Epoch 18, Train Loss: 27.6819, Val Loss: 20.6186
train_e/atom_mae: 0.000460
2025-02-04 23:32:51.922 INFO: train_e/atom_mae: 0.000460
train_e/atom_rmse: 0.000841
2025-02-04 23:32:51.922 INFO: train_e/atom_rmse: 0.000841
train_f_mae: 0.026131
2025-02-04 23:32:51.925 INFO: train_f_mae: 0.026131
train_f_rmse: 0.039873
2025-02-04 23:32:51.925 INFO: train_f_rmse: 0.039873
val_e/atom_mae: 0.000494
2025-02-04 23:32:51.927 INFO: val_e/atom_mae: 0.000494
val_e/atom_rmse: 0.000705
2025-02-04 23:32:51.928 INFO: val_e/atom_rmse: 0.000705
val_f_mae: 0.027839
2025-02-04 23:32:51.928 INFO: val_f_mae: 0.027839
val_f_rmse: 0.047470
2025-02-04 23:32:51.928 INFO: val_f_rmse: 0.047470
##### Step: 258 Learning rate: 2.44140625e-06 #####
2025-02-04 23:34:43.753 INFO: ##### Step: 258 Learning rate: 2.44140625e-06 #####
Epoch 19, Train Loss: 27.5314, Val Loss: 20.6181
2025-02-04 23:34:43.753 INFO: Epoch 19, Train Loss: 27.5314, Val Loss: 20.6181
train_e/atom_mae: 0.000461
2025-02-04 23:34:43.754 INFO: train_e/atom_mae: 0.000461
train_e/atom_rmse: 0.000839
2025-02-04 23:34:43.754 INFO: train_e/atom_rmse: 0.000839
train_f_mae: 0.026175
2025-02-04 23:34:43.757 INFO: train_f_mae: 0.026175
train_f_rmse: 0.039917
2025-02-04 23:34:43.757 INFO: train_f_rmse: 0.039917
val_e/atom_mae: 0.000494
2025-02-04 23:34:43.759 INFO: val_e/atom_mae: 0.000494
val_e/atom_rmse: 0.000705
2025-02-04 23:34:43.760 INFO: val_e/atom_rmse: 0.000705
val_f_mae: 0.027883
2025-02-04 23:34:43.760 INFO: val_f_mae: 0.027883
val_f_rmse: 0.047506
2025-02-04 23:34:43.760 INFO: val_f_rmse: 0.047506
##### Step: 259 Learning rate: 2.44140625e-06 #####
2025-02-04 23:36:51.325 INFO: ##### Step: 259 Learning rate: 2.44140625e-06 #####
Epoch 20, Train Loss: 27.5820, Val Loss: 20.9270
2025-02-04 23:36:51.326 INFO: Epoch 20, Train Loss: 27.5820, Val Loss: 20.9270
train_e/atom_mae: 0.000465
2025-02-04 23:36:51.428 INFO: train_e/atom_mae: 0.000465
train_e/atom_rmse: 0.000840
2025-02-04 23:36:51.519 INFO: train_e/atom_rmse: 0.000840
train_f_mae: 0.026201
2025-02-04 23:36:51.522 INFO: train_f_mae: 0.026201
train_f_rmse: 0.039937
2025-02-04 23:36:51.522 INFO: train_f_rmse: 0.039937
val_e/atom_mae: 0.000501
2025-02-04 23:36:51.524 INFO: val_e/atom_mae: 0.000501
val_e/atom_rmse: 0.000710
2025-02-04 23:36:51.525 INFO: val_e/atom_rmse: 0.000710
val_f_mae: 0.027941
2025-02-04 23:36:51.525 INFO: val_f_mae: 0.027941
val_f_rmse: 0.047556
2025-02-04 23:36:51.526 INFO: val_f_rmse: 0.047556
##### Step: 260 Learning rate: 1.220703125e-06 #####
2025-02-04 23:38:43.348 INFO: ##### Step: 260 Learning rate: 1.220703125e-06 #####
Epoch 21, Train Loss: 27.4024, Val Loss: 20.9209
2025-02-04 23:38:43.348 INFO: Epoch 21, Train Loss: 27.4024, Val Loss: 20.9209
train_e/atom_mae: 0.000459
2025-02-04 23:38:43.349 INFO: train_e/atom_mae: 0.000459
train_e/atom_rmse: 0.000837
2025-02-04 23:38:43.349 INFO: train_e/atom_rmse: 0.000837
train_f_mae: 0.026234
2025-02-04 23:38:43.352 INFO: train_f_mae: 0.026234
train_f_rmse: 0.039968
2025-02-04 23:38:43.352 INFO: train_f_rmse: 0.039968
val_e/atom_mae: 0.000500
2025-02-04 23:38:43.354 INFO: val_e/atom_mae: 0.000500
val_e/atom_rmse: 0.000710
2025-02-04 23:38:43.355 INFO: val_e/atom_rmse: 0.000710
val_f_mae: 0.027933
2025-02-04 23:38:43.355 INFO: val_f_mae: 0.027933
val_f_rmse: 0.047560
2025-02-04 23:38:43.355 INFO: val_f_rmse: 0.047560
##### Step: 261 Learning rate: 1.220703125e-06 #####
2025-02-04 23:40:34.789 INFO: ##### Step: 261 Learning rate: 1.220703125e-06 #####
Epoch 22, Train Loss: 27.2195, Val Loss: 20.6742
2025-02-04 23:40:34.789 INFO: Epoch 22, Train Loss: 27.2195, Val Loss: 20.6742
train_e/atom_mae: 0.000454
2025-02-04 23:40:34.790 INFO: train_e/atom_mae: 0.000454
train_e/atom_rmse: 0.000834
2025-02-04 23:40:34.790 INFO: train_e/atom_rmse: 0.000834
train_f_mae: 0.026174
2025-02-04 23:40:34.797 INFO: train_f_mae: 0.026174
train_f_rmse: 0.039906
2025-02-04 23:40:34.797 INFO: train_f_rmse: 0.039906
val_e/atom_mae: 0.000495
2025-02-04 23:40:34.799 INFO: val_e/atom_mae: 0.000495
val_e/atom_rmse: 0.000706
2025-02-04 23:40:34.799 INFO: val_e/atom_rmse: 0.000706
val_f_mae: 0.027905
2025-02-04 23:40:34.800 INFO: val_f_mae: 0.027905
val_f_rmse: 0.047542
2025-02-04 23:40:34.800 INFO: val_f_rmse: 0.047542
##### Step: 262 Learning rate: 1.220703125e-06 #####
2025-02-04 23:42:26.403 INFO: ##### Step: 262 Learning rate: 1.220703125e-06 #####
Epoch 23, Train Loss: 27.2254, Val Loss: 20.6752
2025-02-04 23:42:26.404 INFO: Epoch 23, Train Loss: 27.2254, Val Loss: 20.6752
train_e/atom_mae: 0.000454
2025-02-04 23:42:26.405 INFO: train_e/atom_mae: 0.000454
train_e/atom_rmse: 0.000834
2025-02-04 23:42:26.405 INFO: train_e/atom_rmse: 0.000834
train_f_mae: 0.026201
2025-02-04 23:42:26.408 INFO: train_f_mae: 0.026201
train_f_rmse: 0.039946
2025-02-04 23:42:26.408 INFO: train_f_rmse: 0.039946
val_e/atom_mae: 0.000495
2025-02-04 23:42:26.410 INFO: val_e/atom_mae: 0.000495
val_e/atom_rmse: 0.000706
2025-02-04 23:42:26.411 INFO: val_e/atom_rmse: 0.000706
val_f_mae: 0.027914
2025-02-04 23:42:26.411 INFO: val_f_mae: 0.027914
val_f_rmse: 0.047553
2025-02-04 23:42:26.411 INFO: val_f_rmse: 0.047553
##### Step: 263 Learning rate: 1.220703125e-06 #####
2025-02-04 23:44:17.865 INFO: ##### Step: 263 Learning rate: 1.220703125e-06 #####
Epoch 24, Train Loss: 27.4066, Val Loss: 20.6455
2025-02-04 23:44:17.866 INFO: Epoch 24, Train Loss: 27.4066, Val Loss: 20.6455
train_e/atom_mae: 0.000456
2025-02-04 23:44:17.867 INFO: train_e/atom_mae: 0.000456
train_e/atom_rmse: 0.000837
2025-02-04 23:44:17.867 INFO: train_e/atom_rmse: 0.000837
train_f_mae: 0.026210
2025-02-04 23:44:17.870 INFO: train_f_mae: 0.026210
train_f_rmse: 0.039952
2025-02-04 23:44:17.870 INFO: train_f_rmse: 0.039952
val_e/atom_mae: 0.000495
2025-02-04 23:44:17.872 INFO: val_e/atom_mae: 0.000495
val_e/atom_rmse: 0.000705
2025-02-04 23:44:17.873 INFO: val_e/atom_rmse: 0.000705
val_f_mae: 0.027923
2025-02-04 23:44:17.873 INFO: val_f_mae: 0.027923
val_f_rmse: 0.047555
2025-02-04 23:44:17.873 INFO: val_f_rmse: 0.047555
##### Step: 264 Learning rate: 1.220703125e-06 #####
2025-02-04 23:46:09.484 INFO: ##### Step: 264 Learning rate: 1.220703125e-06 #####
Epoch 25, Train Loss: 27.1396, Val Loss: 20.7128
2025-02-04 23:46:09.484 INFO: Epoch 25, Train Loss: 27.1396, Val Loss: 20.7128
train_e/atom_mae: 0.000453
2025-02-04 23:46:09.485 INFO: train_e/atom_mae: 0.000453
train_e/atom_rmse: 0.000832
2025-02-04 23:46:09.486 INFO: train_e/atom_rmse: 0.000832
train_f_mae: 0.026207
2025-02-04 23:46:09.488 INFO: train_f_mae: 0.026207
train_f_rmse: 0.039949
2025-02-04 23:46:09.488 INFO: train_f_rmse: 0.039949
val_e/atom_mae: 0.000495
2025-02-04 23:46:09.491 INFO: val_e/atom_mae: 0.000495
val_e/atom_rmse: 0.000706
2025-02-04 23:46:09.491 INFO: val_e/atom_rmse: 0.000706
val_f_mae: 0.027902
2025-02-04 23:46:09.491 INFO: val_f_mae: 0.027902
val_f_rmse: 0.047542
2025-02-04 23:46:09.492 INFO: val_f_rmse: 0.047542
##### Step: 265 Learning rate: 1.220703125e-06 #####
2025-02-04 23:48:01.904 INFO: ##### Step: 265 Learning rate: 1.220703125e-06 #####
Epoch 26, Train Loss: 27.3461, Val Loss: 20.8294
2025-02-04 23:48:01.904 INFO: Epoch 26, Train Loss: 27.3461, Val Loss: 20.8294
train_e/atom_mae: 0.000456
2025-02-04 23:48:01.906 INFO: train_e/atom_mae: 0.000456
train_e/atom_rmse: 0.000836
2025-02-04 23:48:01.907 INFO: train_e/atom_rmse: 0.000836
train_f_mae: 0.026191
2025-02-04 23:48:01.909 INFO: train_f_mae: 0.026191
train_f_rmse: 0.039934
2025-02-04 23:48:01.909 INFO: train_f_rmse: 0.039934
val_e/atom_mae: 0.000497
2025-02-04 23:48:01.912 INFO: val_e/atom_mae: 0.000497
val_e/atom_rmse: 0.000709
2025-02-04 23:48:01.912 INFO: val_e/atom_rmse: 0.000709
val_f_mae: 0.027900
2025-02-04 23:48:01.912 INFO: val_f_mae: 0.027900
val_f_rmse: 0.047536
2025-02-04 23:48:01.913 INFO: val_f_rmse: 0.047536
##### Step: 266 Learning rate: 1.220703125e-06 #####
2025-02-04 23:49:53.808 INFO: ##### Step: 266 Learning rate: 1.220703125e-06 #####
Epoch 27, Train Loss: 25.6865, Val Loss: 21.0085
2025-02-04 23:49:53.809 INFO: Epoch 27, Train Loss: 25.6865, Val Loss: 21.0085
train_e/atom_mae: 0.000453
2025-02-04 23:49:53.810 INFO: train_e/atom_mae: 0.000453
train_e/atom_rmse: 0.000808
2025-02-04 23:49:53.811 INFO: train_e/atom_rmse: 0.000808
train_f_mae: 0.026160
2025-02-04 23:49:53.814 INFO: train_f_mae: 0.026160
train_f_rmse: 0.039888
2025-02-04 23:49:53.814 INFO: train_f_rmse: 0.039888
val_e/atom_mae: 0.000500
2025-02-04 23:49:53.816 INFO: val_e/atom_mae: 0.000500
val_e/atom_rmse: 0.000712
2025-02-04 23:49:53.817 INFO: val_e/atom_rmse: 0.000712
val_f_mae: 0.027879
2025-02-04 23:49:53.817 INFO: val_f_mae: 0.027879
val_f_rmse: 0.047519
2025-02-04 23:49:53.818 INFO: val_f_rmse: 0.047519
##### Step: 267 Learning rate: 1.220703125e-06 #####
2025-02-04 23:51:45.418 INFO: ##### Step: 267 Learning rate: 1.220703125e-06 #####
Epoch 28, Train Loss: 27.3110, Val Loss: 20.8049
2025-02-04 23:51:45.419 INFO: Epoch 28, Train Loss: 27.3110, Val Loss: 20.8049
train_e/atom_mae: 0.000454
2025-02-04 23:51:45.420 INFO: train_e/atom_mae: 0.000454
train_e/atom_rmse: 0.000835
2025-02-04 23:51:45.420 INFO: train_e/atom_rmse: 0.000835
train_f_mae: 0.026194
2025-02-04 23:51:45.423 INFO: train_f_mae: 0.026194
train_f_rmse: 0.039940
2025-02-04 23:51:45.423 INFO: train_f_rmse: 0.039940
val_e/atom_mae: 0.000496
2025-02-04 23:51:45.425 INFO: val_e/atom_mae: 0.000496
val_e/atom_rmse: 0.000708
2025-02-04 23:51:45.425 INFO: val_e/atom_rmse: 0.000708
val_f_mae: 0.027903
2025-02-04 23:51:45.426 INFO: val_f_mae: 0.027903
val_f_rmse: 0.047539
2025-02-04 23:51:45.426 INFO: val_f_rmse: 0.047539
##### Step: 268 Learning rate: 1.220703125e-06 #####
2025-02-04 23:53:36.939 INFO: ##### Step: 268 Learning rate: 1.220703125e-06 #####
Epoch 29, Train Loss: 27.2941, Val Loss: 20.6418
2025-02-04 23:53:36.939 INFO: Epoch 29, Train Loss: 27.2941, Val Loss: 20.6418
train_e/atom_mae: 0.000455
2025-02-04 23:53:36.940 INFO: train_e/atom_mae: 0.000455
train_e/atom_rmse: 0.000835
2025-02-04 23:53:36.941 INFO: train_e/atom_rmse: 0.000835
train_f_mae: 0.026172
2025-02-04 23:53:36.943 INFO: train_f_mae: 0.026172
train_f_rmse: 0.039929
2025-02-04 23:53:36.944 INFO: train_f_rmse: 0.039929
val_e/atom_mae: 0.000493
2025-02-04 23:53:36.946 INFO: val_e/atom_mae: 0.000493
val_e/atom_rmse: 0.000705
2025-02-04 23:53:36.946 INFO: val_e/atom_rmse: 0.000705
val_f_mae: 0.027867
2025-02-04 23:53:36.946 INFO: val_f_mae: 0.027867
val_f_rmse: 0.047513
2025-02-04 23:53:36.947 INFO: val_f_rmse: 0.047513
##### Step: 269 Learning rate: 1.220703125e-06 #####
2025-02-04 23:55:28.708 INFO: ##### Step: 269 Learning rate: 1.220703125e-06 #####
Epoch 30, Train Loss: 27.3286, Val Loss: 20.5898
2025-02-04 23:55:28.709 INFO: Epoch 30, Train Loss: 27.3286, Val Loss: 20.5898
train_e/atom_mae: 0.000454
2025-02-04 23:55:28.710 INFO: train_e/atom_mae: 0.000454
train_e/atom_rmse: 0.000836
2025-02-04 23:55:28.710 INFO: train_e/atom_rmse: 0.000836
train_f_mae: 0.026144
2025-02-04 23:55:28.712 INFO: train_f_mae: 0.026144
train_f_rmse: 0.039888
2025-02-04 23:55:28.713 INFO: train_f_rmse: 0.039888
val_e/atom_mae: 0.000493
2025-02-04 23:55:28.715 INFO: val_e/atom_mae: 0.000493
val_e/atom_rmse: 0.000704
2025-02-04 23:55:28.715 INFO: val_e/atom_rmse: 0.000704
val_f_mae: 0.027864
2025-02-04 23:55:28.716 INFO: val_f_mae: 0.027864
val_f_rmse: 0.047506
2025-02-04 23:55:28.716 INFO: val_f_rmse: 0.047506
##### Step: 270 Learning rate: 1.220703125e-06 #####
2025-02-04 23:57:20.818 INFO: ##### Step: 270 Learning rate: 1.220703125e-06 #####
Epoch 31, Train Loss: 27.2253, Val Loss: 20.7073
2025-02-04 23:57:20.819 INFO: Epoch 31, Train Loss: 27.2253, Val Loss: 20.7073
train_e/atom_mae: 0.000456
2025-02-04 23:57:20.821 INFO: train_e/atom_mae: 0.000456
train_e/atom_rmse: 0.000834
2025-02-04 23:57:20.822 INFO: train_e/atom_rmse: 0.000834
train_f_mae: 0.026175
2025-02-04 23:57:20.824 INFO: train_f_mae: 0.026175
train_f_rmse: 0.039931
2025-02-04 23:57:20.824 INFO: train_f_rmse: 0.039931
val_e/atom_mae: 0.000495
2025-02-04 23:57:20.827 INFO: val_e/atom_mae: 0.000495
val_e/atom_rmse: 0.000706
2025-02-04 23:57:20.827 INFO: val_e/atom_rmse: 0.000706
val_f_mae: 0.027886
2025-02-04 23:57:20.827 INFO: val_f_mae: 0.027886
val_f_rmse: 0.047532
2025-02-04 23:57:20.828 INFO: val_f_rmse: 0.047532
##### Step: 271 Learning rate: 1.220703125e-06 #####
2025-02-04 23:59:12.448 INFO: ##### Step: 271 Learning rate: 1.220703125e-06 #####
Epoch 32, Train Loss: 27.3999, Val Loss: 20.6766
2025-02-04 23:59:12.448 INFO: Epoch 32, Train Loss: 27.3999, Val Loss: 20.6766
train_e/atom_mae: 0.000454
2025-02-04 23:59:12.449 INFO: train_e/atom_mae: 0.000454
train_e/atom_rmse: 0.000837
2025-02-04 23:59:12.449 INFO: train_e/atom_rmse: 0.000837
train_f_mae: 0.026200
2025-02-04 23:59:12.452 INFO: train_f_mae: 0.026200
train_f_rmse: 0.039954
2025-02-04 23:59:12.452 INFO: train_f_rmse: 0.039954
val_e/atom_mae: 0.000493
2025-02-04 23:59:12.454 INFO: val_e/atom_mae: 0.000493
val_e/atom_rmse: 0.000706
2025-02-04 23:59:12.454 INFO: val_e/atom_rmse: 0.000706
val_f_mae: 0.027909
2025-02-04 23:59:12.455 INFO: val_f_mae: 0.027909
val_f_rmse: 0.047552
2025-02-04 23:59:12.455 INFO: val_f_rmse: 0.047552
##### Step: 272 Learning rate: 1.220703125e-06 #####
2025-02-05 00:01:05.055 INFO: ##### Step: 272 Learning rate: 1.220703125e-06 #####
Epoch 33, Train Loss: 27.1521, Val Loss: 20.6293
2025-02-05 00:01:05.056 INFO: Epoch 33, Train Loss: 27.1521, Val Loss: 20.6293
train_e/atom_mae: 0.000452
2025-02-05 00:01:05.057 INFO: train_e/atom_mae: 0.000452
train_e/atom_rmse: 0.000833
2025-02-05 00:01:05.058 INFO: train_e/atom_rmse: 0.000833
train_f_mae: 0.026208
2025-02-05 00:01:05.060 INFO: train_f_mae: 0.026208
train_f_rmse: 0.039962
2025-02-05 00:01:05.061 INFO: train_f_rmse: 0.039962
val_e/atom_mae: 0.000492
2025-02-05 00:01:05.063 INFO: val_e/atom_mae: 0.000492
val_e/atom_rmse: 0.000705
2025-02-05 00:01:05.063 INFO: val_e/atom_rmse: 0.000705
val_f_mae: 0.027901
2025-02-05 00:01:05.064 INFO: val_f_mae: 0.027901
val_f_rmse: 0.047544
2025-02-05 00:01:05.064 INFO: val_f_rmse: 0.047544
##### Step: 273 Learning rate: 1.220703125e-06 #####
2025-02-05 00:02:57.422 INFO: ##### Step: 273 Learning rate: 1.220703125e-06 #####
Epoch 34, Train Loss: 27.1245, Val Loss: 20.6078
2025-02-05 00:02:57.423 INFO: Epoch 34, Train Loss: 27.1245, Val Loss: 20.6078
train_e/atom_mae: 0.000451
2025-02-05 00:02:57.424 INFO: train_e/atom_mae: 0.000451
train_e/atom_rmse: 0.000832
2025-02-05 00:02:57.426 INFO: train_e/atom_rmse: 0.000832
train_f_mae: 0.026196
2025-02-05 00:02:57.429 INFO: train_f_mae: 0.026196
train_f_rmse: 0.039954
2025-02-05 00:02:57.429 INFO: train_f_rmse: 0.039954
val_e/atom_mae: 0.000493
2025-02-05 00:02:57.431 INFO: val_e/atom_mae: 0.000493
val_e/atom_rmse: 0.000704
2025-02-05 00:02:57.431 INFO: val_e/atom_rmse: 0.000704
val_f_mae: 0.027889
2025-02-05 00:02:57.432 INFO: val_f_mae: 0.027889
val_f_rmse: 0.047535
2025-02-05 00:02:57.432 INFO: val_f_rmse: 0.047535
##### Step: 274 Learning rate: 1.220703125e-06 #####
2025-02-05 00:04:48.951 INFO: ##### Step: 274 Learning rate: 1.220703125e-06 #####
Epoch 35, Train Loss: 27.0997, Val Loss: 20.6081
2025-02-05 00:04:48.952 INFO: Epoch 35, Train Loss: 27.0997, Val Loss: 20.6081
train_e/atom_mae: 0.000453
2025-02-05 00:04:48.952 INFO: train_e/atom_mae: 0.000453
train_e/atom_rmse: 0.000832
2025-02-05 00:04:48.953 INFO: train_e/atom_rmse: 0.000832
train_f_mae: 0.026204
2025-02-05 00:04:48.955 INFO: train_f_mae: 0.026204
train_f_rmse: 0.039962
2025-02-05 00:04:48.956 INFO: train_f_rmse: 0.039962
val_e/atom_mae: 0.000494
2025-02-05 00:04:48.958 INFO: val_e/atom_mae: 0.000494
val_e/atom_rmse: 0.000704
2025-02-05 00:04:48.958 INFO: val_e/atom_rmse: 0.000704
val_f_mae: 0.027900
2025-02-05 00:04:48.959 INFO: val_f_mae: 0.027900
val_f_rmse: 0.047542
2025-02-05 00:04:48.959 INFO: val_f_rmse: 0.047542
##### Step: 275 Learning rate: 1.220703125e-06 #####
2025-02-05 00:06:40.710 INFO: ##### Step: 275 Learning rate: 1.220703125e-06 #####
Epoch 36, Train Loss: 27.1417, Val Loss: 20.6485
2025-02-05 00:06:40.711 INFO: Epoch 36, Train Loss: 27.1417, Val Loss: 20.6485
train_e/atom_mae: 0.000453
2025-02-05 00:06:40.712 INFO: train_e/atom_mae: 0.000453
train_e/atom_rmse: 0.000832
2025-02-05 00:06:40.712 INFO: train_e/atom_rmse: 0.000832
train_f_mae: 0.026191
2025-02-05 00:06:40.714 INFO: train_f_mae: 0.026191
train_f_rmse: 0.039933
2025-02-05 00:06:40.715 INFO: train_f_rmse: 0.039933
val_e/atom_mae: 0.000493
2025-02-05 00:06:40.717 INFO: val_e/atom_mae: 0.000493
val_e/atom_rmse: 0.000705
2025-02-05 00:06:40.717 INFO: val_e/atom_rmse: 0.000705
val_f_mae: 0.027910
2025-02-05 00:06:40.718 INFO: val_f_mae: 0.027910
val_f_rmse: 0.047554
2025-02-05 00:06:40.718 INFO: val_f_rmse: 0.047554
##### Step: 276 Learning rate: 1.220703125e-06 #####
2025-02-05 00:08:32.305 INFO: ##### Step: 276 Learning rate: 1.220703125e-06 #####
Epoch 37, Train Loss: 27.2310, Val Loss: 20.8539
2025-02-05 00:08:32.306 INFO: Epoch 37, Train Loss: 27.2310, Val Loss: 20.8539
train_e/atom_mae: 0.000457
2025-02-05 00:08:32.307 INFO: train_e/atom_mae: 0.000457
train_e/atom_rmse: 0.000834
2025-02-05 00:08:32.307 INFO: train_e/atom_rmse: 0.000834
train_f_mae: 0.026213
2025-02-05 00:08:32.310 INFO: train_f_mae: 0.026213
train_f_rmse: 0.039972
2025-02-05 00:08:32.310 INFO: train_f_rmse: 0.039972
val_e/atom_mae: 0.000497
2025-02-05 00:08:32.312 INFO: val_e/atom_mae: 0.000497
val_e/atom_rmse: 0.000709
2025-02-05 00:08:32.312 INFO: val_e/atom_rmse: 0.000709
val_f_mae: 0.027936
2025-02-05 00:08:32.313 INFO: val_f_mae: 0.027936
val_f_rmse: 0.047579
2025-02-05 00:08:32.313 INFO: val_f_rmse: 0.047579
##### Step: 277 Learning rate: 1.220703125e-06 #####
2025-02-05 00:10:23.986 INFO: ##### Step: 277 Learning rate: 1.220703125e-06 #####
Epoch 38, Train Loss: 27.1090, Val Loss: 20.7502
2025-02-05 00:10:23.986 INFO: Epoch 38, Train Loss: 27.1090, Val Loss: 20.7502
train_e/atom_mae: 0.000452
2025-02-05 00:10:23.987 INFO: train_e/atom_mae: 0.000452
train_e/atom_rmse: 0.000832
2025-02-05 00:10:23.987 INFO: train_e/atom_rmse: 0.000832
train_f_mae: 0.026239
2025-02-05 00:10:23.990 INFO: train_f_mae: 0.026239
train_f_rmse: 0.039992
2025-02-05 00:10:23.990 INFO: train_f_rmse: 0.039992
val_e/atom_mae: 0.000495
2025-02-05 00:10:23.992 INFO: val_e/atom_mae: 0.000495
val_e/atom_rmse: 0.000707
2025-02-05 00:10:23.993 INFO: val_e/atom_rmse: 0.000707
val_f_mae: 0.027926
2025-02-05 00:10:23.993 INFO: val_f_mae: 0.027926
val_f_rmse: 0.047569
2025-02-05 00:10:23.993 INFO: val_f_rmse: 0.047569
##### Step: 278 Learning rate: 1.220703125e-06 #####
2025-02-05 00:12:15.811 INFO: ##### Step: 278 Learning rate: 1.220703125e-06 #####
Epoch 39, Train Loss: 27.1343, Val Loss: 20.5970
2025-02-05 00:12:15.812 INFO: Epoch 39, Train Loss: 27.1343, Val Loss: 20.5970
train_e/atom_mae: 0.000450
2025-02-05 00:12:15.814 INFO: train_e/atom_mae: 0.000450
train_e/atom_rmse: 0.000832
2025-02-05 00:12:15.815 INFO: train_e/atom_rmse: 0.000832
train_f_mae: 0.026194
2025-02-05 00:12:15.818 INFO: train_f_mae: 0.026194
train_f_rmse: 0.039953
2025-02-05 00:12:15.818 INFO: train_f_rmse: 0.039953
val_e/atom_mae: 0.000492
2025-02-05 00:12:15.820 INFO: val_e/atom_mae: 0.000492
val_e/atom_rmse: 0.000704
2025-02-05 00:12:15.820 INFO: val_e/atom_rmse: 0.000704
val_f_mae: 0.027901
2025-02-05 00:12:15.821 INFO: val_f_mae: 0.027901
val_f_rmse: 0.047549
2025-02-05 00:12:15.821 INFO: val_f_rmse: 0.047549
##### Step: 279 Learning rate: 1.220703125e-06 #####
2025-02-05 00:14:07.569 INFO: ##### Step: 279 Learning rate: 1.220703125e-06 #####
Epoch 40, Train Loss: 27.2927, Val Loss: 20.6973
2025-02-05 00:14:07.570 INFO: Epoch 40, Train Loss: 27.2927, Val Loss: 20.6973
train_e/atom_mae: 0.000452
2025-02-05 00:14:07.571 INFO: train_e/atom_mae: 0.000452
train_e/atom_rmse: 0.000835
2025-02-05 00:14:07.571 INFO: train_e/atom_rmse: 0.000835
train_f_mae: 0.026191
2025-02-05 00:14:07.574 INFO: train_f_mae: 0.026191
train_f_rmse: 0.039952
2025-02-05 00:14:07.574 INFO: train_f_rmse: 0.039952
val_e/atom_mae: 0.000495
2025-02-05 00:14:07.576 INFO: val_e/atom_mae: 0.000495
val_e/atom_rmse: 0.000706
2025-02-05 00:14:07.576 INFO: val_e/atom_rmse: 0.000706
val_f_mae: 0.027907
2025-02-05 00:14:07.577 INFO: val_f_mae: 0.027907
val_f_rmse: 0.047561
2025-02-05 00:14:07.577 INFO: val_f_rmse: 0.047561
##### Step: 280 Learning rate: 6.103515625e-07 #####
2025-02-05 00:15:59.491 INFO: ##### Step: 280 Learning rate: 6.103515625e-07 #####
Epoch 41, Train Loss: 27.1599, Val Loss: 20.7151
2025-02-05 00:15:59.491 INFO: Epoch 41, Train Loss: 27.1599, Val Loss: 20.7151
train_e/atom_mae: 0.000451
2025-02-05 00:15:59.493 INFO: train_e/atom_mae: 0.000451
train_e/atom_rmse: 0.000833
2025-02-05 00:15:59.494 INFO: train_e/atom_rmse: 0.000833
train_f_mae: 0.026202
2025-02-05 00:15:59.497 INFO: train_f_mae: 0.026202
train_f_rmse: 0.039959
2025-02-05 00:15:59.497 INFO: train_f_rmse: 0.039959
val_e/atom_mae: 0.000494
2025-02-05 00:15:59.499 INFO: val_e/atom_mae: 0.000494
val_e/atom_rmse: 0.000706
2025-02-05 00:15:59.499 INFO: val_e/atom_rmse: 0.000706
val_f_mae: 0.027919
2025-02-05 00:15:59.500 INFO: val_f_mae: 0.027919
val_f_rmse: 0.047565
2025-02-05 00:15:59.500 INFO: val_f_rmse: 0.047565
##### Step: 281 Learning rate: 6.103515625e-07 #####
2025-02-05 00:17:51.455 INFO: ##### Step: 281 Learning rate: 6.103515625e-07 #####
Epoch 42, Train Loss: 26.8606, Val Loss: 20.7087
2025-02-05 00:17:51.456 INFO: Epoch 42, Train Loss: 26.8606, Val Loss: 20.7087
train_e/atom_mae: 0.000447
2025-02-05 00:17:51.457 INFO: train_e/atom_mae: 0.000447
train_e/atom_rmse: 0.000828
2025-02-05 00:17:51.457 INFO: train_e/atom_rmse: 0.000828
train_f_mae: 0.026186
2025-02-05 00:17:51.460 INFO: train_f_mae: 0.026186
train_f_rmse: 0.039943
2025-02-05 00:17:51.460 INFO: train_f_rmse: 0.039943
val_e/atom_mae: 0.000494
2025-02-05 00:17:51.462 INFO: val_e/atom_mae: 0.000494
val_e/atom_rmse: 0.000706
2025-02-05 00:17:51.463 INFO: val_e/atom_rmse: 0.000706
val_f_mae: 0.027901
2025-02-05 00:17:51.463 INFO: val_f_mae: 0.027901
val_f_rmse: 0.047557
2025-02-05 00:17:51.463 INFO: val_f_rmse: 0.047557
##### Step: 282 Learning rate: 6.103515625e-07 #####
2025-02-05 00:19:42.885 INFO: ##### Step: 282 Learning rate: 6.103515625e-07 #####
Epoch 43, Train Loss: 27.1395, Val Loss: 20.7934
2025-02-05 00:19:42.886 INFO: Epoch 43, Train Loss: 27.1395, Val Loss: 20.7934
train_e/atom_mae: 0.000452
2025-02-05 00:19:42.887 INFO: train_e/atom_mae: 0.000452
train_e/atom_rmse: 0.000832
2025-02-05 00:19:42.887 INFO: train_e/atom_rmse: 0.000832
train_f_mae: 0.026198
2025-02-05 00:19:42.890 INFO: train_f_mae: 0.026198
train_f_rmse: 0.039962
2025-02-05 00:19:42.890 INFO: train_f_rmse: 0.039962
val_e/atom_mae: 0.000494
2025-02-05 00:19:42.892 INFO: val_e/atom_mae: 0.000494
val_e/atom_rmse: 0.000708
2025-02-05 00:19:42.892 INFO: val_e/atom_rmse: 0.000708
val_f_mae: 0.027903
2025-02-05 00:19:42.893 INFO: val_f_mae: 0.027903
val_f_rmse: 0.047561
2025-02-05 00:19:42.893 INFO: val_f_rmse: 0.047561
##### Step: 283 Learning rate: 6.103515625e-07 #####
2025-02-05 00:21:34.188 INFO: ##### Step: 283 Learning rate: 6.103515625e-07 #####
Epoch 44, Train Loss: 27.0457, Val Loss: 20.7358
2025-02-05 00:21:34.188 INFO: Epoch 44, Train Loss: 27.0457, Val Loss: 20.7358
train_e/atom_mae: 0.000450
2025-02-05 00:21:34.189 INFO: train_e/atom_mae: 0.000450
train_e/atom_rmse: 0.000831
2025-02-05 00:21:34.189 INFO: train_e/atom_rmse: 0.000831
train_f_mae: 0.026192
2025-02-05 00:21:34.192 INFO: train_f_mae: 0.026192
train_f_rmse: 0.039962
2025-02-05 00:21:34.192 INFO: train_f_rmse: 0.039962
val_e/atom_mae: 0.000494
2025-02-05 00:21:34.194 INFO: val_e/atom_mae: 0.000494
val_e/atom_rmse: 0.000707
2025-02-05 00:21:34.195 INFO: val_e/atom_rmse: 0.000707
val_f_mae: 0.027897
2025-02-05 00:21:34.195 INFO: val_f_mae: 0.027897
val_f_rmse: 0.047557
2025-02-05 00:21:34.196 INFO: val_f_rmse: 0.047557
##### Step: 284 Learning rate: 6.103515625e-07 #####
2025-02-05 00:23:26.780 INFO: ##### Step: 284 Learning rate: 6.103515625e-07 #####
Epoch 45, Train Loss: 27.0401, Val Loss: 20.7376
2025-02-05 00:23:26.780 INFO: Epoch 45, Train Loss: 27.0401, Val Loss: 20.7376
train_e/atom_mae: 0.000450
2025-02-05 00:23:26.807 INFO: train_e/atom_mae: 0.000450
train_e/atom_rmse: 0.000831
2025-02-05 00:23:26.807 INFO: train_e/atom_rmse: 0.000831
train_f_mae: 0.026193
2025-02-05 00:23:26.810 INFO: train_f_mae: 0.026193
train_f_rmse: 0.039961
2025-02-05 00:23:26.810 INFO: train_f_rmse: 0.039961
val_e/atom_mae: 0.000495
2025-02-05 00:23:26.812 INFO: val_e/atom_mae: 0.000495
val_e/atom_rmse: 0.000707
2025-02-05 00:23:26.813 INFO: val_e/atom_rmse: 0.000707
val_f_mae: 0.027906
2025-02-05 00:23:26.813 INFO: val_f_mae: 0.027906
val_f_rmse: 0.047567
2025-02-05 00:23:26.813 INFO: val_f_rmse: 0.047567
##### Step: 285 Learning rate: 6.103515625e-07 #####
2025-02-05 00:25:18.457 INFO: ##### Step: 285 Learning rate: 6.103515625e-07 #####
Epoch 46, Train Loss: 27.0462, Val Loss: 20.8377
2025-02-05 00:25:18.458 INFO: Epoch 46, Train Loss: 27.0462, Val Loss: 20.8377
train_e/atom_mae: 0.000452
2025-02-05 00:25:18.459 INFO: train_e/atom_mae: 0.000452
train_e/atom_rmse: 0.000831
2025-02-05 00:25:18.459 INFO: train_e/atom_rmse: 0.000831
train_f_mae: 0.026193
2025-02-05 00:25:18.462 INFO: train_f_mae: 0.026193
train_f_rmse: 0.039958
2025-02-05 00:25:18.462 INFO: train_f_rmse: 0.039958
val_e/atom_mae: 0.000495
2025-02-05 00:25:18.464 INFO: val_e/atom_mae: 0.000495
val_e/atom_rmse: 0.000709
2025-02-05 00:25:18.464 INFO: val_e/atom_rmse: 0.000709
val_f_mae: 0.027907
2025-02-05 00:25:18.465 INFO: val_f_mae: 0.027907
val_f_rmse: 0.047569
2025-02-05 00:25:18.465 INFO: val_f_rmse: 0.047569
##### Step: 286 Learning rate: 6.103515625e-07 #####
2025-02-05 00:27:10.194 INFO: ##### Step: 286 Learning rate: 6.103515625e-07 #####
Epoch 47, Train Loss: 27.1511, Val Loss: 20.7154
2025-02-05 00:27:10.195 INFO: Epoch 47, Train Loss: 27.1511, Val Loss: 20.7154
train_e/atom_mae: 0.000452
2025-02-05 00:27:10.195 INFO: train_e/atom_mae: 0.000452
train_e/atom_rmse: 0.000833
2025-02-05 00:27:10.196 INFO: train_e/atom_rmse: 0.000833
train_f_mae: 0.026203
2025-02-05 00:27:10.198 INFO: train_f_mae: 0.026203
train_f_rmse: 0.039968
2025-02-05 00:27:10.199 INFO: train_f_rmse: 0.039968
val_e/atom_mae: 0.000493
2025-02-05 00:27:10.201 INFO: val_e/atom_mae: 0.000493
val_e/atom_rmse: 0.000706
2025-02-05 00:27:10.201 INFO: val_e/atom_rmse: 0.000706
val_f_mae: 0.027910
2025-02-05 00:27:10.202 INFO: val_f_mae: 0.027910
val_f_rmse: 0.047569
2025-02-05 00:27:10.202 INFO: val_f_rmse: 0.047569
##### Step: 287 Learning rate: 6.103515625e-07 #####
2025-02-05 00:29:01.965 INFO: ##### Step: 287 Learning rate: 6.103515625e-07 #####
Epoch 48, Train Loss: 27.0073, Val Loss: 20.7520
2025-02-05 00:29:01.966 INFO: Epoch 48, Train Loss: 27.0073, Val Loss: 20.7520
train_e/atom_mae: 0.000450
2025-02-05 00:29:01.967 INFO: train_e/atom_mae: 0.000450
train_e/atom_rmse: 0.000830
2025-02-05 00:29:01.967 INFO: train_e/atom_rmse: 0.000830
train_f_mae: 0.026193
2025-02-05 00:29:01.970 INFO: train_f_mae: 0.026193
train_f_rmse: 0.039959
2025-02-05 00:29:01.970 INFO: train_f_rmse: 0.039959
val_e/atom_mae: 0.000493
2025-02-05 00:29:01.972 INFO: val_e/atom_mae: 0.000493
val_e/atom_rmse: 0.000707
2025-02-05 00:29:01.972 INFO: val_e/atom_rmse: 0.000707
val_f_mae: 0.027901
2025-02-05 00:29:01.973 INFO: val_f_mae: 0.027901
val_f_rmse: 0.047560
2025-02-05 00:29:01.973 INFO: val_f_rmse: 0.047560
##### Step: 288 Learning rate: 6.103515625e-07 #####
2025-02-05 00:30:53.611 INFO: ##### Step: 288 Learning rate: 6.103515625e-07 #####
Epoch 49, Train Loss: 26.9515, Val Loss: 20.7943
2025-02-05 00:30:53.611 INFO: Epoch 49, Train Loss: 26.9515, Val Loss: 20.7943
train_e/atom_mae: 0.000449
2025-02-05 00:30:53.612 INFO: train_e/atom_mae: 0.000449
train_e/atom_rmse: 0.000829
2025-02-05 00:30:53.612 INFO: train_e/atom_rmse: 0.000829
train_f_mae: 0.026187
2025-02-05 00:30:53.615 INFO: train_f_mae: 0.026187
train_f_rmse: 0.039961
2025-02-05 00:30:53.615 INFO: train_f_rmse: 0.039961
val_e/atom_mae: 0.000494
2025-02-05 00:30:53.617 INFO: val_e/atom_mae: 0.000494
val_e/atom_rmse: 0.000708
2025-02-05 00:30:53.618 INFO: val_e/atom_rmse: 0.000708
val_f_mae: 0.027896
2025-02-05 00:30:53.618 INFO: val_f_mae: 0.027896
val_f_rmse: 0.047555
2025-02-05 00:30:53.618 INFO: val_f_rmse: 0.047555
##### Step: 289 Learning rate: 6.103515625e-07 #####
2025-02-05 00:32:45.054 INFO: ##### Step: 289 Learning rate: 6.103515625e-07 #####
Epoch 50, Train Loss: 27.1206, Val Loss: 20.6714
2025-02-05 00:32:45.054 INFO: Epoch 50, Train Loss: 27.1206, Val Loss: 20.6714
train_e/atom_mae: 0.000451
2025-02-05 00:32:45.055 INFO: train_e/atom_mae: 0.000451
train_e/atom_rmse: 0.000832
2025-02-05 00:32:45.055 INFO: train_e/atom_rmse: 0.000832
train_f_mae: 0.026187
2025-02-05 00:32:45.058 INFO: train_f_mae: 0.026187
train_f_rmse: 0.039960
2025-02-05 00:32:45.058 INFO: train_f_rmse: 0.039960
val_e/atom_mae: 0.000492
2025-02-05 00:32:45.060 INFO: val_e/atom_mae: 0.000492
val_e/atom_rmse: 0.000706
2025-02-05 00:32:45.061 INFO: val_e/atom_rmse: 0.000706
val_f_mae: 0.027899
2025-02-05 00:32:45.061 INFO: val_f_mae: 0.027899
val_f_rmse: 0.047559
2025-02-05 00:32:45.061 INFO: val_f_rmse: 0.047559
##### Step: 290 Learning rate: 6.103515625e-07 #####
2025-02-05 00:34:37.098 INFO: ##### Step: 290 Learning rate: 6.103515625e-07 #####
Epoch 51, Train Loss: 26.9599, Val Loss: 20.7157
2025-02-05 00:34:37.098 INFO: Epoch 51, Train Loss: 26.9599, Val Loss: 20.7157
train_e/atom_mae: 0.000449
2025-02-05 00:34:37.100 INFO: train_e/atom_mae: 0.000449
train_e/atom_rmse: 0.000829
2025-02-05 00:34:37.101 INFO: train_e/atom_rmse: 0.000829
train_f_mae: 0.026185
2025-02-05 00:34:37.103 INFO: train_f_mae: 0.026185
train_f_rmse: 0.039952
2025-02-05 00:34:37.104 INFO: train_f_rmse: 0.039952
val_e/atom_mae: 0.000493
2025-02-05 00:34:37.106 INFO: val_e/atom_mae: 0.000493
val_e/atom_rmse: 0.000706
2025-02-05 00:34:37.106 INFO: val_e/atom_rmse: 0.000706
val_f_mae: 0.027897
2025-02-05 00:34:37.106 INFO: val_f_mae: 0.027897
val_f_rmse: 0.047560
2025-02-05 00:34:37.107 INFO: val_f_rmse: 0.047560
##### Step: 291 Learning rate: 6.103515625e-07 #####
2025-02-05 00:36:28.553 INFO: ##### Step: 291 Learning rate: 6.103515625e-07 #####
Epoch 52, Train Loss: 26.7618, Val Loss: 20.7792
2025-02-05 00:36:28.554 INFO: Epoch 52, Train Loss: 26.7618, Val Loss: 20.7792
train_e/atom_mae: 0.000447
2025-02-05 00:36:28.555 INFO: train_e/atom_mae: 0.000447
train_e/atom_rmse: 0.000826
2025-02-05 00:36:28.555 INFO: train_e/atom_rmse: 0.000826
train_f_mae: 0.026179
2025-02-05 00:36:28.558 INFO: train_f_mae: 0.026179
train_f_rmse: 0.039939
2025-02-05 00:36:28.558 INFO: train_f_rmse: 0.039939
val_e/atom_mae: 0.000494
2025-02-05 00:36:28.560 INFO: val_e/atom_mae: 0.000494
val_e/atom_rmse: 0.000708
2025-02-05 00:36:28.560 INFO: val_e/atom_rmse: 0.000708
val_f_mae: 0.027903
2025-02-05 00:36:28.561 INFO: val_f_mae: 0.027903
val_f_rmse: 0.047569
2025-02-05 00:36:28.561 INFO: val_f_rmse: 0.047569
##### Step: 292 Learning rate: 6.103515625e-07 #####
2025-02-05 00:38:20.210 INFO: ##### Step: 292 Learning rate: 6.103515625e-07 #####
Epoch 53, Train Loss: 27.0435, Val Loss: 20.7257
2025-02-05 00:38:20.211 INFO: Epoch 53, Train Loss: 27.0435, Val Loss: 20.7257
train_e/atom_mae: 0.000450
2025-02-05 00:38:20.212 INFO: train_e/atom_mae: 0.000450
train_e/atom_rmse: 0.000831
2025-02-05 00:38:20.212 INFO: train_e/atom_rmse: 0.000831
train_f_mae: 0.026196
2025-02-05 00:38:20.215 INFO: train_f_mae: 0.026196
train_f_rmse: 0.039969
2025-02-05 00:38:20.215 INFO: train_f_rmse: 0.039969
val_e/atom_mae: 0.000493
2025-02-05 00:38:20.217 INFO: val_e/atom_mae: 0.000493
val_e/atom_rmse: 0.000707
2025-02-05 00:38:20.217 INFO: val_e/atom_rmse: 0.000707
val_f_mae: 0.027902
2025-02-05 00:38:20.218 INFO: val_f_mae: 0.027902
val_f_rmse: 0.047565
2025-02-05 00:38:20.218 INFO: val_f_rmse: 0.047565
##### Step: 293 Learning rate: 6.103515625e-07 #####
2025-02-05 00:40:11.795 INFO: ##### Step: 293 Learning rate: 6.103515625e-07 #####
Epoch 54, Train Loss: 26.9679, Val Loss: 20.7119
2025-02-05 00:40:11.796 INFO: Epoch 54, Train Loss: 26.9679, Val Loss: 20.7119
train_e/atom_mae: 0.000448
2025-02-05 00:40:11.797 INFO: train_e/atom_mae: 0.000448
train_e/atom_rmse: 0.000830
2025-02-05 00:40:11.797 INFO: train_e/atom_rmse: 0.000830
train_f_mae: 0.026200
2025-02-05 00:40:11.800 INFO: train_f_mae: 0.026200
train_f_rmse: 0.039972
2025-02-05 00:40:11.800 INFO: train_f_rmse: 0.039972
val_e/atom_mae: 0.000492
2025-02-05 00:40:11.802 INFO: val_e/atom_mae: 0.000492
val_e/atom_rmse: 0.000706
2025-02-05 00:40:11.803 INFO: val_e/atom_rmse: 0.000706
val_f_mae: 0.027903
2025-02-05 00:40:11.803 INFO: val_f_mae: 0.027903
val_f_rmse: 0.047565
2025-02-05 00:40:11.803 INFO: val_f_rmse: 0.047565
##### Step: 294 Learning rate: 6.103515625e-07 #####
2025-02-05 00:42:03.337 INFO: ##### Step: 294 Learning rate: 6.103515625e-07 #####
Epoch 55, Train Loss: 26.9777, Val Loss: 20.7122
2025-02-05 00:42:03.337 INFO: Epoch 55, Train Loss: 26.9777, Val Loss: 20.7122
train_e/atom_mae: 0.000451
2025-02-05 00:42:03.338 INFO: train_e/atom_mae: 0.000451
train_e/atom_rmse: 0.000830
2025-02-05 00:42:03.338 INFO: train_e/atom_rmse: 0.000830
train_f_mae: 0.026201
2025-02-05 00:42:03.341 INFO: train_f_mae: 0.026201
train_f_rmse: 0.039973
2025-02-05 00:42:03.341 INFO: train_f_rmse: 0.039973
val_e/atom_mae: 0.000493
2025-02-05 00:42:03.343 INFO: val_e/atom_mae: 0.000493
val_e/atom_rmse: 0.000706
2025-02-05 00:42:03.344 INFO: val_e/atom_rmse: 0.000706
val_f_mae: 0.027903
2025-02-05 00:42:03.344 INFO: val_f_mae: 0.027903
val_f_rmse: 0.047566
2025-02-05 00:42:03.344 INFO: val_f_rmse: 0.047566
##### Step: 295 Learning rate: 6.103515625e-07 #####
2025-02-05 00:43:54.929 INFO: ##### Step: 295 Learning rate: 6.103515625e-07 #####
Epoch 56, Train Loss: 26.8870, Val Loss: 20.6688
2025-02-05 00:43:54.930 INFO: Epoch 56, Train Loss: 26.8870, Val Loss: 20.6688
train_e/atom_mae: 0.000450
2025-02-05 00:43:54.931 INFO: train_e/atom_mae: 0.000450
train_e/atom_rmse: 0.000828
2025-02-05 00:43:54.931 INFO: train_e/atom_rmse: 0.000828
train_f_mae: 0.026190
2025-02-05 00:43:54.934 INFO: train_f_mae: 0.026190
train_f_rmse: 0.039958
2025-02-05 00:43:54.934 INFO: train_f_rmse: 0.039958
val_e/atom_mae: 0.000493
2025-02-05 00:43:54.936 INFO: val_e/atom_mae: 0.000493
val_e/atom_rmse: 0.000706
2025-02-05 00:43:54.936 INFO: val_e/atom_rmse: 0.000706
val_f_mae: 0.027909
2025-02-05 00:43:54.937 INFO: val_f_mae: 0.027909
val_f_rmse: 0.047572
2025-02-05 00:43:54.937 INFO: val_f_rmse: 0.047572
##### Step: 296 Learning rate: 6.103515625e-07 #####
2025-02-05 00:45:46.590 INFO: ##### Step: 296 Learning rate: 6.103515625e-07 #####
Epoch 57, Train Loss: 26.8911, Val Loss: 20.7801
2025-02-05 00:45:46.590 INFO: Epoch 57, Train Loss: 26.8911, Val Loss: 20.7801
train_e/atom_mae: 0.000450
2025-02-05 00:45:46.591 INFO: train_e/atom_mae: 0.000450
train_e/atom_rmse: 0.000828
2025-02-05 00:45:46.591 INFO: train_e/atom_rmse: 0.000828
train_f_mae: 0.026214
2025-02-05 00:45:46.594 INFO: train_f_mae: 0.026214
train_f_rmse: 0.039985
2025-02-05 00:45:46.594 INFO: train_f_rmse: 0.039985
val_e/atom_mae: 0.000494
2025-02-05 00:45:46.596 INFO: val_e/atom_mae: 0.000494
val_e/atom_rmse: 0.000708
2025-02-05 00:45:46.597 INFO: val_e/atom_rmse: 0.000708
val_f_mae: 0.027917
2025-02-05 00:45:46.597 INFO: val_f_mae: 0.027917
val_f_rmse: 0.047583
2025-02-05 00:45:46.597 INFO: val_f_rmse: 0.047583
##### Step: 297 Learning rate: 6.103515625e-07 #####
2025-02-05 00:47:38.316 INFO: ##### Step: 297 Learning rate: 6.103515625e-07 #####
Epoch 58, Train Loss: 26.8623, Val Loss: 20.8444
2025-02-05 00:47:38.317 INFO: Epoch 58, Train Loss: 26.8623, Val Loss: 20.8444
train_e/atom_mae: 0.000449
2025-02-05 00:47:38.318 INFO: train_e/atom_mae: 0.000449
train_e/atom_rmse: 0.000828
2025-02-05 00:47:38.318 INFO: train_e/atom_rmse: 0.000828
train_f_mae: 0.026202
2025-02-05 00:47:38.321 INFO: train_f_mae: 0.026202
train_f_rmse: 0.039964
2025-02-05 00:47:38.321 INFO: train_f_rmse: 0.039964
val_e/atom_mae: 0.000496
2025-02-05 00:47:38.323 INFO: val_e/atom_mae: 0.000496
val_e/atom_rmse: 0.000709
2025-02-05 00:47:38.323 INFO: val_e/atom_rmse: 0.000709
val_f_mae: 0.027927
2025-02-05 00:47:38.324 INFO: val_f_mae: 0.027927
val_f_rmse: 0.047588
2025-02-05 00:47:38.324 INFO: val_f_rmse: 0.047588
##### Step: 298 Learning rate: 6.103515625e-07 #####
2025-02-05 00:49:30.094 INFO: ##### Step: 298 Learning rate: 6.103515625e-07 #####
Epoch 59, Train Loss: 26.9224, Val Loss: 20.6723
2025-02-05 00:49:30.095 INFO: Epoch 59, Train Loss: 26.9224, Val Loss: 20.6723
train_e/atom_mae: 0.000449
2025-02-05 00:49:30.096 INFO: train_e/atom_mae: 0.000449
train_e/atom_rmse: 0.000829
2025-02-05 00:49:30.096 INFO: train_e/atom_rmse: 0.000829
train_f_mae: 0.026197
2025-02-05 00:49:30.099 INFO: train_f_mae: 0.026197
train_f_rmse: 0.039943
2025-02-05 00:49:30.099 INFO: train_f_rmse: 0.039943
val_e/atom_mae: 0.000493
2025-02-05 00:49:30.101 INFO: val_e/atom_mae: 0.000493
val_e/atom_rmse: 0.000706
2025-02-05 00:49:30.101 INFO: val_e/atom_rmse: 0.000706
val_f_mae: 0.027919
2025-02-05 00:49:30.102 INFO: val_f_mae: 0.027919
val_f_rmse: 0.047586
2025-02-05 00:49:30.102 INFO: val_f_rmse: 0.047586
##### Step: 299 Learning rate: 6.103515625e-07 #####
2025-02-05 00:51:21.766 INFO: ##### Step: 299 Learning rate: 6.103515625e-07 #####
Epoch 60, Train Loss: 26.6808, Val Loss: 20.7338
2025-02-05 00:51:21.767 INFO: Epoch 60, Train Loss: 26.6808, Val Loss: 20.7338
train_e/atom_mae: 0.000448
2025-02-05 00:51:21.767 INFO: train_e/atom_mae: 0.000448
train_e/atom_rmse: 0.000825
2025-02-05 00:51:21.768 INFO: train_e/atom_rmse: 0.000825
train_f_mae: 0.026197
2025-02-05 00:51:21.770 INFO: train_f_mae: 0.026197
train_f_rmse: 0.039967
2025-02-05 00:51:21.770 INFO: train_f_rmse: 0.039967
val_e/atom_mae: 0.000494
2025-02-05 00:51:21.773 INFO: val_e/atom_mae: 0.000494
val_e/atom_rmse: 0.000707
2025-02-05 00:51:21.773 INFO: val_e/atom_rmse: 0.000707
val_f_mae: 0.027920
2025-02-05 00:51:21.773 INFO: val_f_mae: 0.027920
val_f_rmse: 0.047584
2025-02-05 00:51:21.774 INFO: val_f_rmse: 0.047584
##### Step: 300 Learning rate: 3.0517578125e-07 #####
2025-02-05 00:53:13.288 INFO: ##### Step: 300 Learning rate: 3.0517578125e-07 #####
Epoch 61, Train Loss: 26.8272, Val Loss: 20.6667
2025-02-05 00:53:13.288 INFO: Epoch 61, Train Loss: 26.8272, Val Loss: 20.6667
train_e/atom_mae: 0.000448
2025-02-05 00:53:13.289 INFO: train_e/atom_mae: 0.000448
train_e/atom_rmse: 0.000827
2025-02-05 00:53:13.290 INFO: train_e/atom_rmse: 0.000827
train_f_mae: 0.026216
2025-02-05 00:53:13.292 INFO: train_f_mae: 0.026216
train_f_rmse: 0.039992
2025-02-05 00:53:13.292 INFO: train_f_rmse: 0.039992
val_e/atom_mae: 0.000493
2025-02-05 00:53:13.294 INFO: val_e/atom_mae: 0.000493
val_e/atom_rmse: 0.000705
2025-02-05 00:53:13.295 INFO: val_e/atom_rmse: 0.000705
val_f_mae: 0.027919
2025-02-05 00:53:13.295 INFO: val_f_mae: 0.027919
val_f_rmse: 0.047583
2025-02-05 00:53:13.296 INFO: val_f_rmse: 0.047583
##### Step: 301 Learning rate: 3.0517578125e-07 #####
2025-02-05 00:55:04.871 INFO: ##### Step: 301 Learning rate: 3.0517578125e-07 #####
Epoch 62, Train Loss: 26.7696, Val Loss: 20.7071
2025-02-05 00:55:04.872 INFO: Epoch 62, Train Loss: 26.7696, Val Loss: 20.7071
train_e/atom_mae: 0.000447
2025-02-05 00:55:04.873 INFO: train_e/atom_mae: 0.000447
train_e/atom_rmse: 0.000826
2025-02-05 00:55:04.873 INFO: train_e/atom_rmse: 0.000826
train_f_mae: 0.026211
2025-02-05 00:55:04.876 INFO: train_f_mae: 0.026211
train_f_rmse: 0.039988
2025-02-05 00:55:04.876 INFO: train_f_rmse: 0.039988
val_e/atom_mae: 0.000493
2025-02-05 00:55:04.878 INFO: val_e/atom_mae: 0.000493
val_e/atom_rmse: 0.000706
2025-02-05 00:55:04.878 INFO: val_e/atom_rmse: 0.000706
val_f_mae: 0.027919
2025-02-05 00:55:04.879 INFO: val_f_mae: 0.027919
val_f_rmse: 0.047586
2025-02-05 00:55:04.879 INFO: val_f_rmse: 0.047586
##### Step: 302 Learning rate: 3.0517578125e-07 #####
2025-02-05 00:56:56.649 INFO: ##### Step: 302 Learning rate: 3.0517578125e-07 #####
Epoch 63, Train Loss: 26.7726, Val Loss: 20.6882
2025-02-05 00:56:56.649 INFO: Epoch 63, Train Loss: 26.7726, Val Loss: 20.6882
train_e/atom_mae: 0.000447
2025-02-05 00:56:56.650 INFO: train_e/atom_mae: 0.000447
train_e/atom_rmse: 0.000826
2025-02-05 00:56:56.651 INFO: train_e/atom_rmse: 0.000826
train_f_mae: 0.026212
2025-02-05 00:56:56.653 INFO: train_f_mae: 0.026212
train_f_rmse: 0.039990
2025-02-05 00:56:56.653 INFO: train_f_rmse: 0.039990
val_e/atom_mae: 0.000493
2025-02-05 00:56:56.655 INFO: val_e/atom_mae: 0.000493
val_e/atom_rmse: 0.000706
2025-02-05 00:56:56.656 INFO: val_e/atom_rmse: 0.000706
val_f_mae: 0.027921
2025-02-05 00:56:56.656 INFO: val_f_mae: 0.027921
val_f_rmse: 0.047589
2025-02-05 00:56:56.657 INFO: val_f_rmse: 0.047589
##### Step: 303 Learning rate: 3.0517578125e-07 #####
2025-02-05 00:58:48.558 INFO: ##### Step: 303 Learning rate: 3.0517578125e-07 #####
Epoch 64, Train Loss: 26.8059, Val Loss: 20.7089
2025-02-05 00:58:48.559 INFO: Epoch 64, Train Loss: 26.8059, Val Loss: 20.7089
train_e/atom_mae: 0.000447
2025-02-05 00:58:48.560 INFO: train_e/atom_mae: 0.000447
train_e/atom_rmse: 0.000827
2025-02-05 00:58:48.560 INFO: train_e/atom_rmse: 0.000827
train_f_mae: 0.026221
2025-02-05 00:58:48.563 INFO: train_f_mae: 0.026221
train_f_rmse: 0.039998
2025-02-05 00:58:48.563 INFO: train_f_rmse: 0.039998
val_e/atom_mae: 0.000493
2025-02-05 00:58:48.565 INFO: val_e/atom_mae: 0.000493
val_e/atom_rmse: 0.000706
2025-02-05 00:58:48.566 INFO: val_e/atom_rmse: 0.000706
val_f_mae: 0.027923
2025-02-05 00:58:48.566 INFO: val_f_mae: 0.027923
val_f_rmse: 0.047591
2025-02-05 00:58:48.566 INFO: val_f_rmse: 0.047591
##### Step: 304 Learning rate: 3.0517578125e-07 #####
2025-02-05 01:00:40.374 INFO: ##### Step: 304 Learning rate: 3.0517578125e-07 #####
Epoch 65, Train Loss: 26.7867, Val Loss: 20.7037
2025-02-05 01:00:40.374 INFO: Epoch 65, Train Loss: 26.7867, Val Loss: 20.7037
train_e/atom_mae: 0.000448
2025-02-05 01:00:40.375 INFO: train_e/atom_mae: 0.000448
train_e/atom_rmse: 0.000827
2025-02-05 01:00:40.375 INFO: train_e/atom_rmse: 0.000827
train_f_mae: 0.026220
2025-02-05 01:00:40.378 INFO: train_f_mae: 0.026220
train_f_rmse: 0.039997
2025-02-05 01:00:40.378 INFO: train_f_rmse: 0.039997
val_e/atom_mae: 0.000493
2025-02-05 01:00:40.380 INFO: val_e/atom_mae: 0.000493
val_e/atom_rmse: 0.000706
2025-02-05 01:00:40.380 INFO: val_e/atom_rmse: 0.000706
val_f_mae: 0.027925
2025-02-05 01:00:40.381 INFO: val_f_mae: 0.027925
val_f_rmse: 0.047593
2025-02-05 01:00:40.381 INFO: val_f_rmse: 0.047593
##### Step: 305 Learning rate: 3.0517578125e-07 #####
2025-02-05 01:02:32.114 INFO: ##### Step: 305 Learning rate: 3.0517578125e-07 #####
Epoch 66, Train Loss: 26.7653, Val Loss: 20.7764
2025-02-05 01:02:32.114 INFO: Epoch 66, Train Loss: 26.7653, Val Loss: 20.7764
train_e/atom_mae: 0.000448
2025-02-05 01:02:32.115 INFO: train_e/atom_mae: 0.000448
train_e/atom_rmse: 0.000826
2025-02-05 01:02:32.115 INFO: train_e/atom_rmse: 0.000826
train_f_mae: 0.026221
2025-02-05 01:02:32.118 INFO: train_f_mae: 0.026221
train_f_rmse: 0.040000
2025-02-05 01:02:32.118 INFO: train_f_rmse: 0.040000
val_e/atom_mae: 0.000494
2025-02-05 01:02:32.120 INFO: val_e/atom_mae: 0.000494
val_e/atom_rmse: 0.000708
2025-02-05 01:02:32.121 INFO: val_e/atom_rmse: 0.000708
val_f_mae: 0.027925
2025-02-05 01:02:32.121 INFO: val_f_mae: 0.027925
val_f_rmse: 0.047593
2025-02-05 01:02:32.121 INFO: val_f_rmse: 0.047593
##### Step: 306 Learning rate: 3.0517578125e-07 #####
2025-02-05 01:04:23.852 INFO: ##### Step: 306 Learning rate: 3.0517578125e-07 #####
Epoch 67, Train Loss: 26.8315, Val Loss: 20.6776
2025-02-05 01:04:23.853 INFO: Epoch 67, Train Loss: 26.8315, Val Loss: 20.6776
train_e/atom_mae: 0.000447
2025-02-05 01:04:23.853 INFO: train_e/atom_mae: 0.000447
train_e/atom_rmse: 0.000827
2025-02-05 01:04:23.854 INFO: train_e/atom_rmse: 0.000827
train_f_mae: 0.026212
2025-02-05 01:04:23.856 INFO: train_f_mae: 0.026212
train_f_rmse: 0.039989
2025-02-05 01:04:23.857 INFO: train_f_rmse: 0.039989
val_e/atom_mae: 0.000492
2025-02-05 01:04:23.862 INFO: val_e/atom_mae: 0.000492
val_e/atom_rmse: 0.000706
2025-02-05 01:04:23.862 INFO: val_e/atom_rmse: 0.000706
val_f_mae: 0.027922
2025-02-05 01:04:23.863 INFO: val_f_mae: 0.027922
val_f_rmse: 0.047592
2025-02-05 01:04:23.863 INFO: val_f_rmse: 0.047592
##### Step: 307 Learning rate: 3.0517578125e-07 #####
2025-02-05 01:06:15.662 INFO: ##### Step: 307 Learning rate: 3.0517578125e-07 #####
Epoch 68, Train Loss: 26.7914, Val Loss: 20.6826
2025-02-05 01:06:15.663 INFO: Epoch 68, Train Loss: 26.7914, Val Loss: 20.6826
train_e/atom_mae: 0.000448
2025-02-05 01:06:15.664 INFO: train_e/atom_mae: 0.000448
train_e/atom_rmse: 0.000827
2025-02-05 01:06:15.664 INFO: train_e/atom_rmse: 0.000827
train_f_mae: 0.026219
2025-02-05 01:06:15.666 INFO: train_f_mae: 0.026219
train_f_rmse: 0.039998
2025-02-05 01:06:15.667 INFO: train_f_rmse: 0.039998
val_e/atom_mae: 0.000493
2025-02-05 01:06:15.669 INFO: val_e/atom_mae: 0.000493
val_e/atom_rmse: 0.000706
2025-02-05 01:06:15.669 INFO: val_e/atom_rmse: 0.000706
val_f_mae: 0.027923
2025-02-05 01:06:15.670 INFO: val_f_mae: 0.027923
val_f_rmse: 0.047593
2025-02-05 01:06:15.670 INFO: val_f_rmse: 0.047593
##### Step: 308 Learning rate: 3.0517578125e-07 #####
2025-02-05 01:08:07.615 INFO: ##### Step: 308 Learning rate: 3.0517578125e-07 #####
Epoch 69, Train Loss: 26.9038, Val Loss: 20.7541
2025-02-05 01:08:07.616 INFO: Epoch 69, Train Loss: 26.9038, Val Loss: 20.7541
train_e/atom_mae: 0.000448
2025-02-05 01:08:07.617 INFO: train_e/atom_mae: 0.000448
train_e/atom_rmse: 0.000829
2025-02-05 01:08:07.617 INFO: train_e/atom_rmse: 0.000829
train_f_mae: 0.026217
2025-02-05 01:08:07.619 INFO: train_f_mae: 0.026217
train_f_rmse: 0.039998
2025-02-05 01:08:07.620 INFO: train_f_rmse: 0.039998
val_e/atom_mae: 0.000493
2025-02-05 01:08:07.622 INFO: val_e/atom_mae: 0.000493
val_e/atom_rmse: 0.000707
2025-02-05 01:08:07.622 INFO: val_e/atom_rmse: 0.000707
val_f_mae: 0.027923
2025-02-05 01:08:07.623 INFO: val_f_mae: 0.027923
val_f_rmse: 0.047593
2025-02-05 01:08:07.623 INFO: val_f_rmse: 0.047593
##### Step: 309 Learning rate: 3.0517578125e-07 #####
2025-02-05 01:10:10.434 INFO: ##### Step: 309 Learning rate: 3.0517578125e-07 #####
Epoch 70, Train Loss: 26.7401, Val Loss: 20.6830
2025-02-05 01:10:10.435 INFO: Epoch 70, Train Loss: 26.7401, Val Loss: 20.6830
train_e/atom_mae: 0.000446
2025-02-05 01:10:10.498 INFO: train_e/atom_mae: 0.000446
train_e/atom_rmse: 0.000826
2025-02-05 01:10:10.639 INFO: train_e/atom_rmse: 0.000826
train_f_mae: 0.026208
2025-02-05 01:10:10.642 INFO: train_f_mae: 0.026208
train_f_rmse: 0.039978
2025-02-05 01:10:10.642 INFO: train_f_rmse: 0.039978
val_e/atom_mae: 0.000492
2025-02-05 01:10:10.645 INFO: val_e/atom_mae: 0.000492
val_e/atom_rmse: 0.000706
2025-02-05 01:10:10.645 INFO: val_e/atom_rmse: 0.000706
val_f_mae: 0.027925
2025-02-05 01:10:10.645 INFO: val_f_mae: 0.027925
val_f_rmse: 0.047596
2025-02-05 01:10:10.646 INFO: val_f_rmse: 0.047596
##### Step: 310 Learning rate: 3.0517578125e-07 #####
2025-02-05 01:12:03.069 INFO: ##### Step: 310 Learning rate: 3.0517578125e-07 #####
Epoch 71, Train Loss: 26.8444, Val Loss: 20.7037
2025-02-05 01:12:03.069 INFO: Epoch 71, Train Loss: 26.8444, Val Loss: 20.7037
train_e/atom_mae: 0.000449
2025-02-05 01:12:03.070 INFO: train_e/atom_mae: 0.000449
train_e/atom_rmse: 0.000828
2025-02-05 01:12:03.071 INFO: train_e/atom_rmse: 0.000828
train_f_mae: 0.026217
2025-02-05 01:12:03.073 INFO: train_f_mae: 0.026217
train_f_rmse: 0.039999
2025-02-05 01:12:03.073 INFO: train_f_rmse: 0.039999
val_e/atom_mae: 0.000493
2025-02-05 01:12:03.076 INFO: val_e/atom_mae: 0.000493
val_e/atom_rmse: 0.000706
2025-02-05 01:12:03.076 INFO: val_e/atom_rmse: 0.000706
val_f_mae: 0.027924
2025-02-05 01:12:03.076 INFO: val_f_mae: 0.027924
val_f_rmse: 0.047596
2025-02-05 01:12:03.077 INFO: val_f_rmse: 0.047596
##### Step: 311 Learning rate: 3.0517578125e-07 #####
2025-02-05 01:13:55.081 INFO: ##### Step: 311 Learning rate: 3.0517578125e-07 #####
Epoch 72, Train Loss: 26.8786, Val Loss: 20.7336
2025-02-05 01:13:55.082 INFO: Epoch 72, Train Loss: 26.8786, Val Loss: 20.7336
train_e/atom_mae: 0.000447
2025-02-05 01:13:55.083 INFO: train_e/atom_mae: 0.000447
train_e/atom_rmse: 0.000828
2025-02-05 01:13:55.083 INFO: train_e/atom_rmse: 0.000828
train_f_mae: 0.026220
2025-02-05 01:13:55.086 INFO: train_f_mae: 0.026220
train_f_rmse: 0.040002
2025-02-05 01:13:55.086 INFO: train_f_rmse: 0.040002
val_e/atom_mae: 0.000493
2025-02-05 01:13:55.088 INFO: val_e/atom_mae: 0.000493
val_e/atom_rmse: 0.000707
2025-02-05 01:13:55.089 INFO: val_e/atom_rmse: 0.000707
val_f_mae: 0.027923
2025-02-05 01:13:55.089 INFO: val_f_mae: 0.027923
val_f_rmse: 0.047596
2025-02-05 01:13:55.089 INFO: val_f_rmse: 0.047596
##### Step: 312 Learning rate: 3.0517578125e-07 #####
2025-02-05 01:15:47.225 INFO: ##### Step: 312 Learning rate: 3.0517578125e-07 #####
Epoch 73, Train Loss: 26.8378, Val Loss: 20.7130
2025-02-05 01:15:47.225 INFO: Epoch 73, Train Loss: 26.8378, Val Loss: 20.7130
train_e/atom_mae: 0.000447
2025-02-05 01:15:47.226 INFO: train_e/atom_mae: 0.000447
train_e/atom_rmse: 0.000827
2025-02-05 01:15:47.227 INFO: train_e/atom_rmse: 0.000827
train_f_mae: 0.026207
2025-02-05 01:15:47.229 INFO: train_f_mae: 0.026207
train_f_rmse: 0.039980
2025-02-05 01:15:47.230 INFO: train_f_rmse: 0.039980
val_e/atom_mae: 0.000493
2025-02-05 01:15:47.232 INFO: val_e/atom_mae: 0.000493
val_e/atom_rmse: 0.000706
2025-02-05 01:15:47.232 INFO: val_e/atom_rmse: 0.000706
val_f_mae: 0.027927
2025-02-05 01:15:47.232 INFO: val_f_mae: 0.027927
val_f_rmse: 0.047598
2025-02-05 01:15:47.233 INFO: val_f_rmse: 0.047598
##### Step: 313 Learning rate: 3.0517578125e-07 #####
2025-02-05 01:17:39.226 INFO: ##### Step: 313 Learning rate: 3.0517578125e-07 #####
Epoch 74, Train Loss: 26.8263, Val Loss: 20.7373
2025-02-05 01:17:39.226 INFO: Epoch 74, Train Loss: 26.8263, Val Loss: 20.7373
train_e/atom_mae: 0.000447
2025-02-05 01:17:39.227 INFO: train_e/atom_mae: 0.000447
train_e/atom_rmse: 0.000827
2025-02-05 01:17:39.227 INFO: train_e/atom_rmse: 0.000827
train_f_mae: 0.026214
2025-02-05 01:17:39.230 INFO: train_f_mae: 0.026214
train_f_rmse: 0.039984
2025-02-05 01:17:39.230 INFO: train_f_rmse: 0.039984
val_e/atom_mae: 0.000493
2025-02-05 01:17:39.232 INFO: val_e/atom_mae: 0.000493
val_e/atom_rmse: 0.000707
2025-02-05 01:17:39.233 INFO: val_e/atom_rmse: 0.000707
val_f_mae: 0.027932
2025-02-05 01:17:39.233 INFO: val_f_mae: 0.027932
val_f_rmse: 0.047603
2025-02-05 01:17:39.233 INFO: val_f_rmse: 0.047603
##### Step: 314 Learning rate: 3.0517578125e-07 #####
2025-02-05 01:19:31.081 INFO: ##### Step: 314 Learning rate: 3.0517578125e-07 #####
Epoch 75, Train Loss: 26.7458, Val Loss: 20.7715
2025-02-05 01:19:31.081 INFO: Epoch 75, Train Loss: 26.7458, Val Loss: 20.7715
train_e/atom_mae: 0.000445
2025-02-05 01:19:31.082 INFO: train_e/atom_mae: 0.000445
train_e/atom_rmse: 0.000826
2025-02-05 01:19:31.082 INFO: train_e/atom_rmse: 0.000826
train_f_mae: 0.026216
2025-02-05 01:19:31.085 INFO: train_f_mae: 0.026216
train_f_rmse: 0.039990
2025-02-05 01:19:31.085 INFO: train_f_rmse: 0.039990
val_e/atom_mae: 0.000494
2025-02-05 01:19:31.087 INFO: val_e/atom_mae: 0.000494
val_e/atom_rmse: 0.000707
2025-02-05 01:19:31.088 INFO: val_e/atom_rmse: 0.000707
val_f_mae: 0.027936
2025-02-05 01:19:31.088 INFO: val_f_mae: 0.027936
val_f_rmse: 0.047606
2025-02-05 01:19:31.088 INFO: val_f_rmse: 0.047606
##### Step: 315 Learning rate: 3.0517578125e-07 #####
2025-02-05 01:21:22.996 INFO: ##### Step: 315 Learning rate: 3.0517578125e-07 #####
Epoch 76, Train Loss: 26.8246, Val Loss: 20.7313
2025-02-05 01:21:22.997 INFO: Epoch 76, Train Loss: 26.8246, Val Loss: 20.7313
train_e/atom_mae: 0.000448
2025-02-05 01:21:22.998 INFO: train_e/atom_mae: 0.000448
train_e/atom_rmse: 0.000827
2025-02-05 01:21:22.998 INFO: train_e/atom_rmse: 0.000827
train_f_mae: 0.026233
2025-02-05 01:21:23.001 INFO: train_f_mae: 0.026233
train_f_rmse: 0.040012
2025-02-05 01:21:23.001 INFO: train_f_rmse: 0.040012
val_e/atom_mae: 0.000493
2025-02-05 01:21:23.003 INFO: val_e/atom_mae: 0.000493
val_e/atom_rmse: 0.000707
2025-02-05 01:21:23.003 INFO: val_e/atom_rmse: 0.000707
val_f_mae: 0.027938
2025-02-05 01:21:23.004 INFO: val_f_mae: 0.027938
val_f_rmse: 0.047609
2025-02-05 01:21:23.004 INFO: val_f_rmse: 0.047609
##### Step: 316 Learning rate: 3.0517578125e-07 #####
2025-02-05 01:23:15.136 INFO: ##### Step: 316 Learning rate: 3.0517578125e-07 #####
Epoch 77, Train Loss: 26.7452, Val Loss: 20.7320
2025-02-05 01:23:15.137 INFO: Epoch 77, Train Loss: 26.7452, Val Loss: 20.7320
train_e/atom_mae: 0.000448
2025-02-05 01:23:15.138 INFO: train_e/atom_mae: 0.000448
train_e/atom_rmse: 0.000826
2025-02-05 01:23:15.138 INFO: train_e/atom_rmse: 0.000826
train_f_mae: 0.026226
2025-02-05 01:23:15.141 INFO: train_f_mae: 0.026226
train_f_rmse: 0.040006
2025-02-05 01:23:15.141 INFO: train_f_rmse: 0.040006
val_e/atom_mae: 0.000493
2025-02-05 01:23:15.143 INFO: val_e/atom_mae: 0.000493
val_e/atom_rmse: 0.000707
2025-02-05 01:23:15.143 INFO: val_e/atom_rmse: 0.000707
val_f_mae: 0.027939
2025-02-05 01:23:15.144 INFO: val_f_mae: 0.027939
val_f_rmse: 0.047610
2025-02-05 01:23:15.144 INFO: val_f_rmse: 0.047610
##### Step: 317 Learning rate: 3.0517578125e-07 #####
2025-02-05 01:25:07.313 INFO: ##### Step: 317 Learning rate: 3.0517578125e-07 #####
Epoch 78, Train Loss: 26.9407, Val Loss: 20.6709
2025-02-05 01:25:07.314 INFO: Epoch 78, Train Loss: 26.9407, Val Loss: 20.6709
train_e/atom_mae: 0.000448
2025-02-05 01:25:07.315 INFO: train_e/atom_mae: 0.000448
train_e/atom_rmse: 0.000829
2025-02-05 01:25:07.315 INFO: train_e/atom_rmse: 0.000829
train_f_mae: 0.026234
2025-02-05 01:25:07.317 INFO: train_f_mae: 0.026234
train_f_rmse: 0.040013
2025-02-05 01:25:07.318 INFO: train_f_rmse: 0.040013
val_e/atom_mae: 0.000492
2025-02-05 01:25:07.320 INFO: val_e/atom_mae: 0.000492
val_e/atom_rmse: 0.000706
2025-02-05 01:25:07.320 INFO: val_e/atom_rmse: 0.000706
val_f_mae: 0.027939
2025-02-05 01:25:07.321 INFO: val_f_mae: 0.027939
val_f_rmse: 0.047609
2025-02-05 01:25:07.321 INFO: val_f_rmse: 0.047609
##### Step: 318 Learning rate: 3.0517578125e-07 #####
2025-02-05 01:26:59.208 INFO: ##### Step: 318 Learning rate: 3.0517578125e-07 #####
Epoch 79, Train Loss: 26.7997, Val Loss: 20.7012
2025-02-05 01:26:59.209 INFO: Epoch 79, Train Loss: 26.7997, Val Loss: 20.7012
train_e/atom_mae: 0.000447
2025-02-05 01:26:59.210 INFO: train_e/atom_mae: 0.000447
train_e/atom_rmse: 0.000827
2025-02-05 01:26:59.210 INFO: train_e/atom_rmse: 0.000827
train_f_mae: 0.026225
2025-02-05 01:26:59.213 INFO: train_f_mae: 0.026225
train_f_rmse: 0.040005
2025-02-05 01:26:59.213 INFO: train_f_rmse: 0.040005
val_e/atom_mae: 0.000493
2025-02-05 01:26:59.215 INFO: val_e/atom_mae: 0.000493
val_e/atom_rmse: 0.000706
2025-02-05 01:26:59.216 INFO: val_e/atom_rmse: 0.000706
val_f_mae: 0.027935
2025-02-05 01:26:59.216 INFO: val_f_mae: 0.027935
val_f_rmse: 0.047607
2025-02-05 01:26:59.216 INFO: val_f_rmse: 0.047607
##### Step: 319 Learning rate: 3.0517578125e-07 #####
2025-02-05 01:28:51.207 INFO: ##### Step: 319 Learning rate: 3.0517578125e-07 #####
Epoch 80, Train Loss: 26.7709, Val Loss: 20.7071
2025-02-05 01:28:51.207 INFO: Epoch 80, Train Loss: 26.7709, Val Loss: 20.7071
train_e/atom_mae: 0.000447
2025-02-05 01:28:51.208 INFO: train_e/atom_mae: 0.000447
train_e/atom_rmse: 0.000826
2025-02-05 01:28:51.208 INFO: train_e/atom_rmse: 0.000826
train_f_mae: 0.026216
2025-02-05 01:28:51.211 INFO: train_f_mae: 0.026216
train_f_rmse: 0.039989
2025-02-05 01:28:51.211 INFO: train_f_rmse: 0.039989
val_e/atom_mae: 0.000493
2025-02-05 01:28:51.213 INFO: val_e/atom_mae: 0.000493
val_e/atom_rmse: 0.000706
2025-02-05 01:28:51.214 INFO: val_e/atom_rmse: 0.000706
val_f_mae: 0.027936
2025-02-05 01:28:51.214 INFO: val_f_mae: 0.027936
val_f_rmse: 0.047607
2025-02-05 01:28:51.214 INFO: val_f_rmse: 0.047607
##### Step: 320 Learning rate: 1.52587890625e-07 #####
2025-02-05 01:30:43.130 INFO: ##### Step: 320 Learning rate: 1.52587890625e-07 #####
Epoch 81, Train Loss: 26.7955, Val Loss: 20.7892
2025-02-05 01:30:43.131 INFO: Epoch 81, Train Loss: 26.7955, Val Loss: 20.7892
train_e/atom_mae: 0.000448
2025-02-05 01:30:43.131 INFO: train_e/atom_mae: 0.000448
train_e/atom_rmse: 0.000827
2025-02-05 01:30:43.132 INFO: train_e/atom_rmse: 0.000827
train_f_mae: 0.026219
2025-02-05 01:30:43.138 INFO: train_f_mae: 0.026219
train_f_rmse: 0.040000
2025-02-05 01:30:43.138 INFO: train_f_rmse: 0.040000
val_e/atom_mae: 0.000494
2025-02-05 01:30:43.140 INFO: val_e/atom_mae: 0.000494
val_e/atom_rmse: 0.000708
2025-02-05 01:30:43.140 INFO: val_e/atom_rmse: 0.000708
val_f_mae: 0.027931
2025-02-05 01:30:43.141 INFO: val_f_mae: 0.027931
val_f_rmse: 0.047604
2025-02-05 01:30:43.141 INFO: val_f_rmse: 0.047604
##### Step: 321 Learning rate: 1.52587890625e-07 #####
2025-02-05 01:32:34.959 INFO: ##### Step: 321 Learning rate: 1.52587890625e-07 #####
Epoch 82, Train Loss: 26.7782, Val Loss: 20.7251
2025-02-05 01:32:34.959 INFO: Epoch 82, Train Loss: 26.7782, Val Loss: 20.7251
train_e/atom_mae: 0.000446
2025-02-05 01:32:34.960 INFO: train_e/atom_mae: 0.000446
train_e/atom_rmse: 0.000826
2025-02-05 01:32:34.960 INFO: train_e/atom_rmse: 0.000826
train_f_mae: 0.026224
2025-02-05 01:32:34.963 INFO: train_f_mae: 0.026224
train_f_rmse: 0.040007
2025-02-05 01:32:34.963 INFO: train_f_rmse: 0.040007
val_e/atom_mae: 0.000493
2025-02-05 01:32:34.965 INFO: val_e/atom_mae: 0.000493
val_e/atom_rmse: 0.000707
2025-02-05 01:32:34.966 INFO: val_e/atom_rmse: 0.000707
val_f_mae: 0.027930
2025-02-05 01:32:34.966 INFO: val_f_mae: 0.027930
val_f_rmse: 0.047603
2025-02-05 01:32:34.966 INFO: val_f_rmse: 0.047603
##### Step: 322 Learning rate: 1.52587890625e-07 #####
2025-02-05 01:34:26.804 INFO: ##### Step: 322 Learning rate: 1.52587890625e-07 #####
Epoch 83, Train Loss: 26.6047, Val Loss: 20.7164
2025-02-05 01:34:26.804 INFO: Epoch 83, Train Loss: 26.6047, Val Loss: 20.7164
train_e/atom_mae: 0.000445
2025-02-05 01:34:26.805 INFO: train_e/atom_mae: 0.000445
train_e/atom_rmse: 0.000824
2025-02-05 01:34:26.805 INFO: train_e/atom_rmse: 0.000824
train_f_mae: 0.026195
2025-02-05 01:34:26.808 INFO: train_f_mae: 0.026195
train_f_rmse: 0.039960
2025-02-05 01:34:26.808 INFO: train_f_rmse: 0.039960
val_e/atom_mae: 0.000493
2025-02-05 01:34:26.810 INFO: val_e/atom_mae: 0.000493
val_e/atom_rmse: 0.000706
2025-02-05 01:34:26.811 INFO: val_e/atom_rmse: 0.000706
val_f_mae: 0.027930
2025-02-05 01:34:26.811 INFO: val_f_mae: 0.027930
val_f_rmse: 0.047603
2025-02-05 01:34:26.812 INFO: val_f_rmse: 0.047603
##### Step: 323 Learning rate: 1.52587890625e-07 #####
2025-02-05 01:36:18.898 INFO: ##### Step: 323 Learning rate: 1.52587890625e-07 #####
Epoch 84, Train Loss: 26.7821, Val Loss: 20.7059
2025-02-05 01:36:18.898 INFO: Epoch 84, Train Loss: 26.7821, Val Loss: 20.7059
train_e/atom_mae: 0.000446
2025-02-05 01:36:18.899 INFO: train_e/atom_mae: 0.000446
train_e/atom_rmse: 0.000826
2025-02-05 01:36:18.899 INFO: train_e/atom_rmse: 0.000826
train_f_mae: 0.026224
2025-02-05 01:36:18.902 INFO: train_f_mae: 0.026224
train_f_rmse: 0.040007
2025-02-05 01:36:18.902 INFO: train_f_rmse: 0.040007
val_e/atom_mae: 0.000492
2025-02-05 01:36:18.904 INFO: val_e/atom_mae: 0.000492
val_e/atom_rmse: 0.000706
2025-02-05 01:36:18.905 INFO: val_e/atom_rmse: 0.000706
val_f_mae: 0.027930
2025-02-05 01:36:18.905 INFO: val_f_mae: 0.027930
val_f_rmse: 0.047602
2025-02-05 01:36:18.905 INFO: val_f_rmse: 0.047602
##### Step: 324 Learning rate: 1.52587890625e-07 #####
2025-02-05 01:38:11.001 INFO: ##### Step: 324 Learning rate: 1.52587890625e-07 #####
Epoch 85, Train Loss: 26.7847, Val Loss: 20.6907
2025-02-05 01:38:11.002 INFO: Epoch 85, Train Loss: 26.7847, Val Loss: 20.6907
train_e/atom_mae: 0.000446
2025-02-05 01:38:11.003 INFO: train_e/atom_mae: 0.000446
train_e/atom_rmse: 0.000827
2025-02-05 01:38:11.003 INFO: train_e/atom_rmse: 0.000827
train_f_mae: 0.026224
2025-02-05 01:38:11.006 INFO: train_f_mae: 0.026224
train_f_rmse: 0.040007
2025-02-05 01:38:11.006 INFO: train_f_rmse: 0.040007
val_e/atom_mae: 0.000492
2025-02-05 01:38:11.008 INFO: val_e/atom_mae: 0.000492
val_e/atom_rmse: 0.000706
2025-02-05 01:38:11.008 INFO: val_e/atom_rmse: 0.000706
val_f_mae: 0.027929
2025-02-05 01:38:11.009 INFO: val_f_mae: 0.027929
val_f_rmse: 0.047602
2025-02-05 01:38:11.009 INFO: val_f_rmse: 0.047602
##### Step: 325 Learning rate: 1.52587890625e-07 #####
2025-02-05 01:40:02.790 INFO: ##### Step: 325 Learning rate: 1.52587890625e-07 #####
Epoch 86, Train Loss: 26.7185, Val Loss: 20.6916
2025-02-05 01:40:02.791 INFO: Epoch 86, Train Loss: 26.7185, Val Loss: 20.6916
train_e/atom_mae: 0.000445
2025-02-05 01:40:02.792 INFO: train_e/atom_mae: 0.000445
train_e/atom_rmse: 0.000826
2025-02-05 01:40:02.792 INFO: train_e/atom_rmse: 0.000826
train_f_mae: 0.026201
2025-02-05 01:40:02.795 INFO: train_f_mae: 0.026201
train_f_rmse: 0.039966
2025-02-05 01:40:02.795 INFO: train_f_rmse: 0.039966
val_e/atom_mae: 0.000492
2025-02-05 01:40:02.797 INFO: val_e/atom_mae: 0.000492
val_e/atom_rmse: 0.000706
2025-02-05 01:40:02.798 INFO: val_e/atom_rmse: 0.000706
val_f_mae: 0.027929
2025-02-05 01:40:02.798 INFO: val_f_mae: 0.027929
val_f_rmse: 0.047601
2025-02-05 01:40:02.798 INFO: val_f_rmse: 0.047601
##### Step: 326 Learning rate: 1.52587890625e-07 #####
2025-02-05 01:41:54.691 INFO: ##### Step: 326 Learning rate: 1.52587890625e-07 #####
Epoch 87, Train Loss: 26.7525, Val Loss: 20.7180
2025-02-05 01:41:54.691 INFO: Epoch 87, Train Loss: 26.7525, Val Loss: 20.7180
train_e/atom_mae: 0.000447
2025-02-05 01:41:54.692 INFO: train_e/atom_mae: 0.000447
train_e/atom_rmse: 0.000826
2025-02-05 01:41:54.692 INFO: train_e/atom_rmse: 0.000826
train_f_mae: 0.026222
2025-02-05 01:41:54.695 INFO: train_f_mae: 0.026222
train_f_rmse: 0.040007
2025-02-05 01:41:54.695 INFO: train_f_rmse: 0.040007
val_e/atom_mae: 0.000493
2025-02-05 01:41:54.701 INFO: val_e/atom_mae: 0.000493
val_e/atom_rmse: 0.000706
2025-02-05 01:41:54.701 INFO: val_e/atom_rmse: 0.000706
val_f_mae: 0.027925
2025-02-05 01:41:54.702 INFO: val_f_mae: 0.027925
val_f_rmse: 0.047598
2025-02-05 01:41:54.702 INFO: val_f_rmse: 0.047598
##### Step: 327 Learning rate: 1.52587890625e-07 #####
2025-02-05 01:43:46.467 INFO: ##### Step: 327 Learning rate: 1.52587890625e-07 #####
Epoch 88, Train Loss: 26.6108, Val Loss: 20.7125
2025-02-05 01:43:46.467 INFO: Epoch 88, Train Loss: 26.6108, Val Loss: 20.7125
train_e/atom_mae: 0.000445
2025-02-05 01:43:46.468 INFO: train_e/atom_mae: 0.000445
train_e/atom_rmse: 0.000824
2025-02-05 01:43:46.468 INFO: train_e/atom_rmse: 0.000824
train_f_mae: 0.026211
2025-02-05 01:43:46.471 INFO: train_f_mae: 0.026211
train_f_rmse: 0.039973
2025-02-05 01:43:46.471 INFO: train_f_rmse: 0.039973
val_e/atom_mae: 0.000493
2025-02-05 01:43:46.473 INFO: val_e/atom_mae: 0.000493
val_e/atom_rmse: 0.000706
2025-02-05 01:43:46.474 INFO: val_e/atom_rmse: 0.000706
val_f_mae: 0.027924
2025-02-05 01:43:46.474 INFO: val_f_mae: 0.027924
val_f_rmse: 0.047598
2025-02-05 01:43:46.474 INFO: val_f_rmse: 0.047598
##### Step: 328 Learning rate: 1.52587890625e-07 #####
2025-02-05 01:45:38.338 INFO: ##### Step: 328 Learning rate: 1.52587890625e-07 #####
Epoch 89, Train Loss: 26.7349, Val Loss: 20.7610
2025-02-05 01:45:38.339 INFO: Epoch 89, Train Loss: 26.7349, Val Loss: 20.7610
train_e/atom_mae: 0.000447
2025-02-05 01:45:38.339 INFO: train_e/atom_mae: 0.000447
train_e/atom_rmse: 0.000826
2025-02-05 01:45:38.340 INFO: train_e/atom_rmse: 0.000826
train_f_mae: 0.026219
2025-02-05 01:45:38.342 INFO: train_f_mae: 0.026219
train_f_rmse: 0.040005
2025-02-05 01:45:38.342 INFO: train_f_rmse: 0.040005
val_e/atom_mae: 0.000493
2025-02-05 01:45:38.345 INFO: val_e/atom_mae: 0.000493
val_e/atom_rmse: 0.000707
2025-02-05 01:45:38.345 INFO: val_e/atom_rmse: 0.000707
val_f_mae: 0.027925
2025-02-05 01:45:38.345 INFO: val_f_mae: 0.027925
val_f_rmse: 0.047599
2025-02-05 01:45:38.346 INFO: val_f_rmse: 0.047599
##### Step: 329 Learning rate: 1.52587890625e-07 #####
2025-02-05 01:47:30.222 INFO: ##### Step: 329 Learning rate: 1.52587890625e-07 #####
Epoch 90, Train Loss: 26.7500, Val Loss: 20.6995
2025-02-05 01:47:30.222 INFO: Epoch 90, Train Loss: 26.7500, Val Loss: 20.6995
train_e/atom_mae: 0.000446
2025-02-05 01:47:30.223 INFO: train_e/atom_mae: 0.000446
train_e/atom_rmse: 0.000826
2025-02-05 01:47:30.224 INFO: train_e/atom_rmse: 0.000826
train_f_mae: 0.026221
2025-02-05 01:47:30.226 INFO: train_f_mae: 0.026221
train_f_rmse: 0.040006
2025-02-05 01:47:30.226 INFO: train_f_rmse: 0.040006
val_e/atom_mae: 0.000493
2025-02-05 01:47:30.229 INFO: val_e/atom_mae: 0.000493
val_e/atom_rmse: 0.000706
2025-02-05 01:47:30.229 INFO: val_e/atom_rmse: 0.000706
val_f_mae: 0.027923
2025-02-05 01:47:30.229 INFO: val_f_mae: 0.027923
val_f_rmse: 0.047598
2025-02-05 01:47:30.230 INFO: val_f_rmse: 0.047598
##### Step: 330 Learning rate: 1.52587890625e-07 #####
2025-02-05 01:49:22.060 INFO: ##### Step: 330 Learning rate: 1.52587890625e-07 #####
Epoch 91, Train Loss: 26.7613, Val Loss: 20.7068
2025-02-05 01:49:22.060 INFO: Epoch 91, Train Loss: 26.7613, Val Loss: 20.7068
train_e/atom_mae: 0.000446
2025-02-05 01:49:22.061 INFO: train_e/atom_mae: 0.000446
train_e/atom_rmse: 0.000826
2025-02-05 01:49:22.061 INFO: train_e/atom_rmse: 0.000826
train_f_mae: 0.026219
2025-02-05 01:49:22.064 INFO: train_f_mae: 0.026219
train_f_rmse: 0.040005
2025-02-05 01:49:22.064 INFO: train_f_rmse: 0.040005
val_e/atom_mae: 0.000493
2025-02-05 01:49:22.066 INFO: val_e/atom_mae: 0.000493
val_e/atom_rmse: 0.000706
2025-02-05 01:49:22.067 INFO: val_e/atom_rmse: 0.000706
val_f_mae: 0.027923
2025-02-05 01:49:22.067 INFO: val_f_mae: 0.027923
val_f_rmse: 0.047598
2025-02-05 01:49:22.067 INFO: val_f_rmse: 0.047598
##### Step: 331 Learning rate: 1.52587890625e-07 #####
2025-02-05 01:51:13.996 INFO: ##### Step: 331 Learning rate: 1.52587890625e-07 #####
Epoch 92, Train Loss: 26.7429, Val Loss: 20.7357
2025-02-05 01:51:13.997 INFO: Epoch 92, Train Loss: 26.7429, Val Loss: 20.7357
train_e/atom_mae: 0.000446
2025-02-05 01:51:13.997 INFO: train_e/atom_mae: 0.000446
train_e/atom_rmse: 0.000826
2025-02-05 01:51:13.998 INFO: train_e/atom_rmse: 0.000826
train_f_mae: 0.026200
2025-02-05 01:51:14.000 INFO: train_f_mae: 0.026200
train_f_rmse: 0.039969
2025-02-05 01:51:14.001 INFO: train_f_rmse: 0.039969
val_e/atom_mae: 0.000493
2025-02-05 01:51:14.003 INFO: val_e/atom_mae: 0.000493
val_e/atom_rmse: 0.000707
2025-02-05 01:51:14.003 INFO: val_e/atom_rmse: 0.000707
val_f_mae: 0.027925
2025-02-05 01:51:14.004 INFO: val_f_mae: 0.027925
val_f_rmse: 0.047600
2025-02-05 01:51:14.004 INFO: val_f_rmse: 0.047600
##### Step: 332 Learning rate: 1.52587890625e-07 #####
2025-02-05 01:53:05.781 INFO: ##### Step: 332 Learning rate: 1.52587890625e-07 #####
Epoch 93, Train Loss: 26.7254, Val Loss: 20.7230
2025-02-05 01:53:05.782 INFO: Epoch 93, Train Loss: 26.7254, Val Loss: 20.7230
train_e/atom_mae: 0.000446
2025-02-05 01:53:05.782 INFO: train_e/atom_mae: 0.000446
train_e/atom_rmse: 0.000826
2025-02-05 01:53:05.783 INFO: train_e/atom_rmse: 0.000826
train_f_mae: 0.026221
2025-02-05 01:53:05.785 INFO: train_f_mae: 0.026221
train_f_rmse: 0.040007
2025-02-05 01:53:05.785 INFO: train_f_rmse: 0.040007
val_e/atom_mae: 0.000493
2025-02-05 01:53:05.788 INFO: val_e/atom_mae: 0.000493
val_e/atom_rmse: 0.000707
2025-02-05 01:53:05.788 INFO: val_e/atom_rmse: 0.000707
val_f_mae: 0.027923
2025-02-05 01:53:05.788 INFO: val_f_mae: 0.027923
val_f_rmse: 0.047599
2025-02-05 01:53:05.789 INFO: val_f_rmse: 0.047599
##### Step: 333 Learning rate: 1.52587890625e-07 #####
2025-02-05 01:54:57.675 INFO: ##### Step: 333 Learning rate: 1.52587890625e-07 #####
Epoch 94, Train Loss: 26.7256, Val Loss: 20.7259
2025-02-05 01:54:57.676 INFO: Epoch 94, Train Loss: 26.7256, Val Loss: 20.7259
train_e/atom_mae: 0.000446
2025-02-05 01:54:57.676 INFO: train_e/atom_mae: 0.000446
train_e/atom_rmse: 0.000826
2025-02-05 01:54:57.677 INFO: train_e/atom_rmse: 0.000826
train_f_mae: 0.026209
2025-02-05 01:54:57.679 INFO: train_f_mae: 0.026209
train_f_rmse: 0.039994
2025-02-05 01:54:57.679 INFO: train_f_rmse: 0.039994
val_e/atom_mae: 0.000493
2025-02-05 01:54:57.682 INFO: val_e/atom_mae: 0.000493
val_e/atom_rmse: 0.000707
2025-02-05 01:54:57.682 INFO: val_e/atom_rmse: 0.000707
val_f_mae: 0.027923
2025-02-05 01:54:57.682 INFO: val_f_mae: 0.027923
val_f_rmse: 0.047599
2025-02-05 01:54:57.683 INFO: val_f_rmse: 0.047599
##### Step: 334 Learning rate: 1.52587890625e-07 #####
2025-02-05 01:56:49.421 INFO: ##### Step: 334 Learning rate: 1.52587890625e-07 #####
Epoch 95, Train Loss: 26.7447, Val Loss: 20.7181
2025-02-05 01:56:49.421 INFO: Epoch 95, Train Loss: 26.7447, Val Loss: 20.7181
train_e/atom_mae: 0.000446
2025-02-05 01:56:49.422 INFO: train_e/atom_mae: 0.000446
train_e/atom_rmse: 0.000826
2025-02-05 01:56:49.422 INFO: train_e/atom_rmse: 0.000826
train_f_mae: 0.026218
2025-02-05 01:56:49.425 INFO: train_f_mae: 0.026218
train_f_rmse: 0.040005
2025-02-05 01:56:49.425 INFO: train_f_rmse: 0.040005
val_e/atom_mae: 0.000493
2025-02-05 01:56:49.427 INFO: val_e/atom_mae: 0.000493
val_e/atom_rmse: 0.000706
2025-02-05 01:56:49.428 INFO: val_e/atom_rmse: 0.000706
val_f_mae: 0.027923
2025-02-05 01:56:49.428 INFO: val_f_mae: 0.027923
val_f_rmse: 0.047599
2025-02-05 01:56:49.428 INFO: val_f_rmse: 0.047599
##### Step: 335 Learning rate: 1.52587890625e-07 #####
2025-02-05 01:58:41.089 INFO: ##### Step: 335 Learning rate: 1.52587890625e-07 #####
Epoch 96, Train Loss: 26.7121, Val Loss: 20.7832
2025-02-05 01:58:41.090 INFO: Epoch 96, Train Loss: 26.7121, Val Loss: 20.7832
train_e/atom_mae: 0.000447
2025-02-05 01:58:41.091 INFO: train_e/atom_mae: 0.000447
train_e/atom_rmse: 0.000825
2025-02-05 01:58:41.091 INFO: train_e/atom_rmse: 0.000825
train_f_mae: 0.026217
2025-02-05 01:58:41.094 INFO: train_f_mae: 0.026217
train_f_rmse: 0.040005
2025-02-05 01:58:41.094 INFO: train_f_rmse: 0.040005
val_e/atom_mae: 0.000494
2025-02-05 01:58:41.096 INFO: val_e/atom_mae: 0.000494
val_e/atom_rmse: 0.000708
2025-02-05 01:58:41.097 INFO: val_e/atom_rmse: 0.000708
val_f_mae: 0.027924
2025-02-05 01:58:41.097 INFO: val_f_mae: 0.027924
val_f_rmse: 0.047600
2025-02-05 01:58:41.097 INFO: val_f_rmse: 0.047600
##### Step: 336 Learning rate: 1.52587890625e-07 #####
2025-02-05 02:00:32.661 INFO: ##### Step: 336 Learning rate: 1.52587890625e-07 #####
Epoch 97, Train Loss: 26.7324, Val Loss: 20.7301
2025-02-05 02:00:32.662 INFO: Epoch 97, Train Loss: 26.7324, Val Loss: 20.7301
train_e/atom_mae: 0.000447
2025-02-05 02:00:32.663 INFO: train_e/atom_mae: 0.000447
train_e/atom_rmse: 0.000826
2025-02-05 02:00:32.663 INFO: train_e/atom_rmse: 0.000826
train_f_mae: 0.026211
2025-02-05 02:00:32.666 INFO: train_f_mae: 0.026211
train_f_rmse: 0.039998
2025-02-05 02:00:32.666 INFO: train_f_rmse: 0.039998
val_e/atom_mae: 0.000493
2025-02-05 02:00:32.668 INFO: val_e/atom_mae: 0.000493
val_e/atom_rmse: 0.000707
2025-02-05 02:00:32.669 INFO: val_e/atom_rmse: 0.000707
val_f_mae: 0.027923
2025-02-05 02:00:32.669 INFO: val_f_mae: 0.027923
val_f_rmse: 0.047599
2025-02-05 02:00:32.669 INFO: val_f_rmse: 0.047599
##### Step: 337 Learning rate: 1.52587890625e-07 #####
2025-02-05 02:02:24.153 INFO: ##### Step: 337 Learning rate: 1.52587890625e-07 #####
Epoch 98, Train Loss: 26.7182, Val Loss: 20.7032
2025-02-05 02:02:24.154 INFO: Epoch 98, Train Loss: 26.7182, Val Loss: 20.7032
train_e/atom_mae: 0.000446
2025-02-05 02:02:24.155 INFO: train_e/atom_mae: 0.000446
train_e/atom_rmse: 0.000825
2025-02-05 02:02:24.155 INFO: train_e/atom_rmse: 0.000825
train_f_mae: 0.026204
2025-02-05 02:02:24.158 INFO: train_f_mae: 0.026204
train_f_rmse: 0.039990
2025-02-05 02:02:24.158 INFO: train_f_rmse: 0.039990
val_e/atom_mae: 0.000492
2025-02-05 02:02:24.160 INFO: val_e/atom_mae: 0.000492
val_e/atom_rmse: 0.000706
2025-02-05 02:02:24.160 INFO: val_e/atom_rmse: 0.000706
val_f_mae: 0.027921
2025-02-05 02:02:24.161 INFO: val_f_mae: 0.027921
val_f_rmse: 0.047598
2025-02-05 02:02:24.161 INFO: val_f_rmse: 0.047598
##### Step: 338 Learning rate: 1.52587890625e-07 #####
2025-02-05 02:04:15.780 INFO: ##### Step: 338 Learning rate: 1.52587890625e-07 #####
Epoch 99, Train Loss: 26.4894, Val Loss: 20.7108
2025-02-05 02:04:15.781 INFO: Epoch 99, Train Loss: 26.4894, Val Loss: 20.7108
train_e/atom_mae: 0.000444
2025-02-05 02:04:15.782 INFO: train_e/atom_mae: 0.000444
train_e/atom_rmse: 0.000822
2025-02-05 02:04:15.782 INFO: train_e/atom_rmse: 0.000822
train_f_mae: 0.026194
2025-02-05 02:04:15.784 INFO: train_f_mae: 0.026194
train_f_rmse: 0.039952
2025-02-05 02:04:15.785 INFO: train_f_rmse: 0.039952
val_e/atom_mae: 0.000492
2025-02-05 02:04:15.787 INFO: val_e/atom_mae: 0.000492
val_e/atom_rmse: 0.000706
2025-02-05 02:04:15.787 INFO: val_e/atom_rmse: 0.000706
val_f_mae: 0.027921
2025-02-05 02:04:15.788 INFO: val_f_mae: 0.027921
val_f_rmse: 0.047598
2025-02-05 02:04:15.788 INFO: val_f_rmse: 0.047598
##### Step: 339 Learning rate: 1.52587890625e-07 #####
2025-02-05 02:06:07.547 INFO: ##### Step: 339 Learning rate: 1.52587890625e-07 #####
Epoch 100, Train Loss: 26.7187, Val Loss: 20.7477
2025-02-05 02:06:07.548 INFO: Epoch 100, Train Loss: 26.7187, Val Loss: 20.7477
train_e/atom_mae: 0.000447
2025-02-05 02:06:07.549 INFO: train_e/atom_mae: 0.000447
train_e/atom_rmse: 0.000825
2025-02-05 02:06:07.549 INFO: train_e/atom_rmse: 0.000825
train_f_mae: 0.026215
2025-02-05 02:06:07.552 INFO: train_f_mae: 0.026215
train_f_rmse: 0.040003
2025-02-05 02:06:07.552 INFO: train_f_rmse: 0.040003
val_e/atom_mae: 0.000493
2025-02-05 02:06:07.554 INFO: val_e/atom_mae: 0.000493
val_e/atom_rmse: 0.000707
2025-02-05 02:06:07.554 INFO: val_e/atom_rmse: 0.000707
val_f_mae: 0.027921
2025-02-05 02:06:07.555 INFO: val_f_mae: 0.027921
val_f_rmse: 0.047598
2025-02-05 02:06:07.555 INFO: val_f_rmse: 0.047598
2025-02-05 02:06:07.997 INFO: Finished
2025-02-05 02:06:07.997 INFO: Number of trainable parameters: 69320
