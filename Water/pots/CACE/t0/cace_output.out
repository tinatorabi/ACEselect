2025-02-03 23:46:15.153 INFO: reading data
2025-02-03 23:46:18.380 INFO: Loaded 1354 training configurations from 'train_data.xyz'
2025-02-03 23:46:18.380 INFO: Using random 15.0% of training set for validation
2025-02-03 23:46:46.187 INFO: CUDA version: 12.1, CUDA device: 0
2025-02-03 23:46:46.187 INFO: device: cuda
2025-02-03 23:46:46.188 INFO: building CACE representation
2025-02-03 23:46:51.730 INFO: Representation: Cace(
  (node_onehot): NodeEncoder(num_classes=2)
  (node_embedding_sender): NodeEmbedding(num_classes=2, embedding_dim=3)
  (node_embedding_receiver): NodeEmbedding(num_classes=2, embedding_dim=3)
  (edge_coding): EdgeEncoder(directed=True)
  (radial_basis): BesselRBF(cutoff=5.5, n_rbf=6, trainable=True)
  (cutoff_fn): PolynomialCutoff(p=6.0, cutoff=5.5)
  (angular_basis): AngularComponent(l_max=3)
  (radial_transform): SharedRadialLinearTransform(
    (weights): ParameterList(
        (0): Parameter containing: [torch.float32 of size 6x12x9 (cuda:0)]
        (1): Parameter containing: [torch.float32 of size 6x12x9 (cuda:0)]
        (2): Parameter containing: [torch.float32 of size 6x12x9 (cuda:0)]
        (3): Parameter containing: [torch.float32 of size 6x12x9 (cuda:0)]
    )
  )
  (symmetrizer): Symmetrizer()
  (message_passing_list): ModuleList()
)
2025-02-03 23:46:51.730 INFO: building CACE NNP
2025-02-03 23:46:51.731 INFO: First train loop:
2025-02-03 23:46:51.731 INFO: creating training task
2025-02-03 23:47:55.588 INFO: training
##### Step: 0 Learning rate: 0.002 #####
2025-02-03 23:50:05.787 INFO: ##### Step: 0 Learning rate: 0.002 #####
Epoch 1, Train Loss: 797.9471, Val Loss: 216.0416
2025-02-03 23:50:05.787 INFO: Epoch 1, Train Loss: 797.9471, Val Loss: 216.0416
train_e/atom_mae: 0.080747
2025-02-03 23:50:05.963 INFO: train_e/atom_mae: 0.080747
train_e/atom_rmse: 0.110811
2025-02-03 23:50:06.481 INFO: train_e/atom_rmse: 0.110811
train_f_mae: 0.531602
2025-02-03 23:50:06.484 INFO: train_f_mae: 0.531602
train_f_rmse: 0.867572
2025-02-03 23:50:06.484 INFO: train_f_rmse: 0.867572
val_e/atom_mae: 0.057559
2025-02-03 23:50:06.486 INFO: val_e/atom_mae: 0.057559
val_e/atom_rmse: 0.079656
2025-02-03 23:50:06.487 INFO: val_e/atom_rmse: 0.079656
val_f_mae: 0.283593
2025-02-03 23:50:06.487 INFO: val_f_mae: 0.283593
val_f_rmse: 0.438830
2025-02-03 23:50:06.488 INFO: val_f_rmse: 0.438830
##### Step: 1 Learning rate: 0.004 #####
2025-02-03 23:51:08.068 INFO: ##### Step: 1 Learning rate: 0.004 #####
Epoch 2, Train Loss: 167.0265, Val Loss: 132.2409
2025-02-03 23:51:08.068 INFO: Epoch 2, Train Loss: 167.0265, Val Loss: 132.2409
train_e/atom_mae: 0.050066
2025-02-03 23:51:08.069 INFO: train_e/atom_mae: 0.050066
train_e/atom_rmse: 0.067810
2025-02-03 23:51:08.070 INFO: train_e/atom_rmse: 0.067810
train_f_mae: 0.239629
2025-02-03 23:51:08.072 INFO: train_f_mae: 0.239629
train_f_rmse: 0.387396
2025-02-03 23:51:08.073 INFO: train_f_rmse: 0.387396
val_e/atom_mae: 0.041307
2025-02-03 23:51:08.075 INFO: val_e/atom_mae: 0.041307
val_e/atom_rmse: 0.055872
2025-02-03 23:51:08.075 INFO: val_e/atom_rmse: 0.055872
val_f_mae: 0.211850
2025-02-03 23:51:08.076 INFO: val_f_mae: 0.211850
val_f_rmse: 0.347503
2025-02-03 23:51:08.076 INFO: val_f_rmse: 0.347503
##### Step: 2 Learning rate: 0.006 #####
2025-02-03 23:52:12.044 INFO: ##### Step: 2 Learning rate: 0.006 #####
Epoch 3, Train Loss: 108.2995, Val Loss: 83.4767
2025-02-03 23:52:12.044 INFO: Epoch 3, Train Loss: 108.2995, Val Loss: 83.4767
train_e/atom_mae: 0.031447
2025-02-03 23:52:12.045 INFO: train_e/atom_mae: 0.031447
train_e/atom_rmse: 0.042609
2025-02-03 23:52:12.045 INFO: train_e/atom_rmse: 0.042609
train_f_mae: 0.191043
2025-02-03 23:52:12.048 INFO: train_f_mae: 0.191043
train_f_rmse: 0.318758
2025-02-03 23:52:12.048 INFO: train_f_rmse: 0.318758
val_e/atom_mae: 0.018140
2025-02-03 23:52:12.051 INFO: val_e/atom_mae: 0.018140
val_e/atom_rmse: 0.026143
2025-02-03 23:52:12.051 INFO: val_e/atom_rmse: 0.026143
val_f_mae: 0.182235
2025-02-03 23:52:12.051 INFO: val_f_mae: 0.182235
val_f_rmse: 0.284490
2025-02-03 23:52:12.052 INFO: val_f_rmse: 0.284490
##### Step: 3 Learning rate: 0.008 #####
2025-02-03 23:53:29.128 INFO: ##### Step: 3 Learning rate: 0.008 #####
Epoch 4, Train Loss: 83.8637, Val Loss: 63.0086
2025-02-03 23:53:29.129 INFO: Epoch 4, Train Loss: 83.8637, Val Loss: 63.0086
train_e/atom_mae: 0.027997
2025-02-03 23:53:29.130 INFO: train_e/atom_mae: 0.027997
train_e/atom_rmse: 0.036249
2025-02-03 23:53:29.130 INFO: train_e/atom_rmse: 0.036249
train_f_mae: 0.175585
2025-02-03 23:53:29.133 INFO: train_f_mae: 0.175585
train_f_rmse: 0.281104
2025-02-03 23:53:29.133 INFO: train_f_rmse: 0.281104
val_e/atom_mae: 0.022150
2025-02-03 23:53:29.135 INFO: val_e/atom_mae: 0.022150
val_e/atom_rmse: 0.025390
2025-02-03 23:53:29.136 INFO: val_e/atom_rmse: 0.025390
val_f_mae: 0.156566
2025-02-03 23:53:29.136 INFO: val_f_mae: 0.156566
val_f_rmse: 0.246150
2025-02-03 23:53:29.136 INFO: val_f_rmse: 0.246150
##### Step: 4 Learning rate: 0.01 #####
2025-02-03 23:54:30.282 INFO: ##### Step: 4 Learning rate: 0.01 #####
Epoch 5, Train Loss: 66.6105, Val Loss: 55.1866
2025-02-03 23:54:30.282 INFO: Epoch 5, Train Loss: 66.6105, Val Loss: 55.1866
train_e/atom_mae: 0.027987
2025-02-03 23:54:30.283 INFO: train_e/atom_mae: 0.027987
train_e/atom_rmse: 0.035186
2025-02-03 23:54:30.283 INFO: train_e/atom_rmse: 0.035186
train_f_mae: 0.153623
2025-02-03 23:54:30.286 INFO: train_f_mae: 0.153623
train_f_rmse: 0.249091
2025-02-03 23:54:30.286 INFO: train_f_rmse: 0.249091
val_e/atom_mae: 0.013590
2025-02-03 23:54:30.288 INFO: val_e/atom_mae: 0.013590
val_e/atom_rmse: 0.020807
2025-02-03 23:54:30.289 INFO: val_e/atom_rmse: 0.020807
val_f_mae: 0.152242
2025-02-03 23:54:30.289 INFO: val_f_mae: 0.152242
val_f_rmse: 0.231433
2025-02-03 23:54:30.290 INFO: val_f_rmse: 0.231433
##### Step: 5 Learning rate: 0.01 #####
2025-02-03 23:55:31.321 INFO: ##### Step: 5 Learning rate: 0.01 #####
Epoch 6, Train Loss: 61.0159, Val Loss: 51.0563
2025-02-03 23:55:31.322 INFO: Epoch 6, Train Loss: 61.0159, Val Loss: 51.0563
train_e/atom_mae: 0.023035
2025-02-03 23:55:31.322 INFO: train_e/atom_mae: 0.023035
train_e/atom_rmse: 0.030474
2025-02-03 23:55:31.323 INFO: train_e/atom_rmse: 0.030474
train_f_mae: 0.152488
2025-02-03 23:55:31.325 INFO: train_f_mae: 0.152488
train_f_rmse: 0.239984
2025-02-03 23:55:31.326 INFO: train_f_rmse: 0.239984
val_e/atom_mae: 0.018035
2025-02-03 23:55:31.328 INFO: val_e/atom_mae: 0.018035
val_e/atom_rmse: 0.020365
2025-02-03 23:55:31.328 INFO: val_e/atom_rmse: 0.020365
val_f_mae: 0.148000
2025-02-03 23:55:31.329 INFO: val_f_mae: 0.148000
val_f_rmse: 0.222481
2025-02-03 23:55:31.329 INFO: val_f_rmse: 0.222481
##### Step: 6 Learning rate: 0.01 #####
2025-02-03 23:56:32.317 INFO: ##### Step: 6 Learning rate: 0.01 #####
Epoch 7, Train Loss: 53.7444, Val Loss: 45.2613
2025-02-03 23:56:32.317 INFO: Epoch 7, Train Loss: 53.7444, Val Loss: 45.2613
train_e/atom_mae: 0.026279
2025-02-03 23:56:32.318 INFO: train_e/atom_mae: 0.026279
train_e/atom_rmse: 0.033810
2025-02-03 23:56:32.319 INFO: train_e/atom_rmse: 0.033810
train_f_mae: 0.138375
2025-02-03 23:56:32.321 INFO: train_f_mae: 0.138375
train_f_rmse: 0.222554
2025-02-03 23:56:32.321 INFO: train_f_rmse: 0.222554
val_e/atom_mae: 0.023665
2025-02-03 23:56:32.324 INFO: val_e/atom_mae: 0.023665
val_e/atom_rmse: 0.028827
2025-02-03 23:56:32.324 INFO: val_e/atom_rmse: 0.028827
val_f_mae: 0.135328
2025-02-03 23:56:32.325 INFO: val_f_mae: 0.135328
val_f_rmse: 0.205361
2025-02-03 23:56:32.325 INFO: val_f_rmse: 0.205361
##### Step: 7 Learning rate: 0.01 #####
2025-02-03 23:57:33.447 INFO: ##### Step: 7 Learning rate: 0.01 #####
Epoch 8, Train Loss: 51.1885, Val Loss: 43.8563
2025-02-03 23:57:33.447 INFO: Epoch 8, Train Loss: 51.1885, Val Loss: 43.8563
train_e/atom_mae: 0.023537
2025-02-03 23:57:33.448 INFO: train_e/atom_mae: 0.023537
train_e/atom_rmse: 0.029527
2025-02-03 23:57:33.448 INFO: train_e/atom_rmse: 0.029527
train_f_mae: 0.137911
2025-02-03 23:57:33.451 INFO: train_f_mae: 0.137911
train_f_rmse: 0.219031
2025-02-03 23:57:33.451 INFO: train_f_rmse: 0.219031
val_e/atom_mae: 0.010822
2025-02-03 23:57:33.454 INFO: val_e/atom_mae: 0.010822
val_e/atom_rmse: 0.015766
2025-02-03 23:57:33.454 INFO: val_e/atom_rmse: 0.015766
val_f_mae: 0.139841
2025-02-03 23:57:33.454 INFO: val_f_mae: 0.139841
val_f_rmse: 0.207160
2025-02-03 23:57:33.455 INFO: val_f_rmse: 0.207160
##### Step: 8 Learning rate: 0.01 #####
2025-02-03 23:58:34.476 INFO: ##### Step: 8 Learning rate: 0.01 #####
Epoch 9, Train Loss: 46.6720, Val Loss: 38.4590
2025-02-03 23:58:34.476 INFO: Epoch 9, Train Loss: 46.6720, Val Loss: 38.4590
train_e/atom_mae: 0.019313
2025-02-03 23:58:34.477 INFO: train_e/atom_mae: 0.019313
train_e/atom_rmse: 0.025079
2025-02-03 23:58:34.478 INFO: train_e/atom_rmse: 0.025079
train_f_mae: 0.133167
2025-02-03 23:58:34.480 INFO: train_f_mae: 0.133167
train_f_rmse: 0.210602
2025-02-03 23:58:34.481 INFO: train_f_rmse: 0.210602
val_e/atom_mae: 0.012669
2025-02-03 23:58:34.483 INFO: val_e/atom_mae: 0.012669
val_e/atom_rmse: 0.019684
2025-02-03 23:58:34.483 INFO: val_e/atom_rmse: 0.019684
val_f_mae: 0.127899
2025-02-03 23:58:34.484 INFO: val_f_mae: 0.127899
val_f_rmse: 0.192331
2025-02-03 23:58:34.484 INFO: val_f_rmse: 0.192331
##### Step: 9 Learning rate: 0.01 #####
2025-02-03 23:59:35.525 INFO: ##### Step: 9 Learning rate: 0.01 #####
Epoch 10, Train Loss: 48.1676, Val Loss: 37.3207
2025-02-03 23:59:35.525 INFO: Epoch 10, Train Loss: 48.1676, Val Loss: 37.3207
train_e/atom_mae: 0.026147
2025-02-03 23:59:35.526 INFO: train_e/atom_mae: 0.026147
train_e/atom_rmse: 0.032501
2025-02-03 23:59:35.526 INFO: train_e/atom_rmse: 0.032501
train_f_mae: 0.135059
2025-02-03 23:59:35.529 INFO: train_f_mae: 0.135059
train_f_rmse: 0.210413
2025-02-03 23:59:35.529 INFO: train_f_rmse: 0.210413
val_e/atom_mae: 0.020750
2025-02-03 23:59:35.531 INFO: val_e/atom_mae: 0.020750
val_e/atom_rmse: 0.025969
2025-02-03 23:59:35.532 INFO: val_e/atom_rmse: 0.025969
val_f_mae: 0.121465
2025-02-03 23:59:35.532 INFO: val_f_mae: 0.121465
val_f_rmse: 0.186553
2025-02-03 23:59:35.532 INFO: val_f_rmse: 0.186553
##### Step: 10 Learning rate: 0.01 #####
2025-02-04 00:00:39.775 INFO: ##### Step: 10 Learning rate: 0.01 #####
Epoch 11, Train Loss: 43.2034, Val Loss: 6764.6010
2025-02-04 00:00:39.776 INFO: Epoch 11, Train Loss: 43.2034, Val Loss: 6764.6010
train_e/atom_mae: 0.021741
2025-02-04 00:00:39.777 INFO: train_e/atom_mae: 0.021741
train_e/atom_rmse: 0.027258
2025-02-04 00:00:39.777 INFO: train_e/atom_rmse: 0.027258
train_f_mae: 0.128001
2025-02-04 00:00:39.780 INFO: train_f_mae: 0.128001
train_f_rmse: 0.201158
2025-02-04 00:00:39.780 INFO: train_f_rmse: 0.201158
val_e/atom_mae: 0.355810
2025-02-04 00:00:39.782 INFO: val_e/atom_mae: 0.355810
val_e/atom_rmse: 0.361874
2025-02-04 00:00:39.783 INFO: val_e/atom_rmse: 0.361874
val_f_mae: 1.624263
2025-02-04 00:00:39.783 INFO: val_f_mae: 1.624263
val_f_rmse: 2.504677
2025-02-04 00:00:39.783 INFO: val_f_rmse: 2.504677
##### Step: 11 Learning rate: 0.01 #####
2025-02-04 00:02:06.466 INFO: ##### Step: 11 Learning rate: 0.01 #####
Epoch 12, Train Loss: 41.7541, Val Loss: 31.1998
2025-02-04 00:02:06.467 INFO: Epoch 12, Train Loss: 41.7541, Val Loss: 31.1998
train_e/atom_mae: 0.021286
2025-02-04 00:02:06.499 INFO: train_e/atom_mae: 0.021286
train_e/atom_rmse: 0.027194
2025-02-04 00:02:06.610 INFO: train_e/atom_rmse: 0.027194
train_f_mae: 0.125842
2025-02-04 00:02:06.613 INFO: train_f_mae: 0.125842
train_f_rmse: 0.197555
2025-02-04 00:02:06.613 INFO: train_f_rmse: 0.197555
val_e/atom_mae: 0.011378
2025-02-04 00:02:06.616 INFO: val_e/atom_mae: 0.011378
val_e/atom_rmse: 0.016069
2025-02-04 00:02:06.616 INFO: val_e/atom_rmse: 0.016069
val_f_mae: 0.112401
2025-02-04 00:02:06.617 INFO: val_f_mae: 0.112401
val_f_rmse: 0.173834
2025-02-04 00:02:06.617 INFO: val_f_rmse: 0.173834
##### Step: 12 Learning rate: 0.01 #####
2025-02-04 00:03:08.216 INFO: ##### Step: 12 Learning rate: 0.01 #####
Epoch 13, Train Loss: 40.5497, Val Loss: 29.8538
2025-02-04 00:03:08.217 INFO: Epoch 13, Train Loss: 40.5497, Val Loss: 29.8538
train_e/atom_mae: 0.021148
2025-02-04 00:03:08.218 INFO: train_e/atom_mae: 0.021148
train_e/atom_rmse: 0.026971
2025-02-04 00:03:08.218 INFO: train_e/atom_rmse: 0.026971
train_f_mae: 0.124224
2025-02-04 00:03:08.221 INFO: train_f_mae: 0.124224
train_f_rmse: 0.194597
2025-02-04 00:03:08.221 INFO: train_f_rmse: 0.194597
val_e/atom_mae: 0.010861
2025-02-04 00:03:08.223 INFO: val_e/atom_mae: 0.010861
val_e/atom_rmse: 0.015531
2025-02-04 00:03:08.224 INFO: val_e/atom_rmse: 0.015531
val_f_mae: 0.109256
2025-02-04 00:03:08.224 INFO: val_f_mae: 0.109256
val_f_rmse: 0.170108
2025-02-04 00:03:08.224 INFO: val_f_rmse: 0.170108
##### Step: 13 Learning rate: 0.01 #####
2025-02-04 00:04:13.700 INFO: ##### Step: 13 Learning rate: 0.01 #####
Epoch 14, Train Loss: 40.5036, Val Loss: 29.1286
2025-02-04 00:04:13.701 INFO: Epoch 14, Train Loss: 40.5036, Val Loss: 29.1286
train_e/atom_mae: 0.017599
2025-02-04 00:04:13.732 INFO: train_e/atom_mae: 0.017599
train_e/atom_rmse: 0.023140
2025-02-04 00:04:13.733 INFO: train_e/atom_rmse: 0.023140
train_f_mae: 0.126689
2025-02-04 00:04:13.735 INFO: train_f_mae: 0.126689
train_f_rmse: 0.196290
2025-02-04 00:04:13.736 INFO: train_f_rmse: 0.196290
val_e/atom_mae: 0.011194
2025-02-04 00:04:13.738 INFO: val_e/atom_mae: 0.011194
val_e/atom_rmse: 0.015260
2025-02-04 00:04:13.738 INFO: val_e/atom_rmse: 0.015260
val_f_mae: 0.108257
2025-02-04 00:04:13.739 INFO: val_f_mae: 0.108257
val_f_rmse: 0.168058
2025-02-04 00:04:13.739 INFO: val_f_rmse: 0.168058
##### Step: 14 Learning rate: 0.01 #####
2025-02-04 00:05:15.114 INFO: ##### Step: 14 Learning rate: 0.01 #####
Epoch 15, Train Loss: 42.5240, Val Loss: 28.5200
2025-02-04 00:05:15.115 INFO: Epoch 15, Train Loss: 42.5240, Val Loss: 28.5200
train_e/atom_mae: 0.029382
2025-02-04 00:05:15.116 INFO: train_e/atom_mae: 0.029382
train_e/atom_rmse: 0.037473
2025-02-04 00:05:15.127 INFO: train_e/atom_rmse: 0.037473
train_f_mae: 0.122696
2025-02-04 00:05:15.129 INFO: train_f_mae: 0.122696
train_f_rmse: 0.193255
2025-02-04 00:05:15.130 INFO: train_f_rmse: 0.193255
val_e/atom_mae: 0.011549
2025-02-04 00:05:15.132 INFO: val_e/atom_mae: 0.011549
val_e/atom_rmse: 0.015100
2025-02-04 00:05:15.132 INFO: val_e/atom_rmse: 0.015100
val_f_mae: 0.106794
2025-02-04 00:05:15.132 INFO: val_f_mae: 0.106794
val_f_rmse: 0.166300
2025-02-04 00:05:15.133 INFO: val_f_rmse: 0.166300
##### Step: 15 Learning rate: 0.01 #####
2025-02-04 00:06:15.403 INFO: ##### Step: 15 Learning rate: 0.01 #####
Epoch 16, Train Loss: 39.3859, Val Loss: 28.0711
2025-02-04 00:06:15.404 INFO: Epoch 16, Train Loss: 39.3859, Val Loss: 28.0711
train_e/atom_mae: 0.028520
2025-02-04 00:06:15.405 INFO: train_e/atom_mae: 0.028520
train_e/atom_rmse: 0.035700
2025-02-04 00:06:15.405 INFO: train_e/atom_rmse: 0.035700
train_f_mae: 0.117970
2025-02-04 00:06:15.408 INFO: train_f_mae: 0.117970
train_f_rmse: 0.186246
2025-02-04 00:06:15.408 INFO: train_f_rmse: 0.186246
val_e/atom_mae: 0.013308
2025-02-04 00:06:15.410 INFO: val_e/atom_mae: 0.013308
val_e/atom_rmse: 0.015501
2025-02-04 00:06:15.410 INFO: val_e/atom_rmse: 0.015501
val_f_mae: 0.106022
2025-02-04 00:06:15.411 INFO: val_f_mae: 0.106022
val_f_rmse: 0.164807
2025-02-04 00:06:15.411 INFO: val_f_rmse: 0.164807
##### Step: 16 Learning rate: 0.01 #####
2025-02-04 00:07:15.320 INFO: ##### Step: 16 Learning rate: 0.01 #####
Epoch 17, Train Loss: 39.8248, Val Loss: 27.3653
2025-02-04 00:07:15.321 INFO: Epoch 17, Train Loss: 39.8248, Val Loss: 27.3653
train_e/atom_mae: 0.019239
2025-02-04 00:07:15.321 INFO: train_e/atom_mae: 0.019239
train_e/atom_rmse: 0.024401
2025-02-04 00:07:15.322 INFO: train_e/atom_rmse: 0.024401
train_f_mae: 0.126173
2025-02-04 00:07:15.324 INFO: train_f_mae: 0.126173
train_f_rmse: 0.193984
2025-02-04 00:07:15.324 INFO: train_f_rmse: 0.193984
val_e/atom_mae: 0.010975
2025-02-04 00:07:15.327 INFO: val_e/atom_mae: 0.010975
val_e/atom_rmse: 0.013958
2025-02-04 00:07:15.327 INFO: val_e/atom_rmse: 0.013958
val_f_mae: 0.105012
2025-02-04 00:07:15.327 INFO: val_f_mae: 0.105012
val_f_rmse: 0.163171
2025-02-04 00:07:15.328 INFO: val_f_rmse: 0.163171
##### Step: 17 Learning rate: 0.01 #####
2025-02-04 00:08:15.270 INFO: ##### Step: 17 Learning rate: 0.01 #####
Epoch 18, Train Loss: 35.2458, Val Loss: 26.1937
2025-02-04 00:08:15.271 INFO: Epoch 18, Train Loss: 35.2458, Val Loss: 26.1937
train_e/atom_mae: 0.017996
2025-02-04 00:08:15.272 INFO: train_e/atom_mae: 0.017996
train_e/atom_rmse: 0.023168
2025-02-04 00:08:15.272 INFO: train_e/atom_rmse: 0.023168
train_f_mae: 0.115832
2025-02-04 00:08:15.274 INFO: train_f_mae: 0.115832
train_f_rmse: 0.182393
2025-02-04 00:08:15.275 INFO: train_f_rmse: 0.182393
val_e/atom_mae: 0.011206
2025-02-04 00:08:15.277 INFO: val_e/atom_mae: 0.011206
val_e/atom_rmse: 0.013436
2025-02-04 00:08:15.277 INFO: val_e/atom_rmse: 0.013436
val_f_mae: 0.104000
2025-02-04 00:08:15.278 INFO: val_f_mae: 0.104000
val_f_rmse: 0.159710
2025-02-04 00:08:15.278 INFO: val_f_rmse: 0.159710
##### Step: 18 Learning rate: 0.01 #####
2025-02-04 00:09:15.059 INFO: ##### Step: 18 Learning rate: 0.01 #####
Epoch 19, Train Loss: 40.0959, Val Loss: 25.8577
2025-02-04 00:09:15.059 INFO: Epoch 19, Train Loss: 40.0959, Val Loss: 25.8577
train_e/atom_mae: 0.017693
2025-02-04 00:09:15.060 INFO: train_e/atom_mae: 0.017693
train_e/atom_rmse: 0.023106
2025-02-04 00:09:15.060 INFO: train_e/atom_rmse: 0.023106
train_f_mae: 0.126817
2025-02-04 00:09:15.063 INFO: train_f_mae: 0.126817
train_f_rmse: 0.195263
2025-02-04 00:09:15.063 INFO: train_f_rmse: 0.195263
val_e/atom_mae: 0.010163
2025-02-04 00:09:15.065 INFO: val_e/atom_mae: 0.010163
val_e/atom_rmse: 0.013570
2025-02-04 00:09:15.065 INFO: val_e/atom_rmse: 0.013570
val_f_mae: 0.102076
2025-02-04 00:09:15.066 INFO: val_f_mae: 0.102076
val_f_rmse: 0.158620
2025-02-04 00:09:15.066 INFO: val_f_rmse: 0.158620
##### Step: 19 Learning rate: 0.01 #####
2025-02-04 00:10:14.776 INFO: ##### Step: 19 Learning rate: 0.01 #####
Epoch 20, Train Loss: 36.7157, Val Loss: 25.0481
2025-02-04 00:10:14.776 INFO: Epoch 20, Train Loss: 36.7157, Val Loss: 25.0481
train_e/atom_mae: 0.019929
2025-02-04 00:10:14.777 INFO: train_e/atom_mae: 0.019929
train_e/atom_rmse: 0.025571
2025-02-04 00:10:14.777 INFO: train_e/atom_rmse: 0.025571
train_f_mae: 0.120077
2025-02-04 00:10:14.780 INFO: train_f_mae: 0.120077
train_f_rmse: 0.185217
2025-02-04 00:10:14.780 INFO: train_f_rmse: 0.185217
val_e/atom_mae: 0.010929
2025-02-04 00:10:14.782 INFO: val_e/atom_mae: 0.010929
val_e/atom_rmse: 0.014682
2025-02-04 00:10:14.782 INFO: val_e/atom_rmse: 0.014682
val_f_mae: 0.101104
2025-02-04 00:10:14.783 INFO: val_f_mae: 0.101104
val_f_rmse: 0.155679
2025-02-04 00:10:14.783 INFO: val_f_rmse: 0.155679
##### Step: 20 Learning rate: 0.005 #####
2025-02-04 00:11:34.301 INFO: ##### Step: 20 Learning rate: 0.005 #####
Epoch 21, Train Loss: 32.6132, Val Loss: 24.1659
2025-02-04 00:11:34.301 INFO: Epoch 21, Train Loss: 32.6132, Val Loss: 24.1659
train_e/atom_mae: 0.018253
2025-02-04 00:11:34.437 INFO: train_e/atom_mae: 0.018253
train_e/atom_rmse: 0.023566
2025-02-04 00:11:34.636 INFO: train_e/atom_rmse: 0.023566
train_f_mae: 0.111117
2025-02-04 00:11:34.639 INFO: train_f_mae: 0.111117
train_f_rmse: 0.174831
2025-02-04 00:11:34.639 INFO: train_f_rmse: 0.174831
val_e/atom_mae: 0.010670
2025-02-04 00:11:34.642 INFO: val_e/atom_mae: 0.010670
val_e/atom_rmse: 0.013723
2025-02-04 00:11:34.642 INFO: val_e/atom_rmse: 0.013723
val_f_mae: 0.098676
2025-02-04 00:11:34.642 INFO: val_f_mae: 0.098676
val_f_rmse: 0.153149
2025-02-04 00:11:34.643 INFO: val_f_rmse: 0.153149
##### Step: 21 Learning rate: 0.005 #####
2025-02-04 00:12:34.744 INFO: ##### Step: 21 Learning rate: 0.005 #####
Epoch 22, Train Loss: 30.8227, Val Loss: 23.5384
2025-02-04 00:12:34.744 INFO: Epoch 22, Train Loss: 30.8227, Val Loss: 23.5384
train_e/atom_mae: 0.018527
2025-02-04 00:12:34.745 INFO: train_e/atom_mae: 0.018527
train_e/atom_rmse: 0.023495
2025-02-04 00:12:34.745 INFO: train_e/atom_rmse: 0.023495
train_f_mae: 0.106827
2025-02-04 00:12:34.748 INFO: train_f_mae: 0.106827
train_f_rmse: 0.169670
2025-02-04 00:12:34.748 INFO: train_f_rmse: 0.169670
val_e/atom_mae: 0.009742
2025-02-04 00:12:34.750 INFO: val_e/atom_mae: 0.009742
val_e/atom_rmse: 0.012496
2025-02-04 00:12:34.751 INFO: val_e/atom_rmse: 0.012496
val_f_mae: 0.098237
2025-02-04 00:12:34.751 INFO: val_f_mae: 0.098237
val_f_rmse: 0.151488
2025-02-04 00:12:34.751 INFO: val_f_rmse: 0.151488
##### Step: 22 Learning rate: 0.005 #####
2025-02-04 00:13:34.559 INFO: ##### Step: 22 Learning rate: 0.005 #####
Epoch 23, Train Loss: 30.7837, Val Loss: 23.1744
2025-02-04 00:13:34.560 INFO: Epoch 23, Train Loss: 30.7837, Val Loss: 23.1744
train_e/atom_mae: 0.014109
2025-02-04 00:13:34.560 INFO: train_e/atom_mae: 0.014109
train_e/atom_rmse: 0.018608
2025-02-04 00:13:34.561 INFO: train_e/atom_rmse: 0.018608
train_f_mae: 0.110121
2025-02-04 00:13:34.563 INFO: train_f_mae: 0.110121
train_f_rmse: 0.171777
2025-02-04 00:13:34.564 INFO: train_f_rmse: 0.171777
val_e/atom_mae: 0.008932
2025-02-04 00:13:34.566 INFO: val_e/atom_mae: 0.008932
val_e/atom_rmse: 0.012690
2025-02-04 00:13:34.566 INFO: val_e/atom_rmse: 0.012690
val_f_mae: 0.096885
2025-02-04 00:13:34.566 INFO: val_f_mae: 0.096885
val_f_rmse: 0.150227
2025-02-04 00:13:34.567 INFO: val_f_rmse: 0.150227
##### Step: 23 Learning rate: 0.005 #####
2025-02-04 00:14:34.298 INFO: ##### Step: 23 Learning rate: 0.005 #####
Epoch 24, Train Loss: 28.7330, Val Loss: 22.6510
2025-02-04 00:14:34.299 INFO: Epoch 24, Train Loss: 28.7330, Val Loss: 22.6510
train_e/atom_mae: 0.014821
2025-02-04 00:14:34.300 INFO: train_e/atom_mae: 0.014821
train_e/atom_rmse: 0.019551
2025-02-04 00:14:34.300 INFO: train_e/atom_rmse: 0.019551
train_f_mae: 0.104017
2025-02-04 00:14:34.303 INFO: train_f_mae: 0.104017
train_f_rmse: 0.165300
2025-02-04 00:14:34.303 INFO: train_f_rmse: 0.165300
val_e/atom_mae: 0.009210
2025-02-04 00:14:34.305 INFO: val_e/atom_mae: 0.009210
val_e/atom_rmse: 0.011964
2025-02-04 00:14:34.306 INFO: val_e/atom_rmse: 0.011964
val_f_mae: 0.096568
2025-02-04 00:14:34.306 INFO: val_f_mae: 0.096568
val_f_rmse: 0.148700
2025-02-04 00:14:34.306 INFO: val_f_rmse: 0.148700
##### Step: 24 Learning rate: 0.005 #####
2025-02-04 00:15:34.119 INFO: ##### Step: 24 Learning rate: 0.005 #####
Epoch 25, Train Loss: 29.1786, Val Loss: 22.5061
2025-02-04 00:15:34.119 INFO: Epoch 25, Train Loss: 29.1786, Val Loss: 22.5061
train_e/atom_mae: 0.014809
2025-02-04 00:15:34.120 INFO: train_e/atom_mae: 0.014809
train_e/atom_rmse: 0.019190
2025-02-04 00:15:34.120 INFO: train_e/atom_rmse: 0.019190
train_f_mae: 0.106137
2025-02-04 00:15:34.123 INFO: train_f_mae: 0.106137
train_f_rmse: 0.166797
2025-02-04 00:15:34.123 INFO: train_f_rmse: 0.166797
val_e/atom_mae: 0.008469
2025-02-04 00:15:34.125 INFO: val_e/atom_mae: 0.008469
val_e/atom_rmse: 0.011790
2025-02-04 00:15:34.125 INFO: val_e/atom_rmse: 0.011790
val_f_mae: 0.096303
2025-02-04 00:15:34.126 INFO: val_f_mae: 0.096303
val_f_rmse: 0.148265
2025-02-04 00:15:34.126 INFO: val_f_rmse: 0.148265
##### Step: 25 Learning rate: 0.005 #####
2025-02-04 00:16:33.987 INFO: ##### Step: 25 Learning rate: 0.005 #####
Epoch 26, Train Loss: 28.5021, Val Loss: 21.9711
2025-02-04 00:16:33.988 INFO: Epoch 26, Train Loss: 28.5021, Val Loss: 21.9711
train_e/atom_mae: 0.014192
2025-02-04 00:16:33.989 INFO: train_e/atom_mae: 0.014192
train_e/atom_rmse: 0.018628
2025-02-04 00:16:33.989 INFO: train_e/atom_rmse: 0.018628
train_f_mae: 0.104621
2025-02-04 00:16:33.991 INFO: train_f_mae: 0.104621
train_f_rmse: 0.164993
2025-02-04 00:16:33.992 INFO: train_f_rmse: 0.164993
val_e/atom_mae: 0.008917
2025-02-04 00:16:33.994 INFO: val_e/atom_mae: 0.008917
val_e/atom_rmse: 0.011566
2025-02-04 00:16:33.994 INFO: val_e/atom_rmse: 0.011566
val_f_mae: 0.095093
2025-02-04 00:16:33.995 INFO: val_f_mae: 0.095093
val_f_rmse: 0.146521
2025-02-04 00:16:33.995 INFO: val_f_rmse: 0.146521
##### Step: 26 Learning rate: 0.005 #####
2025-02-04 00:17:33.730 INFO: ##### Step: 26 Learning rate: 0.005 #####
Epoch 27, Train Loss: 27.8441, Val Loss: 21.8116
2025-02-04 00:17:33.731 INFO: Epoch 27, Train Loss: 27.8441, Val Loss: 21.8116
train_e/atom_mae: 0.017310
2025-02-04 00:17:33.731 INFO: train_e/atom_mae: 0.017310
train_e/atom_rmse: 0.022185
2025-02-04 00:17:33.732 INFO: train_e/atom_rmse: 0.022185
train_f_mae: 0.101312
2025-02-04 00:17:33.734 INFO: train_f_mae: 0.101312
train_f_rmse: 0.161337
2025-02-04 00:17:33.734 INFO: train_f_rmse: 0.161337
val_e/atom_mae: 0.009158
2025-02-04 00:17:33.736 INFO: val_e/atom_mae: 0.009158
val_e/atom_rmse: 0.011440
2025-02-04 00:17:33.737 INFO: val_e/atom_rmse: 0.011440
val_f_mae: 0.094520
2025-02-04 00:17:33.737 INFO: val_f_mae: 0.094520
val_f_rmse: 0.146020
2025-02-04 00:17:33.738 INFO: val_f_rmse: 0.146020
##### Step: 27 Learning rate: 0.005 #####
2025-02-04 00:18:33.438 INFO: ##### Step: 27 Learning rate: 0.005 #####
Epoch 28, Train Loss: 27.2989, Val Loss: 21.3738
2025-02-04 00:18:33.439 INFO: Epoch 28, Train Loss: 27.2989, Val Loss: 21.3738
train_e/atom_mae: 0.017341
2025-02-04 00:18:33.440 INFO: train_e/atom_mae: 0.017341
train_e/atom_rmse: 0.021704
2025-02-04 00:18:33.440 INFO: train_e/atom_rmse: 0.021704
train_f_mae: 0.100206
2025-02-04 00:18:33.443 INFO: train_f_mae: 0.100206
train_f_rmse: 0.159882
2025-02-04 00:18:33.443 INFO: train_f_rmse: 0.159882
val_e/atom_mae: 0.009013
2025-02-04 00:18:33.445 INFO: val_e/atom_mae: 0.009013
val_e/atom_rmse: 0.011554
2025-02-04 00:18:33.445 INFO: val_e/atom_rmse: 0.011554
val_f_mae: 0.093665
2025-02-04 00:18:33.446 INFO: val_f_mae: 0.093665
val_f_rmse: 0.144482
2025-02-04 00:18:33.446 INFO: val_f_rmse: 0.144482
##### Step: 28 Learning rate: 0.005 #####
2025-02-04 00:19:33.323 INFO: ##### Step: 28 Learning rate: 0.005 #####
Epoch 29, Train Loss: 25.4685, Val Loss: 20.8862
2025-02-04 00:19:33.323 INFO: Epoch 29, Train Loss: 25.4685, Val Loss: 20.8862
train_e/atom_mae: 0.012822
2025-02-04 00:19:33.324 INFO: train_e/atom_mae: 0.012822
train_e/atom_rmse: 0.016861
2025-02-04 00:19:33.324 INFO: train_e/atom_rmse: 0.016861
train_f_mae: 0.097028
2025-02-04 00:19:33.327 INFO: train_f_mae: 0.097028
train_f_rmse: 0.156270
2025-02-04 00:19:33.327 INFO: train_f_rmse: 0.156270
val_e/atom_mae: 0.008727
2025-02-04 00:19:33.329 INFO: val_e/atom_mae: 0.008727
val_e/atom_rmse: 0.011073
2025-02-04 00:19:33.330 INFO: val_e/atom_rmse: 0.011073
val_f_mae: 0.093040
2025-02-04 00:19:33.330 INFO: val_f_mae: 0.093040
val_f_rmse: 0.142921
2025-02-04 00:19:33.330 INFO: val_f_rmse: 0.142921
##### Step: 29 Learning rate: 0.005 #####
2025-02-04 00:20:33.075 INFO: ##### Step: 29 Learning rate: 0.005 #####
Epoch 30, Train Loss: 26.6160, Val Loss: 20.7389
2025-02-04 00:20:33.077 INFO: Epoch 30, Train Loss: 26.6160, Val Loss: 20.7389
train_e/atom_mae: 0.014627
2025-02-04 00:20:33.078 INFO: train_e/atom_mae: 0.014627
train_e/atom_rmse: 0.018536
2025-02-04 00:20:33.078 INFO: train_e/atom_rmse: 0.018536
train_f_mae: 0.100480
2025-02-04 00:20:33.081 INFO: train_f_mae: 0.100480
train_f_rmse: 0.159215
2025-02-04 00:20:33.081 INFO: train_f_rmse: 0.159215
val_e/atom_mae: 0.008255
2025-02-04 00:20:33.083 INFO: val_e/atom_mae: 0.008255
val_e/atom_rmse: 0.010933
2025-02-04 00:20:33.083 INFO: val_e/atom_rmse: 0.010933
val_f_mae: 0.092208
2025-02-04 00:20:33.084 INFO: val_f_mae: 0.092208
val_f_rmse: 0.142460
2025-02-04 00:20:33.084 INFO: val_f_rmse: 0.142460
##### Step: 30 Learning rate: 0.005 #####
2025-02-04 00:21:32.866 INFO: ##### Step: 30 Learning rate: 0.005 #####
Epoch 31, Train Loss: 27.0917, Val Loss: 20.7083
2025-02-04 00:21:32.867 INFO: Epoch 31, Train Loss: 27.0917, Val Loss: 20.7083
train_e/atom_mae: 0.016472
2025-02-04 00:21:32.868 INFO: train_e/atom_mae: 0.016472
train_e/atom_rmse: 0.020520
2025-02-04 00:21:32.868 INFO: train_e/atom_rmse: 0.020520
train_f_mae: 0.101915
2025-02-04 00:21:32.871 INFO: train_f_mae: 0.101915
train_f_rmse: 0.159811
2025-02-04 00:21:32.871 INFO: train_f_rmse: 0.159811
val_e/atom_mae: 0.007819
2025-02-04 00:21:32.873 INFO: val_e/atom_mae: 0.007819
val_e/atom_rmse: 0.010984
2025-02-04 00:21:32.873 INFO: val_e/atom_rmse: 0.010984
val_f_mae: 0.091705
2025-02-04 00:21:32.874 INFO: val_f_mae: 0.091705
val_f_rmse: 0.142334
2025-02-04 00:21:32.874 INFO: val_f_rmse: 0.142334
##### Step: 31 Learning rate: 0.005 #####
2025-02-04 00:22:32.748 INFO: ##### Step: 31 Learning rate: 0.005 #####
Epoch 32, Train Loss: 26.9930, Val Loss: 20.0896
2025-02-04 00:22:32.749 INFO: Epoch 32, Train Loss: 26.9930, Val Loss: 20.0896
train_e/atom_mae: 0.021671
2025-02-04 00:22:32.750 INFO: train_e/atom_mae: 0.021671
train_e/atom_rmse: 0.026028
2025-02-04 00:22:32.750 INFO: train_e/atom_rmse: 0.026028
train_f_mae: 0.098793
2025-02-04 00:22:32.753 INFO: train_f_mae: 0.098793
train_f_rmse: 0.156511
2025-02-04 00:22:32.753 INFO: train_f_rmse: 0.156511
val_e/atom_mae: 0.007668
2025-02-04 00:22:32.755 INFO: val_e/atom_mae: 0.007668
val_e/atom_rmse: 0.011084
2025-02-04 00:22:32.755 INFO: val_e/atom_rmse: 0.011084
val_f_mae: 0.090848
2025-02-04 00:22:32.756 INFO: val_f_mae: 0.090848
val_f_rmse: 0.140111
2025-02-04 00:22:32.756 INFO: val_f_rmse: 0.140111
##### Step: 32 Learning rate: 0.005 #####
2025-02-04 00:23:32.700 INFO: ##### Step: 32 Learning rate: 0.005 #####
Epoch 33, Train Loss: 27.2376, Val Loss: 20.1136
2025-02-04 00:23:32.700 INFO: Epoch 33, Train Loss: 27.2376, Val Loss: 20.1136
train_e/atom_mae: 0.022618
2025-02-04 00:23:32.701 INFO: train_e/atom_mae: 0.022618
train_e/atom_rmse: 0.028511
2025-02-04 00:23:32.701 INFO: train_e/atom_rmse: 0.028511
train_f_mae: 0.097794
2025-02-04 00:23:32.704 INFO: train_f_mae: 0.097794
train_f_rmse: 0.155695
2025-02-04 00:23:32.704 INFO: train_f_rmse: 0.155695
val_e/atom_mae: 0.007460
2025-02-04 00:23:32.706 INFO: val_e/atom_mae: 0.007460
val_e/atom_rmse: 0.010518
2025-02-04 00:23:32.707 INFO: val_e/atom_rmse: 0.010518
val_f_mae: 0.091366
2025-02-04 00:23:32.707 INFO: val_f_mae: 0.091366
val_f_rmse: 0.140365
2025-02-04 00:23:32.707 INFO: val_f_rmse: 0.140365
##### Step: 33 Learning rate: 0.005 #####
2025-02-04 00:24:32.507 INFO: ##### Step: 33 Learning rate: 0.005 #####
Epoch 34, Train Loss: 26.3367, Val Loss: 20.0420
2025-02-04 00:24:32.508 INFO: Epoch 34, Train Loss: 26.3367, Val Loss: 20.0420
train_e/atom_mae: 0.021210
2025-02-04 00:24:32.508 INFO: train_e/atom_mae: 0.021210
train_e/atom_rmse: 0.025822
2025-02-04 00:24:32.509 INFO: train_e/atom_rmse: 0.025822
train_f_mae: 0.096346
2025-02-04 00:24:32.511 INFO: train_f_mae: 0.096346
train_f_rmse: 0.154527
2025-02-04 00:24:32.511 INFO: train_f_rmse: 0.154527
val_e/atom_mae: 0.008782
2025-02-04 00:24:32.514 INFO: val_e/atom_mae: 0.008782
val_e/atom_rmse: 0.010608
2025-02-04 00:24:32.514 INFO: val_e/atom_rmse: 0.010608
val_f_mae: 0.090618
2025-02-04 00:24:32.514 INFO: val_f_mae: 0.090618
val_f_rmse: 0.140079
2025-02-04 00:24:32.515 INFO: val_f_rmse: 0.140079
##### Step: 34 Learning rate: 0.005 #####
2025-02-04 00:25:32.455 INFO: ##### Step: 34 Learning rate: 0.005 #####
Epoch 35, Train Loss: 25.0381, Val Loss: 19.5670
2025-02-04 00:25:32.456 INFO: Epoch 35, Train Loss: 25.0381, Val Loss: 19.5670
train_e/atom_mae: 0.015859
2025-02-04 00:25:32.457 INFO: train_e/atom_mae: 0.015859
train_e/atom_rmse: 0.020269
2025-02-04 00:25:32.457 INFO: train_e/atom_rmse: 0.020269
train_f_mae: 0.095643
2025-02-04 00:25:32.459 INFO: train_f_mae: 0.095643
train_f_rmse: 0.153374
2025-02-04 00:25:32.460 INFO: train_f_rmse: 0.153374
val_e/atom_mae: 0.007369
2025-02-04 00:25:32.462 INFO: val_e/atom_mae: 0.007369
val_e/atom_rmse: 0.010277
2025-02-04 00:25:32.462 INFO: val_e/atom_rmse: 0.010277
val_f_mae: 0.089786
2025-02-04 00:25:32.463 INFO: val_f_mae: 0.089786
val_f_rmse: 0.138467
2025-02-04 00:25:32.463 INFO: val_f_rmse: 0.138467
##### Step: 35 Learning rate: 0.005 #####
2025-02-04 00:26:32.297 INFO: ##### Step: 35 Learning rate: 0.005 #####
Epoch 36, Train Loss: 24.4039, Val Loss: 19.2339
2025-02-04 00:26:32.297 INFO: Epoch 36, Train Loss: 24.4039, Val Loss: 19.2339
train_e/atom_mae: 0.016494
2025-02-04 00:26:32.298 INFO: train_e/atom_mae: 0.016494
train_e/atom_rmse: 0.020523
2025-02-04 00:26:32.298 INFO: train_e/atom_rmse: 0.020523
train_f_mae: 0.094308
2025-02-04 00:26:32.301 INFO: train_f_mae: 0.094308
train_f_rmse: 0.151166
2025-02-04 00:26:32.301 INFO: train_f_rmse: 0.151166
val_e/atom_mae: 0.007237
2025-02-04 00:26:32.303 INFO: val_e/atom_mae: 0.007237
val_e/atom_rmse: 0.010365
2025-02-04 00:26:32.303 INFO: val_e/atom_rmse: 0.010365
val_f_mae: 0.089118
2025-02-04 00:26:32.304 INFO: val_f_mae: 0.089118
val_f_rmse: 0.137236
2025-02-04 00:26:32.304 INFO: val_f_rmse: 0.137236
##### Step: 36 Learning rate: 0.005 #####
2025-02-04 00:27:32.205 INFO: ##### Step: 36 Learning rate: 0.005 #####
Epoch 37, Train Loss: 24.2523, Val Loss: 19.0679
2025-02-04 00:27:32.206 INFO: Epoch 37, Train Loss: 24.2523, Val Loss: 19.0679
train_e/atom_mae: 0.013417
2025-02-04 00:27:32.207 INFO: train_e/atom_mae: 0.013417
train_e/atom_rmse: 0.017471
2025-02-04 00:27:32.207 INFO: train_e/atom_rmse: 0.017471
train_f_mae: 0.095569
2025-02-04 00:27:32.209 INFO: train_f_mae: 0.095569
train_f_rmse: 0.152076
2025-02-04 00:27:32.210 INFO: train_f_rmse: 0.152076
val_e/atom_mae: 0.007876
2025-02-04 00:27:32.212 INFO: val_e/atom_mae: 0.007876
val_e/atom_rmse: 0.009913
2025-02-04 00:27:32.212 INFO: val_e/atom_rmse: 0.009913
val_f_mae: 0.088186
2025-02-04 00:27:32.212 INFO: val_f_mae: 0.088186
val_f_rmse: 0.136757
2025-02-04 00:27:32.213 INFO: val_f_rmse: 0.136757
##### Step: 37 Learning rate: 0.005 #####
2025-02-04 00:28:32.053 INFO: ##### Step: 37 Learning rate: 0.005 #####
Epoch 38, Train Loss: 24.2382, Val Loss: 18.6330
2025-02-04 00:28:32.053 INFO: Epoch 38, Train Loss: 24.2382, Val Loss: 18.6330
train_e/atom_mae: 0.014187
2025-02-04 00:28:32.054 INFO: train_e/atom_mae: 0.014187
train_e/atom_rmse: 0.018681
2025-02-04 00:28:32.054 INFO: train_e/atom_rmse: 0.018681
train_f_mae: 0.095664
2025-02-04 00:28:32.057 INFO: train_f_mae: 0.095664
train_f_rmse: 0.151498
2025-02-04 00:28:32.057 INFO: train_f_rmse: 0.151498
val_e/atom_mae: 0.007246
2025-02-04 00:28:32.059 INFO: val_e/atom_mae: 0.007246
val_e/atom_rmse: 0.009514
2025-02-04 00:28:32.059 INFO: val_e/atom_rmse: 0.009514
val_f_mae: 0.088217
2025-02-04 00:28:32.060 INFO: val_f_mae: 0.088217
val_f_rmse: 0.135254
2025-02-04 00:28:32.060 INFO: val_f_rmse: 0.135254
##### Step: 38 Learning rate: 0.005 #####
2025-02-04 00:29:32.089 INFO: ##### Step: 38 Learning rate: 0.005 #####
Epoch 39, Train Loss: 23.9167, Val Loss: 18.4716
2025-02-04 00:29:32.090 INFO: Epoch 39, Train Loss: 23.9167, Val Loss: 18.4716
train_e/atom_mae: 0.011439
2025-02-04 00:29:32.090 INFO: train_e/atom_mae: 0.011439
train_e/atom_rmse: 0.015009
2025-02-04 00:29:32.091 INFO: train_e/atom_rmse: 0.015009
train_f_mae: 0.096134
2025-02-04 00:29:32.093 INFO: train_f_mae: 0.096134
train_f_rmse: 0.151942
2025-02-04 00:29:32.094 INFO: train_f_rmse: 0.151942
val_e/atom_mae: 0.007544
2025-02-04 00:29:32.096 INFO: val_e/atom_mae: 0.007544
val_e/atom_rmse: 0.009650
2025-02-04 00:29:32.096 INFO: val_e/atom_rmse: 0.009650
val_f_mae: 0.086768
2025-02-04 00:29:32.096 INFO: val_f_mae: 0.086768
val_f_rmse: 0.134626
2025-02-04 00:29:32.097 INFO: val_f_rmse: 0.134626
##### Step: 39 Learning rate: 0.005 #####
2025-02-04 00:30:32.249 INFO: ##### Step: 39 Learning rate: 0.005 #####
Epoch 40, Train Loss: 22.6637, Val Loss: 18.1744
2025-02-04 00:30:32.249 INFO: Epoch 40, Train Loss: 22.6637, Val Loss: 18.1744
train_e/atom_mae: 0.013795
2025-02-04 00:30:32.250 INFO: train_e/atom_mae: 0.013795
train_e/atom_rmse: 0.017896
2025-02-04 00:30:32.250 INFO: train_e/atom_rmse: 0.017896
train_f_mae: 0.091068
2025-02-04 00:30:32.253 INFO: train_f_mae: 0.091068
train_f_rmse: 0.146571
2025-02-04 00:30:32.253 INFO: train_f_rmse: 0.146571
val_e/atom_mae: 0.006870
2025-02-04 00:30:32.255 INFO: val_e/atom_mae: 0.006870
val_e/atom_rmse: 0.009435
2025-02-04 00:30:32.255 INFO: val_e/atom_rmse: 0.009435
val_f_mae: 0.086659
2025-02-04 00:30:32.256 INFO: val_f_mae: 0.086659
val_f_rmse: 0.133576
2025-02-04 00:30:32.256 INFO: val_f_rmse: 0.133576
2025-02-04 00:30:32.463 INFO: training
##### Step: 0 Learning rate: 0.002 #####
2025-02-04 00:31:32.537 INFO: ##### Step: 0 Learning rate: 0.002 #####
Epoch 1, Train Loss: 271.1399, Val Loss: 35.3097
2025-02-04 00:31:32.538 INFO: Epoch 1, Train Loss: 271.1399, Val Loss: 35.3097
train_e/atom_mae: 0.076576
2025-02-04 00:31:32.538 INFO: train_e/atom_mae: 0.076576
train_e/atom_rmse: 0.168854
2025-02-04 00:31:32.539 INFO: train_e/atom_rmse: 0.168854
train_f_mae: 0.204914
2025-02-04 00:31:32.541 INFO: train_f_mae: 0.204914
train_f_rmse: 0.407473
2025-02-04 00:31:32.542 INFO: train_f_rmse: 0.407473
val_e/atom_mae: 0.041584
2025-02-04 00:31:32.544 INFO: val_e/atom_mae: 0.041584
val_e/atom_rmse: 0.043546
2025-02-04 00:31:32.544 INFO: val_e/atom_rmse: 0.043546
val_f_mae: 0.112184
2025-02-04 00:31:32.544 INFO: val_f_mae: 0.112184
val_f_rmse: 0.168211
2025-02-04 00:31:32.545 INFO: val_f_rmse: 0.168211
##### Step: 1 Learning rate: 0.004 #####
2025-02-04 00:32:33.617 INFO: ##### Step: 1 Learning rate: 0.004 #####
Epoch 2, Train Loss: 51.4027, Val Loss: 46.0521
2025-02-04 00:32:33.617 INFO: Epoch 2, Train Loss: 51.4027, Val Loss: 46.0521
train_e/atom_mae: 0.036909
2025-02-04 00:32:33.636 INFO: train_e/atom_mae: 0.036909
train_e/atom_rmse: 0.047686
2025-02-04 00:32:33.636 INFO: train_e/atom_rmse: 0.047686
train_f_mae: 0.139562
2025-02-04 00:32:33.639 INFO: train_f_mae: 0.139562
train_f_rmse: 0.207413
2025-02-04 00:32:33.639 INFO: train_f_rmse: 0.207413
val_e/atom_mae: 0.048177
2025-02-04 00:32:33.641 INFO: val_e/atom_mae: 0.048177
val_e/atom_rmse: 0.050370
2025-02-04 00:32:33.641 INFO: val_e/atom_rmse: 0.050370
val_f_mae: 0.137649
2025-02-04 00:32:33.642 INFO: val_f_mae: 0.137649
val_f_rmse: 0.191497
2025-02-04 00:32:33.642 INFO: val_f_rmse: 0.191497
##### Step: 2 Learning rate: 0.006 #####
2025-02-04 00:33:33.336 INFO: ##### Step: 2 Learning rate: 0.006 #####
Epoch 3, Train Loss: 47.6163, Val Loss: 26.8407
2025-02-04 00:33:33.337 INFO: Epoch 3, Train Loss: 47.6163, Val Loss: 26.8407
train_e/atom_mae: 0.040291
2025-02-04 00:33:33.338 INFO: train_e/atom_mae: 0.040291
train_e/atom_rmse: 0.049688
2025-02-04 00:33:33.338 INFO: train_e/atom_rmse: 0.049688
train_f_mae: 0.131780
2025-02-04 00:33:33.340 INFO: train_f_mae: 0.131780
train_f_rmse: 0.196253
2025-02-04 00:33:33.341 INFO: train_f_rmse: 0.196253
val_e/atom_mae: 0.012336
2025-02-04 00:33:33.343 INFO: val_e/atom_mae: 0.012336
val_e/atom_rmse: 0.017623
2025-02-04 00:33:33.343 INFO: val_e/atom_rmse: 0.017623
val_f_mae: 0.108615
2025-02-04 00:33:33.343 INFO: val_f_mae: 0.108615
val_f_rmse: 0.160353
2025-02-04 00:33:33.344 INFO: val_f_rmse: 0.160353
##### Step: 3 Learning rate: 0.008 #####
2025-02-04 00:34:33.125 INFO: ##### Step: 3 Learning rate: 0.008 #####
Epoch 4, Train Loss: 30.3906, Val Loss: 17.4788
2025-02-04 00:34:33.125 INFO: Epoch 4, Train Loss: 30.3906, Val Loss: 17.4788
train_e/atom_mae: 0.021332
2025-02-04 00:34:33.126 INFO: train_e/atom_mae: 0.021332
train_e/atom_rmse: 0.026624
2025-02-04 00:34:33.126 INFO: train_e/atom_rmse: 0.026624
train_f_mae: 0.110551
2025-02-04 00:34:33.129 INFO: train_f_mae: 0.110551
train_f_rmse: 0.166666
2025-02-04 00:34:33.129 INFO: train_f_rmse: 0.166666
val_e/atom_mae: 0.006134
2025-02-04 00:34:33.131 INFO: val_e/atom_mae: 0.006134
val_e/atom_rmse: 0.008660
2025-02-04 00:34:33.132 INFO: val_e/atom_rmse: 0.008660
val_f_mae: 0.087055
2025-02-04 00:34:33.132 INFO: val_f_mae: 0.087055
val_f_rmse: 0.131143
2025-02-04 00:34:33.132 INFO: val_f_rmse: 0.131143
##### Step: 4 Learning rate: 0.01 #####
2025-02-04 00:35:32.998 INFO: ##### Step: 4 Learning rate: 0.01 #####
Epoch 5, Train Loss: 21.0046, Val Loss: 31.4140
2025-02-04 00:35:32.999 INFO: Epoch 5, Train Loss: 21.0046, Val Loss: 31.4140
train_e/atom_mae: 0.014665
2025-02-04 00:35:33.000 INFO: train_e/atom_mae: 0.014665
train_e/atom_rmse: 0.018370
2025-02-04 00:35:33.000 INFO: train_e/atom_rmse: 0.018370
train_f_mae: 0.089974
2025-02-04 00:35:33.002 INFO: train_f_mae: 0.089974
train_f_rmse: 0.140572
2025-02-04 00:35:33.003 INFO: train_f_rmse: 0.140572
val_e/atom_mae: 0.064494
2025-02-04 00:35:33.005 INFO: val_e/atom_mae: 0.064494
val_e/atom_rmse: 0.064838
2025-02-04 00:35:33.005 INFO: val_e/atom_rmse: 0.064838
val_f_mae: 0.085385
2025-02-04 00:35:33.006 INFO: val_f_mae: 0.085385
val_f_rmse: 0.126197
2025-02-04 00:35:33.006 INFO: val_f_rmse: 0.126197
##### Step: 5 Learning rate: 0.01 #####
2025-02-04 00:36:32.913 INFO: ##### Step: 5 Learning rate: 0.01 #####
Epoch 6, Train Loss: 24.2431, Val Loss: 49.1032
2025-02-04 00:36:32.914 INFO: Epoch 6, Train Loss: 24.2431, Val Loss: 49.1032
train_e/atom_mae: 0.021086
2025-02-04 00:36:32.915 INFO: train_e/atom_mae: 0.021086
train_e/atom_rmse: 0.025649
2025-02-04 00:36:32.915 INFO: train_e/atom_rmse: 0.025649
train_f_mae: 0.099026
2025-02-04 00:36:32.918 INFO: train_f_mae: 0.099026
train_f_rmse: 0.147709
2025-02-04 00:36:32.918 INFO: train_f_rmse: 0.147709
val_e/atom_mae: 0.052536
2025-02-04 00:36:32.920 INFO: val_e/atom_mae: 0.052536
val_e/atom_rmse: 0.052811
2025-02-04 00:36:32.920 INFO: val_e/atom_rmse: 0.052811
val_f_mae: 0.146105
2025-02-04 00:36:32.921 INFO: val_f_mae: 0.146105
val_f_rmse: 0.196951
2025-02-04 00:36:32.921 INFO: val_f_rmse: 0.196951
##### Step: 6 Learning rate: 0.01 #####
2025-02-04 00:37:32.833 INFO: ##### Step: 6 Learning rate: 0.01 #####
Epoch 7, Train Loss: 31.6132, Val Loss: 28.9212
2025-02-04 00:37:32.834 INFO: Epoch 7, Train Loss: 31.6132, Val Loss: 28.9212
train_e/atom_mae: 0.026403
2025-02-04 00:37:32.835 INFO: train_e/atom_mae: 0.026403
train_e/atom_rmse: 0.032423
2025-02-04 00:37:32.835 INFO: train_e/atom_rmse: 0.032423
train_f_mae: 0.114184
2025-02-04 00:37:32.837 INFO: train_f_mae: 0.114184
train_f_rmse: 0.166547
2025-02-04 00:37:32.838 INFO: train_f_rmse: 0.166547
val_e/atom_mae: 0.025089
2025-02-04 00:37:32.840 INFO: val_e/atom_mae: 0.025089
val_e/atom_rmse: 0.026506
2025-02-04 00:37:32.840 INFO: val_e/atom_rmse: 0.026506
val_f_mae: 0.122279
2025-02-04 00:37:32.840 INFO: val_f_mae: 0.122279
val_f_rmse: 0.162178
2025-02-04 00:37:32.841 INFO: val_f_rmse: 0.162178
##### Step: 7 Learning rate: 0.01 #####
2025-02-04 00:38:32.713 INFO: ##### Step: 7 Learning rate: 0.01 #####
Epoch 8, Train Loss: 21.1876, Val Loss: 18.7167
2025-02-04 00:38:32.714 INFO: Epoch 8, Train Loss: 21.1876, Val Loss: 18.7167
train_e/atom_mae: 0.024513
2025-02-04 00:38:32.715 INFO: train_e/atom_mae: 0.024513
train_e/atom_rmse: 0.029051
2025-02-04 00:38:32.715 INFO: train_e/atom_rmse: 0.029051
train_f_mae: 0.087307
2025-02-04 00:38:32.717 INFO: train_f_mae: 0.087307
train_f_rmse: 0.134448
2025-02-04 00:38:32.718 INFO: train_f_rmse: 0.134448
val_e/atom_mae: 0.030910
2025-02-04 00:38:32.720 INFO: val_e/atom_mae: 0.030910
val_e/atom_rmse: 0.031423
2025-02-04 00:38:32.720 INFO: val_e/atom_rmse: 0.031423
val_f_mae: 0.081731
2025-02-04 00:38:32.721 INFO: val_f_mae: 0.081731
val_f_rmse: 0.122788
2025-02-04 00:38:32.721 INFO: val_f_rmse: 0.122788
##### Step: 8 Learning rate: 0.01 #####
2025-02-04 00:39:32.331 INFO: ##### Step: 8 Learning rate: 0.01 #####
Epoch 9, Train Loss: 15.5835, Val Loss: 15.6732
2025-02-04 00:39:32.331 INFO: Epoch 9, Train Loss: 15.5835, Val Loss: 15.6732
train_e/atom_mae: 0.015207
2025-02-04 00:39:32.332 INFO: train_e/atom_mae: 0.015207
train_e/atom_rmse: 0.018988
2025-02-04 00:39:32.332 INFO: train_e/atom_rmse: 0.018988
train_f_mae: 0.076408
2025-02-04 00:39:32.335 INFO: train_f_mae: 0.076408
train_f_rmse: 0.119392
2025-02-04 00:39:32.335 INFO: train_f_rmse: 0.119392
val_e/atom_mae: 0.028768
2025-02-04 00:39:32.337 INFO: val_e/atom_mae: 0.028768
val_e/atom_rmse: 0.029836
2025-02-04 00:39:32.338 INFO: val_e/atom_rmse: 0.029836
val_f_mae: 0.074441
2025-02-04 00:39:32.338 INFO: val_f_mae: 0.074441
val_f_rmse: 0.111326
2025-02-04 00:39:32.338 INFO: val_f_rmse: 0.111326
##### Step: 9 Learning rate: 0.01 #####
2025-02-04 00:40:31.761 INFO: ##### Step: 9 Learning rate: 0.01 #####
Epoch 10, Train Loss: 15.9209, Val Loss: 10.7809
2025-02-04 00:40:31.762 INFO: Epoch 10, Train Loss: 15.9209, Val Loss: 10.7809
train_e/atom_mae: 0.014415
2025-02-04 00:40:31.763 INFO: train_e/atom_mae: 0.014415
train_e/atom_rmse: 0.018234
2025-02-04 00:40:31.763 INFO: train_e/atom_rmse: 0.018234
train_f_mae: 0.077685
2025-02-04 00:40:31.766 INFO: train_f_mae: 0.077685
train_f_rmse: 0.121224
2025-02-04 00:40:31.766 INFO: train_f_rmse: 0.121224
val_e/atom_mae: 0.011672
2025-02-04 00:40:31.768 INFO: val_e/atom_mae: 0.011672
val_e/atom_rmse: 0.012366
2025-02-04 00:40:31.768 INFO: val_e/atom_rmse: 0.012366
val_f_mae: 0.065614
2025-02-04 00:40:31.769 INFO: val_f_mae: 0.065614
val_f_rmse: 0.101082
2025-02-04 00:40:31.769 INFO: val_f_rmse: 0.101082
##### Step: 10 Learning rate: 0.01 #####
2025-02-04 00:41:31.616 INFO: ##### Step: 10 Learning rate: 0.01 #####
Epoch 11, Train Loss: 21.5021, Val Loss: 9.4560
2025-02-04 00:41:31.617 INFO: Epoch 11, Train Loss: 21.5021, Val Loss: 9.4560
train_e/atom_mae: 0.023141
2025-02-04 00:41:31.618 INFO: train_e/atom_mae: 0.023141
train_e/atom_rmse: 0.029647
2025-02-04 00:41:31.618 INFO: train_e/atom_rmse: 0.029647
train_f_mae: 0.088868
2025-02-04 00:41:31.621 INFO: train_f_mae: 0.088868
train_f_rmse: 0.135137
2025-02-04 00:41:31.621 INFO: train_f_rmse: 0.135137
val_e/atom_mae: 0.003853
2025-02-04 00:41:31.623 INFO: val_e/atom_mae: 0.003853
val_e/atom_rmse: 0.005815
2025-02-04 00:41:31.623 INFO: val_e/atom_rmse: 0.005815
val_f_mae: 0.060342
2025-02-04 00:41:31.624 INFO: val_f_mae: 0.060342
val_f_rmse: 0.096621
2025-02-04 00:41:31.624 INFO: val_f_rmse: 0.096621
##### Step: 11 Learning rate: 0.01 #####
2025-02-04 00:42:31.385 INFO: ##### Step: 11 Learning rate: 0.01 #####
Epoch 12, Train Loss: 15.7182, Val Loss: 9.0089
2025-02-04 00:42:31.385 INFO: Epoch 12, Train Loss: 15.7182, Val Loss: 9.0089
train_e/atom_mae: 0.011263
2025-02-04 00:42:31.386 INFO: train_e/atom_mae: 0.011263
train_e/atom_rmse: 0.015383
2025-02-04 00:42:31.386 INFO: train_e/atom_rmse: 0.015383
train_f_mae: 0.079162
2025-02-04 00:42:31.389 INFO: train_f_mae: 0.079162
train_f_rmse: 0.121844
2025-02-04 00:42:31.389 INFO: train_f_rmse: 0.121844
val_e/atom_mae: 0.005123
2025-02-04 00:42:31.391 INFO: val_e/atom_mae: 0.005123
val_e/atom_rmse: 0.006729
2025-02-04 00:42:31.391 INFO: val_e/atom_rmse: 0.006729
val_f_mae: 0.058221
2025-02-04 00:42:31.392 INFO: val_f_mae: 0.058221
val_f_rmse: 0.094057
2025-02-04 00:42:31.392 INFO: val_f_rmse: 0.094057
##### Step: 12 Learning rate: 0.01 #####
2025-02-04 00:43:31.205 INFO: ##### Step: 12 Learning rate: 0.01 #####
Epoch 13, Train Loss: 15.8916, Val Loss: 8.6232
2025-02-04 00:43:31.205 INFO: Epoch 13, Train Loss: 15.8916, Val Loss: 8.6232
train_e/atom_mae: 0.017765
2025-02-04 00:43:31.206 INFO: train_e/atom_mae: 0.017765
train_e/atom_rmse: 0.022949
2025-02-04 00:43:31.206 INFO: train_e/atom_rmse: 0.022949
train_f_mae: 0.077311
2025-02-04 00:43:31.209 INFO: train_f_mae: 0.077311
train_f_rmse: 0.118111
2025-02-04 00:43:31.209 INFO: train_f_rmse: 0.118111
val_e/atom_mae: 0.003263
2025-02-04 00:43:31.211 INFO: val_e/atom_mae: 0.003263
val_e/atom_rmse: 0.005090
2025-02-04 00:43:31.211 INFO: val_e/atom_rmse: 0.005090
val_f_mae: 0.056601
2025-02-04 00:43:31.212 INFO: val_f_mae: 0.056601
val_f_rmse: 0.092376
2025-02-04 00:43:31.212 INFO: val_f_rmse: 0.092376
##### Step: 13 Learning rate: 0.01 #####
2025-02-04 00:44:31.007 INFO: ##### Step: 13 Learning rate: 0.01 #####
Epoch 14, Train Loss: 13.9818, Val Loss: 8.5115
2025-02-04 00:44:31.008 INFO: Epoch 14, Train Loss: 13.9818, Val Loss: 8.5115
train_e/atom_mae: 0.014089
2025-02-04 00:44:31.008 INFO: train_e/atom_mae: 0.014089
train_e/atom_rmse: 0.016956
2025-02-04 00:44:31.009 INFO: train_e/atom_rmse: 0.016956
train_f_mae: 0.073506
2025-02-04 00:44:31.011 INFO: train_f_mae: 0.073506
train_f_rmse: 0.113674
2025-02-04 00:44:31.011 INFO: train_f_rmse: 0.113674
val_e/atom_mae: 0.003306
2025-02-04 00:44:31.014 INFO: val_e/atom_mae: 0.003306
val_e/atom_rmse: 0.004874
2025-02-04 00:44:31.014 INFO: val_e/atom_rmse: 0.004874
val_f_mae: 0.056334
2025-02-04 00:44:31.014 INFO: val_f_mae: 0.056334
val_f_rmse: 0.091807
2025-02-04 00:44:31.015 INFO: val_f_rmse: 0.091807
##### Step: 14 Learning rate: 0.01 #####
2025-02-04 00:45:30.813 INFO: ##### Step: 14 Learning rate: 0.01 #####
Epoch 15, Train Loss: 14.4074, Val Loss: 8.6105
2025-02-04 00:45:30.814 INFO: Epoch 15, Train Loss: 14.4074, Val Loss: 8.6105
train_e/atom_mae: 0.012541
2025-02-04 00:45:30.815 INFO: train_e/atom_mae: 0.012541
train_e/atom_rmse: 0.015674
2025-02-04 00:45:30.815 INFO: train_e/atom_rmse: 0.015674
train_f_mae: 0.074045
2025-02-04 00:45:30.817 INFO: train_f_mae: 0.074045
train_f_rmse: 0.116197
2025-02-04 00:45:30.818 INFO: train_f_rmse: 0.116197
val_e/atom_mae: 0.004624
2025-02-04 00:45:30.820 INFO: val_e/atom_mae: 0.004624
val_e/atom_rmse: 0.006046
2025-02-04 00:45:30.820 INFO: val_e/atom_rmse: 0.006046
val_f_mae: 0.055945
2025-02-04 00:45:30.821 INFO: val_f_mae: 0.055945
val_f_rmse: 0.092091
2025-02-04 00:45:30.821 INFO: val_f_rmse: 0.092091
##### Step: 15 Learning rate: 0.01 #####
2025-02-04 00:46:30.649 INFO: ##### Step: 15 Learning rate: 0.01 #####
Epoch 16, Train Loss: 12.3825, Val Loss: 8.1749
2025-02-04 00:46:30.649 INFO: Epoch 16, Train Loss: 12.3825, Val Loss: 8.1749
train_e/atom_mae: 0.008769
2025-02-04 00:46:30.650 INFO: train_e/atom_mae: 0.008769
train_e/atom_rmse: 0.011181
2025-02-04 00:46:30.650 INFO: train_e/atom_rmse: 0.011181
train_f_mae: 0.070305
2025-02-04 00:46:30.653 INFO: train_f_mae: 0.070305
train_f_rmse: 0.109186
2025-02-04 00:46:30.653 INFO: train_f_rmse: 0.109186
val_e/atom_mae: 0.003819
2025-02-04 00:46:30.655 INFO: val_e/atom_mae: 0.003819
val_e/atom_rmse: 0.005156
2025-02-04 00:46:30.655 INFO: val_e/atom_rmse: 0.005156
val_f_mae: 0.054532
2025-02-04 00:46:30.656 INFO: val_f_mae: 0.054532
val_f_rmse: 0.089909
2025-02-04 00:46:30.656 INFO: val_f_rmse: 0.089909
##### Step: 16 Learning rate: 0.01 #####
2025-02-04 00:47:30.623 INFO: ##### Step: 16 Learning rate: 0.01 #####
Epoch 17, Train Loss: 17.5825, Val Loss: 9.2861
2025-02-04 00:47:30.624 INFO: Epoch 17, Train Loss: 17.5825, Val Loss: 9.2861
train_e/atom_mae: 0.026368
2025-02-04 00:47:30.624 INFO: train_e/atom_mae: 0.026368
train_e/atom_rmse: 0.032293
2025-02-04 00:47:30.625 INFO: train_e/atom_rmse: 0.032293
train_f_mae: 0.076120
2025-02-04 00:47:30.627 INFO: train_f_mae: 0.076120
train_f_rmse: 0.117210
2025-02-04 00:47:30.628 INFO: train_f_rmse: 0.117210
val_e/atom_mae: 0.010539
2025-02-04 00:47:30.630 INFO: val_e/atom_mae: 0.010539
val_e/atom_rmse: 0.011148
2025-02-04 00:47:30.630 INFO: val_e/atom_rmse: 0.011148
val_f_mae: 0.057553
2025-02-04 00:47:30.631 INFO: val_f_mae: 0.057553
val_f_rmse: 0.093985
2025-02-04 00:47:30.631 INFO: val_f_rmse: 0.093985
##### Step: 17 Learning rate: 0.01 #####
2025-02-04 00:48:30.512 INFO: ##### Step: 17 Learning rate: 0.01 #####
Epoch 18, Train Loss: 13.5697, Val Loss: 7.9994
2025-02-04 00:48:30.513 INFO: Epoch 18, Train Loss: 13.5697, Val Loss: 7.9994
train_e/atom_mae: 0.017002
2025-02-04 00:48:30.513 INFO: train_e/atom_mae: 0.017002
train_e/atom_rmse: 0.020751
2025-02-04 00:48:30.514 INFO: train_e/atom_rmse: 0.020751
train_f_mae: 0.069623
2025-02-04 00:48:30.516 INFO: train_f_mae: 0.069623
train_f_rmse: 0.109464
2025-02-04 00:48:30.516 INFO: train_f_rmse: 0.109464
val_e/atom_mae: 0.002921
2025-02-04 00:48:30.518 INFO: val_e/atom_mae: 0.002921
val_e/atom_rmse: 0.004548
2025-02-04 00:48:30.519 INFO: val_e/atom_rmse: 0.004548
val_f_mae: 0.054254
2025-02-04 00:48:30.519 INFO: val_f_mae: 0.054254
val_f_rmse: 0.089044
2025-02-04 00:48:30.520 INFO: val_f_rmse: 0.089044
##### Step: 18 Learning rate: 0.01 #####
2025-02-04 00:49:30.439 INFO: ##### Step: 18 Learning rate: 0.01 #####
Epoch 19, Train Loss: 12.1488, Val Loss: 7.6427
2025-02-04 00:49:30.439 INFO: Epoch 19, Train Loss: 12.1488, Val Loss: 7.6427
train_e/atom_mae: 0.015126
2025-02-04 00:49:30.440 INFO: train_e/atom_mae: 0.015126
train_e/atom_rmse: 0.018368
2025-02-04 00:49:30.440 INFO: train_e/atom_rmse: 0.018368
train_f_mae: 0.066947
2025-02-04 00:49:30.443 INFO: train_f_mae: 0.066947
train_f_rmse: 0.104427
2025-02-04 00:49:30.443 INFO: train_f_rmse: 0.104427
val_e/atom_mae: 0.004041
2025-02-04 00:49:30.445 INFO: val_e/atom_mae: 0.004041
val_e/atom_rmse: 0.005225
2025-02-04 00:49:30.446 INFO: val_e/atom_rmse: 0.005225
val_f_mae: 0.053450
2025-02-04 00:49:30.446 INFO: val_f_mae: 0.053450
val_f_rmse: 0.086869
2025-02-04 00:49:30.446 INFO: val_f_rmse: 0.086869
##### Step: 19 Learning rate: 0.01 #####
2025-02-04 00:50:30.373 INFO: ##### Step: 19 Learning rate: 0.01 #####
Epoch 20, Train Loss: 11.3392, Val Loss: 7.5257
2025-02-04 00:50:30.374 INFO: Epoch 20, Train Loss: 11.3392, Val Loss: 7.5257
train_e/atom_mae: 0.011241
2025-02-04 00:50:30.375 INFO: train_e/atom_mae: 0.011241
train_e/atom_rmse: 0.014557
2025-02-04 00:50:30.375 INFO: train_e/atom_rmse: 0.014557
train_f_mae: 0.065299
2025-02-04 00:50:30.377 INFO: train_f_mae: 0.065299
train_f_rmse: 0.102752
2025-02-04 00:50:30.378 INFO: train_f_rmse: 0.102752
val_e/atom_mae: 0.003571
2025-02-04 00:50:30.380 INFO: val_e/atom_mae: 0.003571
val_e/atom_rmse: 0.005207
2025-02-04 00:50:30.380 INFO: val_e/atom_rmse: 0.005207
val_f_mae: 0.052453
2025-02-04 00:50:30.380 INFO: val_f_mae: 0.052453
val_f_rmse: 0.086204
2025-02-04 00:50:30.381 INFO: val_f_rmse: 0.086204
##### Step: 20 Learning rate: 0.005 #####
2025-02-04 00:51:30.352 INFO: ##### Step: 20 Learning rate: 0.005 #####
Epoch 21, Train Loss: 9.5872, Val Loss: 7.1959
2025-02-04 00:51:30.352 INFO: Epoch 21, Train Loss: 9.5872, Val Loss: 7.1959
train_e/atom_mae: 0.006769
2025-02-04 00:51:30.353 INFO: train_e/atom_mae: 0.006769
train_e/atom_rmse: 0.008477
2025-02-04 00:51:30.353 INFO: train_e/atom_rmse: 0.008477
train_f_mae: 0.061571
2025-02-04 00:51:30.356 INFO: train_f_mae: 0.061571
train_f_rmse: 0.096552
2025-02-04 00:51:30.356 INFO: train_f_rmse: 0.096552
val_e/atom_mae: 0.002355
2025-02-04 00:51:30.358 INFO: val_e/atom_mae: 0.002355
val_e/atom_rmse: 0.003741
2025-02-04 00:51:30.358 INFO: val_e/atom_rmse: 0.003741
val_f_mae: 0.051577
2025-02-04 00:51:30.359 INFO: val_f_mae: 0.051577
val_f_rmse: 0.084549
2025-02-04 00:51:30.359 INFO: val_f_rmse: 0.084549
##### Step: 21 Learning rate: 0.005 #####
2025-02-04 00:52:30.097 INFO: ##### Step: 21 Learning rate: 0.005 #####
Epoch 22, Train Loss: 8.8043, Val Loss: 7.1644
2025-02-04 00:52:30.097 INFO: Epoch 22, Train Loss: 8.8043, Val Loss: 7.1644
train_e/atom_mae: 0.008738
2025-02-04 00:52:30.098 INFO: train_e/atom_mae: 0.008738
train_e/atom_rmse: 0.010658
2025-02-04 00:52:30.099 INFO: train_e/atom_rmse: 0.010658
train_f_mae: 0.056315
2025-02-04 00:52:30.101 INFO: train_f_mae: 0.056315
train_f_rmse: 0.091572
2025-02-04 00:52:30.101 INFO: train_f_rmse: 0.091572
val_e/atom_mae: 0.002318
2025-02-04 00:52:30.103 INFO: val_e/atom_mae: 0.002318
val_e/atom_rmse: 0.003428
2025-02-04 00:52:30.104 INFO: val_e/atom_rmse: 0.003428
val_f_mae: 0.051016
2025-02-04 00:52:30.104 INFO: val_f_mae: 0.051016
val_f_rmse: 0.084417
2025-02-04 00:52:30.105 INFO: val_f_rmse: 0.084417
##### Step: 22 Learning rate: 0.005 #####
2025-02-04 00:53:30.111 INFO: ##### Step: 22 Learning rate: 0.005 #####
Epoch 23, Train Loss: 10.0233, Val Loss: 7.2182
2025-02-04 00:53:30.111 INFO: Epoch 23, Train Loss: 10.0233, Val Loss: 7.2182
train_e/atom_mae: 0.009342
2025-02-04 00:53:30.112 INFO: train_e/atom_mae: 0.009342
train_e/atom_rmse: 0.012810
2025-02-04 00:53:30.112 INFO: train_e/atom_rmse: 0.012810
train_f_mae: 0.062470
2025-02-04 00:53:30.115 INFO: train_f_mae: 0.062470
train_f_rmse: 0.097048
2025-02-04 00:53:30.115 INFO: train_f_rmse: 0.097048
val_e/atom_mae: 0.002554
2025-02-04 00:53:30.117 INFO: val_e/atom_mae: 0.002554
val_e/atom_rmse: 0.003705
2025-02-04 00:53:30.118 INFO: val_e/atom_rmse: 0.003705
val_f_mae: 0.051330
2025-02-04 00:53:30.118 INFO: val_f_mae: 0.051330
val_f_rmse: 0.084677
2025-02-04 00:53:30.118 INFO: val_f_rmse: 0.084677
##### Step: 23 Learning rate: 0.005 #####
2025-02-04 00:54:30.100 INFO: ##### Step: 23 Learning rate: 0.005 #####
Epoch 24, Train Loss: 9.8168, Val Loss: 6.9882
2025-02-04 00:54:30.101 INFO: Epoch 24, Train Loss: 9.8168, Val Loss: 6.9882
train_e/atom_mae: 0.008518
2025-02-04 00:54:30.101 INFO: train_e/atom_mae: 0.008518
train_e/atom_rmse: 0.010348
2025-02-04 00:54:30.102 INFO: train_e/atom_rmse: 0.010348
train_f_mae: 0.058612
2025-02-04 00:54:30.104 INFO: train_f_mae: 0.058612
train_f_rmse: 0.097067
2025-02-04 00:54:30.104 INFO: train_f_rmse: 0.097067
val_e/atom_mae: 0.002116
2025-02-04 00:54:30.107 INFO: val_e/atom_mae: 0.002116
val_e/atom_rmse: 0.003410
2025-02-04 00:54:30.107 INFO: val_e/atom_rmse: 0.003410
val_f_mae: 0.050447
2025-02-04 00:54:30.107 INFO: val_f_mae: 0.050447
val_f_rmse: 0.083355
2025-02-04 00:54:30.108 INFO: val_f_rmse: 0.083355
##### Step: 24 Learning rate: 0.005 #####
2025-02-04 00:55:30.044 INFO: ##### Step: 24 Learning rate: 0.005 #####
Epoch 25, Train Loss: 8.7242, Val Loss: 6.8404
2025-02-04 00:55:30.044 INFO: Epoch 25, Train Loss: 8.7242, Val Loss: 6.8404
train_e/atom_mae: 0.009355
2025-02-04 00:55:30.045 INFO: train_e/atom_mae: 0.009355
train_e/atom_rmse: 0.011614
2025-02-04 00:55:30.045 INFO: train_e/atom_rmse: 0.011614
train_f_mae: 0.057168
2025-02-04 00:55:30.048 INFO: train_f_mae: 0.057168
train_f_rmse: 0.090703
2025-02-04 00:55:30.048 INFO: train_f_rmse: 0.090703
val_e/atom_mae: 0.002037
2025-02-04 00:55:30.050 INFO: val_e/atom_mae: 0.002037
val_e/atom_rmse: 0.003317
2025-02-04 00:55:30.051 INFO: val_e/atom_rmse: 0.003317
val_f_mae: 0.050127
2025-02-04 00:55:30.051 INFO: val_f_mae: 0.050127
val_f_rmse: 0.082491
2025-02-04 00:55:30.051 INFO: val_f_rmse: 0.082491
##### Step: 25 Learning rate: 0.005 #####
2025-02-04 00:56:29.959 INFO: ##### Step: 25 Learning rate: 0.005 #####
Epoch 26, Train Loss: 8.1532, Val Loss: 6.7374
2025-02-04 00:56:29.959 INFO: Epoch 26, Train Loss: 8.1532, Val Loss: 6.7374
train_e/atom_mae: 0.006661
2025-02-04 00:56:29.960 INFO: train_e/atom_mae: 0.006661
train_e/atom_rmse: 0.008474
2025-02-04 00:56:29.960 INFO: train_e/atom_rmse: 0.008474
train_f_mae: 0.055266
2025-02-04 00:56:29.963 INFO: train_f_mae: 0.055266
train_f_rmse: 0.088817
2025-02-04 00:56:29.963 INFO: train_f_rmse: 0.088817
val_e/atom_mae: 0.002175
2025-02-04 00:56:29.965 INFO: val_e/atom_mae: 0.002175
val_e/atom_rmse: 0.003240
2025-02-04 00:56:29.966 INFO: val_e/atom_rmse: 0.003240
val_f_mae: 0.049761
2025-02-04 00:56:29.966 INFO: val_f_mae: 0.049761
val_f_rmse: 0.081866
2025-02-04 00:56:29.966 INFO: val_f_rmse: 0.081866
##### Step: 26 Learning rate: 0.005 #####
2025-02-04 00:57:29.743 INFO: ##### Step: 26 Learning rate: 0.005 #####
Epoch 27, Train Loss: 8.3188, Val Loss: 6.8127
2025-02-04 00:57:29.743 INFO: Epoch 27, Train Loss: 8.3188, Val Loss: 6.8127
train_e/atom_mae: 0.009835
2025-02-04 00:57:29.744 INFO: train_e/atom_mae: 0.009835
train_e/atom_rmse: 0.012290
2025-02-04 00:57:29.744 INFO: train_e/atom_rmse: 0.012290
train_f_mae: 0.054823
2025-02-04 00:57:29.747 INFO: train_f_mae: 0.054823
train_f_rmse: 0.088102
2025-02-04 00:57:29.747 INFO: train_f_rmse: 0.088102
val_e/atom_mae: 0.002385
2025-02-04 00:57:29.749 INFO: val_e/atom_mae: 0.002385
val_e/atom_rmse: 0.003550
2025-02-04 00:57:29.749 INFO: val_e/atom_rmse: 0.003550
val_f_mae: 0.049663
2025-02-04 00:57:29.750 INFO: val_f_mae: 0.049663
val_f_rmse: 0.082280
2025-02-04 00:57:29.750 INFO: val_f_rmse: 0.082280
##### Step: 27 Learning rate: 0.005 #####
2025-02-04 00:58:29.541 INFO: ##### Step: 27 Learning rate: 0.005 #####
Epoch 28, Train Loss: 8.8827, Val Loss: 6.6998
2025-02-04 00:58:29.541 INFO: Epoch 28, Train Loss: 8.8827, Val Loss: 6.6998
train_e/atom_mae: 0.007178
2025-02-04 00:58:29.542 INFO: train_e/atom_mae: 0.007178
train_e/atom_rmse: 0.008806
2025-02-04 00:58:29.542 INFO: train_e/atom_rmse: 0.008806
train_f_mae: 0.059957
2025-02-04 00:58:29.545 INFO: train_f_mae: 0.059957
train_f_rmse: 0.092719
2025-02-04 00:58:29.545 INFO: train_f_rmse: 0.092719
val_e/atom_mae: 0.002271
2025-02-04 00:58:29.547 INFO: val_e/atom_mae: 0.002271
val_e/atom_rmse: 0.003545
2025-02-04 00:58:29.547 INFO: val_e/atom_rmse: 0.003545
val_f_mae: 0.049332
2025-02-04 00:58:29.548 INFO: val_f_mae: 0.049332
val_f_rmse: 0.081586
2025-02-04 00:58:29.548 INFO: val_f_rmse: 0.081586
##### Step: 28 Learning rate: 0.005 #####
2025-02-04 00:59:29.380 INFO: ##### Step: 28 Learning rate: 0.005 #####
Epoch 29, Train Loss: 9.3128, Val Loss: 6.7465
2025-02-04 00:59:29.380 INFO: Epoch 29, Train Loss: 9.3128, Val Loss: 6.7465
train_e/atom_mae: 0.007134
2025-02-04 00:59:29.381 INFO: train_e/atom_mae: 0.007134
train_e/atom_rmse: 0.009169
2025-02-04 00:59:29.381 INFO: train_e/atom_rmse: 0.009169
train_f_mae: 0.061602
2025-02-04 00:59:29.384 INFO: train_f_mae: 0.061602
train_f_rmse: 0.094884
2025-02-04 00:59:29.384 INFO: train_f_rmse: 0.094884
val_e/atom_mae: 0.002252
2025-02-04 00:59:29.386 INFO: val_e/atom_mae: 0.002252
val_e/atom_rmse: 0.003327
2025-02-04 00:59:29.387 INFO: val_e/atom_rmse: 0.003327
val_f_mae: 0.049441
2025-02-04 00:59:29.387 INFO: val_f_mae: 0.049441
val_f_rmse: 0.081915
2025-02-04 00:59:29.387 INFO: val_f_rmse: 0.081915
##### Step: 29 Learning rate: 0.005 #####
2025-02-04 01:00:29.208 INFO: ##### Step: 29 Learning rate: 0.005 #####
Epoch 30, Train Loss: 8.4270, Val Loss: 6.7050
2025-02-04 01:00:29.209 INFO: Epoch 30, Train Loss: 8.4270, Val Loss: 6.7050
train_e/atom_mae: 0.007493
2025-02-04 01:00:29.210 INFO: train_e/atom_mae: 0.007493
train_e/atom_rmse: 0.009440
2025-02-04 01:00:29.210 INFO: train_e/atom_rmse: 0.009440
train_f_mae: 0.056717
2025-02-04 01:00:29.213 INFO: train_f_mae: 0.056717
train_f_rmse: 0.089992
2025-02-04 01:00:29.213 INFO: train_f_rmse: 0.089992
val_e/atom_mae: 0.002299
2025-02-04 01:00:29.215 INFO: val_e/atom_mae: 0.002299
val_e/atom_rmse: 0.003220
2025-02-04 01:00:29.215 INFO: val_e/atom_rmse: 0.003220
val_f_mae: 0.049034
2025-02-04 01:00:29.216 INFO: val_f_mae: 0.049034
val_f_rmse: 0.081680
2025-02-04 01:00:29.216 INFO: val_f_rmse: 0.081680
##### Step: 30 Learning rate: 0.005 #####
2025-02-04 01:01:29.019 INFO: ##### Step: 30 Learning rate: 0.005 #####
Epoch 31, Train Loss: 8.7261, Val Loss: 6.9290
2025-02-04 01:01:29.020 INFO: Epoch 31, Train Loss: 8.7261, Val Loss: 6.9290
train_e/atom_mae: 0.007916
2025-02-04 01:01:29.020 INFO: train_e/atom_mae: 0.007916
train_e/atom_rmse: 0.010171
2025-02-04 01:01:29.021 INFO: train_e/atom_rmse: 0.010171
train_f_mae: 0.057655
2025-02-04 01:01:29.023 INFO: train_f_mae: 0.057655
train_f_rmse: 0.091349
2025-02-04 01:01:29.023 INFO: train_f_rmse: 0.091349
val_e/atom_mae: 0.002977
2025-02-04 01:01:29.026 INFO: val_e/atom_mae: 0.002977
val_e/atom_rmse: 0.004290
2025-02-04 01:01:29.026 INFO: val_e/atom_rmse: 0.004290
val_f_mae: 0.049439
2025-02-04 01:01:29.026 INFO: val_f_mae: 0.049439
val_f_rmse: 0.082852
2025-02-04 01:01:29.027 INFO: val_f_rmse: 0.082852
##### Step: 31 Learning rate: 0.005 #####
2025-02-04 01:02:28.888 INFO: ##### Step: 31 Learning rate: 0.005 #####
Epoch 32, Train Loss: 8.7666, Val Loss: 6.5606
2025-02-04 01:02:28.889 INFO: Epoch 32, Train Loss: 8.7666, Val Loss: 6.5606
train_e/atom_mae: 0.009283
2025-02-04 01:02:28.890 INFO: train_e/atom_mae: 0.009283
train_e/atom_rmse: 0.011199
2025-02-04 01:02:28.890 INFO: train_e/atom_rmse: 0.011199
train_f_mae: 0.057894
2025-02-04 01:02:28.892 INFO: train_f_mae: 0.057894
train_f_rmse: 0.091128
2025-02-04 01:02:28.893 INFO: train_f_rmse: 0.091128
val_e/atom_mae: 0.002361
2025-02-04 01:02:28.895 INFO: val_e/atom_mae: 0.002361
val_e/atom_rmse: 0.003731
2025-02-04 01:02:28.895 INFO: val_e/atom_rmse: 0.003731
val_f_mae: 0.048944
2025-02-04 01:02:28.896 INFO: val_f_mae: 0.048944
val_f_rmse: 0.080698
2025-02-04 01:02:28.896 INFO: val_f_rmse: 0.080698
##### Step: 32 Learning rate: 0.005 #####
2025-02-04 01:03:28.765 INFO: ##### Step: 32 Learning rate: 0.005 #####
Epoch 33, Train Loss: 7.9453, Val Loss: 6.5839
2025-02-04 01:03:28.766 INFO: Epoch 33, Train Loss: 7.9453, Val Loss: 6.5839
train_e/atom_mae: 0.007536
2025-02-04 01:03:28.766 INFO: train_e/atom_mae: 0.007536
train_e/atom_rmse: 0.009384
2025-02-04 01:03:28.767 INFO: train_e/atom_rmse: 0.009384
train_f_mae: 0.055261
2025-02-04 01:03:28.769 INFO: train_f_mae: 0.055261
train_f_rmse: 0.087296
2025-02-04 01:03:28.770 INFO: train_f_rmse: 0.087296
val_e/atom_mae: 0.002342
2025-02-04 01:03:28.772 INFO: val_e/atom_mae: 0.002342
val_e/atom_rmse: 0.003425
2025-02-04 01:03:28.772 INFO: val_e/atom_rmse: 0.003425
val_f_mae: 0.048792
2025-02-04 01:03:28.772 INFO: val_f_mae: 0.048792
val_f_rmse: 0.080893
2025-02-04 01:03:28.773 INFO: val_f_rmse: 0.080893
##### Step: 33 Learning rate: 0.005 #####
2025-02-04 01:04:28.812 INFO: ##### Step: 33 Learning rate: 0.005 #####
Epoch 34, Train Loss: 11.7166, Val Loss: 6.9118
2025-02-04 01:04:28.812 INFO: Epoch 34, Train Loss: 11.7166, Val Loss: 6.9118
train_e/atom_mae: 0.008606
2025-02-04 01:04:28.813 INFO: train_e/atom_mae: 0.008606
train_e/atom_rmse: 0.010970
2025-02-04 01:04:28.813 INFO: train_e/atom_rmse: 0.010970
train_f_mae: 0.071331
2025-02-04 01:04:28.816 INFO: train_f_mae: 0.071331
train_f_rmse: 0.106174
2025-02-04 01:04:28.816 INFO: train_f_rmse: 0.106174
val_e/atom_mae: 0.002883
2025-02-04 01:04:28.818 INFO: val_e/atom_mae: 0.002883
val_e/atom_rmse: 0.004147
2025-02-04 01:04:28.818 INFO: val_e/atom_rmse: 0.004147
val_f_mae: 0.050444
2025-02-04 01:04:28.819 INFO: val_f_mae: 0.050444
val_f_rmse: 0.082785
2025-02-04 01:04:28.819 INFO: val_f_rmse: 0.082785
##### Step: 34 Learning rate: 0.005 #####
2025-02-04 01:05:28.880 INFO: ##### Step: 34 Learning rate: 0.005 #####
Epoch 35, Train Loss: 14.0643, Val Loss: 6.9622
2025-02-04 01:05:28.880 INFO: Epoch 35, Train Loss: 14.0643, Val Loss: 6.9622
train_e/atom_mae: 0.015246
2025-02-04 01:05:28.881 INFO: train_e/atom_mae: 0.015246
train_e/atom_rmse: 0.018994
2025-02-04 01:05:28.881 INFO: train_e/atom_rmse: 0.018994
train_f_mae: 0.075792
2025-02-04 01:05:28.884 INFO: train_f_mae: 0.075792
train_f_rmse: 0.112847
2025-02-04 01:05:28.884 INFO: train_f_rmse: 0.112847
val_e/atom_mae: 0.003046
2025-02-04 01:05:28.886 INFO: val_e/atom_mae: 0.003046
val_e/atom_rmse: 0.004244
2025-02-04 01:05:28.886 INFO: val_e/atom_rmse: 0.004244
val_f_mae: 0.049899
2025-02-04 01:05:28.887 INFO: val_f_mae: 0.049899
val_f_rmse: 0.083049
2025-02-04 01:05:28.887 INFO: val_f_rmse: 0.083049
##### Step: 35 Learning rate: 0.005 #####
2025-02-04 01:06:28.900 INFO: ##### Step: 35 Learning rate: 0.005 #####
Epoch 36, Train Loss: 8.6107, Val Loss: 6.4111
2025-02-04 01:06:28.900 INFO: Epoch 36, Train Loss: 8.6107, Val Loss: 6.4111
train_e/atom_mae: 0.008752
2025-02-04 01:06:28.901 INFO: train_e/atom_mae: 0.008752
train_e/atom_rmse: 0.011585
2025-02-04 01:06:28.902 INFO: train_e/atom_rmse: 0.011585
train_f_mae: 0.056509
2025-02-04 01:06:28.904 INFO: train_f_mae: 0.056509
train_f_rmse: 0.090088
2025-02-04 01:06:28.904 INFO: train_f_rmse: 0.090088
val_e/atom_mae: 0.002221
2025-02-04 01:06:28.906 INFO: val_e/atom_mae: 0.002221
val_e/atom_rmse: 0.003152
2025-02-04 01:06:28.907 INFO: val_e/atom_rmse: 0.003152
val_f_mae: 0.048739
2025-02-04 01:06:28.907 INFO: val_f_mae: 0.048739
val_f_rmse: 0.079853
2025-02-04 01:06:28.907 INFO: val_f_rmse: 0.079853
##### Step: 36 Learning rate: 0.005 #####
2025-02-04 01:07:29.052 INFO: ##### Step: 36 Learning rate: 0.005 #####
Epoch 37, Train Loss: 8.5184, Val Loss: 6.4317
2025-02-04 01:07:29.053 INFO: Epoch 37, Train Loss: 8.5184, Val Loss: 6.4317
train_e/atom_mae: 0.007782
2025-02-04 01:07:29.054 INFO: train_e/atom_mae: 0.007782
train_e/atom_rmse: 0.009975
2025-02-04 01:07:29.055 INFO: train_e/atom_rmse: 0.009975
train_f_mae: 0.057600
2025-02-04 01:07:29.057 INFO: train_f_mae: 0.057600
train_f_rmse: 0.090286
2025-02-04 01:07:29.057 INFO: train_f_rmse: 0.090286
val_e/atom_mae: 0.002177
2025-02-04 01:07:29.060 INFO: val_e/atom_mae: 0.002177
val_e/atom_rmse: 0.003465
2025-02-04 01:07:29.060 INFO: val_e/atom_rmse: 0.003465
val_f_mae: 0.048620
2025-02-04 01:07:29.060 INFO: val_f_mae: 0.048620
val_f_rmse: 0.079941
2025-02-04 01:07:29.061 INFO: val_f_rmse: 0.079941
##### Step: 37 Learning rate: 0.005 #####
2025-02-04 01:08:29.144 INFO: ##### Step: 37 Learning rate: 0.005 #####
Epoch 38, Train Loss: 8.6380, Val Loss: 6.5055
2025-02-04 01:08:29.144 INFO: Epoch 38, Train Loss: 8.6380, Val Loss: 6.5055
train_e/atom_mae: 0.006854
2025-02-04 01:08:29.145 INFO: train_e/atom_mae: 0.006854
train_e/atom_rmse: 0.009097
2025-02-04 01:08:29.145 INFO: train_e/atom_rmse: 0.009097
train_f_mae: 0.059124
2025-02-04 01:08:29.148 INFO: train_f_mae: 0.059124
train_f_rmse: 0.091285
2025-02-04 01:08:29.148 INFO: train_f_rmse: 0.091285
val_e/atom_mae: 0.002178
2025-02-04 01:08:29.150 INFO: val_e/atom_mae: 0.002178
val_e/atom_rmse: 0.003057
2025-02-04 01:08:29.151 INFO: val_e/atom_rmse: 0.003057
val_f_mae: 0.049320
2025-02-04 01:08:29.151 INFO: val_f_mae: 0.049320
val_f_rmse: 0.080466
2025-02-04 01:08:29.151 INFO: val_f_rmse: 0.080466
##### Step: 38 Learning rate: 0.005 #####
2025-02-04 01:09:29.226 INFO: ##### Step: 38 Learning rate: 0.005 #####
Epoch 39, Train Loss: 9.4855, Val Loss: 6.3953
2025-02-04 01:09:29.227 INFO: Epoch 39, Train Loss: 9.4855, Val Loss: 6.3953
train_e/atom_mae: 0.007970
2025-02-04 01:09:29.228 INFO: train_e/atom_mae: 0.007970
train_e/atom_rmse: 0.010097
2025-02-04 01:09:29.228 INFO: train_e/atom_rmse: 0.010097
train_f_mae: 0.062345
2025-02-04 01:09:29.231 INFO: train_f_mae: 0.062345
train_f_rmse: 0.095445
2025-02-04 01:09:29.231 INFO: train_f_rmse: 0.095445
val_e/atom_mae: 0.002268
2025-02-04 01:09:29.233 INFO: val_e/atom_mae: 0.002268
val_e/atom_rmse: 0.003654
2025-02-04 01:09:29.233 INFO: val_e/atom_rmse: 0.003654
val_f_mae: 0.048248
2025-02-04 01:09:29.234 INFO: val_f_mae: 0.048248
val_f_rmse: 0.079693
2025-02-04 01:09:29.234 INFO: val_f_rmse: 0.079693
##### Step: 39 Learning rate: 0.005 #####
2025-02-04 01:10:29.208 INFO: ##### Step: 39 Learning rate: 0.005 #####
Epoch 40, Train Loss: 7.4749, Val Loss: 6.3696
2025-02-04 01:10:29.208 INFO: Epoch 40, Train Loss: 7.4749, Val Loss: 6.3696
train_e/atom_mae: 0.007514
2025-02-04 01:10:29.209 INFO: train_e/atom_mae: 0.007514
train_e/atom_rmse: 0.009979
2025-02-04 01:10:29.209 INFO: train_e/atom_rmse: 0.009979
train_f_mae: 0.053238
2025-02-04 01:10:29.212 INFO: train_f_mae: 0.053238
train_f_rmse: 0.084308
2025-02-04 01:10:29.212 INFO: train_f_rmse: 0.084308
val_e/atom_mae: 0.002052
2025-02-04 01:10:29.214 INFO: val_e/atom_mae: 0.002052
val_e/atom_rmse: 0.003247
2025-02-04 01:10:29.214 INFO: val_e/atom_rmse: 0.003247
val_f_mae: 0.047745
2025-02-04 01:10:29.215 INFO: val_f_mae: 0.047745
val_f_rmse: 0.079594
2025-02-04 01:10:29.215 INFO: val_f_rmse: 0.079594
2025-02-04 01:10:29.264 INFO: training
##### Step: 0 Learning rate: 0.002 #####
2025-02-04 01:11:28.898 INFO: ##### Step: 0 Learning rate: 0.002 #####
Epoch 1, Train Loss: 6.9382, Val Loss: 6.5048
2025-02-04 01:11:28.899 INFO: Epoch 1, Train Loss: 6.9382, Val Loss: 6.5048
train_e/atom_mae: 0.005222
2025-02-04 01:11:28.899 INFO: train_e/atom_mae: 0.005222
train_e/atom_rmse: 0.007011
2025-02-04 01:11:28.900 INFO: train_e/atom_rmse: 0.007011
train_f_mae: 0.051355
2025-02-04 01:11:28.902 INFO: train_f_mae: 0.051355
train_f_rmse: 0.082201
2025-02-04 01:11:28.902 INFO: train_f_rmse: 0.082201
val_e/atom_mae: 0.004386
2025-02-04 01:11:28.905 INFO: val_e/atom_mae: 0.004386
val_e/atom_rmse: 0.004689
2025-02-04 01:11:28.905 INFO: val_e/atom_rmse: 0.004689
val_f_mae: 0.049250
2025-02-04 01:11:28.905 INFO: val_f_mae: 0.049250
val_f_rmse: 0.080171
2025-02-04 01:11:28.906 INFO: val_f_rmse: 0.080171
##### Step: 1 Learning rate: 0.004 #####
2025-02-04 01:12:28.594 INFO: ##### Step: 1 Learning rate: 0.004 #####
Epoch 2, Train Loss: 7.2748, Val Loss: 7.1926
2025-02-04 01:12:28.594 INFO: Epoch 2, Train Loss: 7.2748, Val Loss: 7.1926
train_e/atom_mae: 0.008111
2025-02-04 01:12:28.595 INFO: train_e/atom_mae: 0.008111
train_e/atom_rmse: 0.009950
2025-02-04 01:12:28.595 INFO: train_e/atom_rmse: 0.009950
train_f_mae: 0.052422
2025-02-04 01:12:28.598 INFO: train_f_mae: 0.052422
train_f_rmse: 0.083125
2025-02-04 01:12:28.598 INFO: train_f_rmse: 0.083125
val_e/atom_mae: 0.006160
2025-02-04 01:12:28.600 INFO: val_e/atom_mae: 0.006160
val_e/atom_rmse: 0.007202
2025-02-04 01:12:28.601 INFO: val_e/atom_rmse: 0.007202
val_f_mae: 0.051971
2025-02-04 01:12:28.601 INFO: val_f_mae: 0.051971
val_f_rmse: 0.083712
2025-02-04 01:12:28.601 INFO: val_f_rmse: 0.083712
##### Step: 2 Learning rate: 0.006 #####
2025-02-04 01:13:28.223 INFO: ##### Step: 2 Learning rate: 0.006 #####
Epoch 3, Train Loss: 7.8300, Val Loss: 7.6980
2025-02-04 01:13:28.223 INFO: Epoch 3, Train Loss: 7.8300, Val Loss: 7.6980
train_e/atom_mae: 0.007655
2025-02-04 01:13:28.224 INFO: train_e/atom_mae: 0.007655
train_e/atom_rmse: 0.009508
2025-02-04 01:13:28.225 INFO: train_e/atom_rmse: 0.009508
train_f_mae: 0.055177
2025-02-04 01:13:28.227 INFO: train_f_mae: 0.055177
train_f_rmse: 0.086584
2025-02-04 01:13:28.227 INFO: train_f_rmse: 0.086584
val_e/atom_mae: 0.005286
2025-02-04 01:13:28.229 INFO: val_e/atom_mae: 0.005286
val_e/atom_rmse: 0.005992
2025-02-04 01:13:28.230 INFO: val_e/atom_rmse: 0.005992
val_f_mae: 0.056166
2025-02-04 01:13:28.230 INFO: val_f_mae: 0.056166
val_f_rmse: 0.086980
2025-02-04 01:13:28.230 INFO: val_f_rmse: 0.086980
##### Step: 3 Learning rate: 0.008 #####
2025-02-04 01:14:27.890 INFO: ##### Step: 3 Learning rate: 0.008 #####
Epoch 4, Train Loss: 9.0408, Val Loss: 7.3789
2025-02-04 01:14:27.890 INFO: Epoch 4, Train Loss: 9.0408, Val Loss: 7.3789
train_e/atom_mae: 0.009828
2025-02-04 01:14:27.891 INFO: train_e/atom_mae: 0.009828
train_e/atom_rmse: 0.012628
2025-02-04 01:14:27.891 INFO: train_e/atom_rmse: 0.012628
train_f_mae: 0.059847
2025-02-04 01:14:27.894 INFO: train_f_mae: 0.059847
train_f_rmse: 0.091940
2025-02-04 01:14:27.894 INFO: train_f_rmse: 0.091940
val_e/atom_mae: 0.003522
2025-02-04 01:14:27.896 INFO: val_e/atom_mae: 0.003522
val_e/atom_rmse: 0.004638
2025-02-04 01:14:27.896 INFO: val_e/atom_rmse: 0.004638
val_f_mae: 0.053663
2025-02-04 01:14:27.897 INFO: val_f_mae: 0.053663
val_f_rmse: 0.085450
2025-02-04 01:14:27.897 INFO: val_f_rmse: 0.085450
##### Step: 4 Learning rate: 0.01 #####
2025-02-04 01:15:27.622 INFO: ##### Step: 4 Learning rate: 0.01 #####
Epoch 5, Train Loss: 9.5834, Val Loss: 10.4379
2025-02-04 01:15:27.622 INFO: Epoch 5, Train Loss: 9.5834, Val Loss: 10.4379
train_e/atom_mae: 0.009162
2025-02-04 01:15:27.623 INFO: train_e/atom_mae: 0.009162
train_e/atom_rmse: 0.012191
2025-02-04 01:15:27.623 INFO: train_e/atom_rmse: 0.012191
train_f_mae: 0.061277
2025-02-04 01:15:27.626 INFO: train_f_mae: 0.061277
train_f_rmse: 0.095056
2025-02-04 01:15:27.626 INFO: train_f_rmse: 0.095056
val_e/atom_mae: 0.010842
2025-02-04 01:15:27.628 INFO: val_e/atom_mae: 0.010842
val_e/atom_rmse: 0.011223
2025-02-04 01:15:27.629 INFO: val_e/atom_rmse: 0.011223
val_f_mae: 0.068262
2025-02-04 01:15:27.629 INFO: val_f_mae: 0.068262
val_f_rmse: 0.099890
2025-02-04 01:15:27.629 INFO: val_f_rmse: 0.099890
##### Step: 5 Learning rate: 0.01 #####
2025-02-04 01:16:27.229 INFO: ##### Step: 5 Learning rate: 0.01 #####
Epoch 6, Train Loss: 11.8914, Val Loss: 9.4323
2025-02-04 01:16:27.230 INFO: Epoch 6, Train Loss: 11.8914, Val Loss: 9.4323
train_e/atom_mae: 0.013203
2025-02-04 01:16:27.231 INFO: train_e/atom_mae: 0.013203
train_e/atom_rmse: 0.016308
2025-02-04 01:16:27.231 INFO: train_e/atom_rmse: 0.016308
train_f_mae: 0.067228
2025-02-04 01:16:27.234 INFO: train_f_mae: 0.067228
train_f_rmse: 0.104456
2025-02-04 01:16:27.234 INFO: train_f_rmse: 0.104456
val_e/atom_mae: 0.011512
2025-02-04 01:16:27.236 INFO: val_e/atom_mae: 0.011512
val_e/atom_rmse: 0.012484
2025-02-04 01:16:27.236 INFO: val_e/atom_rmse: 0.012484
val_f_mae: 0.059795
2025-02-04 01:16:27.237 INFO: val_f_mae: 0.059795
val_f_rmse: 0.094166
2025-02-04 01:16:27.237 INFO: val_f_rmse: 0.094166
##### Step: 6 Learning rate: 0.01 #####
2025-02-04 01:17:26.789 INFO: ##### Step: 6 Learning rate: 0.01 #####
Epoch 7, Train Loss: 12.4069, Val Loss: 10.7453
2025-02-04 01:17:26.789 INFO: Epoch 7, Train Loss: 12.4069, Val Loss: 10.7453
train_e/atom_mae: 0.014063
2025-02-04 01:17:26.790 INFO: train_e/atom_mae: 0.014063
train_e/atom_rmse: 0.017758
2025-02-04 01:17:26.790 INFO: train_e/atom_rmse: 0.017758
train_f_mae: 0.069725
2025-02-04 01:17:26.793 INFO: train_f_mae: 0.069725
train_f_rmse: 0.106039
2025-02-04 01:17:26.793 INFO: train_f_rmse: 0.106039
val_e/atom_mae: 0.008668
2025-02-04 01:17:26.795 INFO: val_e/atom_mae: 0.008668
val_e/atom_rmse: 0.009750
2025-02-04 01:17:26.796 INFO: val_e/atom_rmse: 0.009750
val_f_mae: 0.069995
2025-02-04 01:17:26.796 INFO: val_f_mae: 0.069995
val_f_rmse: 0.101961
2025-02-04 01:17:26.796 INFO: val_f_rmse: 0.101961
##### Step: 7 Learning rate: 0.01 #####
2025-02-04 01:18:26.385 INFO: ##### Step: 7 Learning rate: 0.01 #####
Epoch 8, Train Loss: 10.2633, Val Loss: 9.2840
2025-02-04 01:18:26.385 INFO: Epoch 8, Train Loss: 10.2633, Val Loss: 9.2840
train_e/atom_mae: 0.014723
2025-02-04 01:18:26.386 INFO: train_e/atom_mae: 0.014723
train_e/atom_rmse: 0.018404
2025-02-04 01:18:26.386 INFO: train_e/atom_rmse: 0.018404
train_f_mae: 0.060947
2025-02-04 01:18:26.389 INFO: train_f_mae: 0.060947
train_f_rmse: 0.094946
2025-02-04 01:18:26.389 INFO: train_f_rmse: 0.094946
val_e/atom_mae: 0.003208
2025-02-04 01:18:26.391 INFO: val_e/atom_mae: 0.003208
val_e/atom_rmse: 0.003970
2025-02-04 01:18:26.391 INFO: val_e/atom_rmse: 0.003970
val_f_mae: 0.054217
2025-02-04 01:18:26.392 INFO: val_f_mae: 0.054217
val_f_rmse: 0.096106
2025-02-04 01:18:26.392 INFO: val_f_rmse: 0.096106
##### Step: 8 Learning rate: 0.01 #####
2025-02-04 01:19:26.060 INFO: ##### Step: 8 Learning rate: 0.01 #####
Epoch 9, Train Loss: 11.0444, Val Loss: 10.8928
2025-02-04 01:19:26.061 INFO: Epoch 9, Train Loss: 11.0444, Val Loss: 10.8928
train_e/atom_mae: 0.008983
2025-02-04 01:19:26.062 INFO: train_e/atom_mae: 0.008983
train_e/atom_rmse: 0.011610
2025-02-04 01:19:26.062 INFO: train_e/atom_rmse: 0.011610
train_f_mae: 0.065053
2025-02-04 01:19:26.064 INFO: train_f_mae: 0.065053
train_f_rmse: 0.102701
2025-02-04 01:19:26.065 INFO: train_f_rmse: 0.102701
val_e/atom_mae: 0.005416
2025-02-04 01:19:26.067 INFO: val_e/atom_mae: 0.005416
val_e/atom_rmse: 0.007673
2025-02-04 01:19:26.067 INFO: val_e/atom_rmse: 0.007673
val_f_mae: 0.069431
2025-02-04 01:19:26.068 INFO: val_f_mae: 0.069431
val_f_rmse: 0.103334
2025-02-04 01:19:26.068 INFO: val_f_rmse: 0.103334
##### Step: 9 Learning rate: 0.01 #####
2025-02-04 01:20:25.636 INFO: ##### Step: 9 Learning rate: 0.01 #####
Epoch 10, Train Loss: 22.3930, Val Loss: 12.9434
2025-02-04 01:20:25.637 INFO: Epoch 10, Train Loss: 22.3930, Val Loss: 12.9434
train_e/atom_mae: 0.017911
2025-02-04 01:20:25.638 INFO: train_e/atom_mae: 0.017911
train_e/atom_rmse: 0.024292
2025-02-04 01:20:25.638 INFO: train_e/atom_rmse: 0.024292
train_f_mae: 0.094464
2025-02-04 01:20:25.641 INFO: train_f_mae: 0.094464
train_f_rmse: 0.142189
2025-02-04 01:20:25.641 INFO: train_f_rmse: 0.142189
val_e/atom_mae: 0.022440
2025-02-04 01:20:25.643 INFO: val_e/atom_mae: 0.022440
val_e/atom_rmse: 0.022978
2025-02-04 01:20:25.643 INFO: val_e/atom_rmse: 0.022978
val_f_mae: 0.068517
2025-02-04 01:20:25.644 INFO: val_f_mae: 0.068517
val_f_rmse: 0.104855
2025-02-04 01:20:25.644 INFO: val_f_rmse: 0.104855
##### Step: 10 Learning rate: 0.01 #####
2025-02-04 01:21:25.759 INFO: ##### Step: 10 Learning rate: 0.01 #####
Epoch 11, Train Loss: 15.0075, Val Loss: 6.8757
2025-02-04 01:21:25.760 INFO: Epoch 11, Train Loss: 15.0075, Val Loss: 6.8757
train_e/atom_mae: 0.017127
2025-02-04 01:21:25.761 INFO: train_e/atom_mae: 0.017127
train_e/atom_rmse: 0.021545
2025-02-04 01:21:25.761 INFO: train_e/atom_rmse: 0.021545
train_f_mae: 0.077869
2025-02-04 01:21:25.764 INFO: train_f_mae: 0.077869
train_f_rmse: 0.115310
2025-02-04 01:21:25.764 INFO: train_f_rmse: 0.115310
val_e/atom_mae: 0.005968
2025-02-04 01:21:25.766 INFO: val_e/atom_mae: 0.005968
val_e/atom_rmse: 0.006678
2025-02-04 01:21:25.766 INFO: val_e/atom_rmse: 0.006678
val_f_mae: 0.050822
2025-02-04 01:21:25.767 INFO: val_f_mae: 0.050822
val_f_rmse: 0.081951
2025-02-04 01:21:25.767 INFO: val_f_rmse: 0.081951
##### Step: 11 Learning rate: 0.01 #####
2025-02-04 01:22:25.630 INFO: ##### Step: 11 Learning rate: 0.01 #####
Epoch 12, Train Loss: 10.3112, Val Loss: 6.6952
2025-02-04 01:22:25.630 INFO: Epoch 12, Train Loss: 10.3112, Val Loss: 6.6952
train_e/atom_mae: 0.010681
2025-02-04 01:22:25.631 INFO: train_e/atom_mae: 0.010681
train_e/atom_rmse: 0.013284
2025-02-04 01:22:25.631 INFO: train_e/atom_rmse: 0.013284
train_f_mae: 0.063831
2025-02-04 01:22:25.634 INFO: train_f_mae: 0.063831
train_f_rmse: 0.098289
2025-02-04 01:22:25.634 INFO: train_f_rmse: 0.098289
val_e/atom_mae: 0.004741
2025-02-04 01:22:25.636 INFO: val_e/atom_mae: 0.004741
val_e/atom_rmse: 0.005786
2025-02-04 01:22:25.636 INFO: val_e/atom_rmse: 0.005786
val_f_mae: 0.049311
2025-02-04 01:22:25.637 INFO: val_f_mae: 0.049311
val_f_rmse: 0.081098
2025-02-04 01:22:25.637 INFO: val_f_rmse: 0.081098
##### Step: 12 Learning rate: 0.01 #####
2025-02-04 01:23:25.424 INFO: ##### Step: 12 Learning rate: 0.01 #####
Epoch 13, Train Loss: 16.0815, Val Loss: 6.9161
2025-02-04 01:23:25.425 INFO: Epoch 13, Train Loss: 16.0815, Val Loss: 6.9161
train_e/atom_mae: 0.010745
2025-02-04 01:23:25.426 INFO: train_e/atom_mae: 0.010745
train_e/atom_rmse: 0.013641
2025-02-04 01:23:25.426 INFO: train_e/atom_rmse: 0.013641
train_f_mae: 0.082680
2025-02-04 01:23:25.429 INFO: train_f_mae: 0.082680
train_f_rmse: 0.124079
2025-02-04 01:23:25.429 INFO: train_f_rmse: 0.124079
val_e/atom_mae: 0.006229
2025-02-04 01:23:25.431 INFO: val_e/atom_mae: 0.006229
val_e/atom_rmse: 0.007080
2025-02-04 01:23:25.432 INFO: val_e/atom_rmse: 0.007080
val_f_mae: 0.050232
2025-02-04 01:23:25.432 INFO: val_f_mae: 0.050232
val_f_rmse: 0.082061
2025-02-04 01:23:25.432 INFO: val_f_rmse: 0.082061
##### Step: 13 Learning rate: 0.01 #####
2025-02-04 01:24:25.212 INFO: ##### Step: 13 Learning rate: 0.01 #####
Epoch 14, Train Loss: 11.9421, Val Loss: 6.4973
2025-02-04 01:24:25.212 INFO: Epoch 14, Train Loss: 11.9421, Val Loss: 6.4973
train_e/atom_mae: 0.010924
2025-02-04 01:24:25.213 INFO: train_e/atom_mae: 0.010924
train_e/atom_rmse: 0.014223
2025-02-04 01:24:25.213 INFO: train_e/atom_rmse: 0.014223
train_f_mae: 0.069270
2025-02-04 01:24:25.216 INFO: train_f_mae: 0.069270
train_f_rmse: 0.105813
2025-02-04 01:24:25.216 INFO: train_f_rmse: 0.105813
val_e/atom_mae: 0.004693
2025-02-04 01:24:25.218 INFO: val_e/atom_mae: 0.004693
val_e/atom_rmse: 0.005712
2025-02-04 01:24:25.219 INFO: val_e/atom_rmse: 0.005712
val_f_mae: 0.049496
2025-02-04 01:24:25.219 INFO: val_f_mae: 0.049496
val_f_rmse: 0.079878
2025-02-04 01:24:25.219 INFO: val_f_rmse: 0.079878
##### Step: 14 Learning rate: 0.01 #####
2025-02-04 01:25:25.046 INFO: ##### Step: 14 Learning rate: 0.01 #####
Epoch 15, Train Loss: 12.2017, Val Loss: 6.5276
2025-02-04 01:25:25.046 INFO: Epoch 15, Train Loss: 12.2017, Val Loss: 6.5276
train_e/atom_mae: 0.009815
2025-02-04 01:25:25.047 INFO: train_e/atom_mae: 0.009815
train_e/atom_rmse: 0.012735
2025-02-04 01:25:25.047 INFO: train_e/atom_rmse: 0.012735
train_f_mae: 0.070754
2025-02-04 01:25:25.050 INFO: train_f_mae: 0.070754
train_f_rmse: 0.107721
2025-02-04 01:25:25.050 INFO: train_f_rmse: 0.107721
val_e/atom_mae: 0.003369
2025-02-04 01:25:25.052 INFO: val_e/atom_mae: 0.003369
val_e/atom_rmse: 0.004461
2025-02-04 01:25:25.053 INFO: val_e/atom_rmse: 0.004461
val_f_mae: 0.049304
2025-02-04 01:25:25.053 INFO: val_f_mae: 0.049304
val_f_rmse: 0.080365
2025-02-04 01:25:25.053 INFO: val_f_rmse: 0.080365
##### Step: 15 Learning rate: 0.01 #####
2025-02-04 01:26:24.876 INFO: ##### Step: 15 Learning rate: 0.01 #####
Epoch 16, Train Loss: 11.0011, Val Loss: 6.2966
2025-02-04 01:26:24.876 INFO: Epoch 16, Train Loss: 11.0011, Val Loss: 6.2966
train_e/atom_mae: 0.009661
2025-02-04 01:26:24.877 INFO: train_e/atom_mae: 0.009661
train_e/atom_rmse: 0.013074
2025-02-04 01:26:24.877 INFO: train_e/atom_rmse: 0.013074
train_f_mae: 0.059927
2025-02-04 01:26:24.880 INFO: train_f_mae: 0.059927
train_f_rmse: 0.101838
2025-02-04 01:26:24.880 INFO: train_f_rmse: 0.101838
val_e/atom_mae: 0.002246
2025-02-04 01:26:24.882 INFO: val_e/atom_mae: 0.002246
val_e/atom_rmse: 0.003514
2025-02-04 01:26:24.882 INFO: val_e/atom_rmse: 0.003514
val_f_mae: 0.048599
2025-02-04 01:26:24.883 INFO: val_f_mae: 0.048599
val_f_rmse: 0.079101
2025-02-04 01:26:24.883 INFO: val_f_rmse: 0.079101
##### Step: 16 Learning rate: 0.01 #####
2025-02-04 01:27:24.673 INFO: ##### Step: 16 Learning rate: 0.01 #####
Epoch 17, Train Loss: 11.3123, Val Loss: 6.3776
2025-02-04 01:27:24.674 INFO: Epoch 17, Train Loss: 11.3123, Val Loss: 6.3776
train_e/atom_mae: 0.012912
2025-02-04 01:27:24.674 INFO: train_e/atom_mae: 0.012912
train_e/atom_rmse: 0.016564
2025-02-04 01:27:24.675 INFO: train_e/atom_rmse: 0.016564
train_f_mae: 0.065797
2025-02-04 01:27:24.677 INFO: train_f_mae: 0.065797
train_f_rmse: 0.101493
2025-02-04 01:27:24.678 INFO: train_f_rmse: 0.101493
val_e/atom_mae: 0.003509
2025-02-04 01:27:24.680 INFO: val_e/atom_mae: 0.003509
val_e/atom_rmse: 0.004654
2025-02-04 01:27:24.680 INFO: val_e/atom_rmse: 0.004654
val_f_mae: 0.048995
2025-02-04 01:27:24.680 INFO: val_f_mae: 0.048995
val_f_rmse: 0.079378
2025-02-04 01:27:24.681 INFO: val_f_rmse: 0.079378
##### Step: 17 Learning rate: 0.01 #####
2025-02-04 01:28:24.553 INFO: ##### Step: 17 Learning rate: 0.01 #####
Epoch 18, Train Loss: 8.8477, Val Loss: 6.3779
2025-02-04 01:28:24.553 INFO: Epoch 18, Train Loss: 8.8477, Val Loss: 6.3779
train_e/atom_mae: 0.009189
2025-02-04 01:28:24.554 INFO: train_e/atom_mae: 0.009189
train_e/atom_rmse: 0.012450
2025-02-04 01:28:24.554 INFO: train_e/atom_rmse: 0.012450
train_f_mae: 0.058397
2025-02-04 01:28:24.557 INFO: train_f_mae: 0.058397
train_f_rmse: 0.090974
2025-02-04 01:28:24.557 INFO: train_f_rmse: 0.090974
val_e/atom_mae: 0.002137
2025-02-04 01:28:24.559 INFO: val_e/atom_mae: 0.002137
val_e/atom_rmse: 0.003304
2025-02-04 01:28:24.560 INFO: val_e/atom_rmse: 0.003304
val_f_mae: 0.048491
2025-02-04 01:28:24.560 INFO: val_f_mae: 0.048491
val_f_rmse: 0.079642
2025-02-04 01:28:24.560 INFO: val_f_rmse: 0.079642
##### Step: 18 Learning rate: 0.01 #####
2025-02-04 01:29:24.385 INFO: ##### Step: 18 Learning rate: 0.01 #####
Epoch 19, Train Loss: 16.5946, Val Loss: 7.3360
2025-02-04 01:29:24.386 INFO: Epoch 19, Train Loss: 16.5946, Val Loss: 7.3360
train_e/atom_mae: 0.017404
2025-02-04 01:29:24.387 INFO: train_e/atom_mae: 0.017404
train_e/atom_rmse: 0.021219
2025-02-04 01:29:24.387 INFO: train_e/atom_rmse: 0.021219
train_f_mae: 0.081763
2025-02-04 01:29:24.389 INFO: train_f_mae: 0.081763
train_f_rmse: 0.122208
2025-02-04 01:29:24.390 INFO: train_f_rmse: 0.122208
val_e/atom_mae: 0.010596
2025-02-04 01:29:24.392 INFO: val_e/atom_mae: 0.010596
val_e/atom_rmse: 0.011076
2025-02-04 01:29:24.392 INFO: val_e/atom_rmse: 0.011076
val_f_mae: 0.050771
2025-02-04 01:29:24.393 INFO: val_f_mae: 0.050771
val_f_rmse: 0.083008
2025-02-04 01:29:24.393 INFO: val_f_rmse: 0.083008
##### Step: 19 Learning rate: 0.01 #####
2025-02-04 01:30:24.265 INFO: ##### Step: 19 Learning rate: 0.01 #####
Epoch 20, Train Loss: 9.6457, Val Loss: 6.5495
2025-02-04 01:30:24.265 INFO: Epoch 20, Train Loss: 9.6457, Val Loss: 6.5495
train_e/atom_mae: 0.011901
2025-02-04 01:30:24.266 INFO: train_e/atom_mae: 0.011901
train_e/atom_rmse: 0.015794
2025-02-04 01:30:24.266 INFO: train_e/atom_rmse: 0.015794
train_f_mae: 0.060566
2025-02-04 01:30:24.269 INFO: train_f_mae: 0.060566
train_f_rmse: 0.093414
2025-02-04 01:30:24.269 INFO: train_f_rmse: 0.093414
val_e/atom_mae: 0.005433
2025-02-04 01:30:24.271 INFO: val_e/atom_mae: 0.005433
val_e/atom_rmse: 0.006279
2025-02-04 01:30:24.271 INFO: val_e/atom_rmse: 0.006279
val_f_mae: 0.049091
2025-02-04 01:30:24.272 INFO: val_f_mae: 0.049091
val_f_rmse: 0.080054
2025-02-04 01:30:24.272 INFO: val_f_rmse: 0.080054
##### Step: 20 Learning rate: 0.005 #####
2025-02-04 01:31:54.060 INFO: ##### Step: 20 Learning rate: 0.005 #####
Epoch 21, Train Loss: 8.2726, Val Loss: 6.2477
2025-02-04 01:31:54.060 INFO: Epoch 21, Train Loss: 8.2726, Val Loss: 6.2477
train_e/atom_mae: 0.009718
2025-02-04 01:31:54.316 INFO: train_e/atom_mae: 0.009718
train_e/atom_rmse: 0.012161
2025-02-04 01:31:54.519 INFO: train_e/atom_rmse: 0.012161
train_f_mae: 0.056704
2025-02-04 01:31:54.522 INFO: train_f_mae: 0.056704
train_f_rmse: 0.087906
2025-02-04 01:31:54.522 INFO: train_f_rmse: 0.087906
val_e/atom_mae: 0.002114
2025-02-04 01:31:54.524 INFO: val_e/atom_mae: 0.002114
val_e/atom_rmse: 0.003051
2025-02-04 01:31:54.525 INFO: val_e/atom_rmse: 0.003051
val_f_mae: 0.047670
2025-02-04 01:31:54.525 INFO: val_f_mae: 0.047670
val_f_rmse: 0.078851
2025-02-04 01:31:54.525 INFO: val_f_rmse: 0.078851
##### Step: 21 Learning rate: 0.005 #####
2025-02-04 01:32:55.599 INFO: ##### Step: 21 Learning rate: 0.005 #####
Epoch 22, Train Loss: 7.0711, Val Loss: 6.1258
2025-02-04 01:32:55.600 INFO: Epoch 22, Train Loss: 7.0711, Val Loss: 6.1258
train_e/atom_mae: 0.006616
2025-02-04 01:32:55.601 INFO: train_e/atom_mae: 0.006616
train_e/atom_rmse: 0.009158
2025-02-04 01:32:55.601 INFO: train_e/atom_rmse: 0.009158
train_f_mae: 0.052244
2025-02-04 01:32:55.603 INFO: train_f_mae: 0.052244
train_f_rmse: 0.082231
2025-02-04 01:32:55.604 INFO: train_f_rmse: 0.082231
val_e/atom_mae: 0.002000
2025-02-04 01:32:55.606 INFO: val_e/atom_mae: 0.002000
val_e/atom_rmse: 0.003313
2025-02-04 01:32:55.606 INFO: val_e/atom_rmse: 0.003313
val_f_mae: 0.047177
2025-02-04 01:32:55.607 INFO: val_f_mae: 0.047177
val_f_rmse: 0.078037
2025-02-04 01:32:55.607 INFO: val_f_rmse: 0.078037
##### Step: 22 Learning rate: 0.005 #####
2025-02-04 01:33:55.340 INFO: ##### Step: 22 Learning rate: 0.005 #####
Epoch 23, Train Loss: 7.4773, Val Loss: 6.0482
2025-02-04 01:33:55.340 INFO: Epoch 23, Train Loss: 7.4773, Val Loss: 6.0482
train_e/atom_mae: 0.007275
2025-02-04 01:33:55.341 INFO: train_e/atom_mae: 0.007275
train_e/atom_rmse: 0.009128
2025-02-04 01:33:55.341 INFO: train_e/atom_rmse: 0.009128
train_f_mae: 0.054450
2025-02-04 01:33:55.344 INFO: train_f_mae: 0.054450
train_f_rmse: 0.084677
2025-02-04 01:33:55.344 INFO: train_f_rmse: 0.084677
val_e/atom_mae: 0.003281
2025-02-04 01:33:55.346 INFO: val_e/atom_mae: 0.003281
val_e/atom_rmse: 0.004316
2025-02-04 01:33:55.346 INFO: val_e/atom_rmse: 0.004316
val_f_mae: 0.047177
2025-02-04 01:33:55.347 INFO: val_f_mae: 0.047177
val_f_rmse: 0.077353
2025-02-04 01:33:55.347 INFO: val_f_rmse: 0.077353
##### Step: 23 Learning rate: 0.005 #####
2025-02-04 01:34:55.081 INFO: ##### Step: 23 Learning rate: 0.005 #####
Epoch 24, Train Loss: 8.0029, Val Loss: 6.2319
2025-02-04 01:34:55.081 INFO: Epoch 24, Train Loss: 8.0029, Val Loss: 6.2319
train_e/atom_mae: 0.006770
2025-02-04 01:34:55.082 INFO: train_e/atom_mae: 0.006770
train_e/atom_rmse: 0.008654
2025-02-04 01:34:55.082 INFO: train_e/atom_rmse: 0.008654
train_f_mae: 0.056832
2025-02-04 01:34:55.085 INFO: train_f_mae: 0.056832
train_f_rmse: 0.087902
2025-02-04 01:34:55.085 INFO: train_f_rmse: 0.087902
val_e/atom_mae: 0.002129
2025-02-04 01:34:55.087 INFO: val_e/atom_mae: 0.002129
val_e/atom_rmse: 0.003410
2025-02-04 01:34:55.088 INFO: val_e/atom_rmse: 0.003410
val_f_mae: 0.047458
2025-02-04 01:34:55.088 INFO: val_f_mae: 0.047458
val_f_rmse: 0.078708
2025-02-04 01:34:55.088 INFO: val_f_rmse: 0.078708
##### Step: 24 Learning rate: 0.005 #####
2025-02-04 01:35:54.950 INFO: ##### Step: 24 Learning rate: 0.005 #####
Epoch 25, Train Loss: 7.7193, Val Loss: 6.0289
2025-02-04 01:35:54.950 INFO: Epoch 25, Train Loss: 7.7193, Val Loss: 6.0289
train_e/atom_mae: 0.007603
2025-02-04 01:35:54.951 INFO: train_e/atom_mae: 0.007603
train_e/atom_rmse: 0.009740
2025-02-04 01:35:54.951 INFO: train_e/atom_rmse: 0.009740
train_f_mae: 0.055568
2025-02-04 01:35:54.954 INFO: train_f_mae: 0.055568
train_f_rmse: 0.085846
2025-02-04 01:35:54.954 INFO: train_f_rmse: 0.085846
val_e/atom_mae: 0.001946
2025-02-04 01:35:54.956 INFO: val_e/atom_mae: 0.001946
val_e/atom_rmse: 0.003155
2025-02-04 01:35:54.956 INFO: val_e/atom_rmse: 0.003155
val_f_mae: 0.046636
2025-02-04 01:35:54.957 INFO: val_f_mae: 0.046636
val_f_rmse: 0.077444
2025-02-04 01:35:54.957 INFO: val_f_rmse: 0.077444
##### Step: 25 Learning rate: 0.005 #####
2025-02-04 01:36:54.697 INFO: ##### Step: 25 Learning rate: 0.005 #####
Epoch 26, Train Loss: 6.8816, Val Loss: 5.9410
2025-02-04 01:36:54.698 INFO: Epoch 26, Train Loss: 6.8816, Val Loss: 5.9410
train_e/atom_mae: 0.008429
2025-02-04 01:36:54.698 INFO: train_e/atom_mae: 0.008429
train_e/atom_rmse: 0.010261
2025-02-04 01:36:54.699 INFO: train_e/atom_rmse: 0.010261
train_f_mae: 0.051200
2025-02-04 01:36:54.701 INFO: train_f_mae: 0.051200
train_f_rmse: 0.080582
2025-02-04 01:36:54.701 INFO: train_f_rmse: 0.080582
val_e/atom_mae: 0.001999
2025-02-04 01:36:54.704 INFO: val_e/atom_mae: 0.001999
val_e/atom_rmse: 0.003262
2025-02-04 01:36:54.704 INFO: val_e/atom_rmse: 0.003262
val_f_mae: 0.046520
2025-02-04 01:36:54.704 INFO: val_f_mae: 0.046520
val_f_rmse: 0.076852
2025-02-04 01:36:54.705 INFO: val_f_rmse: 0.076852
##### Step: 26 Learning rate: 0.005 #####
2025-02-04 01:37:54.433 INFO: ##### Step: 26 Learning rate: 0.005 #####
Epoch 27, Train Loss: 7.0192, Val Loss: 5.9598
2025-02-04 01:37:54.433 INFO: Epoch 27, Train Loss: 7.0192, Val Loss: 5.9598
train_e/atom_mae: 0.005332
2025-02-04 01:37:54.434 INFO: train_e/atom_mae: 0.005332
train_e/atom_rmse: 0.006989
2025-02-04 01:37:54.434 INFO: train_e/atom_rmse: 0.006989
train_f_mae: 0.052536
2025-02-04 01:37:54.437 INFO: train_f_mae: 0.052536
train_f_rmse: 0.082699
2025-02-04 01:37:54.437 INFO: train_f_rmse: 0.082699
val_e/atom_mae: 0.002141
2025-02-04 01:37:54.439 INFO: val_e/atom_mae: 0.002141
val_e/atom_rmse: 0.003312
2025-02-04 01:37:54.440 INFO: val_e/atom_rmse: 0.003312
val_f_mae: 0.046858
2025-02-04 01:37:54.440 INFO: val_f_mae: 0.046858
val_f_rmse: 0.076974
2025-02-04 01:37:54.440 INFO: val_f_rmse: 0.076974
##### Step: 27 Learning rate: 0.005 #####
2025-02-04 01:38:54.210 INFO: ##### Step: 27 Learning rate: 0.005 #####
Epoch 28, Train Loss: 6.9217, Val Loss: 5.8875
2025-02-04 01:38:54.210 INFO: Epoch 28, Train Loss: 6.9217, Val Loss: 5.8875
train_e/atom_mae: 0.005536
2025-02-04 01:38:54.211 INFO: train_e/atom_mae: 0.005536
train_e/atom_rmse: 0.007002
2025-02-04 01:38:54.211 INFO: train_e/atom_rmse: 0.007002
train_f_mae: 0.052935
2025-02-04 01:38:54.214 INFO: train_f_mae: 0.052935
train_f_rmse: 0.082103
2025-02-04 01:38:54.214 INFO: train_f_rmse: 0.082103
val_e/atom_mae: 0.001828
2025-02-04 01:38:54.216 INFO: val_e/atom_mae: 0.001828
val_e/atom_rmse: 0.002807
2025-02-04 01:38:54.216 INFO: val_e/atom_rmse: 0.002807
val_f_mae: 0.046155
2025-02-04 01:38:54.217 INFO: val_f_mae: 0.046155
val_f_rmse: 0.076571
2025-02-04 01:38:54.217 INFO: val_f_rmse: 0.076571
##### Step: 28 Learning rate: 0.005 #####
2025-02-04 01:39:53.896 INFO: ##### Step: 28 Learning rate: 0.005 #####
Epoch 29, Train Loss: 6.7327, Val Loss: 5.9032
2025-02-04 01:39:53.896 INFO: Epoch 29, Train Loss: 6.7327, Val Loss: 5.9032
train_e/atom_mae: 0.005609
2025-02-04 01:39:53.897 INFO: train_e/atom_mae: 0.005609
train_e/atom_rmse: 0.007151
2025-02-04 01:39:53.897 INFO: train_e/atom_rmse: 0.007151
train_f_mae: 0.051718
2025-02-04 01:39:53.900 INFO: train_f_mae: 0.051718
train_f_rmse: 0.080896
2025-02-04 01:39:53.900 INFO: train_f_rmse: 0.080896
val_e/atom_mae: 0.001905
2025-02-04 01:39:53.902 INFO: val_e/atom_mae: 0.001905
val_e/atom_rmse: 0.002947
2025-02-04 01:39:53.902 INFO: val_e/atom_rmse: 0.002947
val_f_mae: 0.046201
2025-02-04 01:39:53.903 INFO: val_f_mae: 0.046201
val_f_rmse: 0.076654
2025-02-04 01:39:53.903 INFO: val_f_rmse: 0.076654
##### Step: 29 Learning rate: 0.005 #####
2025-02-04 01:40:53.678 INFO: ##### Step: 29 Learning rate: 0.005 #####
Epoch 30, Train Loss: 6.9991, Val Loss: 5.9239
2025-02-04 01:40:53.678 INFO: Epoch 30, Train Loss: 6.9991, Val Loss: 5.9239
train_e/atom_mae: 0.008412
2025-02-04 01:40:53.679 INFO: train_e/atom_mae: 0.008412
train_e/atom_rmse: 0.010158
2025-02-04 01:40:53.680 INFO: train_e/atom_rmse: 0.010158
train_f_mae: 0.052466
2025-02-04 01:40:53.682 INFO: train_f_mae: 0.052466
train_f_rmse: 0.081355
2025-02-04 01:40:53.682 INFO: train_f_rmse: 0.081355
val_e/atom_mae: 0.003303
2025-02-04 01:40:53.685 INFO: val_e/atom_mae: 0.003303
val_e/atom_rmse: 0.004293
2025-02-04 01:40:53.685 INFO: val_e/atom_rmse: 0.004293
val_f_mae: 0.046150
2025-02-04 01:40:53.685 INFO: val_f_mae: 0.046150
val_f_rmse: 0.076555
2025-02-04 01:40:53.686 INFO: val_f_rmse: 0.076555
##### Step: 30 Learning rate: 0.005 #####
2025-02-04 01:41:53.439 INFO: ##### Step: 30 Learning rate: 0.005 #####
Epoch 31, Train Loss: 7.3089, Val Loss: 5.7985
2025-02-04 01:41:53.440 INFO: Epoch 31, Train Loss: 7.3089, Val Loss: 5.7985
train_e/atom_mae: 0.005290
2025-02-04 01:41:53.441 INFO: train_e/atom_mae: 0.005290
train_e/atom_rmse: 0.006916
2025-02-04 01:41:53.441 INFO: train_e/atom_rmse: 0.006916
train_f_mae: 0.054805
2025-02-04 01:41:53.443 INFO: train_f_mae: 0.054805
train_f_rmse: 0.084455
2025-02-04 01:41:53.444 INFO: train_f_rmse: 0.084455
val_e/atom_mae: 0.002462
2025-02-04 01:41:53.446 INFO: val_e/atom_mae: 0.002462
val_e/atom_rmse: 0.003677
2025-02-04 01:41:53.446 INFO: val_e/atom_rmse: 0.003677
val_f_mae: 0.046240
2025-02-04 01:41:53.447 INFO: val_f_mae: 0.046240
val_f_rmse: 0.075849
2025-02-04 01:41:53.447 INFO: val_f_rmse: 0.075849
##### Step: 31 Learning rate: 0.005 #####
2025-02-04 01:42:53.187 INFO: ##### Step: 31 Learning rate: 0.005 #####
Epoch 32, Train Loss: 7.5690, Val Loss: 5.7956
2025-02-04 01:42:53.188 INFO: Epoch 32, Train Loss: 7.5690, Val Loss: 5.7956
train_e/atom_mae: 0.010615
2025-02-04 01:42:53.189 INFO: train_e/atom_mae: 0.010615
train_e/atom_rmse: 0.013256
2025-02-04 01:42:53.189 INFO: train_e/atom_rmse: 0.013256
train_f_mae: 0.053781
2025-02-04 01:42:53.192 INFO: train_f_mae: 0.053781
train_f_rmse: 0.083193
2025-02-04 01:42:53.192 INFO: train_f_rmse: 0.083193
val_e/atom_mae: 0.001879
2025-02-04 01:42:53.194 INFO: val_e/atom_mae: 0.001879
val_e/atom_rmse: 0.003116
2025-02-04 01:42:53.194 INFO: val_e/atom_rmse: 0.003116
val_f_mae: 0.046151
2025-02-04 01:42:53.195 INFO: val_f_mae: 0.046151
val_f_rmse: 0.075924
2025-02-04 01:42:53.195 INFO: val_f_rmse: 0.075924
##### Step: 32 Learning rate: 0.005 #####
2025-02-04 01:43:52.931 INFO: ##### Step: 32 Learning rate: 0.005 #####
Epoch 33, Train Loss: 7.3652, Val Loss: 5.8852
2025-02-04 01:43:52.931 INFO: Epoch 33, Train Loss: 7.3652, Val Loss: 5.8852
train_e/atom_mae: 0.008001
2025-02-04 01:43:52.932 INFO: train_e/atom_mae: 0.008001
train_e/atom_rmse: 0.009840
2025-02-04 01:43:52.932 INFO: train_e/atom_rmse: 0.009840
train_f_mae: 0.054870
2025-02-04 01:43:52.935 INFO: train_f_mae: 0.054870
train_f_rmse: 0.083715
2025-02-04 01:43:52.935 INFO: train_f_rmse: 0.083715
val_e/atom_mae: 0.002295
2025-02-04 01:43:52.937 INFO: val_e/atom_mae: 0.002295
val_e/atom_rmse: 0.003484
2025-02-04 01:43:52.937 INFO: val_e/atom_rmse: 0.003484
val_f_mae: 0.046093
2025-02-04 01:43:52.938 INFO: val_f_mae: 0.046093
val_f_rmse: 0.076447
2025-02-04 01:43:52.938 INFO: val_f_rmse: 0.076447
##### Step: 33 Learning rate: 0.005 #####
2025-02-04 01:44:52.693 INFO: ##### Step: 33 Learning rate: 0.005 #####
Epoch 34, Train Loss: 7.8691, Val Loss: 5.8923
2025-02-04 01:44:52.694 INFO: Epoch 34, Train Loss: 7.8691, Val Loss: 5.8923
train_e/atom_mae: 0.005552
2025-02-04 01:44:52.695 INFO: train_e/atom_mae: 0.005552
train_e/atom_rmse: 0.006968
2025-02-04 01:44:52.695 INFO: train_e/atom_rmse: 0.006968
train_f_mae: 0.056226
2025-02-04 01:44:52.697 INFO: train_f_mae: 0.056226
train_f_rmse: 0.087693
2025-02-04 01:44:52.698 INFO: train_f_rmse: 0.087693
val_e/atom_mae: 0.002807
2025-02-04 01:44:52.700 INFO: val_e/atom_mae: 0.002807
val_e/atom_rmse: 0.003813
2025-02-04 01:44:52.700 INFO: val_e/atom_rmse: 0.003813
val_f_mae: 0.046196
2025-02-04 01:44:52.700 INFO: val_f_mae: 0.046196
val_f_rmse: 0.076435
2025-02-04 01:44:52.701 INFO: val_f_rmse: 0.076435
##### Step: 34 Learning rate: 0.005 #####
2025-02-04 01:45:52.405 INFO: ##### Step: 34 Learning rate: 0.005 #####
Epoch 35, Train Loss: 6.9054, Val Loss: 5.7124
2025-02-04 01:45:52.406 INFO: Epoch 35, Train Loss: 6.9054, Val Loss: 5.7124
train_e/atom_mae: 0.008059
2025-02-04 01:45:52.407 INFO: train_e/atom_mae: 0.008059
train_e/atom_rmse: 0.010086
2025-02-04 01:45:52.407 INFO: train_e/atom_rmse: 0.010086
train_f_mae: 0.051765
2025-02-04 01:45:52.409 INFO: train_f_mae: 0.051765
train_f_rmse: 0.080811
2025-02-04 01:45:52.410 INFO: train_f_rmse: 0.080811
val_e/atom_mae: 0.002149
2025-02-04 01:45:52.412 INFO: val_e/atom_mae: 0.002149
val_e/atom_rmse: 0.003368
2025-02-04 01:45:52.412 INFO: val_e/atom_rmse: 0.003368
val_f_mae: 0.045629
2025-02-04 01:45:52.412 INFO: val_f_mae: 0.045629
val_f_rmse: 0.075333
2025-02-04 01:45:52.413 INFO: val_f_rmse: 0.075333
##### Step: 35 Learning rate: 0.005 #####
2025-02-04 01:46:52.185 INFO: ##### Step: 35 Learning rate: 0.005 #####
Epoch 36, Train Loss: 6.6205, Val Loss: 5.6853
2025-02-04 01:46:52.185 INFO: Epoch 36, Train Loss: 6.6205, Val Loss: 5.6853
train_e/atom_mae: 0.006081
2025-02-04 01:46:52.186 INFO: train_e/atom_mae: 0.006081
train_e/atom_rmse: 0.007818
2025-02-04 01:46:52.186 INFO: train_e/atom_rmse: 0.007818
train_f_mae: 0.051316
2025-02-04 01:46:52.189 INFO: train_f_mae: 0.051316
train_f_rmse: 0.079970
2025-02-04 01:46:52.189 INFO: train_f_rmse: 0.079970
val_e/atom_mae: 0.002364
2025-02-04 01:46:52.191 INFO: val_e/atom_mae: 0.002364
val_e/atom_rmse: 0.003353
2025-02-04 01:46:52.192 INFO: val_e/atom_rmse: 0.003353
val_f_mae: 0.045724
2025-02-04 01:46:52.192 INFO: val_f_mae: 0.045724
val_f_rmse: 0.075138
2025-02-04 01:46:52.192 INFO: val_f_rmse: 0.075138
##### Step: 36 Learning rate: 0.005 #####
2025-02-04 01:47:51.921 INFO: ##### Step: 36 Learning rate: 0.005 #####
Epoch 37, Train Loss: 6.8635, Val Loss: 5.7334
2025-02-04 01:47:51.921 INFO: Epoch 37, Train Loss: 6.8635, Val Loss: 5.7334
train_e/atom_mae: 0.006786
2025-02-04 01:47:51.922 INFO: train_e/atom_mae: 0.006786
train_e/atom_rmse: 0.009197
2025-02-04 01:47:51.922 INFO: train_e/atom_rmse: 0.009197
train_f_mae: 0.051573
2025-02-04 01:47:51.925 INFO: train_f_mae: 0.051573
train_f_rmse: 0.080943
2025-02-04 01:47:51.925 INFO: train_f_rmse: 0.080943
val_e/atom_mae: 0.002029
2025-02-04 01:47:51.927 INFO: val_e/atom_mae: 0.002029
val_e/atom_rmse: 0.003326
2025-02-04 01:47:51.927 INFO: val_e/atom_rmse: 0.003326
val_f_mae: 0.045784
2025-02-04 01:47:51.928 INFO: val_f_mae: 0.045784
val_f_rmse: 0.075473
2025-02-04 01:47:51.928 INFO: val_f_rmse: 0.075473
##### Step: 37 Learning rate: 0.005 #####
2025-02-04 01:48:51.638 INFO: ##### Step: 37 Learning rate: 0.005 #####
Epoch 38, Train Loss: 6.5479, Val Loss: 5.7514
2025-02-04 01:48:51.639 INFO: Epoch 38, Train Loss: 6.5479, Val Loss: 5.7514
train_e/atom_mae: 0.005740
2025-02-04 01:48:51.639 INFO: train_e/atom_mae: 0.005740
train_e/atom_rmse: 0.006997
2025-02-04 01:48:51.640 INFO: train_e/atom_rmse: 0.006997
train_f_mae: 0.050976
2025-02-04 01:48:51.642 INFO: train_f_mae: 0.050976
train_f_rmse: 0.079796
2025-02-04 01:48:51.642 INFO: train_f_rmse: 0.079796
val_e/atom_mae: 0.001763
2025-02-04 01:48:51.645 INFO: val_e/atom_mae: 0.001763
val_e/atom_rmse: 0.002670
2025-02-04 01:48:51.645 INFO: val_e/atom_rmse: 0.002670
val_f_mae: 0.045529
2025-02-04 01:48:51.645 INFO: val_f_mae: 0.045529
val_f_rmse: 0.075693
2025-02-04 01:48:51.646 INFO: val_f_rmse: 0.075693
##### Step: 38 Learning rate: 0.005 #####
2025-02-04 01:49:51.391 INFO: ##### Step: 38 Learning rate: 0.005 #####
Epoch 39, Train Loss: 7.2147, Val Loss: 5.9486
2025-02-04 01:49:51.392 INFO: Epoch 39, Train Loss: 7.2147, Val Loss: 5.9486
train_e/atom_mae: 0.007073
2025-02-04 01:49:51.393 INFO: train_e/atom_mae: 0.007073
train_e/atom_rmse: 0.008776
2025-02-04 01:49:51.393 INFO: train_e/atom_rmse: 0.008776
train_f_mae: 0.054291
2025-02-04 01:49:51.396 INFO: train_f_mae: 0.054291
train_f_rmse: 0.083251
2025-02-04 01:49:51.396 INFO: train_f_rmse: 0.083251
val_e/atom_mae: 0.001995
2025-02-04 01:49:51.398 INFO: val_e/atom_mae: 0.001995
val_e/atom_rmse: 0.003113
2025-02-04 01:49:51.398 INFO: val_e/atom_rmse: 0.003113
val_f_mae: 0.045661
2025-02-04 01:49:51.399 INFO: val_f_mae: 0.045661
val_f_rmse: 0.076935
2025-02-04 01:49:51.399 INFO: val_f_rmse: 0.076935
##### Step: 39 Learning rate: 0.005 #####
2025-02-04 01:50:51.192 INFO: ##### Step: 39 Learning rate: 0.005 #####
Epoch 40, Train Loss: 7.5545, Val Loss: 5.8451
2025-02-04 01:50:51.192 INFO: Epoch 40, Train Loss: 7.5545, Val Loss: 5.8451
train_e/atom_mae: 0.007766
2025-02-04 01:50:51.193 INFO: train_e/atom_mae: 0.007766
train_e/atom_rmse: 0.009814
2025-02-04 01:50:51.193 INFO: train_e/atom_rmse: 0.009814
train_f_mae: 0.055778
2025-02-04 01:50:51.196 INFO: train_f_mae: 0.055778
train_f_rmse: 0.084850
2025-02-04 01:50:51.196 INFO: train_f_rmse: 0.084850
val_e/atom_mae: 0.002330
2025-02-04 01:50:51.198 INFO: val_e/atom_mae: 0.002330
val_e/atom_rmse: 0.003543
2025-02-04 01:50:51.198 INFO: val_e/atom_rmse: 0.003543
val_f_mae: 0.045820
2025-02-04 01:50:51.199 INFO: val_f_mae: 0.045820
val_f_rmse: 0.076182
2025-02-04 01:50:51.199 INFO: val_f_rmse: 0.076182
2025-02-04 01:50:51.351 INFO: training
##### Step: 0 Learning rate: 0.002 #####
2025-02-04 01:51:50.661 INFO: ##### Step: 0 Learning rate: 0.002 #####
Epoch 1, Train Loss: 5.8151, Val Loss: 5.7731
2025-02-04 01:51:50.661 INFO: Epoch 1, Train Loss: 5.8151, Val Loss: 5.7731
train_e/atom_mae: 0.004337
2025-02-04 01:51:50.662 INFO: train_e/atom_mae: 0.004337
train_e/atom_rmse: 0.006381
2025-02-04 01:51:50.662 INFO: train_e/atom_rmse: 0.006381
train_f_mae: 0.047563
2025-02-04 01:51:50.665 INFO: train_f_mae: 0.047563
train_f_rmse: 0.075267
2025-02-04 01:51:50.665 INFO: train_f_rmse: 0.075267
val_e/atom_mae: 0.002232
2025-02-04 01:51:50.667 INFO: val_e/atom_mae: 0.002232
val_e/atom_rmse: 0.002885
2025-02-04 01:51:50.668 INFO: val_e/atom_rmse: 0.002885
val_f_mae: 0.046085
2025-02-04 01:51:50.668 INFO: val_f_mae: 0.046085
val_f_rmse: 0.075797
2025-02-04 01:51:50.668 INFO: val_f_rmse: 0.075797
##### Step: 1 Learning rate: 0.004 #####
2025-02-04 01:52:49.985 INFO: ##### Step: 1 Learning rate: 0.004 #####
Epoch 2, Train Loss: 6.0823, Val Loss: 6.3655
2025-02-04 01:52:49.985 INFO: Epoch 2, Train Loss: 6.0823, Val Loss: 6.3655
train_e/atom_mae: 0.007962
2025-02-04 01:52:49.986 INFO: train_e/atom_mae: 0.007962
train_e/atom_rmse: 0.010205
2025-02-04 01:52:49.987 INFO: train_e/atom_rmse: 0.010205
train_f_mae: 0.047894
2025-02-04 01:52:49.989 INFO: train_f_mae: 0.047894
train_f_rmse: 0.075487
2025-02-04 01:52:49.989 INFO: train_f_rmse: 0.075487
val_e/atom_mae: 0.002714
2025-02-04 01:52:49.991 INFO: val_e/atom_mae: 0.002714
val_e/atom_rmse: 0.003048
2025-02-04 01:52:49.992 INFO: val_e/atom_rmse: 0.003048
val_f_mae: 0.050150
2025-02-04 01:52:49.992 INFO: val_f_mae: 0.050150
val_f_rmse: 0.079590
2025-02-04 01:52:49.992 INFO: val_f_rmse: 0.079590
##### Step: 2 Learning rate: 0.006 #####
2025-02-04 01:54:05.177 INFO: ##### Step: 2 Learning rate: 0.006 #####
Epoch 3, Train Loss: 9.8668, Val Loss: 6.7964
2025-02-04 01:54:05.177 INFO: Epoch 3, Train Loss: 9.8668, Val Loss: 6.7964
train_e/atom_mae: 0.005862
2025-02-04 01:54:05.333 INFO: train_e/atom_mae: 0.005862
train_e/atom_rmse: 0.007397
2025-02-04 01:54:05.582 INFO: train_e/atom_rmse: 0.007397
train_f_mae: 0.065490
2025-02-04 01:54:05.585 INFO: train_f_mae: 0.065490
train_f_rmse: 0.098311
2025-02-04 01:54:05.585 INFO: train_f_rmse: 0.098311
val_e/atom_mae: 0.008544
2025-02-04 01:54:05.587 INFO: val_e/atom_mae: 0.008544
val_e/atom_rmse: 0.009321
2025-02-04 01:54:05.587 INFO: val_e/atom_rmse: 0.009321
val_f_mae: 0.050480
2025-02-04 01:54:05.588 INFO: val_f_mae: 0.050480
val_f_rmse: 0.080495
2025-02-04 01:54:05.588 INFO: val_f_rmse: 0.080495
##### Step: 3 Learning rate: 0.008 #####
2025-02-04 01:55:05.031 INFO: ##### Step: 3 Learning rate: 0.008 #####
Epoch 4, Train Loss: 8.2680, Val Loss: 10.9761
2025-02-04 01:55:05.032 INFO: Epoch 4, Train Loss: 8.2680, Val Loss: 10.9761
train_e/atom_mae: 0.007692
2025-02-04 01:55:05.033 INFO: train_e/atom_mae: 0.007692
train_e/atom_rmse: 0.009552
2025-02-04 01:55:05.033 INFO: train_e/atom_rmse: 0.009552
train_f_mae: 0.057759
2025-02-04 01:55:05.036 INFO: train_f_mae: 0.057759
train_f_rmse: 0.089060
2025-02-04 01:55:05.036 INFO: train_f_rmse: 0.089060
val_e/atom_mae: 0.015144
2025-02-04 01:55:05.038 INFO: val_e/atom_mae: 0.015144
val_e/atom_rmse: 0.015610
2025-02-04 01:55:05.038 INFO: val_e/atom_rmse: 0.015610
val_f_mae: 0.067363
2025-02-04 01:55:05.039 INFO: val_f_mae: 0.067363
val_f_rmse: 0.100371
2025-02-04 01:55:05.039 INFO: val_f_rmse: 0.100371
##### Step: 4 Learning rate: 0.01 #####
2025-02-04 01:56:04.364 INFO: ##### Step: 4 Learning rate: 0.01 #####
Epoch 5, Train Loss: 10.0961, Val Loss: 8.5598
2025-02-04 01:56:04.364 INFO: Epoch 5, Train Loss: 10.0961, Val Loss: 8.5598
train_e/atom_mae: 0.012098
2025-02-04 01:56:04.365 INFO: train_e/atom_mae: 0.012098
train_e/atom_rmse: 0.014681
2025-02-04 01:56:04.365 INFO: train_e/atom_rmse: 0.014681
train_f_mae: 0.061720
2025-02-04 01:56:04.368 INFO: train_f_mae: 0.061720
train_f_rmse: 0.096445
2025-02-04 01:56:04.368 INFO: train_f_rmse: 0.096445
val_e/atom_mae: 0.009835
2025-02-04 01:56:04.370 INFO: val_e/atom_mae: 0.009835
val_e/atom_rmse: 0.010385
2025-02-04 01:56:04.371 INFO: val_e/atom_rmse: 0.010385
val_f_mae: 0.059082
2025-02-04 01:56:04.371 INFO: val_f_mae: 0.059082
val_f_rmse: 0.090365
2025-02-04 01:56:04.371 INFO: val_f_rmse: 0.090365
##### Step: 5 Learning rate: 0.01 #####
2025-02-04 01:57:03.794 INFO: ##### Step: 5 Learning rate: 0.01 #####
Epoch 6, Train Loss: 13.8676, Val Loss: 10.1539
2025-02-04 01:57:03.794 INFO: Epoch 6, Train Loss: 13.8676, Val Loss: 10.1539
train_e/atom_mae: 0.014578
2025-02-04 01:57:03.795 INFO: train_e/atom_mae: 0.014578
train_e/atom_rmse: 0.017556
2025-02-04 01:57:03.795 INFO: train_e/atom_rmse: 0.017556
train_f_mae: 0.075169
2025-02-04 01:57:03.798 INFO: train_f_mae: 0.075169
train_f_rmse: 0.112834
2025-02-04 01:57:03.798 INFO: train_f_rmse: 0.112834
val_e/atom_mae: 0.017016
2025-02-04 01:57:03.800 INFO: val_e/atom_mae: 0.017016
val_e/atom_rmse: 0.017326
2025-02-04 01:57:03.801 INFO: val_e/atom_rmse: 0.017326
val_f_mae: 0.061878
2025-02-04 01:57:03.801 INFO: val_f_mae: 0.061878
val_f_rmse: 0.095103
2025-02-04 01:57:03.801 INFO: val_f_rmse: 0.095103
##### Step: 6 Learning rate: 0.01 #####
2025-02-04 01:58:03.175 INFO: ##### Step: 6 Learning rate: 0.01 #####
Epoch 7, Train Loss: 11.4301, Val Loss: 15.1291
2025-02-04 01:58:03.175 INFO: Epoch 7, Train Loss: 11.4301, Val Loss: 15.1291
train_e/atom_mae: 0.010317
2025-02-04 01:58:03.176 INFO: train_e/atom_mae: 0.010317
train_e/atom_rmse: 0.012824
2025-02-04 01:58:03.176 INFO: train_e/atom_rmse: 0.012824
train_f_mae: 0.069316
2025-02-04 01:58:03.179 INFO: train_f_mae: 0.069316
train_f_rmse: 0.104038
2025-02-04 01:58:03.179 INFO: train_f_rmse: 0.104038
val_e/atom_mae: 0.019901
2025-02-04 01:58:03.181 INFO: val_e/atom_mae: 0.019901
val_e/atom_rmse: 0.020320
2025-02-04 01:58:03.182 INFO: val_e/atom_rmse: 0.020320
val_f_mae: 0.083083
2025-02-04 01:58:03.182 INFO: val_f_mae: 0.083083
val_f_rmse: 0.116651
2025-02-04 01:58:03.182 INFO: val_f_rmse: 0.116651
##### Step: 7 Learning rate: 0.01 #####
2025-02-04 01:59:02.558 INFO: ##### Step: 7 Learning rate: 0.01 #####
Epoch 8, Train Loss: 11.0017, Val Loss: 9.0144
2025-02-04 01:59:02.558 INFO: Epoch 8, Train Loss: 11.0017, Val Loss: 9.0144
train_e/atom_mae: 0.012276
2025-02-04 01:59:02.559 INFO: train_e/atom_mae: 0.012276
train_e/atom_rmse: 0.015694
2025-02-04 01:59:02.559 INFO: train_e/atom_rmse: 0.015694
train_f_mae: 0.066902
2025-02-04 01:59:02.562 INFO: train_f_mae: 0.066902
train_f_rmse: 0.100468
2025-02-04 01:59:02.562 INFO: train_f_rmse: 0.100468
val_e/atom_mae: 0.011513
2025-02-04 01:59:02.564 INFO: val_e/atom_mae: 0.011513
val_e/atom_rmse: 0.012108
2025-02-04 01:59:02.565 INFO: val_e/atom_rmse: 0.012108
val_f_mae: 0.061176
2025-02-04 01:59:02.565 INFO: val_f_mae: 0.061176
val_f_rmse: 0.092081
2025-02-04 01:59:02.565 INFO: val_f_rmse: 0.092081
##### Step: 8 Learning rate: 0.01 #####
2025-02-04 02:00:01.955 INFO: ##### Step: 8 Learning rate: 0.01 #####
Epoch 9, Train Loss: 9.1194, Val Loss: 9.7779
2025-02-04 02:00:01.956 INFO: Epoch 9, Train Loss: 9.1194, Val Loss: 9.7779
train_e/atom_mae: 0.006443
2025-02-04 02:00:01.957 INFO: train_e/atom_mae: 0.006443
train_e/atom_rmse: 0.008082
2025-02-04 02:00:01.957 INFO: train_e/atom_rmse: 0.008082
train_f_mae: 0.061634
2025-02-04 02:00:01.960 INFO: train_f_mae: 0.061634
train_f_rmse: 0.094227
2025-02-04 02:00:01.960 INFO: train_f_rmse: 0.094227
val_e/atom_mae: 0.006120
2025-02-04 02:00:01.962 INFO: val_e/atom_mae: 0.006120
val_e/atom_rmse: 0.007242
2025-02-04 02:00:01.962 INFO: val_e/atom_rmse: 0.007242
val_f_mae: 0.065958
2025-02-04 02:00:01.963 INFO: val_f_mae: 0.065958
val_f_rmse: 0.097958
2025-02-04 02:00:01.963 INFO: val_f_rmse: 0.097958
##### Step: 9 Learning rate: 0.01 #####
2025-02-04 02:01:01.393 INFO: ##### Step: 9 Learning rate: 0.01 #####
Epoch 10, Train Loss: 8.9165, Val Loss: 7.4465
2025-02-04 02:01:01.394 INFO: Epoch 10, Train Loss: 8.9165, Val Loss: 7.4465
train_e/atom_mae: 0.010417
2025-02-04 02:01:01.395 INFO: train_e/atom_mae: 0.010417
train_e/atom_rmse: 0.013238
2025-02-04 02:01:01.395 INFO: train_e/atom_rmse: 0.013238
train_f_mae: 0.059932
2025-02-04 02:01:01.398 INFO: train_f_mae: 0.059932
train_f_rmse: 0.090942
2025-02-04 02:01:01.398 INFO: train_f_rmse: 0.090942
val_e/atom_mae: 0.002081
2025-02-04 02:01:01.400 INFO: val_e/atom_mae: 0.002081
val_e/atom_rmse: 0.002746
2025-02-04 02:01:01.400 INFO: val_e/atom_rmse: 0.002746
val_f_mae: 0.054705
2025-02-04 02:01:01.401 INFO: val_f_mae: 0.054705
val_f_rmse: 0.086164
2025-02-04 02:01:01.401 INFO: val_f_rmse: 0.086164
##### Step: 10 Learning rate: 0.01 #####
2025-02-04 02:02:01.588 INFO: ##### Step: 10 Learning rate: 0.01 #####
Epoch 11, Train Loss: 9.3697, Val Loss: 5.9737
2025-02-04 02:02:01.588 INFO: Epoch 11, Train Loss: 9.3697, Val Loss: 5.9737
train_e/atom_mae: 0.008611
2025-02-04 02:02:01.589 INFO: train_e/atom_mae: 0.008611
train_e/atom_rmse: 0.011533
2025-02-04 02:02:01.589 INFO: train_e/atom_rmse: 0.011533
train_f_mae: 0.062298
2025-02-04 02:02:01.592 INFO: train_f_mae: 0.062298
train_f_rmse: 0.094230
2025-02-04 02:02:01.592 INFO: train_f_rmse: 0.094230
val_e/atom_mae: 0.004220
2025-02-04 02:02:01.594 INFO: val_e/atom_mae: 0.004220
val_e/atom_rmse: 0.004963
2025-02-04 02:02:01.595 INFO: val_e/atom_rmse: 0.004963
val_f_mae: 0.047139
2025-02-04 02:02:01.595 INFO: val_f_mae: 0.047139
val_f_rmse: 0.076718
2025-02-04 02:02:01.595 INFO: val_f_rmse: 0.076718
##### Step: 11 Learning rate: 0.01 #####
2025-02-04 02:03:01.481 INFO: ##### Step: 11 Learning rate: 0.01 #####
Epoch 12, Train Loss: 9.9408, Val Loss: 5.9988
2025-02-04 02:03:01.482 INFO: Epoch 12, Train Loss: 9.9408, Val Loss: 5.9988
train_e/atom_mae: 0.010200
2025-02-04 02:03:01.483 INFO: train_e/atom_mae: 0.010200
train_e/atom_rmse: 0.012833
2025-02-04 02:03:01.483 INFO: train_e/atom_rmse: 0.012833
train_f_mae: 0.063334
2025-02-04 02:03:01.486 INFO: train_f_mae: 0.063334
train_f_rmse: 0.096611
2025-02-04 02:03:01.486 INFO: train_f_rmse: 0.096611
val_e/atom_mae: 0.003272
2025-02-04 02:03:01.488 INFO: val_e/atom_mae: 0.003272
val_e/atom_rmse: 0.004275
2025-02-04 02:03:01.488 INFO: val_e/atom_rmse: 0.004275
val_f_mae: 0.046936
2025-02-04 02:03:01.489 INFO: val_f_mae: 0.046936
val_f_rmse: 0.077040
2025-02-04 02:03:01.489 INFO: val_f_rmse: 0.077040
##### Step: 12 Learning rate: 0.01 #####
2025-02-04 02:04:01.327 INFO: ##### Step: 12 Learning rate: 0.01 #####
Epoch 13, Train Loss: 13.3302, Val Loss: 7.1320
2025-02-04 02:04:01.328 INFO: Epoch 13, Train Loss: 13.3302, Val Loss: 7.1320
train_e/atom_mae: 0.016492
2025-02-04 02:04:01.328 INFO: train_e/atom_mae: 0.016492
train_e/atom_rmse: 0.019521
2025-02-04 02:04:01.329 INFO: train_e/atom_rmse: 0.019521
train_f_mae: 0.072107
2025-02-04 02:04:01.331 INFO: train_f_mae: 0.072107
train_f_rmse: 0.109204
2025-02-04 02:04:01.331 INFO: train_f_rmse: 0.109204
val_e/atom_mae: 0.011087
2025-02-04 02:04:01.333 INFO: val_e/atom_mae: 0.011087
val_e/atom_rmse: 0.011573
2025-02-04 02:04:01.334 INFO: val_e/atom_rmse: 0.011573
val_f_mae: 0.051242
2025-02-04 02:04:01.334 INFO: val_f_mae: 0.051242
val_f_rmse: 0.081504
2025-02-04 02:04:01.335 INFO: val_f_rmse: 0.081504
##### Step: 13 Learning rate: 0.01 #####
2025-02-04 02:05:01.119 INFO: ##### Step: 13 Learning rate: 0.01 #####
Epoch 14, Train Loss: 9.7838, Val Loss: 6.1704
2025-02-04 02:05:01.119 INFO: Epoch 14, Train Loss: 9.7838, Val Loss: 6.1704
train_e/atom_mae: 0.007319
2025-02-04 02:05:01.120 INFO: train_e/atom_mae: 0.007319
train_e/atom_rmse: 0.009824
2025-02-04 02:05:01.120 INFO: train_e/atom_rmse: 0.009824
train_f_mae: 0.064489
2025-02-04 02:05:01.123 INFO: train_f_mae: 0.064489
train_f_rmse: 0.097098
2025-02-04 02:05:01.123 INFO: train_f_rmse: 0.097098
val_e/atom_mae: 0.004401
2025-02-04 02:05:01.125 INFO: val_e/atom_mae: 0.004401
val_e/atom_rmse: 0.005263
2025-02-04 02:05:01.126 INFO: val_e/atom_rmse: 0.005263
val_f_mae: 0.047544
2025-02-04 02:05:01.126 INFO: val_f_mae: 0.047544
val_f_rmse: 0.077933
2025-02-04 02:05:01.126 INFO: val_f_rmse: 0.077933
##### Step: 14 Learning rate: 0.01 #####
2025-02-04 02:06:00.882 INFO: ##### Step: 14 Learning rate: 0.01 #####
Epoch 15, Train Loss: 9.4217, Val Loss: 6.0955
2025-02-04 02:06:00.882 INFO: Epoch 15, Train Loss: 9.4217, Val Loss: 6.0955
train_e/atom_mae: 0.007922
2025-02-04 02:06:00.883 INFO: train_e/atom_mae: 0.007922
train_e/atom_rmse: 0.010558
2025-02-04 02:06:00.883 INFO: train_e/atom_rmse: 0.010558
train_f_mae: 0.062209
2025-02-04 02:06:00.886 INFO: train_f_mae: 0.062209
train_f_rmse: 0.094925
2025-02-04 02:06:00.886 INFO: train_f_rmse: 0.094925
val_e/atom_mae: 0.004527
2025-02-04 02:06:00.888 INFO: val_e/atom_mae: 0.004527
val_e/atom_rmse: 0.005302
2025-02-04 02:06:00.888 INFO: val_e/atom_rmse: 0.005302
val_f_mae: 0.046931
2025-02-04 02:06:00.889 INFO: val_f_mae: 0.046931
val_f_rmse: 0.077441
2025-02-04 02:06:00.889 INFO: val_f_rmse: 0.077441
##### Step: 15 Learning rate: 0.01 #####
2025-02-04 02:07:00.657 INFO: ##### Step: 15 Learning rate: 0.01 #####
Epoch 16, Train Loss: 11.5366, Val Loss: 6.1816
2025-02-04 02:07:00.657 INFO: Epoch 16, Train Loss: 11.5366, Val Loss: 6.1816
train_e/atom_mae: 0.014672
2025-02-04 02:07:00.658 INFO: train_e/atom_mae: 0.014672
train_e/atom_rmse: 0.019217
2025-02-04 02:07:00.658 INFO: train_e/atom_rmse: 0.019217
train_f_mae: 0.065763
2025-02-04 02:07:00.661 INFO: train_f_mae: 0.065763
train_f_rmse: 0.100873
2025-02-04 02:07:00.661 INFO: train_f_rmse: 0.100873
val_e/atom_mae: 0.007075
2025-02-04 02:07:00.663 INFO: val_e/atom_mae: 0.007075
val_e/atom_rmse: 0.007659
2025-02-04 02:07:00.664 INFO: val_e/atom_rmse: 0.007659
val_f_mae: 0.047493
2025-02-04 02:07:00.664 INFO: val_f_mae: 0.047493
val_f_rmse: 0.077276
2025-02-04 02:07:00.664 INFO: val_f_rmse: 0.077276
##### Step: 16 Learning rate: 0.01 #####
2025-02-04 02:08:00.445 INFO: ##### Step: 16 Learning rate: 0.01 #####
Epoch 17, Train Loss: 9.7008, Val Loss: 6.0864
2025-02-04 02:08:00.446 INFO: Epoch 17, Train Loss: 9.7008, Val Loss: 6.0864
train_e/atom_mae: 0.012113
2025-02-04 02:08:00.447 INFO: train_e/atom_mae: 0.012113
train_e/atom_rmse: 0.015616
2025-02-04 02:08:00.447 INFO: train_e/atom_rmse: 0.015616
train_f_mae: 0.061556
2025-02-04 02:08:00.450 INFO: train_f_mae: 0.061556
train_f_rmse: 0.093818
2025-02-04 02:08:00.450 INFO: train_f_rmse: 0.093818
val_e/atom_mae: 0.005568
2025-02-04 02:08:00.452 INFO: val_e/atom_mae: 0.005568
val_e/atom_rmse: 0.006297
2025-02-04 02:08:00.452 INFO: val_e/atom_rmse: 0.006297
val_f_mae: 0.047224
2025-02-04 02:08:00.453 INFO: val_f_mae: 0.047224
val_f_rmse: 0.077100
2025-02-04 02:08:00.453 INFO: val_f_rmse: 0.077100
##### Step: 17 Learning rate: 0.01 #####
2025-02-04 02:09:00.238 INFO: ##### Step: 17 Learning rate: 0.01 #####
Epoch 18, Train Loss: 9.0764, Val Loss: 5.9873
2025-02-04 02:09:00.238 INFO: Epoch 18, Train Loss: 9.0764, Val Loss: 5.9873
train_e/atom_mae: 0.008674
2025-02-04 02:09:00.239 INFO: train_e/atom_mae: 0.008674
train_e/atom_rmse: 0.011379
2025-02-04 02:09:00.239 INFO: train_e/atom_rmse: 0.011379
train_f_mae: 0.060525
2025-02-04 02:09:00.242 INFO: train_f_mae: 0.060525
train_f_rmse: 0.092731
2025-02-04 02:09:00.242 INFO: train_f_rmse: 0.092731
val_e/atom_mae: 0.002577
2025-02-04 02:09:00.244 INFO: val_e/atom_mae: 0.002577
val_e/atom_rmse: 0.003851
2025-02-04 02:09:00.245 INFO: val_e/atom_rmse: 0.003851
val_f_mae: 0.047203
2025-02-04 02:09:00.245 INFO: val_f_mae: 0.047203
val_f_rmse: 0.077052
2025-02-04 02:09:00.245 INFO: val_f_rmse: 0.077052
##### Step: 18 Learning rate: 0.01 #####
2025-02-04 02:10:00.022 INFO: ##### Step: 18 Learning rate: 0.01 #####
Epoch 19, Train Loss: 8.0985, Val Loss: 5.9229
2025-02-04 02:10:00.023 INFO: Epoch 19, Train Loss: 8.0985, Val Loss: 5.9229
train_e/atom_mae: 0.007429
2025-02-04 02:10:00.023 INFO: train_e/atom_mae: 0.007429
train_e/atom_rmse: 0.009481
2025-02-04 02:10:00.024 INFO: train_e/atom_rmse: 0.009481
train_f_mae: 0.057813
2025-02-04 02:10:00.026 INFO: train_f_mae: 0.057813
train_f_rmse: 0.088131
2025-02-04 02:10:00.026 INFO: train_f_rmse: 0.088131
val_e/atom_mae: 0.003857
2025-02-04 02:10:00.029 INFO: val_e/atom_mae: 0.003857
val_e/atom_rmse: 0.004718
2025-02-04 02:10:00.029 INFO: val_e/atom_rmse: 0.004718
val_f_mae: 0.046863
2025-02-04 02:10:00.029 INFO: val_f_mae: 0.046863
val_f_rmse: 0.076452
2025-02-04 02:10:00.030 INFO: val_f_rmse: 0.076452
##### Step: 19 Learning rate: 0.01 #####
2025-02-04 02:10:59.811 INFO: ##### Step: 19 Learning rate: 0.01 #####
Epoch 20, Train Loss: 9.3892, Val Loss: 6.8633
2025-02-04 02:10:59.811 INFO: Epoch 20, Train Loss: 9.3892, Val Loss: 6.8633
train_e/atom_mae: 0.011681
2025-02-04 02:10:59.812 INFO: train_e/atom_mae: 0.011681
train_e/atom_rmse: 0.014843
2025-02-04 02:10:59.812 INFO: train_e/atom_rmse: 0.014843
train_f_mae: 0.060780
2025-02-04 02:10:59.815 INFO: train_f_mae: 0.060780
train_f_rmse: 0.092612
2025-02-04 02:10:59.815 INFO: train_f_rmse: 0.092612
val_e/atom_mae: 0.012757
2025-02-04 02:10:59.817 INFO: val_e/atom_mae: 0.012757
val_e/atom_rmse: 0.013102
2025-02-04 02:10:59.818 INFO: val_e/atom_rmse: 0.013102
val_f_mae: 0.049358
2025-02-04 02:10:59.818 INFO: val_f_mae: 0.049358
val_f_rmse: 0.078967
2025-02-04 02:10:59.818 INFO: val_f_rmse: 0.078967
##### Step: 20 Learning rate: 0.005 #####
2025-02-04 02:11:59.669 INFO: ##### Step: 20 Learning rate: 0.005 #####
Epoch 21, Train Loss: 8.0691, Val Loss: 5.7664
2025-02-04 02:11:59.669 INFO: Epoch 21, Train Loss: 8.0691, Val Loss: 5.7664
train_e/atom_mae: 0.007356
2025-02-04 02:11:59.670 INFO: train_e/atom_mae: 0.007356
train_e/atom_rmse: 0.009185
2025-02-04 02:11:59.670 INFO: train_e/atom_rmse: 0.009185
train_f_mae: 0.058098
2025-02-04 02:11:59.673 INFO: train_f_mae: 0.058098
train_f_rmse: 0.088080
2025-02-04 02:11:59.673 INFO: train_f_rmse: 0.088080
val_e/atom_mae: 0.002038
2025-02-04 02:11:59.675 INFO: val_e/atom_mae: 0.002038
val_e/atom_rmse: 0.003072
2025-02-04 02:11:59.675 INFO: val_e/atom_rmse: 0.003072
val_f_mae: 0.046051
2025-02-04 02:11:59.676 INFO: val_f_mae: 0.046051
val_f_rmse: 0.075740
2025-02-04 02:11:59.676 INFO: val_f_rmse: 0.075740
##### Step: 21 Learning rate: 0.005 #####
2025-02-04 02:12:59.446 INFO: ##### Step: 21 Learning rate: 0.005 #####
Epoch 22, Train Loss: 6.7160, Val Loss: 5.8279
2025-02-04 02:12:59.447 INFO: Epoch 22, Train Loss: 6.7160, Val Loss: 5.8279
train_e/atom_mae: 0.007318
2025-02-04 02:12:59.447 INFO: train_e/atom_mae: 0.007318
train_e/atom_rmse: 0.009479
2025-02-04 02:12:59.448 INFO: train_e/atom_rmse: 0.009479
train_f_mae: 0.051692
2025-02-04 02:12:59.450 INFO: train_f_mae: 0.051692
train_f_rmse: 0.079905
2025-02-04 02:12:59.450 INFO: train_f_rmse: 0.079905
val_e/atom_mae: 0.001998
2025-02-04 02:12:59.452 INFO: val_e/atom_mae: 0.001998
val_e/atom_rmse: 0.002895
2025-02-04 02:12:59.453 INFO: val_e/atom_rmse: 0.002895
val_f_mae: 0.045834
2025-02-04 02:12:59.453 INFO: val_f_mae: 0.045834
val_f_rmse: 0.076172
2025-02-04 02:12:59.454 INFO: val_f_rmse: 0.076172
##### Step: 22 Learning rate: 0.005 #####
2025-02-04 02:14:39.934 INFO: ##### Step: 22 Learning rate: 0.005 #####
Epoch 23, Train Loss: 6.7911, Val Loss: 5.8200
2025-02-04 02:14:39.934 INFO: Epoch 23, Train Loss: 6.7911, Val Loss: 5.8200
train_e/atom_mae: 0.006569
2025-02-04 02:14:40.269 INFO: train_e/atom_mae: 0.006569
train_e/atom_rmse: 0.008527
2025-02-04 02:14:40.512 INFO: train_e/atom_rmse: 0.008527
train_f_mae: 0.052123
2025-02-04 02:14:40.514 INFO: train_f_mae: 0.052123
train_f_rmse: 0.080765
2025-02-04 02:14:40.514 INFO: train_f_rmse: 0.080765
val_e/atom_mae: 0.002849
2025-02-04 02:14:40.516 INFO: val_e/atom_mae: 0.002849
val_e/atom_rmse: 0.003813
2025-02-04 02:14:40.517 INFO: val_e/atom_rmse: 0.003813
val_f_mae: 0.046083
2025-02-04 02:14:40.517 INFO: val_f_mae: 0.046083
val_f_rmse: 0.075965
2025-02-04 02:14:40.518 INFO: val_f_rmse: 0.075965
##### Step: 23 Learning rate: 0.005 #####
2025-02-04 02:15:40.532 INFO: ##### Step: 23 Learning rate: 0.005 #####
Epoch 24, Train Loss: 8.0375, Val Loss: 5.6538
2025-02-04 02:15:40.532 INFO: Epoch 24, Train Loss: 8.0375, Val Loss: 5.6538
train_e/atom_mae: 0.007304
2025-02-04 02:15:40.533 INFO: train_e/atom_mae: 0.007304
train_e/atom_rmse: 0.008898
2025-02-04 02:15:40.533 INFO: train_e/atom_rmse: 0.008898
train_f_mae: 0.057666
2025-02-04 02:15:40.536 INFO: train_f_mae: 0.057666
train_f_rmse: 0.088009
2025-02-04 02:15:40.536 INFO: train_f_rmse: 0.088009
val_e/atom_mae: 0.002377
2025-02-04 02:15:40.538 INFO: val_e/atom_mae: 0.002377
val_e/atom_rmse: 0.003456
2025-02-04 02:15:40.538 INFO: val_e/atom_rmse: 0.003456
val_f_mae: 0.045514
2025-02-04 02:15:40.539 INFO: val_f_mae: 0.045514
val_f_rmse: 0.074929
2025-02-04 02:15:40.539 INFO: val_f_rmse: 0.074929
##### Step: 24 Learning rate: 0.005 #####
2025-02-04 02:16:41.332 INFO: ##### Step: 24 Learning rate: 0.005 #####
Epoch 25, Train Loss: 7.5572, Val Loss: 5.6885
2025-02-04 02:16:41.332 INFO: Epoch 25, Train Loss: 7.5572, Val Loss: 5.6885
train_e/atom_mae: 0.007402
2025-02-04 02:16:41.333 INFO: train_e/atom_mae: 0.007402
train_e/atom_rmse: 0.010319
2025-02-04 02:16:41.333 INFO: train_e/atom_rmse: 0.010319
train_f_mae: 0.055751
2025-02-04 02:16:41.336 INFO: train_f_mae: 0.055751
train_f_rmse: 0.084644
2025-02-04 02:16:41.336 INFO: train_f_rmse: 0.084644
val_e/atom_mae: 0.001916
2025-02-04 02:16:41.338 INFO: val_e/atom_mae: 0.001916
val_e/atom_rmse: 0.002862
2025-02-04 02:16:41.339 INFO: val_e/atom_rmse: 0.002862
val_f_mae: 0.045465
2025-02-04 02:16:41.339 INFO: val_f_mae: 0.045465
val_f_rmse: 0.075250
2025-02-04 02:16:41.339 INFO: val_f_rmse: 0.075250
##### Step: 25 Learning rate: 0.005 #####
2025-02-04 02:17:41.300 INFO: ##### Step: 25 Learning rate: 0.005 #####
Epoch 26, Train Loss: 6.1710, Val Loss: 5.6594
2025-02-04 02:17:41.300 INFO: Epoch 26, Train Loss: 6.1710, Val Loss: 5.6594
train_e/atom_mae: 0.004282
2025-02-04 02:17:41.301 INFO: train_e/atom_mae: 0.004282
train_e/atom_rmse: 0.005758
2025-02-04 02:17:41.301 INFO: train_e/atom_rmse: 0.005758
train_f_mae: 0.050014
2025-02-04 02:17:41.304 INFO: train_f_mae: 0.050014
train_f_rmse: 0.077774
2025-02-04 02:17:41.304 INFO: train_f_rmse: 0.077774
val_e/atom_mae: 0.002740
2025-02-04 02:17:41.306 INFO: val_e/atom_mae: 0.002740
val_e/atom_rmse: 0.003763
2025-02-04 02:17:41.307 INFO: val_e/atom_rmse: 0.003763
val_f_mae: 0.045304
2025-02-04 02:17:41.307 INFO: val_f_mae: 0.045304
val_f_rmse: 0.074907
2025-02-04 02:17:41.307 INFO: val_f_rmse: 0.074907
##### Step: 26 Learning rate: 0.005 #####
2025-02-04 02:18:41.444 INFO: ##### Step: 26 Learning rate: 0.005 #####
Epoch 27, Train Loss: 7.1907, Val Loss: 5.5885
2025-02-04 02:18:41.445 INFO: Epoch 27, Train Loss: 7.1907, Val Loss: 5.5885
train_e/atom_mae: 0.011380
2025-02-04 02:18:41.446 INFO: train_e/atom_mae: 0.011380
train_e/atom_rmse: 0.014153
2025-02-04 02:18:41.446 INFO: train_e/atom_rmse: 0.014153
train_f_mae: 0.052368
2025-02-04 02:18:41.448 INFO: train_f_mae: 0.052368
train_f_rmse: 0.080326
2025-02-04 02:18:41.449 INFO: train_f_rmse: 0.080326
val_e/atom_mae: 0.002118
2025-02-04 02:18:41.451 INFO: val_e/atom_mae: 0.002118
val_e/atom_rmse: 0.003040
2025-02-04 02:18:41.451 INFO: val_e/atom_rmse: 0.003040
val_f_mae: 0.045157
2025-02-04 02:18:41.451 INFO: val_f_mae: 0.045157
val_f_rmse: 0.074560
2025-02-04 02:18:41.452 INFO: val_f_rmse: 0.074560
##### Step: 27 Learning rate: 0.005 #####
2025-02-04 02:19:43.570 INFO: ##### Step: 27 Learning rate: 0.005 #####
Epoch 28, Train Loss: 6.3897, Val Loss: 5.5713
2025-02-04 02:19:43.571 INFO: Epoch 28, Train Loss: 6.3897, Val Loss: 5.5713
train_e/atom_mae: 0.004901
2025-02-04 02:19:43.572 INFO: train_e/atom_mae: 0.004901
train_e/atom_rmse: 0.006396
2025-02-04 02:19:43.572 INFO: train_e/atom_rmse: 0.006396
train_f_mae: 0.051021
2025-02-04 02:19:43.574 INFO: train_f_mae: 0.051021
train_f_rmse: 0.078987
2025-02-04 02:19:43.575 INFO: train_f_rmse: 0.078987
val_e/atom_mae: 0.002073
2025-02-04 02:19:43.577 INFO: val_e/atom_mae: 0.002073
val_e/atom_rmse: 0.003203
2025-02-04 02:19:43.577 INFO: val_e/atom_rmse: 0.003203
val_f_mae: 0.045121
2025-02-04 02:19:43.578 INFO: val_f_mae: 0.045121
val_f_rmse: 0.074416
2025-02-04 02:19:43.578 INFO: val_f_rmse: 0.074416
##### Step: 28 Learning rate: 0.005 #####
2025-02-04 02:20:43.311 INFO: ##### Step: 28 Learning rate: 0.005 #####
Epoch 29, Train Loss: 6.6744, Val Loss: 5.5884
2025-02-04 02:20:43.311 INFO: Epoch 29, Train Loss: 6.6744, Val Loss: 5.5884
train_e/atom_mae: 0.006680
2025-02-04 02:20:43.312 INFO: train_e/atom_mae: 0.006680
train_e/atom_rmse: 0.008442
2025-02-04 02:20:43.312 INFO: train_e/atom_rmse: 0.008442
train_f_mae: 0.052396
2025-02-04 02:20:43.315 INFO: train_f_mae: 0.052396
train_f_rmse: 0.080073
2025-02-04 02:20:43.315 INFO: train_f_rmse: 0.080073
val_e/atom_mae: 0.001828
2025-02-04 02:20:43.317 INFO: val_e/atom_mae: 0.001828
val_e/atom_rmse: 0.002788
2025-02-04 02:20:43.318 INFO: val_e/atom_rmse: 0.002788
val_f_mae: 0.045148
2025-02-04 02:20:43.318 INFO: val_f_mae: 0.045148
val_f_rmse: 0.074594
2025-02-04 02:20:43.318 INFO: val_f_rmse: 0.074594
##### Step: 29 Learning rate: 0.005 #####
2025-02-04 02:21:43.073 INFO: ##### Step: 29 Learning rate: 0.005 #####
Epoch 30, Train Loss: 7.1782, Val Loss: 5.4819
2025-02-04 02:21:43.073 INFO: Epoch 30, Train Loss: 7.1782, Val Loss: 5.4819
train_e/atom_mae: 0.008610
2025-02-04 02:21:43.074 INFO: train_e/atom_mae: 0.008610
train_e/atom_rmse: 0.011077
2025-02-04 02:21:43.074 INFO: train_e/atom_rmse: 0.011077
train_f_mae: 0.054030
2025-02-04 02:21:43.077 INFO: train_f_mae: 0.054030
train_f_rmse: 0.082012
2025-02-04 02:21:43.077 INFO: train_f_rmse: 0.082012
val_e/atom_mae: 0.001732
2025-02-04 02:21:43.079 INFO: val_e/atom_mae: 0.001732
val_e/atom_rmse: 0.002577
2025-02-04 02:21:43.080 INFO: val_e/atom_rmse: 0.002577
val_f_mae: 0.045114
2025-02-04 02:21:43.080 INFO: val_f_mae: 0.045114
val_f_rmse: 0.073893
2025-02-04 02:21:43.080 INFO: val_f_rmse: 0.073893
##### Step: 30 Learning rate: 0.005 #####
2025-02-04 02:23:11.129 INFO: ##### Step: 30 Learning rate: 0.005 #####
Epoch 31, Train Loss: 6.4822, Val Loss: 5.5229
2025-02-04 02:23:11.130 INFO: Epoch 31, Train Loss: 6.4822, Val Loss: 5.5229
train_e/atom_mae: 0.004941
2025-02-04 02:23:11.364 INFO: train_e/atom_mae: 0.004941
train_e/atom_rmse: 0.006413
2025-02-04 02:23:11.696 INFO: train_e/atom_rmse: 0.006413
train_f_mae: 0.051941
2025-02-04 02:23:11.699 INFO: train_f_mae: 0.051941
train_f_rmse: 0.079565
2025-02-04 02:23:11.699 INFO: train_f_rmse: 0.079565
val_e/atom_mae: 0.001820
2025-02-04 02:23:11.701 INFO: val_e/atom_mae: 0.001820
val_e/atom_rmse: 0.002815
2025-02-04 02:23:11.702 INFO: val_e/atom_rmse: 0.002815
val_f_mae: 0.044911
2025-02-04 02:23:11.702 INFO: val_f_mae: 0.044911
val_f_rmse: 0.074141
2025-02-04 02:23:11.702 INFO: val_f_rmse: 0.074141
##### Step: 31 Learning rate: 0.005 #####
2025-02-04 02:24:11.468 INFO: ##### Step: 31 Learning rate: 0.005 #####
Epoch 32, Train Loss: 6.5376, Val Loss: 5.5492
2025-02-04 02:24:11.469 INFO: Epoch 32, Train Loss: 6.5376, Val Loss: 5.5492
train_e/atom_mae: 0.007237
2025-02-04 02:24:11.470 INFO: train_e/atom_mae: 0.007237
train_e/atom_rmse: 0.008997
2025-02-04 02:24:11.470 INFO: train_e/atom_rmse: 0.008997
train_f_mae: 0.051710
2025-02-04 02:24:11.472 INFO: train_f_mae: 0.051710
train_f_rmse: 0.078988
2025-02-04 02:24:11.473 INFO: train_f_rmse: 0.078988
val_e/atom_mae: 0.001682
2025-02-04 02:24:11.475 INFO: val_e/atom_mae: 0.001682
val_e/atom_rmse: 0.002648
2025-02-04 02:24:11.475 INFO: val_e/atom_rmse: 0.002648
val_f_mae: 0.044895
2025-02-04 02:24:11.475 INFO: val_f_mae: 0.044895
val_f_rmse: 0.074351
2025-02-04 02:24:11.476 INFO: val_f_rmse: 0.074351
##### Step: 32 Learning rate: 0.005 #####
2025-02-04 02:25:11.205 INFO: ##### Step: 32 Learning rate: 0.005 #####
Epoch 33, Train Loss: 6.5425, Val Loss: 5.5596
2025-02-04 02:25:11.206 INFO: Epoch 33, Train Loss: 6.5425, Val Loss: 5.5596
train_e/atom_mae: 0.007100
2025-02-04 02:25:11.206 INFO: train_e/atom_mae: 0.007100
train_e/atom_rmse: 0.008701
2025-02-04 02:25:11.207 INFO: train_e/atom_rmse: 0.008701
train_f_mae: 0.051381
2025-02-04 02:25:11.209 INFO: train_f_mae: 0.051381
train_f_rmse: 0.079142
2025-02-04 02:25:11.209 INFO: train_f_rmse: 0.079142
val_e/atom_mae: 0.001710
2025-02-04 02:25:11.212 INFO: val_e/atom_mae: 0.001710
val_e/atom_rmse: 0.002868
2025-02-04 02:25:11.212 INFO: val_e/atom_rmse: 0.002868
val_f_mae: 0.044823
2025-02-04 02:25:11.212 INFO: val_f_mae: 0.044823
val_f_rmse: 0.074389
2025-02-04 02:25:11.213 INFO: val_f_rmse: 0.074389
##### Step: 33 Learning rate: 0.005 #####
2025-02-04 02:26:10.964 INFO: ##### Step: 33 Learning rate: 0.005 #####
Epoch 34, Train Loss: 6.0652, Val Loss: 5.5122
2025-02-04 02:26:10.965 INFO: Epoch 34, Train Loss: 6.0652, Val Loss: 5.5122
train_e/atom_mae: 0.006473
2025-02-04 02:26:10.966 INFO: train_e/atom_mae: 0.006473
train_e/atom_rmse: 0.008176
2025-02-04 02:26:10.966 INFO: train_e/atom_rmse: 0.008176
train_f_mae: 0.049068
2025-02-04 02:26:10.969 INFO: train_f_mae: 0.049068
train_f_rmse: 0.076281
2025-02-04 02:26:10.969 INFO: train_f_rmse: 0.076281
val_e/atom_mae: 0.001898
2025-02-04 02:26:10.971 INFO: val_e/atom_mae: 0.001898
val_e/atom_rmse: 0.003039
2025-02-04 02:26:10.971 INFO: val_e/atom_rmse: 0.003039
val_f_mae: 0.044825
2025-02-04 02:26:10.972 INFO: val_f_mae: 0.044825
val_f_rmse: 0.074044
2025-02-04 02:26:10.972 INFO: val_f_rmse: 0.074044
##### Step: 34 Learning rate: 0.005 #####
2025-02-04 02:27:10.721 INFO: ##### Step: 34 Learning rate: 0.005 #####
Epoch 35, Train Loss: 6.1078, Val Loss: 5.4572
2025-02-04 02:27:10.722 INFO: Epoch 35, Train Loss: 6.1078, Val Loss: 5.4572
train_e/atom_mae: 0.005680
2025-02-04 02:27:10.723 INFO: train_e/atom_mae: 0.005680
train_e/atom_rmse: 0.007077
2025-02-04 02:27:10.723 INFO: train_e/atom_rmse: 0.007077
train_f_mae: 0.049603
2025-02-04 02:27:10.725 INFO: train_f_mae: 0.049603
train_f_rmse: 0.076962
2025-02-04 02:27:10.726 INFO: train_f_rmse: 0.076962
val_e/atom_mae: 0.002365
2025-02-04 02:27:10.728 INFO: val_e/atom_mae: 0.002365
val_e/atom_rmse: 0.003354
2025-02-04 02:27:10.728 INFO: val_e/atom_rmse: 0.003354
val_f_mae: 0.044647
2025-02-04 02:27:10.728 INFO: val_f_mae: 0.044647
val_f_rmse: 0.073621
2025-02-04 02:27:10.729 INFO: val_f_rmse: 0.073621
##### Step: 35 Learning rate: 0.005 #####
2025-02-04 02:28:10.690 INFO: ##### Step: 35 Learning rate: 0.005 #####
Epoch 36, Train Loss: 6.2595, Val Loss: 5.4800
2025-02-04 02:28:10.690 INFO: Epoch 36, Train Loss: 6.2595, Val Loss: 5.4800
train_e/atom_mae: 0.005411
2025-02-04 02:28:10.691 INFO: train_e/atom_mae: 0.005411
train_e/atom_rmse: 0.006846
2025-02-04 02:28:10.692 INFO: train_e/atom_rmse: 0.006846
train_f_mae: 0.050672
2025-02-04 02:28:10.694 INFO: train_f_mae: 0.050672
train_f_rmse: 0.078017
2025-02-04 02:28:10.694 INFO: train_f_rmse: 0.078017
val_e/atom_mae: 0.002001
2025-02-04 02:28:10.696 INFO: val_e/atom_mae: 0.002001
val_e/atom_rmse: 0.002942
2025-02-04 02:28:10.697 INFO: val_e/atom_rmse: 0.002942
val_f_mae: 0.044482
2025-02-04 02:28:10.697 INFO: val_f_mae: 0.044482
val_f_rmse: 0.073833
2025-02-04 02:28:10.697 INFO: val_f_rmse: 0.073833
##### Step: 36 Learning rate: 0.005 #####
2025-02-04 02:29:10.408 INFO: ##### Step: 36 Learning rate: 0.005 #####
Epoch 37, Train Loss: 5.9857, Val Loss: 5.4742
2025-02-04 02:29:10.409 INFO: Epoch 37, Train Loss: 5.9857, Val Loss: 5.4742
train_e/atom_mae: 0.004728
2025-02-04 02:29:10.410 INFO: train_e/atom_mae: 0.004728
train_e/atom_rmse: 0.006051
2025-02-04 02:29:10.410 INFO: train_e/atom_rmse: 0.006051
train_f_mae: 0.049535
2025-02-04 02:29:10.412 INFO: train_f_mae: 0.049535
train_f_rmse: 0.076490
2025-02-04 02:29:10.413 INFO: train_f_rmse: 0.076490
val_e/atom_mae: 0.001779
2025-02-04 02:29:10.415 INFO: val_e/atom_mae: 0.001779
val_e/atom_rmse: 0.002779
2025-02-04 02:29:10.415 INFO: val_e/atom_rmse: 0.002779
val_f_mae: 0.044565
2025-02-04 02:29:10.416 INFO: val_f_mae: 0.044565
val_f_rmse: 0.073821
2025-02-04 02:29:10.416 INFO: val_f_rmse: 0.073821
##### Step: 37 Learning rate: 0.005 #####
2025-02-04 02:30:10.109 INFO: ##### Step: 37 Learning rate: 0.005 #####
Epoch 38, Train Loss: 5.8341, Val Loss: 5.4025
2025-02-04 02:30:10.110 INFO: Epoch 38, Train Loss: 5.8341, Val Loss: 5.4025
train_e/atom_mae: 0.005119
2025-02-04 02:30:10.111 INFO: train_e/atom_mae: 0.005119
train_e/atom_rmse: 0.006553
2025-02-04 02:30:10.111 INFO: train_e/atom_rmse: 0.006553
train_f_mae: 0.048449
2025-02-04 02:30:10.113 INFO: train_f_mae: 0.048449
train_f_rmse: 0.075338
2025-02-04 02:30:10.114 INFO: train_f_rmse: 0.075338
val_e/atom_mae: 0.001779
2025-02-04 02:30:10.116 INFO: val_e/atom_mae: 0.001779
val_e/atom_rmse: 0.002696
2025-02-04 02:30:10.116 INFO: val_e/atom_rmse: 0.002696
val_f_mae: 0.044412
2025-02-04 02:30:10.116 INFO: val_f_mae: 0.044412
val_f_rmse: 0.073342
2025-02-04 02:30:10.117 INFO: val_f_rmse: 0.073342
##### Step: 38 Learning rate: 0.005 #####
2025-02-04 02:31:09.837 INFO: ##### Step: 38 Learning rate: 0.005 #####
Epoch 39, Train Loss: 6.3399, Val Loss: 5.5341
2025-02-04 02:31:09.837 INFO: Epoch 39, Train Loss: 6.3399, Val Loss: 5.5341
train_e/atom_mae: 0.007466
2025-02-04 02:31:09.838 INFO: train_e/atom_mae: 0.007466
train_e/atom_rmse: 0.009656
2025-02-04 02:31:09.838 INFO: train_e/atom_rmse: 0.009656
train_f_mae: 0.050067
2025-02-04 02:31:09.841 INFO: train_f_mae: 0.050067
train_f_rmse: 0.077435
2025-02-04 02:31:09.841 INFO: train_f_rmse: 0.077435
val_e/atom_mae: 0.003588
2025-02-04 02:31:09.843 INFO: val_e/atom_mae: 0.003588
val_e/atom_rmse: 0.004361
2025-02-04 02:31:09.844 INFO: val_e/atom_rmse: 0.004361
val_f_mae: 0.044763
2025-02-04 02:31:09.844 INFO: val_f_mae: 0.044763
val_f_rmse: 0.073953
2025-02-04 02:31:09.844 INFO: val_f_rmse: 0.073953
##### Step: 39 Learning rate: 0.005 #####
2025-02-04 02:32:09.539 INFO: ##### Step: 39 Learning rate: 0.005 #####
Epoch 40, Train Loss: 6.1567, Val Loss: 5.4812
2025-02-04 02:32:09.540 INFO: Epoch 40, Train Loss: 6.1567, Val Loss: 5.4812
train_e/atom_mae: 0.005731
2025-02-04 02:32:09.540 INFO: train_e/atom_mae: 0.005731
train_e/atom_rmse: 0.007178
2025-02-04 02:32:09.541 INFO: train_e/atom_rmse: 0.007178
train_f_mae: 0.050402
2025-02-04 02:32:09.543 INFO: train_f_mae: 0.050402
train_f_rmse: 0.077245
2025-02-04 02:32:09.543 INFO: train_f_rmse: 0.077245
val_e/atom_mae: 0.001749
2025-02-04 02:32:09.546 INFO: val_e/atom_mae: 0.001749
val_e/atom_rmse: 0.002855
2025-02-04 02:32:09.546 INFO: val_e/atom_rmse: 0.002855
val_f_mae: 0.044599
2025-02-04 02:32:09.546 INFO: val_f_mae: 0.044599
val_f_rmse: 0.073856
2025-02-04 02:32:09.547 INFO: val_f_rmse: 0.073856
2025-02-04 02:32:09.783 INFO: training
##### Step: 0 Learning rate: 0.002 #####
2025-02-04 02:33:09.112 INFO: ##### Step: 0 Learning rate: 0.002 #####
Epoch 1, Train Loss: 5.4537, Val Loss: 6.1815
2025-02-04 02:33:09.112 INFO: Epoch 1, Train Loss: 5.4537, Val Loss: 6.1815
train_e/atom_mae: 0.006831
2025-02-04 02:33:09.113 INFO: train_e/atom_mae: 0.006831
train_e/atom_rmse: 0.008286
2025-02-04 02:33:09.113 INFO: train_e/atom_rmse: 0.008286
train_f_mae: 0.045630
2025-02-04 02:33:09.116 INFO: train_f_mae: 0.045630
train_f_rmse: 0.072115
2025-02-04 02:33:09.116 INFO: train_f_rmse: 0.072115
val_e/atom_mae: 0.007667
2025-02-04 02:33:09.118 INFO: val_e/atom_mae: 0.007667
val_e/atom_rmse: 0.008214
2025-02-04 02:33:09.119 INFO: val_e/atom_rmse: 0.008214
val_f_mae: 0.047018
2025-02-04 02:33:09.119 INFO: val_f_mae: 0.047018
val_f_rmse: 0.077056
2025-02-04 02:33:09.119 INFO: val_f_rmse: 0.077056
##### Step: 1 Learning rate: 0.004 #####
2025-02-04 02:34:08.458 INFO: ##### Step: 1 Learning rate: 0.004 #####
Epoch 2, Train Loss: 6.0810, Val Loss: 6.2370
2025-02-04 02:34:08.459 INFO: Epoch 2, Train Loss: 6.0810, Val Loss: 6.2370
train_e/atom_mae: 0.007239
2025-02-04 02:34:08.459 INFO: train_e/atom_mae: 0.007239
train_e/atom_rmse: 0.008923
2025-02-04 02:34:08.460 INFO: train_e/atom_rmse: 0.008923
train_f_mae: 0.049748
2025-02-04 02:34:08.462 INFO: train_f_mae: 0.049748
train_f_rmse: 0.076076
2025-02-04 02:34:08.462 INFO: train_f_rmse: 0.076076
val_e/atom_mae: 0.004626
2025-02-04 02:34:08.465 INFO: val_e/atom_mae: 0.004626
val_e/atom_rmse: 0.005014
2025-02-04 02:34:08.465 INFO: val_e/atom_rmse: 0.005014
val_f_mae: 0.048731
2025-02-04 02:34:08.465 INFO: val_f_mae: 0.048731
val_f_rmse: 0.078414
2025-02-04 02:34:08.465 INFO: val_f_rmse: 0.078414
##### Step: 2 Learning rate: 0.006 #####
2025-02-04 02:35:07.879 INFO: ##### Step: 2 Learning rate: 0.006 #####
Epoch 3, Train Loss: 6.1328, Val Loss: 7.6681
2025-02-04 02:35:07.879 INFO: Epoch 3, Train Loss: 6.1328, Val Loss: 7.6681
train_e/atom_mae: 0.006504
2025-02-04 02:35:07.880 INFO: train_e/atom_mae: 0.006504
train_e/atom_rmse: 0.008210
2025-02-04 02:35:07.880 INFO: train_e/atom_rmse: 0.008210
train_f_mae: 0.049616
2025-02-04 02:35:07.883 INFO: train_f_mae: 0.049616
train_f_rmse: 0.076709
2025-02-04 02:35:07.883 INFO: train_f_rmse: 0.076709
val_e/atom_mae: 0.015153
2025-02-04 02:35:07.885 INFO: val_e/atom_mae: 0.015153
val_e/atom_rmse: 0.015520
2025-02-04 02:35:07.886 INFO: val_e/atom_rmse: 0.015520
val_f_mae: 0.052058
2025-02-04 02:35:07.886 INFO: val_f_mae: 0.052058
val_f_rmse: 0.082351
2025-02-04 02:35:07.886 INFO: val_f_rmse: 0.082351
##### Step: 3 Learning rate: 0.008 #####
2025-02-04 02:36:07.207 INFO: ##### Step: 3 Learning rate: 0.008 #####
Epoch 4, Train Loss: 6.5021, Val Loss: 6.9048
2025-02-04 02:36:07.207 INFO: Epoch 4, Train Loss: 6.5021, Val Loss: 6.9048
train_e/atom_mae: 0.006201
2025-02-04 02:36:07.208 INFO: train_e/atom_mae: 0.006201
train_e/atom_rmse: 0.008072
2025-02-04 02:36:07.208 INFO: train_e/atom_rmse: 0.008072
train_f_mae: 0.051441
2025-02-04 02:36:07.211 INFO: train_f_mae: 0.051441
train_f_rmse: 0.079132
2025-02-04 02:36:07.211 INFO: train_f_rmse: 0.079132
val_e/atom_mae: 0.005417
2025-02-04 02:36:07.213 INFO: val_e/atom_mae: 0.005417
val_e/atom_rmse: 0.005900
2025-02-04 02:36:07.213 INFO: val_e/atom_rmse: 0.005900
val_f_mae: 0.051218
2025-02-04 02:36:07.214 INFO: val_f_mae: 0.051218
val_f_rmse: 0.082340
2025-02-04 02:36:07.214 INFO: val_f_rmse: 0.082340
##### Step: 4 Learning rate: 0.01 #####
2025-02-04 02:37:06.542 INFO: ##### Step: 4 Learning rate: 0.01 #####
Epoch 5, Train Loss: 9.2026, Val Loss: 11.4349
2025-02-04 02:37:06.542 INFO: Epoch 5, Train Loss: 9.2026, Val Loss: 11.4349
train_e/atom_mae: 0.009336
2025-02-04 02:37:06.543 INFO: train_e/atom_mae: 0.009336
train_e/atom_rmse: 0.012827
2025-02-04 02:37:06.544 INFO: train_e/atom_rmse: 0.012827
train_f_mae: 0.060598
2025-02-04 02:37:06.546 INFO: train_f_mae: 0.060598
train_f_rmse: 0.092715
2025-02-04 02:37:06.546 INFO: train_f_rmse: 0.092715
val_e/atom_mae: 0.010096
2025-02-04 02:37:06.548 INFO: val_e/atom_mae: 0.010096
val_e/atom_rmse: 0.010894
2025-02-04 02:37:06.549 INFO: val_e/atom_rmse: 0.010894
val_f_mae: 0.070383
2025-02-04 02:37:06.549 INFO: val_f_mae: 0.070383
val_f_rmse: 0.104867
2025-02-04 02:37:06.549 INFO: val_f_rmse: 0.104867
##### Step: 5 Learning rate: 0.01 #####
2025-02-04 02:38:05.875 INFO: ##### Step: 5 Learning rate: 0.01 #####
Epoch 6, Train Loss: 9.4172, Val Loss: 9.1491
2025-02-04 02:38:05.875 INFO: Epoch 6, Train Loss: 9.4172, Val Loss: 9.1491
train_e/atom_mae: 0.010953
2025-02-04 02:38:05.876 INFO: train_e/atom_mae: 0.010953
train_e/atom_rmse: 0.014043
2025-02-04 02:38:05.877 INFO: train_e/atom_rmse: 0.014043
train_f_mae: 0.061024
2025-02-04 02:38:05.879 INFO: train_f_mae: 0.061024
train_f_rmse: 0.093221
2025-02-04 02:38:05.879 INFO: train_f_rmse: 0.093221
val_e/atom_mae: 0.003692
2025-02-04 02:38:05.881 INFO: val_e/atom_mae: 0.003692
val_e/atom_rmse: 0.004411
2025-02-04 02:38:05.882 INFO: val_e/atom_rmse: 0.004411
val_f_mae: 0.066454
2025-02-04 02:38:05.882 INFO: val_f_mae: 0.066454
val_f_rmse: 0.095308
2025-02-04 02:38:05.882 INFO: val_f_rmse: 0.095308
##### Step: 6 Learning rate: 0.01 #####
2025-02-04 02:39:05.222 INFO: ##### Step: 6 Learning rate: 0.01 #####
Epoch 7, Train Loss: 10.5559, Val Loss: 11.8581
2025-02-04 02:39:05.222 INFO: Epoch 7, Train Loss: 10.5559, Val Loss: 11.8581
train_e/atom_mae: 0.008798
2025-02-04 02:39:05.223 INFO: train_e/atom_mae: 0.008798
train_e/atom_rmse: 0.011511
2025-02-04 02:39:05.223 INFO: train_e/atom_rmse: 0.011511
train_f_mae: 0.066755
2025-02-04 02:39:05.226 INFO: train_f_mae: 0.066755
train_f_rmse: 0.100336
2025-02-04 02:39:05.226 INFO: train_f_rmse: 0.100336
val_e/atom_mae: 0.015978
2025-02-04 02:39:05.228 INFO: val_e/atom_mae: 0.015978
val_e/atom_rmse: 0.016444
2025-02-04 02:39:05.229 INFO: val_e/atom_rmse: 0.016444
val_f_mae: 0.070102
2025-02-04 02:39:05.229 INFO: val_f_mae: 0.070102
val_f_rmse: 0.104229
2025-02-04 02:39:05.229 INFO: val_f_rmse: 0.104229
##### Step: 7 Learning rate: 0.01 #####
2025-02-04 02:40:04.724 INFO: ##### Step: 7 Learning rate: 0.01 #####
Epoch 8, Train Loss: 9.3600, Val Loss: 9.0171
2025-02-04 02:40:04.725 INFO: Epoch 8, Train Loss: 9.3600, Val Loss: 9.0171
train_e/atom_mae: 0.010003
2025-02-04 02:40:04.726 INFO: train_e/atom_mae: 0.010003
train_e/atom_rmse: 0.012704
2025-02-04 02:40:04.726 INFO: train_e/atom_rmse: 0.012704
train_f_mae: 0.062990
2025-02-04 02:40:04.728 INFO: train_f_mae: 0.062990
train_f_rmse: 0.093622
2025-02-04 02:40:04.729 INFO: train_f_rmse: 0.093622
val_e/atom_mae: 0.018483
2025-02-04 02:40:04.731 INFO: val_e/atom_mae: 0.018483
val_e/atom_rmse: 0.018777
2025-02-04 02:40:04.731 INFO: val_e/atom_rmse: 0.018777
val_f_mae: 0.058939
2025-02-04 02:40:04.732 INFO: val_f_mae: 0.058939
val_f_rmse: 0.087861
2025-02-04 02:40:04.732 INFO: val_f_rmse: 0.087861
##### Step: 8 Learning rate: 0.01 #####
2025-02-04 02:41:04.106 INFO: ##### Step: 8 Learning rate: 0.01 #####
Epoch 9, Train Loss: 8.7179, Val Loss: 12.6799
2025-02-04 02:41:04.107 INFO: Epoch 9, Train Loss: 8.7179, Val Loss: 12.6799
train_e/atom_mae: 0.008719
2025-02-04 02:41:04.108 INFO: train_e/atom_mae: 0.008719
train_e/atom_rmse: 0.011151
2025-02-04 02:41:04.108 INFO: train_e/atom_rmse: 0.011151
train_f_mae: 0.060047
2025-02-04 02:41:04.110 INFO: train_f_mae: 0.060047
train_f_rmse: 0.090882
2025-02-04 02:41:04.111 INFO: train_f_rmse: 0.090882
val_e/atom_mae: 0.018370
2025-02-04 02:41:04.113 INFO: val_e/atom_mae: 0.018370
val_e/atom_rmse: 0.018733
2025-02-04 02:41:04.113 INFO: val_e/atom_rmse: 0.018733
val_f_mae: 0.074402
2025-02-04 02:41:04.114 INFO: val_f_mae: 0.074402
val_f_rmse: 0.106714
2025-02-04 02:41:04.114 INFO: val_f_rmse: 0.106714
##### Step: 9 Learning rate: 0.01 #####
2025-02-04 02:42:03.507 INFO: ##### Step: 9 Learning rate: 0.01 #####
Epoch 10, Train Loss: 11.1497, Val Loss: 7.5968
2025-02-04 02:42:03.507 INFO: Epoch 10, Train Loss: 11.1497, Val Loss: 7.5968
train_e/atom_mae: 0.013833
2025-02-04 02:42:03.508 INFO: train_e/atom_mae: 0.013833
train_e/atom_rmse: 0.017452
2025-02-04 02:42:03.508 INFO: train_e/atom_rmse: 0.017452
train_f_mae: 0.067238
2025-02-04 02:42:03.510 INFO: train_f_mae: 0.067238
train_f_rmse: 0.100135
2025-02-04 02:42:03.511 INFO: train_f_rmse: 0.100135
val_e/atom_mae: 0.005676
2025-02-04 02:42:03.513 INFO: val_e/atom_mae: 0.005676
val_e/atom_rmse: 0.006040
2025-02-04 02:42:03.513 INFO: val_e/atom_rmse: 0.006040
val_f_mae: 0.055521
2025-02-04 02:42:03.514 INFO: val_f_mae: 0.055521
val_f_rmse: 0.086399
2025-02-04 02:42:03.514 INFO: val_f_rmse: 0.086399
##### Step: 10 Learning rate: 0.01 #####
2025-02-04 02:43:03.339 INFO: ##### Step: 10 Learning rate: 0.01 #####
Epoch 11, Train Loss: 9.0223, Val Loss: 5.7009
2025-02-04 02:43:03.339 INFO: Epoch 11, Train Loss: 9.0223, Val Loss: 5.7009
train_e/atom_mae: 0.008920
2025-02-04 02:43:03.340 INFO: train_e/atom_mae: 0.008920
train_e/atom_rmse: 0.011521
2025-02-04 02:43:03.340 INFO: train_e/atom_rmse: 0.011521
train_f_mae: 0.061463
2025-02-04 02:43:03.343 INFO: train_f_mae: 0.061463
train_f_rmse: 0.092374
2025-02-04 02:43:03.343 INFO: train_f_rmse: 0.092374
val_e/atom_mae: 0.003340
2025-02-04 02:43:03.345 INFO: val_e/atom_mae: 0.003340
val_e/atom_rmse: 0.004262
2025-02-04 02:43:03.345 INFO: val_e/atom_rmse: 0.004262
val_f_mae: 0.045608
2025-02-04 02:43:03.346 INFO: val_f_mae: 0.045608
val_f_rmse: 0.075082
2025-02-04 02:43:03.346 INFO: val_f_rmse: 0.075082
##### Step: 11 Learning rate: 0.01 #####
2025-02-04 02:44:03.155 INFO: ##### Step: 11 Learning rate: 0.01 #####
Epoch 12, Train Loss: 7.7094, Val Loss: 5.6948
2025-02-04 02:44:03.155 INFO: Epoch 12, Train Loss: 7.7094, Val Loss: 5.6948
train_e/atom_mae: 0.007472
2025-02-04 02:44:03.156 INFO: train_e/atom_mae: 0.007472
train_e/atom_rmse: 0.009140
2025-02-04 02:44:03.157 INFO: train_e/atom_rmse: 0.009140
train_f_mae: 0.056364
2025-02-04 02:44:03.159 INFO: train_f_mae: 0.056364
train_f_rmse: 0.086032
2025-02-04 02:44:03.159 INFO: train_f_rmse: 0.086032
val_e/atom_mae: 0.003873
2025-02-04 02:44:03.161 INFO: val_e/atom_mae: 0.003873
val_e/atom_rmse: 0.004653
2025-02-04 02:44:03.162 INFO: val_e/atom_rmse: 0.004653
val_f_mae: 0.045607
2025-02-04 02:44:03.162 INFO: val_f_mae: 0.045607
val_f_rmse: 0.074964
2025-02-04 02:44:03.163 INFO: val_f_rmse: 0.074964
##### Step: 12 Learning rate: 0.01 #####
2025-02-04 02:45:02.959 INFO: ##### Step: 12 Learning rate: 0.01 #####
Epoch 13, Train Loss: 7.7381, Val Loss: 5.7861
2025-02-04 02:45:02.959 INFO: Epoch 13, Train Loss: 7.7381, Val Loss: 5.7861
train_e/atom_mae: 0.008796
2025-02-04 02:45:02.960 INFO: train_e/atom_mae: 0.008796
train_e/atom_rmse: 0.011603
2025-02-04 02:45:02.961 INFO: train_e/atom_rmse: 0.011603
train_f_mae: 0.056016
2025-02-04 02:45:02.963 INFO: train_f_mae: 0.056016
train_f_rmse: 0.085099
2025-02-04 02:45:02.963 INFO: train_f_rmse: 0.085099
val_e/atom_mae: 0.003363
2025-02-04 02:45:02.965 INFO: val_e/atom_mae: 0.003363
val_e/atom_rmse: 0.004339
2025-02-04 02:45:02.966 INFO: val_e/atom_rmse: 0.004339
val_f_mae: 0.046480
2025-02-04 02:45:02.966 INFO: val_f_mae: 0.046480
val_f_rmse: 0.075627
2025-02-04 02:45:02.966 INFO: val_f_rmse: 0.075627
##### Step: 13 Learning rate: 0.01 #####
2025-02-04 02:46:02.770 INFO: ##### Step: 13 Learning rate: 0.01 #####
Epoch 14, Train Loss: 10.4299, Val Loss: 5.8542
2025-02-04 02:46:02.771 INFO: Epoch 14, Train Loss: 10.4299, Val Loss: 5.8542
train_e/atom_mae: 0.014383
2025-02-04 02:46:02.772 INFO: train_e/atom_mae: 0.014383
train_e/atom_rmse: 0.017020
2025-02-04 02:46:02.772 INFO: train_e/atom_rmse: 0.017020
train_f_mae: 0.064127
2025-02-04 02:46:02.774 INFO: train_f_mae: 0.064127
train_f_rmse: 0.096757
2025-02-04 02:46:02.775 INFO: train_f_rmse: 0.096757
val_e/atom_mae: 0.002987
2025-02-04 02:46:02.777 INFO: val_e/atom_mae: 0.002987
val_e/atom_rmse: 0.004223
2025-02-04 02:46:02.777 INFO: val_e/atom_rmse: 0.004223
val_f_mae: 0.046234
2025-02-04 02:46:02.778 INFO: val_f_mae: 0.046234
val_f_rmse: 0.076111
2025-02-04 02:46:02.778 INFO: val_f_rmse: 0.076111
##### Step: 14 Learning rate: 0.01 #####
2025-02-04 02:47:02.539 INFO: ##### Step: 14 Learning rate: 0.01 #####
Epoch 15, Train Loss: 8.5535, Val Loss: 5.9031
2025-02-04 02:47:02.539 INFO: Epoch 15, Train Loss: 8.5535, Val Loss: 5.9031
train_e/atom_mae: 0.009064
2025-02-04 02:47:02.540 INFO: train_e/atom_mae: 0.009064
train_e/atom_rmse: 0.011549
2025-02-04 02:47:02.541 INFO: train_e/atom_rmse: 0.011549
train_f_mae: 0.059192
2025-02-04 02:47:02.543 INFO: train_f_mae: 0.059192
train_f_rmse: 0.089787
2025-02-04 02:47:02.543 INFO: train_f_rmse: 0.089787
val_e/atom_mae: 0.007163
2025-02-04 02:47:02.545 INFO: val_e/atom_mae: 0.007163
val_e/atom_rmse: 0.007636
2025-02-04 02:47:02.546 INFO: val_e/atom_rmse: 0.007636
val_f_mae: 0.046002
2025-02-04 02:47:02.546 INFO: val_f_mae: 0.046002
val_f_rmse: 0.075452
2025-02-04 02:47:02.546 INFO: val_f_rmse: 0.075452
##### Step: 15 Learning rate: 0.01 #####
2025-02-04 02:48:02.337 INFO: ##### Step: 15 Learning rate: 0.01 #####
Epoch 16, Train Loss: 9.6777, Val Loss: 5.8207
2025-02-04 02:48:02.337 INFO: Epoch 16, Train Loss: 9.6777, Val Loss: 5.8207
train_e/atom_mae: 0.007300
2025-02-04 02:48:02.338 INFO: train_e/atom_mae: 0.007300
train_e/atom_rmse: 0.009054
2025-02-04 02:48:02.338 INFO: train_e/atom_rmse: 0.009054
train_f_mae: 0.064020
2025-02-04 02:48:02.341 INFO: train_f_mae: 0.064020
train_f_rmse: 0.096827
2025-02-04 02:48:02.341 INFO: train_f_rmse: 0.096827
val_e/atom_mae: 0.003831
2025-02-04 02:48:02.343 INFO: val_e/atom_mae: 0.003831
val_e/atom_rmse: 0.004713
2025-02-04 02:48:02.343 INFO: val_e/atom_rmse: 0.004713
val_f_mae: 0.045873
2025-02-04 02:48:02.344 INFO: val_f_mae: 0.045873
val_f_rmse: 0.075784
2025-02-04 02:48:02.344 INFO: val_f_rmse: 0.075784
##### Step: 16 Learning rate: 0.01 #####
2025-02-04 02:49:02.102 INFO: ##### Step: 16 Learning rate: 0.01 #####
Epoch 17, Train Loss: 10.0275, Val Loss: 5.7366
2025-02-04 02:49:02.103 INFO: Epoch 17, Train Loss: 10.0275, Val Loss: 5.7366
train_e/atom_mae: 0.011241
2025-02-04 02:49:02.104 INFO: train_e/atom_mae: 0.011241
train_e/atom_rmse: 0.014186
2025-02-04 02:49:02.104 INFO: train_e/atom_rmse: 0.014186
train_f_mae: 0.062964
2025-02-04 02:49:02.107 INFO: train_f_mae: 0.062964
train_f_rmse: 0.096362
2025-02-04 02:49:02.107 INFO: train_f_rmse: 0.096362
val_e/atom_mae: 0.003605
2025-02-04 02:49:02.109 INFO: val_e/atom_mae: 0.003605
val_e/atom_rmse: 0.004542
2025-02-04 02:49:02.109 INFO: val_e/atom_rmse: 0.004542
val_f_mae: 0.045844
2025-02-04 02:49:02.110 INFO: val_f_mae: 0.045844
val_f_rmse: 0.075262
2025-02-04 02:49:02.110 INFO: val_f_rmse: 0.075262
##### Step: 17 Learning rate: 0.01 #####
2025-02-04 02:50:01.927 INFO: ##### Step: 17 Learning rate: 0.01 #####
Epoch 18, Train Loss: 8.9897, Val Loss: 6.0244
2025-02-04 02:50:01.927 INFO: Epoch 18, Train Loss: 8.9897, Val Loss: 6.0244
train_e/atom_mae: 0.009323
2025-02-04 02:50:01.928 INFO: train_e/atom_mae: 0.009323
train_e/atom_rmse: 0.011550
2025-02-04 02:50:01.928 INFO: train_e/atom_rmse: 0.011550
train_f_mae: 0.060650
2025-02-04 02:50:01.931 INFO: train_f_mae: 0.060650
train_f_rmse: 0.092185
2025-02-04 02:50:01.931 INFO: train_f_rmse: 0.092185
val_e/atom_mae: 0.005326
2025-02-04 02:50:01.933 INFO: val_e/atom_mae: 0.005326
val_e/atom_rmse: 0.006068
2025-02-04 02:50:01.933 INFO: val_e/atom_rmse: 0.006068
val_f_mae: 0.047049
2025-02-04 02:50:01.934 INFO: val_f_mae: 0.047049
val_f_rmse: 0.076759
2025-02-04 02:50:01.934 INFO: val_f_rmse: 0.076759
##### Step: 18 Learning rate: 0.01 #####
2025-02-04 02:51:01.710 INFO: ##### Step: 18 Learning rate: 0.01 #####
Epoch 19, Train Loss: 16.9188, Val Loss: 6.5749
2025-02-04 02:51:01.710 INFO: Epoch 19, Train Loss: 16.9188, Val Loss: 6.5749
train_e/atom_mae: 0.013377
2025-02-04 02:51:01.711 INFO: train_e/atom_mae: 0.013377
train_e/atom_rmse: 0.017985
2025-02-04 02:51:01.711 INFO: train_e/atom_rmse: 0.017985
train_f_mae: 0.072894
2025-02-04 02:51:01.714 INFO: train_f_mae: 0.072894
train_f_rmse: 0.125405
2025-02-04 02:51:01.714 INFO: train_f_rmse: 0.125405
val_e/atom_mae: 0.008519
2025-02-04 02:51:01.716 INFO: val_e/atom_mae: 0.008519
val_e/atom_rmse: 0.009031
2025-02-04 02:51:01.717 INFO: val_e/atom_rmse: 0.009031
val_f_mae: 0.047590
2025-02-04 02:51:01.717 INFO: val_f_mae: 0.047590
val_f_rmse: 0.079243
2025-02-04 02:51:01.717 INFO: val_f_rmse: 0.079243
##### Step: 19 Learning rate: 0.01 #####
2025-02-04 02:52:01.522 INFO: ##### Step: 19 Learning rate: 0.01 #####
Epoch 20, Train Loss: 9.9968, Val Loss: 6.0774
2025-02-04 02:52:01.523 INFO: Epoch 20, Train Loss: 9.9968, Val Loss: 6.0774
train_e/atom_mae: 0.011000
2025-02-04 02:52:01.524 INFO: train_e/atom_mae: 0.011000
train_e/atom_rmse: 0.013310
2025-02-04 02:52:01.524 INFO: train_e/atom_rmse: 0.013310
train_f_mae: 0.064335
2025-02-04 02:52:01.527 INFO: train_f_mae: 0.064335
train_f_rmse: 0.096663
2025-02-04 02:52:01.527 INFO: train_f_rmse: 0.096663
val_e/atom_mae: 0.003803
2025-02-04 02:52:01.529 INFO: val_e/atom_mae: 0.003803
val_e/atom_rmse: 0.004770
2025-02-04 02:52:01.529 INFO: val_e/atom_rmse: 0.004770
val_f_mae: 0.046364
2025-02-04 02:52:01.530 INFO: val_f_mae: 0.046364
val_f_rmse: 0.077443
2025-02-04 02:52:01.530 INFO: val_f_rmse: 0.077443
##### Step: 20 Learning rate: 0.005 #####
2025-02-04 02:53:01.302 INFO: ##### Step: 20 Learning rate: 0.005 #####
Epoch 21, Train Loss: 6.6699, Val Loss: 5.7154
2025-02-04 02:53:01.303 INFO: Epoch 21, Train Loss: 6.6699, Val Loss: 5.7154
train_e/atom_mae: 0.007859
2025-02-04 02:53:01.303 INFO: train_e/atom_mae: 0.007859
train_e/atom_rmse: 0.009824
2025-02-04 02:53:01.304 INFO: train_e/atom_rmse: 0.009824
train_f_mae: 0.051596
2025-02-04 02:53:01.306 INFO: train_f_mae: 0.051596
train_f_rmse: 0.079462
2025-02-04 02:53:01.307 INFO: train_f_rmse: 0.079462
val_e/atom_mae: 0.002441
2025-02-04 02:53:01.309 INFO: val_e/atom_mae: 0.002441
val_e/atom_rmse: 0.003393
2025-02-04 02:53:01.309 INFO: val_e/atom_rmse: 0.003393
val_f_mae: 0.045316
2025-02-04 02:53:01.309 INFO: val_f_mae: 0.045316
val_f_rmse: 0.075337
2025-02-04 02:53:01.310 INFO: val_f_rmse: 0.075337
##### Step: 21 Learning rate: 0.005 #####
2025-02-04 02:54:01.025 INFO: ##### Step: 21 Learning rate: 0.005 #####
Epoch 22, Train Loss: 5.9628, Val Loss: 5.5744
2025-02-04 02:54:01.026 INFO: Epoch 22, Train Loss: 5.9628, Val Loss: 5.5744
train_e/atom_mae: 0.005656
2025-02-04 02:54:01.026 INFO: train_e/atom_mae: 0.005656
train_e/atom_rmse: 0.007147
2025-02-04 02:54:01.027 INFO: train_e/atom_rmse: 0.007147
train_f_mae: 0.049270
2025-02-04 02:54:01.029 INFO: train_f_mae: 0.049270
train_f_rmse: 0.075990
2025-02-04 02:54:01.029 INFO: train_f_rmse: 0.075990
val_e/atom_mae: 0.002234
2025-02-04 02:54:01.032 INFO: val_e/atom_mae: 0.002234
val_e/atom_rmse: 0.003128
2025-02-04 02:54:01.032 INFO: val_e/atom_rmse: 0.003128
val_f_mae: 0.044951
2025-02-04 02:54:01.032 INFO: val_f_mae: 0.044951
val_f_rmse: 0.074444
2025-02-04 02:54:01.033 INFO: val_f_rmse: 0.074444
##### Step: 22 Learning rate: 0.005 #####
2025-02-04 02:55:00.800 INFO: ##### Step: 22 Learning rate: 0.005 #####
Epoch 23, Train Loss: 6.3141, Val Loss: 5.5631
2025-02-04 02:55:00.800 INFO: Epoch 23, Train Loss: 6.3141, Val Loss: 5.5631
train_e/atom_mae: 0.007166
2025-02-04 02:55:00.801 INFO: train_e/atom_mae: 0.007166
train_e/atom_rmse: 0.009129
2025-02-04 02:55:00.801 INFO: train_e/atom_rmse: 0.009129
train_f_mae: 0.050336
2025-02-04 02:55:00.804 INFO: train_f_mae: 0.050336
train_f_rmse: 0.077504
2025-02-04 02:55:00.804 INFO: train_f_rmse: 0.077504
val_e/atom_mae: 0.002285
2025-02-04 02:55:00.806 INFO: val_e/atom_mae: 0.002285
val_e/atom_rmse: 0.003416
2025-02-04 02:55:00.807 INFO: val_e/atom_rmse: 0.003416
val_f_mae: 0.044666
2025-02-04 02:55:00.807 INFO: val_f_mae: 0.044666
val_f_rmse: 0.074328
2025-02-04 02:55:00.807 INFO: val_f_rmse: 0.074328
##### Step: 23 Learning rate: 0.005 #####
2025-02-04 02:56:00.538 INFO: ##### Step: 23 Learning rate: 0.005 #####
Epoch 24, Train Loss: 6.1435, Val Loss: 5.4700
2025-02-04 02:56:00.538 INFO: Epoch 24, Train Loss: 6.1435, Val Loss: 5.4700
train_e/atom_mae: 0.005773
2025-02-04 02:56:00.539 INFO: train_e/atom_mae: 0.005773
train_e/atom_rmse: 0.007272
2025-02-04 02:56:00.540 INFO: train_e/atom_rmse: 0.007272
train_f_mae: 0.050012
2025-02-04 02:56:00.542 INFO: train_f_mae: 0.050012
train_f_rmse: 0.077127
2025-02-04 02:56:00.542 INFO: train_f_rmse: 0.077127
val_e/atom_mae: 0.002240
2025-02-04 02:56:00.545 INFO: val_e/atom_mae: 0.002240
val_e/atom_rmse: 0.003291
2025-02-04 02:56:00.545 INFO: val_e/atom_rmse: 0.003291
val_f_mae: 0.044405
2025-02-04 02:56:00.545 INFO: val_f_mae: 0.044405
val_f_rmse: 0.073718
2025-02-04 02:56:00.546 INFO: val_f_rmse: 0.073718
##### Step: 24 Learning rate: 0.005 #####
2025-02-04 02:57:00.346 INFO: ##### Step: 24 Learning rate: 0.005 #####
Epoch 25, Train Loss: 5.8835, Val Loss: 5.3595
2025-02-04 02:57:00.346 INFO: Epoch 25, Train Loss: 5.8835, Val Loss: 5.3595
train_e/atom_mae: 0.005247
2025-02-04 02:57:00.347 INFO: train_e/atom_mae: 0.005247
train_e/atom_rmse: 0.006678
2025-02-04 02:57:00.347 INFO: train_e/atom_rmse: 0.006678
train_f_mae: 0.049343
2025-02-04 02:57:00.350 INFO: train_f_mae: 0.049343
train_f_rmse: 0.075625
2025-02-04 02:57:00.350 INFO: train_f_rmse: 0.075625
val_e/atom_mae: 0.001816
2025-02-04 02:57:00.352 INFO: val_e/atom_mae: 0.001816
val_e/atom_rmse: 0.002764
2025-02-04 02:57:00.353 INFO: val_e/atom_rmse: 0.002764
val_f_mae: 0.044245
2025-02-04 02:57:00.353 INFO: val_f_mae: 0.044245
val_f_rmse: 0.073030
2025-02-04 02:57:00.353 INFO: val_f_rmse: 0.073030
##### Step: 25 Learning rate: 0.005 #####
2025-02-04 02:58:00.158 INFO: ##### Step: 25 Learning rate: 0.005 #####
Epoch 26, Train Loss: 6.0936, Val Loss: 5.3991
2025-02-04 02:58:00.159 INFO: Epoch 26, Train Loss: 6.0936, Val Loss: 5.3991
train_e/atom_mae: 0.004944
2025-02-04 02:58:00.160 INFO: train_e/atom_mae: 0.004944
train_e/atom_rmse: 0.006448
2025-02-04 02:58:00.160 INFO: train_e/atom_rmse: 0.006448
train_f_mae: 0.050546
2025-02-04 02:58:00.162 INFO: train_f_mae: 0.050546
train_f_rmse: 0.077073
2025-02-04 02:58:00.163 INFO: train_f_rmse: 0.077073
val_e/atom_mae: 0.001712
2025-02-04 02:58:00.165 INFO: val_e/atom_mae: 0.001712
val_e/atom_rmse: 0.002681
2025-02-04 02:58:00.165 INFO: val_e/atom_rmse: 0.002681
val_f_mae: 0.044301
2025-02-04 02:58:00.165 INFO: val_f_mae: 0.044301
val_f_rmse: 0.073321
2025-02-04 02:58:00.166 INFO: val_f_rmse: 0.073321
##### Step: 26 Learning rate: 0.005 #####
2025-02-04 02:58:59.942 INFO: ##### Step: 26 Learning rate: 0.005 #####
Epoch 27, Train Loss: 6.0276, Val Loss: 5.3455
2025-02-04 02:58:59.942 INFO: Epoch 27, Train Loss: 6.0276, Val Loss: 5.3455
train_e/atom_mae: 0.009037
2025-02-04 02:58:59.943 INFO: train_e/atom_mae: 0.009037
train_e/atom_rmse: 0.011411
2025-02-04 02:58:59.943 INFO: train_e/atom_rmse: 0.011411
train_f_mae: 0.048236
2025-02-04 02:58:59.946 INFO: train_f_mae: 0.048236
train_f_rmse: 0.074482
2025-02-04 02:58:59.946 INFO: train_f_rmse: 0.074482
val_e/atom_mae: 0.001875
2025-02-04 02:58:59.948 INFO: val_e/atom_mae: 0.001875
val_e/atom_rmse: 0.002980
2025-02-04 02:58:59.949 INFO: val_e/atom_rmse: 0.002980
val_f_mae: 0.044133
2025-02-04 02:58:59.949 INFO: val_f_mae: 0.044133
val_f_rmse: 0.072912
2025-02-04 02:58:59.949 INFO: val_f_rmse: 0.072912
##### Step: 27 Learning rate: 0.005 #####
2025-02-04 02:59:59.746 INFO: ##### Step: 27 Learning rate: 0.005 #####
Epoch 28, Train Loss: 5.9893, Val Loss: 5.4274
2025-02-04 02:59:59.746 INFO: Epoch 28, Train Loss: 5.9893, Val Loss: 5.4274
train_e/atom_mae: 0.005186
2025-02-04 02:59:59.747 INFO: train_e/atom_mae: 0.005186
train_e/atom_rmse: 0.006702
2025-02-04 02:59:59.747 INFO: train_e/atom_rmse: 0.006702
train_f_mae: 0.049507
2025-02-04 02:59:59.750 INFO: train_f_mae: 0.049507
train_f_rmse: 0.076313
2025-02-04 02:59:59.750 INFO: train_f_rmse: 0.076313
val_e/atom_mae: 0.001800
2025-02-04 02:59:59.752 INFO: val_e/atom_mae: 0.001800
val_e/atom_rmse: 0.002897
2025-02-04 02:59:59.753 INFO: val_e/atom_rmse: 0.002897
val_f_mae: 0.044452
2025-02-04 02:59:59.753 INFO: val_f_mae: 0.044452
val_f_rmse: 0.073481
2025-02-04 02:59:59.753 INFO: val_f_rmse: 0.073481
##### Step: 28 Learning rate: 0.005 #####
2025-02-04 03:00:59.561 INFO: ##### Step: 28 Learning rate: 0.005 #####
Epoch 29, Train Loss: 5.7768, Val Loss: 5.3540
2025-02-04 03:00:59.561 INFO: Epoch 29, Train Loss: 5.7768, Val Loss: 5.3540
train_e/atom_mae: 0.005789
2025-02-04 03:00:59.562 INFO: train_e/atom_mae: 0.005789
train_e/atom_rmse: 0.007195
2025-02-04 03:00:59.562 INFO: train_e/atom_rmse: 0.007195
train_f_mae: 0.048499
2025-02-04 03:00:59.565 INFO: train_f_mae: 0.048499
train_f_rmse: 0.074740
2025-02-04 03:00:59.565 INFO: train_f_rmse: 0.074740
val_e/atom_mae: 0.001757
2025-02-04 03:00:59.567 INFO: val_e/atom_mae: 0.001757
val_e/atom_rmse: 0.002752
2025-02-04 03:00:59.567 INFO: val_e/atom_rmse: 0.002752
val_f_mae: 0.043950
2025-02-04 03:00:59.568 INFO: val_f_mae: 0.043950
val_f_rmse: 0.073008
2025-02-04 03:00:59.568 INFO: val_f_rmse: 0.073008
##### Step: 29 Learning rate: 0.005 #####
2025-02-04 03:01:59.342 INFO: ##### Step: 29 Learning rate: 0.005 #####
Epoch 30, Train Loss: 5.3983, Val Loss: 5.3591
2025-02-04 03:01:59.342 INFO: Epoch 30, Train Loss: 5.3983, Val Loss: 5.3591
train_e/atom_mae: 0.004192
2025-02-04 03:01:59.343 INFO: train_e/atom_mae: 0.004192
train_e/atom_rmse: 0.005435
2025-02-04 03:01:59.343 INFO: train_e/atom_rmse: 0.005435
train_f_mae: 0.047103
2025-02-04 03:01:59.346 INFO: train_f_mae: 0.047103
train_f_rmse: 0.072728
2025-02-04 03:01:59.346 INFO: train_f_rmse: 0.072728
val_e/atom_mae: 0.001585
2025-02-04 03:01:59.348 INFO: val_e/atom_mae: 0.001585
val_e/atom_rmse: 0.002520
2025-02-04 03:01:59.349 INFO: val_e/atom_rmse: 0.002520
val_f_mae: 0.044138
2025-02-04 03:01:59.349 INFO: val_f_mae: 0.044138
val_f_rmse: 0.073071
2025-02-04 03:01:59.349 INFO: val_f_rmse: 0.073071
##### Step: 30 Learning rate: 0.005 #####
2025-02-04 03:02:59.194 INFO: ##### Step: 30 Learning rate: 0.005 #####
Epoch 31, Train Loss: 5.6041, Val Loss: 5.3726
2025-02-04 03:02:59.195 INFO: Epoch 31, Train Loss: 5.6041, Val Loss: 5.3726
train_e/atom_mae: 0.005826
2025-02-04 03:02:59.196 INFO: train_e/atom_mae: 0.005826
train_e/atom_rmse: 0.007093
2025-02-04 03:02:59.196 INFO: train_e/atom_rmse: 0.007093
train_f_mae: 0.047408
2025-02-04 03:02:59.199 INFO: train_f_mae: 0.047408
train_f_rmse: 0.073611
2025-02-04 03:02:59.199 INFO: train_f_rmse: 0.073611
val_e/atom_mae: 0.001700
2025-02-04 03:02:59.201 INFO: val_e/atom_mae: 0.001700
val_e/atom_rmse: 0.002794
2025-02-04 03:02:59.201 INFO: val_e/atom_rmse: 0.002794
val_f_mae: 0.044074
2025-02-04 03:02:59.202 INFO: val_f_mae: 0.044074
val_f_rmse: 0.073126
2025-02-04 03:02:59.202 INFO: val_f_rmse: 0.073126
##### Step: 31 Learning rate: 0.005 #####
2025-02-04 03:03:59.027 INFO: ##### Step: 31 Learning rate: 0.005 #####
Epoch 32, Train Loss: 6.0147, Val Loss: 5.2713
2025-02-04 03:03:59.027 INFO: Epoch 32, Train Loss: 6.0147, Val Loss: 5.2713
train_e/atom_mae: 0.004616
2025-02-04 03:03:59.028 INFO: train_e/atom_mae: 0.004616
train_e/atom_rmse: 0.005953
2025-02-04 03:03:59.028 INFO: train_e/atom_rmse: 0.005953
train_f_mae: 0.050059
2025-02-04 03:03:59.031 INFO: train_f_mae: 0.050059
train_f_rmse: 0.076708
2025-02-04 03:03:59.031 INFO: train_f_rmse: 0.076708
val_e/atom_mae: 0.001938
2025-02-04 03:03:59.033 INFO: val_e/atom_mae: 0.001938
val_e/atom_rmse: 0.002890
2025-02-04 03:03:59.034 INFO: val_e/atom_rmse: 0.002890
val_f_mae: 0.043855
2025-02-04 03:03:59.034 INFO: val_f_mae: 0.043855
val_f_rmse: 0.072413
2025-02-04 03:03:59.034 INFO: val_f_rmse: 0.072413
##### Step: 32 Learning rate: 0.005 #####
2025-02-04 03:04:58.898 INFO: ##### Step: 32 Learning rate: 0.005 #####
Epoch 33, Train Loss: 5.8131, Val Loss: 5.3815
2025-02-04 03:04:58.898 INFO: Epoch 33, Train Loss: 5.8131, Val Loss: 5.3815
train_e/atom_mae: 0.005249
2025-02-04 03:04:58.899 INFO: train_e/atom_mae: 0.005249
train_e/atom_rmse: 0.006574
2025-02-04 03:04:58.899 INFO: train_e/atom_rmse: 0.006574
train_f_mae: 0.048831
2025-02-04 03:04:58.902 INFO: train_f_mae: 0.048831
train_f_rmse: 0.075192
2025-02-04 03:04:58.902 INFO: train_f_rmse: 0.075192
val_e/atom_mae: 0.001862
2025-02-04 03:04:58.904 INFO: val_e/atom_mae: 0.001862
val_e/atom_rmse: 0.003149
2025-02-04 03:04:58.904 INFO: val_e/atom_rmse: 0.003149
val_f_mae: 0.043905
2025-02-04 03:04:58.905 INFO: val_f_mae: 0.043905
val_f_rmse: 0.073132
2025-02-04 03:04:58.905 INFO: val_f_rmse: 0.073132
##### Step: 33 Learning rate: 0.005 #####
2025-02-04 03:05:58.710 INFO: ##### Step: 33 Learning rate: 0.005 #####
Epoch 34, Train Loss: 5.8383, Val Loss: 5.3013
2025-02-04 03:05:58.710 INFO: Epoch 34, Train Loss: 5.8383, Val Loss: 5.3013
train_e/atom_mae: 0.005394
2025-02-04 03:05:58.711 INFO: train_e/atom_mae: 0.005394
train_e/atom_rmse: 0.007015
2025-02-04 03:05:58.711 INFO: train_e/atom_rmse: 0.007015
train_f_mae: 0.048899
2025-02-04 03:05:58.714 INFO: train_f_mae: 0.048899
train_f_rmse: 0.075212
2025-02-04 03:05:58.714 INFO: train_f_rmse: 0.075212
val_e/atom_mae: 0.001730
2025-02-04 03:05:58.716 INFO: val_e/atom_mae: 0.001730
val_e/atom_rmse: 0.002611
2025-02-04 03:05:58.717 INFO: val_e/atom_rmse: 0.002611
val_f_mae: 0.043797
2025-02-04 03:05:58.717 INFO: val_f_mae: 0.043797
val_f_rmse: 0.072658
2025-02-04 03:05:58.717 INFO: val_f_rmse: 0.072658
##### Step: 34 Learning rate: 0.005 #####
2025-02-04 03:06:58.522 INFO: ##### Step: 34 Learning rate: 0.005 #####
Epoch 35, Train Loss: 5.7888, Val Loss: 5.3380
2025-02-04 03:06:58.522 INFO: Epoch 35, Train Loss: 5.7888, Val Loss: 5.3380
train_e/atom_mae: 0.005041
2025-02-04 03:06:58.523 INFO: train_e/atom_mae: 0.005041
train_e/atom_rmse: 0.006470
2025-02-04 03:06:58.524 INFO: train_e/atom_rmse: 0.006470
train_f_mae: 0.048598
2025-02-04 03:06:58.526 INFO: train_f_mae: 0.048598
train_f_rmse: 0.075063
2025-02-04 03:06:58.526 INFO: train_f_rmse: 0.075063
val_e/atom_mae: 0.002697
2025-02-04 03:06:58.528 INFO: val_e/atom_mae: 0.002697
val_e/atom_rmse: 0.003646
2025-02-04 03:06:58.529 INFO: val_e/atom_rmse: 0.003646
val_f_mae: 0.043752
2025-02-04 03:06:58.529 INFO: val_f_mae: 0.043752
val_f_rmse: 0.072752
2025-02-04 03:06:58.530 INFO: val_f_rmse: 0.072752
##### Step: 35 Learning rate: 0.005 #####
2025-02-04 03:07:58.263 INFO: ##### Step: 35 Learning rate: 0.005 #####
Epoch 36, Train Loss: 6.3392, Val Loss: 5.5092
2025-02-04 03:07:58.264 INFO: Epoch 36, Train Loss: 6.3392, Val Loss: 5.5092
train_e/atom_mae: 0.005090
2025-02-04 03:07:58.265 INFO: train_e/atom_mae: 0.005090
train_e/atom_rmse: 0.006422
2025-02-04 03:07:58.265 INFO: train_e/atom_rmse: 0.006422
train_f_mae: 0.051501
2025-02-04 03:07:58.267 INFO: train_f_mae: 0.051501
train_f_rmse: 0.078659
2025-02-04 03:07:58.268 INFO: train_f_rmse: 0.078659
val_e/atom_mae: 0.002145
2025-02-04 03:07:58.270 INFO: val_e/atom_mae: 0.002145
val_e/atom_rmse: 0.003099
2025-02-04 03:07:58.270 INFO: val_e/atom_rmse: 0.003099
val_f_mae: 0.044340
2025-02-04 03:07:58.271 INFO: val_f_mae: 0.044340
val_f_rmse: 0.074009
2025-02-04 03:07:58.271 INFO: val_f_rmse: 0.074009
##### Step: 36 Learning rate: 0.005 #####
2025-02-04 03:08:58.036 INFO: ##### Step: 36 Learning rate: 0.005 #####
Epoch 37, Train Loss: 6.1403, Val Loss: 5.3404
2025-02-04 03:08:58.037 INFO: Epoch 37, Train Loss: 6.1403, Val Loss: 5.3404
train_e/atom_mae: 0.008584
2025-02-04 03:08:58.038 INFO: train_e/atom_mae: 0.008584
train_e/atom_rmse: 0.010502
2025-02-04 03:08:58.038 INFO: train_e/atom_rmse: 0.010502
train_f_mae: 0.049122
2025-02-04 03:08:58.040 INFO: train_f_mae: 0.049122
train_f_rmse: 0.075721
2025-02-04 03:08:58.041 INFO: train_f_rmse: 0.075721
val_e/atom_mae: 0.002496
2025-02-04 03:08:58.043 INFO: val_e/atom_mae: 0.002496
val_e/atom_rmse: 0.003408
2025-02-04 03:08:58.043 INFO: val_e/atom_rmse: 0.003408
val_f_mae: 0.043786
2025-02-04 03:08:58.043 INFO: val_f_mae: 0.043786
val_f_rmse: 0.072809
2025-02-04 03:08:58.044 INFO: val_f_rmse: 0.072809
##### Step: 37 Learning rate: 0.005 #####
2025-02-04 03:09:57.810 INFO: ##### Step: 37 Learning rate: 0.005 #####
Epoch 38, Train Loss: 6.0287, Val Loss: 5.3160
2025-02-04 03:09:57.811 INFO: Epoch 38, Train Loss: 6.0287, Val Loss: 5.3160
train_e/atom_mae: 0.006391
2025-02-04 03:09:57.811 INFO: train_e/atom_mae: 0.006391
train_e/atom_rmse: 0.008601
2025-02-04 03:09:57.812 INFO: train_e/atom_rmse: 0.008601
train_f_mae: 0.049834
2025-02-04 03:09:57.814 INFO: train_f_mae: 0.049834
train_f_rmse: 0.075868
2025-02-04 03:09:57.814 INFO: train_f_rmse: 0.075868
val_e/atom_mae: 0.001797
2025-02-04 03:09:57.816 INFO: val_e/atom_mae: 0.001797
val_e/atom_rmse: 0.002853
2025-02-04 03:09:57.817 INFO: val_e/atom_rmse: 0.002853
val_f_mae: 0.043830
2025-02-04 03:09:57.817 INFO: val_f_mae: 0.043830
val_f_rmse: 0.072732
2025-02-04 03:09:57.818 INFO: val_f_rmse: 0.072732
##### Step: 38 Learning rate: 0.005 #####
2025-02-04 03:10:57.573 INFO: ##### Step: 38 Learning rate: 0.005 #####
Epoch 39, Train Loss: 5.7940, Val Loss: 5.4015
2025-02-04 03:10:57.573 INFO: Epoch 39, Train Loss: 5.7940, Val Loss: 5.4015
train_e/atom_mae: 0.005005
2025-02-04 03:10:57.574 INFO: train_e/atom_mae: 0.005005
train_e/atom_rmse: 0.006625
2025-02-04 03:10:57.574 INFO: train_e/atom_rmse: 0.006625
train_f_mae: 0.048924
2025-02-04 03:10:57.577 INFO: train_f_mae: 0.048924
train_f_rmse: 0.075048
2025-02-04 03:10:57.577 INFO: train_f_rmse: 0.075048
val_e/atom_mae: 0.002023
2025-02-04 03:10:57.579 INFO: val_e/atom_mae: 0.002023
val_e/atom_rmse: 0.002843
2025-02-04 03:10:57.579 INFO: val_e/atom_rmse: 0.002843
val_f_mae: 0.043955
2025-02-04 03:10:57.580 INFO: val_f_mae: 0.043955
val_f_rmse: 0.073312
2025-02-04 03:10:57.580 INFO: val_f_rmse: 0.073312
##### Step: 39 Learning rate: 0.005 #####
2025-02-04 03:11:57.328 INFO: ##### Step: 39 Learning rate: 0.005 #####
Epoch 40, Train Loss: 5.5965, Val Loss: 5.2911
2025-02-04 03:11:57.328 INFO: Epoch 40, Train Loss: 5.5965, Val Loss: 5.2911
train_e/atom_mae: 0.005641
2025-02-04 03:11:57.329 INFO: train_e/atom_mae: 0.005641
train_e/atom_rmse: 0.007305
2025-02-04 03:11:57.329 INFO: train_e/atom_rmse: 0.007305
train_f_mae: 0.047454
2025-02-04 03:11:57.332 INFO: train_f_mae: 0.047454
train_f_rmse: 0.073483
2025-02-04 03:11:57.332 INFO: train_f_rmse: 0.073483
val_e/atom_mae: 0.002690
2025-02-04 03:11:57.334 INFO: val_e/atom_mae: 0.002690
val_e/atom_rmse: 0.003621
2025-02-04 03:11:57.335 INFO: val_e/atom_rmse: 0.003621
val_f_mae: 0.043882
2025-02-04 03:11:57.335 INFO: val_f_mae: 0.043882
val_f_rmse: 0.072426
2025-02-04 03:11:57.335 INFO: val_f_rmse: 0.072426
2025-02-04 03:11:58.169 INFO: Second train loop:
2025-02-04 03:11:58.170 INFO: training
##### Step: 40 Learning rate: 0.0025 #####
2025-02-04 03:13:03.605 INFO: ##### Step: 40 Learning rate: 0.0025 #####
Epoch 1, Train Loss: 6.7934, Val Loss: 5.4658
2025-02-04 03:13:03.606 INFO: Epoch 1, Train Loss: 6.7934, Val Loss: 5.4658
train_e/atom_mae: 0.005754
2025-02-04 03:13:03.682 INFO: train_e/atom_mae: 0.005754
train_e/atom_rmse: 0.007005
2025-02-04 03:13:03.698 INFO: train_e/atom_rmse: 0.007005
train_f_mae: 0.045558
2025-02-04 03:13:03.700 INFO: train_f_mae: 0.045558
train_f_rmse: 0.070602
2025-02-04 03:13:03.701 INFO: train_f_rmse: 0.070602
val_e/atom_mae: 0.001739
2025-02-04 03:13:03.703 INFO: val_e/atom_mae: 0.001739
val_e/atom_rmse: 0.002386
2025-02-04 03:13:03.703 INFO: val_e/atom_rmse: 0.002386
val_f_mae: 0.043946
2025-02-04 03:13:03.704 INFO: val_f_mae: 0.043946
val_f_rmse: 0.072524
2025-02-04 03:13:03.704 INFO: val_f_rmse: 0.072524
##### Step: 41 Learning rate: 0.0025 #####
2025-02-04 03:14:03.486 INFO: ##### Step: 41 Learning rate: 0.0025 #####
Epoch 2, Train Loss: 6.9381, Val Loss: 5.5321
2025-02-04 03:14:03.487 INFO: Epoch 2, Train Loss: 6.9381, Val Loss: 5.5321
train_e/atom_mae: 0.005530
2025-02-04 03:14:03.487 INFO: train_e/atom_mae: 0.005530
train_e/atom_rmse: 0.007387
2025-02-04 03:14:03.488 INFO: train_e/atom_rmse: 0.007387
train_f_mae: 0.045286
2025-02-04 03:14:03.490 INFO: train_f_mae: 0.045286
train_f_rmse: 0.070187
2025-02-04 03:14:03.491 INFO: train_f_rmse: 0.070187
val_e/atom_mae: 0.001601
2025-02-04 03:14:03.493 INFO: val_e/atom_mae: 0.001601
val_e/atom_rmse: 0.002280
2025-02-04 03:14:03.493 INFO: val_e/atom_rmse: 0.002280
val_f_mae: 0.044031
2025-02-04 03:14:03.493 INFO: val_f_mae: 0.044031
val_f_rmse: 0.073100
2025-02-04 03:14:03.494 INFO: val_f_rmse: 0.073100
##### Step: 42 Learning rate: 0.0025 #####
2025-02-04 03:15:03.375 INFO: ##### Step: 42 Learning rate: 0.0025 #####
Epoch 3, Train Loss: 9.5358, Val Loss: 5.5465
2025-02-04 03:15:03.376 INFO: Epoch 3, Train Loss: 9.5358, Val Loss: 5.5465
train_e/atom_mae: 0.008924
2025-02-04 03:15:03.377 INFO: train_e/atom_mae: 0.008924
train_e/atom_rmse: 0.010487
2025-02-04 03:15:03.377 INFO: train_e/atom_rmse: 0.010487
train_f_mae: 0.048870
2025-02-04 03:15:03.379 INFO: train_f_mae: 0.048870
train_f_rmse: 0.074039
2025-02-04 03:15:03.380 INFO: train_f_rmse: 0.074039
val_e/atom_mae: 0.001729
2025-02-04 03:15:03.382 INFO: val_e/atom_mae: 0.001729
val_e/atom_rmse: 0.002427
2025-02-04 03:15:03.382 INFO: val_e/atom_rmse: 0.002427
val_f_mae: 0.044000
2025-02-04 03:15:03.383 INFO: val_f_mae: 0.044000
val_f_rmse: 0.073031
2025-02-04 03:15:03.383 INFO: val_f_rmse: 0.073031
##### Step: 43 Learning rate: 0.0025 #####
2025-02-04 03:16:03.179 INFO: ##### Step: 43 Learning rate: 0.0025 #####
Epoch 4, Train Loss: 7.4176, Val Loss: 5.5661
2025-02-04 03:16:03.180 INFO: Epoch 4, Train Loss: 7.4176, Val Loss: 5.5661
train_e/atom_mae: 0.005946
2025-02-04 03:16:03.181 INFO: train_e/atom_mae: 0.005946
train_e/atom_rmse: 0.007495
2025-02-04 03:16:03.181 INFO: train_e/atom_rmse: 0.007495
train_f_mae: 0.047934
2025-02-04 03:16:03.183 INFO: train_f_mae: 0.047934
train_f_rmse: 0.073122
2025-02-04 03:16:03.184 INFO: train_f_rmse: 0.073122
val_e/atom_mae: 0.001955
2025-02-04 03:16:03.186 INFO: val_e/atom_mae: 0.001955
val_e/atom_rmse: 0.002662
2025-02-04 03:16:03.186 INFO: val_e/atom_rmse: 0.002662
val_f_mae: 0.044236
2025-02-04 03:16:03.186 INFO: val_f_mae: 0.044236
val_f_rmse: 0.072854
2025-02-04 03:16:03.187 INFO: val_f_rmse: 0.072854
##### Step: 44 Learning rate: 0.0025 #####
2025-02-04 03:17:02.999 INFO: ##### Step: 44 Learning rate: 0.0025 #####
Epoch 5, Train Loss: 7.0407, Val Loss: 5.4742
2025-02-04 03:17:02.999 INFO: Epoch 5, Train Loss: 7.0407, Val Loss: 5.4742
train_e/atom_mae: 0.005258
2025-02-04 03:17:03.000 INFO: train_e/atom_mae: 0.005258
train_e/atom_rmse: 0.007011
2025-02-04 03:17:03.000 INFO: train_e/atom_rmse: 0.007011
train_f_mae: 0.047307
2025-02-04 03:17:03.003 INFO: train_f_mae: 0.047307
train_f_rmse: 0.072308
2025-02-04 03:17:03.003 INFO: train_f_rmse: 0.072308
val_e/atom_mae: 0.001608
2025-02-04 03:17:03.005 INFO: val_e/atom_mae: 0.001608
val_e/atom_rmse: 0.002247
2025-02-04 03:17:03.006 INFO: val_e/atom_rmse: 0.002247
val_f_mae: 0.043781
2025-02-04 03:17:03.006 INFO: val_f_mae: 0.043781
val_f_rmse: 0.072740
2025-02-04 03:17:03.006 INFO: val_f_rmse: 0.072740
##### Step: 45 Learning rate: 0.0025 #####
2025-02-04 03:18:02.826 INFO: ##### Step: 45 Learning rate: 0.0025 #####
Epoch 6, Train Loss: 6.3815, Val Loss: 5.4564
2025-02-04 03:18:02.826 INFO: Epoch 6, Train Loss: 6.3815, Val Loss: 5.4564
train_e/atom_mae: 0.004748
2025-02-04 03:18:02.827 INFO: train_e/atom_mae: 0.004748
train_e/atom_rmse: 0.005961
2025-02-04 03:18:02.828 INFO: train_e/atom_rmse: 0.005961
train_f_mae: 0.046516
2025-02-04 03:18:02.830 INFO: train_f_mae: 0.046516
train_f_rmse: 0.071215
2025-02-04 03:18:02.830 INFO: train_f_rmse: 0.071215
val_e/atom_mae: 0.001377
2025-02-04 03:18:02.833 INFO: val_e/atom_mae: 0.001377
val_e/atom_rmse: 0.002059
2025-02-04 03:18:02.833 INFO: val_e/atom_rmse: 0.002059
val_f_mae: 0.043897
2025-02-04 03:18:02.833 INFO: val_f_mae: 0.043897
val_f_rmse: 0.072821
2025-02-04 03:18:02.834 INFO: val_f_rmse: 0.072821
##### Step: 46 Learning rate: 0.0025 #####
2025-02-04 03:19:02.651 INFO: ##### Step: 46 Learning rate: 0.0025 #####
Epoch 7, Train Loss: 5.5189, Val Loss: 5.3541
2025-02-04 03:19:02.652 INFO: Epoch 7, Train Loss: 5.5189, Val Loss: 5.3541
train_e/atom_mae: 0.003170
2025-02-04 03:19:02.653 INFO: train_e/atom_mae: 0.003170
train_e/atom_rmse: 0.004176
2025-02-04 03:19:02.653 INFO: train_e/atom_rmse: 0.004176
train_f_mae: 0.045253
2025-02-04 03:19:02.655 INFO: train_f_mae: 0.045253
train_f_rmse: 0.069828
2025-02-04 03:19:02.656 INFO: train_f_rmse: 0.069828
val_e/atom_mae: 0.001418
2025-02-04 03:19:02.658 INFO: val_e/atom_mae: 0.001418
val_e/atom_rmse: 0.002056
2025-02-04 03:19:02.658 INFO: val_e/atom_rmse: 0.002056
val_f_mae: 0.043861
2025-02-04 03:19:02.658 INFO: val_f_mae: 0.043861
val_f_rmse: 0.072117
2025-02-04 03:19:02.659 INFO: val_f_rmse: 0.072117
##### Step: 47 Learning rate: 0.0025 #####
2025-02-04 03:20:02.511 INFO: ##### Step: 47 Learning rate: 0.0025 #####
Epoch 8, Train Loss: 6.1011, Val Loss: 5.4458
2025-02-04 03:20:02.512 INFO: Epoch 8, Train Loss: 6.1011, Val Loss: 5.4458
train_e/atom_mae: 0.004229
2025-02-04 03:20:02.513 INFO: train_e/atom_mae: 0.004229
train_e/atom_rmse: 0.005426
2025-02-04 03:20:02.513 INFO: train_e/atom_rmse: 0.005426
train_f_mae: 0.046110
2025-02-04 03:20:02.515 INFO: train_f_mae: 0.046110
train_f_rmse: 0.070824
2025-02-04 03:20:02.516 INFO: train_f_rmse: 0.070824
val_e/atom_mae: 0.001507
2025-02-04 03:20:02.518 INFO: val_e/atom_mae: 0.001507
val_e/atom_rmse: 0.002078
2025-02-04 03:20:02.518 INFO: val_e/atom_rmse: 0.002078
val_f_mae: 0.043894
2025-02-04 03:20:02.518 INFO: val_f_mae: 0.043894
val_f_rmse: 0.072727
2025-02-04 03:20:02.519 INFO: val_f_rmse: 0.072727
##### Step: 48 Learning rate: 0.0025 #####
2025-02-04 03:21:02.331 INFO: ##### Step: 48 Learning rate: 0.0025 #####
Epoch 9, Train Loss: 5.5174, Val Loss: 5.4290
2025-02-04 03:21:02.332 INFO: Epoch 9, Train Loss: 5.5174, Val Loss: 5.4290
train_e/atom_mae: 0.003305
2025-02-04 03:21:02.332 INFO: train_e/atom_mae: 0.003305
train_e/atom_rmse: 0.004320
2025-02-04 03:21:02.333 INFO: train_e/atom_rmse: 0.004320
train_f_mae: 0.045036
2025-02-04 03:21:02.335 INFO: train_f_mae: 0.045036
train_f_rmse: 0.069493
2025-02-04 03:21:02.335 INFO: train_f_rmse: 0.069493
val_e/atom_mae: 0.001787
2025-02-04 03:21:02.338 INFO: val_e/atom_mae: 0.001787
val_e/atom_rmse: 0.002441
2025-02-04 03:21:02.338 INFO: val_e/atom_rmse: 0.002441
val_f_mae: 0.043648
2025-02-04 03:21:02.338 INFO: val_f_mae: 0.043648
val_f_rmse: 0.072197
2025-02-04 03:21:02.339 INFO: val_f_rmse: 0.072197
##### Step: 49 Learning rate: 0.0025 #####
2025-02-04 03:22:02.149 INFO: ##### Step: 49 Learning rate: 0.0025 #####
Epoch 10, Train Loss: 5.7466, Val Loss: 5.3946
2025-02-04 03:22:02.149 INFO: Epoch 10, Train Loss: 5.7466, Val Loss: 5.3946
train_e/atom_mae: 0.003795
2025-02-04 03:22:02.150 INFO: train_e/atom_mae: 0.003795
train_e/atom_rmse: 0.004903
2025-02-04 03:22:02.150 INFO: train_e/atom_rmse: 0.004903
train_f_mae: 0.045351
2025-02-04 03:22:02.153 INFO: train_f_mae: 0.045351
train_f_rmse: 0.069718
2025-02-04 03:22:02.153 INFO: train_f_rmse: 0.069718
val_e/atom_mae: 0.001311
2025-02-04 03:22:02.155 INFO: val_e/atom_mae: 0.001311
val_e/atom_rmse: 0.001943
2025-02-04 03:22:02.156 INFO: val_e/atom_rmse: 0.001943
val_f_mae: 0.043735
2025-02-04 03:22:02.156 INFO: val_f_mae: 0.043735
val_f_rmse: 0.072511
2025-02-04 03:22:02.156 INFO: val_f_rmse: 0.072511
##### Step: 50 Learning rate: 0.0025 #####
2025-02-04 03:23:08.420 INFO: ##### Step: 50 Learning rate: 0.0025 #####
Epoch 11, Train Loss: 7.0956, Val Loss: 5.3570
2025-02-04 03:23:08.421 INFO: Epoch 11, Train Loss: 7.0956, Val Loss: 5.3570
train_e/atom_mae: 0.006423
2025-02-04 03:23:08.557 INFO: train_e/atom_mae: 0.006423
train_e/atom_rmse: 0.007589
2025-02-04 03:23:08.580 INFO: train_e/atom_rmse: 0.007589
train_f_mae: 0.045968
2025-02-04 03:23:08.582 INFO: train_f_mae: 0.045968
train_f_rmse: 0.070518
2025-02-04 03:23:08.583 INFO: train_f_rmse: 0.070518
val_e/atom_mae: 0.001564
2025-02-04 03:23:08.585 INFO: val_e/atom_mae: 0.001564
val_e/atom_rmse: 0.002075
2025-02-04 03:23:08.585 INFO: val_e/atom_rmse: 0.002075
val_f_mae: 0.043672
2025-02-04 03:23:08.586 INFO: val_f_mae: 0.043672
val_f_rmse: 0.072110
2025-02-04 03:23:08.586 INFO: val_f_rmse: 0.072110
##### Step: 51 Learning rate: 0.0025 #####
2025-02-04 03:24:08.462 INFO: ##### Step: 51 Learning rate: 0.0025 #####
Epoch 12, Train Loss: 5.7910, Val Loss: 5.3711
2025-02-04 03:24:08.462 INFO: Epoch 12, Train Loss: 5.7910, Val Loss: 5.3711
train_e/atom_mae: 0.003848
2025-02-04 03:24:08.463 INFO: train_e/atom_mae: 0.003848
train_e/atom_rmse: 0.004849
2025-02-04 03:24:08.463 INFO: train_e/atom_rmse: 0.004849
train_f_mae: 0.045727
2025-02-04 03:24:08.466 INFO: train_f_mae: 0.045727
train_f_rmse: 0.070172
2025-02-04 03:24:08.466 INFO: train_f_rmse: 0.070172
val_e/atom_mae: 0.001255
2025-02-04 03:24:08.468 INFO: val_e/atom_mae: 0.001255
val_e/atom_rmse: 0.001912
2025-02-04 03:24:08.469 INFO: val_e/atom_rmse: 0.001912
val_f_mae: 0.043755
2025-02-04 03:24:08.469 INFO: val_f_mae: 0.043755
val_f_rmse: 0.072372
2025-02-04 03:24:08.469 INFO: val_f_rmse: 0.072372
##### Step: 52 Learning rate: 0.0025 #####
2025-02-04 03:25:08.252 INFO: ##### Step: 52 Learning rate: 0.0025 #####
Epoch 13, Train Loss: 5.8920, Val Loss: 5.3739
2025-02-04 03:25:08.253 INFO: Epoch 13, Train Loss: 5.8920, Val Loss: 5.3739
train_e/atom_mae: 0.004005
2025-02-04 03:25:08.254 INFO: train_e/atom_mae: 0.004005
train_e/atom_rmse: 0.005160
2025-02-04 03:25:08.254 INFO: train_e/atom_rmse: 0.005160
train_f_mae: 0.045360
2025-02-04 03:25:08.256 INFO: train_f_mae: 0.045360
train_f_rmse: 0.070076
2025-02-04 03:25:08.257 INFO: train_f_rmse: 0.070076
val_e/atom_mae: 0.001464
2025-02-04 03:25:08.259 INFO: val_e/atom_mae: 0.001464
val_e/atom_rmse: 0.002164
2025-02-04 03:25:08.259 INFO: val_e/atom_rmse: 0.002164
val_f_mae: 0.043736
2025-02-04 03:25:08.260 INFO: val_f_mae: 0.043736
val_f_rmse: 0.072144
2025-02-04 03:25:08.260 INFO: val_f_rmse: 0.072144
##### Step: 53 Learning rate: 0.0025 #####
2025-02-04 03:26:08.024 INFO: ##### Step: 53 Learning rate: 0.0025 #####
Epoch 14, Train Loss: 6.8766, Val Loss: 5.4776
2025-02-04 03:26:08.024 INFO: Epoch 14, Train Loss: 6.8766, Val Loss: 5.4776
train_e/atom_mae: 0.004872
2025-02-04 03:26:08.025 INFO: train_e/atom_mae: 0.004872
train_e/atom_rmse: 0.006166
2025-02-04 03:26:08.025 INFO: train_e/atom_rmse: 0.006166
train_f_mae: 0.048784
2025-02-04 03:26:08.028 INFO: train_f_mae: 0.048784
train_f_rmse: 0.073994
2025-02-04 03:26:08.028 INFO: train_f_rmse: 0.073994
val_e/atom_mae: 0.001465
2025-02-04 03:26:08.030 INFO: val_e/atom_mae: 0.001465
val_e/atom_rmse: 0.002225
2025-02-04 03:26:08.030 INFO: val_e/atom_rmse: 0.002225
val_f_mae: 0.043822
2025-02-04 03:26:08.031 INFO: val_f_mae: 0.043822
val_f_rmse: 0.072794
2025-02-04 03:26:08.031 INFO: val_f_rmse: 0.072794
##### Step: 54 Learning rate: 0.0025 #####
2025-02-04 03:27:07.821 INFO: ##### Step: 54 Learning rate: 0.0025 #####
Epoch 15, Train Loss: 6.3105, Val Loss: 5.4036
2025-02-04 03:27:07.821 INFO: Epoch 15, Train Loss: 6.3105, Val Loss: 5.4036
train_e/atom_mae: 0.004413
2025-02-04 03:27:07.822 INFO: train_e/atom_mae: 0.004413
train_e/atom_rmse: 0.005708
2025-02-04 03:27:07.822 INFO: train_e/atom_rmse: 0.005708
train_f_mae: 0.046692
2025-02-04 03:27:07.825 INFO: train_f_mae: 0.046692
train_f_rmse: 0.071481
2025-02-04 03:27:07.825 INFO: train_f_rmse: 0.071481
val_e/atom_mae: 0.001387
2025-02-04 03:27:07.827 INFO: val_e/atom_mae: 0.001387
val_e/atom_rmse: 0.002033
2025-02-04 03:27:07.828 INFO: val_e/atom_rmse: 0.002033
val_f_mae: 0.043744
2025-02-04 03:27:07.828 INFO: val_f_mae: 0.043744
val_f_rmse: 0.072485
2025-02-04 03:27:07.828 INFO: val_f_rmse: 0.072485
##### Step: 55 Learning rate: 0.0025 #####
2025-02-04 03:28:07.615 INFO: ##### Step: 55 Learning rate: 0.0025 #####
Epoch 16, Train Loss: 5.7285, Val Loss: 5.3630
2025-02-04 03:28:07.615 INFO: Epoch 16, Train Loss: 5.7285, Val Loss: 5.3630
train_e/atom_mae: 0.003545
2025-02-04 03:28:07.616 INFO: train_e/atom_mae: 0.003545
train_e/atom_rmse: 0.004742
2025-02-04 03:28:07.616 INFO: train_e/atom_rmse: 0.004742
train_f_mae: 0.045221
2025-02-04 03:28:07.619 INFO: train_f_mae: 0.045221
train_f_rmse: 0.069998
2025-02-04 03:28:07.619 INFO: train_f_rmse: 0.069998
val_e/atom_mae: 0.001406
2025-02-04 03:28:07.621 INFO: val_e/atom_mae: 0.001406
val_e/atom_rmse: 0.002036
2025-02-04 03:28:07.622 INFO: val_e/atom_rmse: 0.002036
val_f_mae: 0.043700
2025-02-04 03:28:07.622 INFO: val_f_mae: 0.043700
val_f_rmse: 0.072195
2025-02-04 03:28:07.622 INFO: val_f_rmse: 0.072195
##### Step: 56 Learning rate: 0.0025 #####
2025-02-04 03:29:07.485 INFO: ##### Step: 56 Learning rate: 0.0025 #####
Epoch 17, Train Loss: 6.4998, Val Loss: 5.4732
2025-02-04 03:29:07.485 INFO: Epoch 17, Train Loss: 6.4998, Val Loss: 5.4732
train_e/atom_mae: 0.005381
2025-02-04 03:29:07.486 INFO: train_e/atom_mae: 0.005381
train_e/atom_rmse: 0.006576
2025-02-04 03:29:07.486 INFO: train_e/atom_rmse: 0.006576
train_f_mae: 0.045517
2025-02-04 03:29:07.489 INFO: train_f_mae: 0.045517
train_f_rmse: 0.070040
2025-02-04 03:29:07.489 INFO: train_f_rmse: 0.070040
val_e/atom_mae: 0.001767
2025-02-04 03:29:07.491 INFO: val_e/atom_mae: 0.001767
val_e/atom_rmse: 0.002308
2025-02-04 03:29:07.492 INFO: val_e/atom_rmse: 0.002308
val_f_mae: 0.043810
2025-02-04 03:29:07.492 INFO: val_f_mae: 0.043810
val_f_rmse: 0.072658
2025-02-04 03:29:07.492 INFO: val_f_rmse: 0.072658
##### Step: 57 Learning rate: 0.0025 #####
2025-02-04 03:30:07.294 INFO: ##### Step: 57 Learning rate: 0.0025 #####
Epoch 18, Train Loss: 7.0546, Val Loss: 5.3237
2025-02-04 03:30:07.295 INFO: Epoch 18, Train Loss: 7.0546, Val Loss: 5.3237
train_e/atom_mae: 0.004483
2025-02-04 03:30:07.295 INFO: train_e/atom_mae: 0.004483
train_e/atom_rmse: 0.005968
2025-02-04 03:30:07.296 INFO: train_e/atom_rmse: 0.005968
train_f_mae: 0.050575
2025-02-04 03:30:07.298 INFO: train_f_mae: 0.050575
train_f_rmse: 0.075775
2025-02-04 03:30:07.299 INFO: train_f_rmse: 0.075775
val_e/atom_mae: 0.001369
2025-02-04 03:30:07.301 INFO: val_e/atom_mae: 0.001369
val_e/atom_rmse: 0.002026
2025-02-04 03:30:07.301 INFO: val_e/atom_rmse: 0.002026
val_f_mae: 0.043796
2025-02-04 03:30:07.301 INFO: val_f_mae: 0.043796
val_f_rmse: 0.071936
2025-02-04 03:30:07.302 INFO: val_f_rmse: 0.071936
##### Step: 58 Learning rate: 0.0025 #####
2025-02-04 03:31:07.247 INFO: ##### Step: 58 Learning rate: 0.0025 #####
Epoch 19, Train Loss: 6.6446, Val Loss: 5.6177
2025-02-04 03:31:07.247 INFO: Epoch 19, Train Loss: 6.6446, Val Loss: 5.6177
train_e/atom_mae: 0.004880
2025-02-04 03:31:07.248 INFO: train_e/atom_mae: 0.004880
train_e/atom_rmse: 0.006623
2025-02-04 03:31:07.248 INFO: train_e/atom_rmse: 0.006623
train_f_mae: 0.046075
2025-02-04 03:31:07.251 INFO: train_f_mae: 0.046075
train_f_rmse: 0.070904
2025-02-04 03:31:07.251 INFO: train_f_rmse: 0.070904
val_e/atom_mae: 0.002069
2025-02-04 03:31:07.253 INFO: val_e/atom_mae: 0.002069
val_e/atom_rmse: 0.002641
2025-02-04 03:31:07.254 INFO: val_e/atom_rmse: 0.002641
val_f_mae: 0.044601
2025-02-04 03:31:07.254 INFO: val_f_mae: 0.044601
val_f_rmse: 0.073231
2025-02-04 03:31:07.254 INFO: val_f_rmse: 0.073231
##### Step: 59 Learning rate: 0.0025 #####
2025-02-04 03:32:07.140 INFO: ##### Step: 59 Learning rate: 0.0025 #####
Epoch 20, Train Loss: 7.3305, Val Loss: 5.3954
2025-02-04 03:32:07.141 INFO: Epoch 20, Train Loss: 7.3305, Val Loss: 5.3954
train_e/atom_mae: 0.006122
2025-02-04 03:32:07.142 INFO: train_e/atom_mae: 0.006122
train_e/atom_rmse: 0.007578
2025-02-04 03:32:07.142 INFO: train_e/atom_rmse: 0.007578
train_f_mae: 0.047102
2025-02-04 03:32:07.144 INFO: train_f_mae: 0.047102
train_f_rmse: 0.072205
2025-02-04 03:32:07.145 INFO: train_f_rmse: 0.072205
val_e/atom_mae: 0.001343
2025-02-04 03:32:07.147 INFO: val_e/atom_mae: 0.001343
val_e/atom_rmse: 0.002064
2025-02-04 03:32:07.147 INFO: val_e/atom_rmse: 0.002064
val_f_mae: 0.043737
2025-02-04 03:32:07.148 INFO: val_f_mae: 0.043737
val_f_rmse: 0.072394
2025-02-04 03:32:07.148 INFO: val_f_rmse: 0.072394
##### Step: 60 Learning rate: 0.00125 #####
2025-02-04 03:33:06.924 INFO: ##### Step: 60 Learning rate: 0.00125 #####
Epoch 21, Train Loss: 5.1020, Val Loss: 5.3834
2025-02-04 03:33:06.924 INFO: Epoch 21, Train Loss: 5.1020, Val Loss: 5.3834
train_e/atom_mae: 0.002930
2025-02-04 03:33:06.925 INFO: train_e/atom_mae: 0.002930
train_e/atom_rmse: 0.003944
2025-02-04 03:33:06.926 INFO: train_e/atom_rmse: 0.003944
train_f_mae: 0.043225
2025-02-04 03:33:06.928 INFO: train_f_mae: 0.043225
train_f_rmse: 0.067295
2025-02-04 03:33:06.928 INFO: train_f_rmse: 0.067295
val_e/atom_mae: 0.001292
2025-02-04 03:33:06.930 INFO: val_e/atom_mae: 0.001292
val_e/atom_rmse: 0.001976
2025-02-04 03:33:06.931 INFO: val_e/atom_rmse: 0.001976
val_f_mae: 0.043577
2025-02-04 03:33:06.931 INFO: val_f_mae: 0.043577
val_f_rmse: 0.072402
2025-02-04 03:33:06.931 INFO: val_f_rmse: 0.072402
##### Step: 61 Learning rate: 0.00125 #####
2025-02-04 03:34:06.716 INFO: ##### Step: 61 Learning rate: 0.00125 #####
Epoch 22, Train Loss: 4.8806, Val Loss: 5.3705
2025-02-04 03:34:06.717 INFO: Epoch 22, Train Loss: 4.8806, Val Loss: 5.3705
train_e/atom_mae: 0.002454
2025-02-04 03:34:06.717 INFO: train_e/atom_mae: 0.002454
train_e/atom_rmse: 0.003316
2025-02-04 03:34:06.718 INFO: train_e/atom_rmse: 0.003316
train_f_mae: 0.042836
2025-02-04 03:34:06.720 INFO: train_f_mae: 0.042836
train_f_rmse: 0.066898
2025-02-04 03:34:06.720 INFO: train_f_rmse: 0.066898
val_e/atom_mae: 0.001432
2025-02-04 03:34:06.723 INFO: val_e/atom_mae: 0.001432
val_e/atom_rmse: 0.002066
2025-02-04 03:34:06.723 INFO: val_e/atom_rmse: 0.002066
val_f_mae: 0.043537
2025-02-04 03:34:06.723 INFO: val_f_mae: 0.043537
val_f_rmse: 0.072219
2025-02-04 03:34:06.724 INFO: val_f_rmse: 0.072219
##### Step: 62 Learning rate: 0.00125 #####
2025-02-04 03:35:58.935 INFO: ##### Step: 62 Learning rate: 0.00125 #####
Epoch 23, Train Loss: 4.9078, Val Loss: 5.3593
2025-02-04 03:35:58.936 INFO: Epoch 23, Train Loss: 4.9078, Val Loss: 5.3593
train_e/atom_mae: 0.002360
2025-02-04 03:35:59.665 INFO: train_e/atom_mae: 0.002360
train_e/atom_rmse: 0.003265
2025-02-04 03:36:00.135 INFO: train_e/atom_rmse: 0.003265
train_f_mae: 0.042970
2025-02-04 03:36:00.138 INFO: train_f_mae: 0.042970
train_f_rmse: 0.067192
2025-02-04 03:36:00.138 INFO: train_f_rmse: 0.067192
val_e/atom_mae: 0.001302
2025-02-04 03:36:00.140 INFO: val_e/atom_mae: 0.001302
val_e/atom_rmse: 0.001977
2025-02-04 03:36:00.141 INFO: val_e/atom_rmse: 0.001977
val_f_mae: 0.043451
2025-02-04 03:36:00.141 INFO: val_f_mae: 0.043451
val_f_rmse: 0.072234
2025-02-04 03:36:00.141 INFO: val_f_rmse: 0.072234
##### Step: 63 Learning rate: 0.00125 #####
2025-02-04 03:36:59.965 INFO: ##### Step: 63 Learning rate: 0.00125 #####
Epoch 24, Train Loss: 5.2913, Val Loss: 5.3476
2025-02-04 03:36:59.965 INFO: Epoch 24, Train Loss: 5.2913, Val Loss: 5.3476
train_e/atom_mae: 0.003390
2025-02-04 03:36:59.966 INFO: train_e/atom_mae: 0.003390
train_e/atom_rmse: 0.004344
2025-02-04 03:36:59.966 INFO: train_e/atom_rmse: 0.004344
train_f_mae: 0.043691
2025-02-04 03:36:59.969 INFO: train_f_mae: 0.043691
train_f_rmse: 0.067792
2025-02-04 03:36:59.969 INFO: train_f_rmse: 0.067792
val_e/atom_mae: 0.001346
2025-02-04 03:36:59.971 INFO: val_e/atom_mae: 0.001346
val_e/atom_rmse: 0.002047
2025-02-04 03:36:59.972 INFO: val_e/atom_rmse: 0.002047
val_f_mae: 0.043490
2025-02-04 03:36:59.972 INFO: val_f_mae: 0.043490
val_f_rmse: 0.072078
2025-02-04 03:36:59.972 INFO: val_f_rmse: 0.072078
##### Step: 64 Learning rate: 0.00125 #####
2025-02-04 03:37:59.778 INFO: ##### Step: 64 Learning rate: 0.00125 #####
Epoch 25, Train Loss: 4.9106, Val Loss: 5.3309
2025-02-04 03:37:59.779 INFO: Epoch 25, Train Loss: 4.9106, Val Loss: 5.3309
train_e/atom_mae: 0.002690
2025-02-04 03:37:59.779 INFO: train_e/atom_mae: 0.002690
train_e/atom_rmse: 0.003558
2025-02-04 03:37:59.780 INFO: train_e/atom_rmse: 0.003558
train_f_mae: 0.042691
2025-02-04 03:37:59.782 INFO: train_f_mae: 0.042691
train_f_rmse: 0.066663
2025-02-04 03:37:59.783 INFO: train_f_rmse: 0.066663
val_e/atom_mae: 0.001366
2025-02-04 03:37:59.785 INFO: val_e/atom_mae: 0.001366
val_e/atom_rmse: 0.002070
2025-02-04 03:37:59.785 INFO: val_e/atom_rmse: 0.002070
val_f_mae: 0.043360
2025-02-04 03:37:59.785 INFO: val_f_mae: 0.043360
val_f_rmse: 0.071941
2025-02-04 03:37:59.786 INFO: val_f_rmse: 0.071941
##### Step: 65 Learning rate: 0.00125 #####
2025-02-04 03:38:59.576 INFO: ##### Step: 65 Learning rate: 0.00125 #####
Epoch 26, Train Loss: 4.8577, Val Loss: 5.3290
2025-02-04 03:38:59.576 INFO: Epoch 26, Train Loss: 4.8577, Val Loss: 5.3290
train_e/atom_mae: 0.002468
2025-02-04 03:38:59.577 INFO: train_e/atom_mae: 0.002468
train_e/atom_rmse: 0.003367
2025-02-04 03:38:59.578 INFO: train_e/atom_rmse: 0.003367
train_f_mae: 0.042628
2025-02-04 03:38:59.580 INFO: train_f_mae: 0.042628
train_f_rmse: 0.066631
2025-02-04 03:38:59.580 INFO: train_f_rmse: 0.066631
val_e/atom_mae: 0.001303
2025-02-04 03:38:59.582 INFO: val_e/atom_mae: 0.001303
val_e/atom_rmse: 0.001945
2025-02-04 03:38:59.583 INFO: val_e/atom_rmse: 0.001945
val_f_mae: 0.043384
2025-02-04 03:38:59.583 INFO: val_f_mae: 0.043384
val_f_rmse: 0.072055
2025-02-04 03:38:59.583 INFO: val_f_rmse: 0.072055
##### Step: 66 Learning rate: 0.00125 #####
2025-02-04 03:39:59.432 INFO: ##### Step: 66 Learning rate: 0.00125 #####
Epoch 27, Train Loss: 4.9325, Val Loss: 5.3024
2025-02-04 03:39:59.433 INFO: Epoch 27, Train Loss: 4.9325, Val Loss: 5.3024
train_e/atom_mae: 0.002725
2025-02-04 03:39:59.434 INFO: train_e/atom_mae: 0.002725
train_e/atom_rmse: 0.003515
2025-02-04 03:39:59.434 INFO: train_e/atom_rmse: 0.003515
train_f_mae: 0.042876
2025-02-04 03:39:59.437 INFO: train_f_mae: 0.042876
train_f_rmse: 0.066911
2025-02-04 03:39:59.437 INFO: train_f_rmse: 0.066911
val_e/atom_mae: 0.001400
2025-02-04 03:39:59.439 INFO: val_e/atom_mae: 0.001400
val_e/atom_rmse: 0.001996
2025-02-04 03:39:59.439 INFO: val_e/atom_rmse: 0.001996
val_f_mae: 0.043330
2025-02-04 03:39:59.440 INFO: val_f_mae: 0.043330
val_f_rmse: 0.071815
2025-02-04 03:39:59.440 INFO: val_f_rmse: 0.071815
##### Step: 67 Learning rate: 0.00125 #####
2025-02-04 03:41:00.473 INFO: ##### Step: 67 Learning rate: 0.00125 #####
Epoch 28, Train Loss: 5.0282, Val Loss: 5.3352
2025-02-04 03:41:00.473 INFO: Epoch 28, Train Loss: 5.0282, Val Loss: 5.3352
train_e/atom_mae: 0.002757
2025-02-04 03:41:00.474 INFO: train_e/atom_mae: 0.002757
train_e/atom_rmse: 0.003751
2025-02-04 03:41:00.474 INFO: train_e/atom_rmse: 0.003751
train_f_mae: 0.043151
2025-02-04 03:41:00.477 INFO: train_f_mae: 0.043151
train_f_rmse: 0.067152
2025-02-04 03:41:00.477 INFO: train_f_rmse: 0.067152
val_e/atom_mae: 0.001412
2025-02-04 03:41:00.479 INFO: val_e/atom_mae: 0.001412
val_e/atom_rmse: 0.002091
2025-02-04 03:41:00.480 INFO: val_e/atom_rmse: 0.002091
val_f_mae: 0.043324
2025-02-04 03:41:00.480 INFO: val_f_mae: 0.043324
val_f_rmse: 0.071951
2025-02-04 03:41:00.480 INFO: val_f_rmse: 0.071951
##### Step: 68 Learning rate: 0.00125 #####
2025-02-04 03:42:03.525 INFO: ##### Step: 68 Learning rate: 0.00125 #####
Epoch 29, Train Loss: 4.8575, Val Loss: 5.3127
2025-02-04 03:42:03.526 INFO: Epoch 29, Train Loss: 4.8575, Val Loss: 5.3127
train_e/atom_mae: 0.002178
2025-02-04 03:42:03.574 INFO: train_e/atom_mae: 0.002178
train_e/atom_rmse: 0.003056
2025-02-04 03:42:03.586 INFO: train_e/atom_rmse: 0.003056
train_f_mae: 0.043068
2025-02-04 03:42:03.588 INFO: train_f_mae: 0.043068
train_f_rmse: 0.067181
2025-02-04 03:42:03.589 INFO: train_f_rmse: 0.067181
val_e/atom_mae: 0.001423
2025-02-04 03:42:03.591 INFO: val_e/atom_mae: 0.001423
val_e/atom_rmse: 0.001994
2025-02-04 03:42:03.591 INFO: val_e/atom_rmse: 0.001994
val_f_mae: 0.043376
2025-02-04 03:42:03.591 INFO: val_f_mae: 0.043376
val_f_rmse: 0.071892
2025-02-04 03:42:03.592 INFO: val_f_rmse: 0.071892
##### Step: 69 Learning rate: 0.00125 #####
2025-02-04 03:43:03.581 INFO: ##### Step: 69 Learning rate: 0.00125 #####
Epoch 30, Train Loss: 4.8162, Val Loss: 5.3344
2025-02-04 03:43:03.581 INFO: Epoch 30, Train Loss: 4.8162, Val Loss: 5.3344
train_e/atom_mae: 0.002113
2025-02-04 03:43:03.582 INFO: train_e/atom_mae: 0.002113
train_e/atom_rmse: 0.002987
2025-02-04 03:43:03.582 INFO: train_e/atom_rmse: 0.002987
train_f_mae: 0.043024
2025-02-04 03:43:03.585 INFO: train_f_mae: 0.043024
train_f_rmse: 0.066987
2025-02-04 03:43:03.585 INFO: train_f_rmse: 0.066987
val_e/atom_mae: 0.001323
2025-02-04 03:43:03.587 INFO: val_e/atom_mae: 0.001323
val_e/atom_rmse: 0.001879
2025-02-04 03:43:03.587 INFO: val_e/atom_rmse: 0.001879
val_f_mae: 0.043406
2025-02-04 03:43:03.588 INFO: val_f_mae: 0.043406
val_f_rmse: 0.072156
2025-02-04 03:43:03.588 INFO: val_f_rmse: 0.072156
##### Step: 70 Learning rate: 0.00125 #####
2025-02-04 03:44:06.307 INFO: ##### Step: 70 Learning rate: 0.00125 #####
Epoch 31, Train Loss: 5.5086, Val Loss: 5.3161
2025-02-04 03:44:06.307 INFO: Epoch 31, Train Loss: 5.5086, Val Loss: 5.3161
train_e/atom_mae: 0.003895
2025-02-04 03:44:06.308 INFO: train_e/atom_mae: 0.003895
train_e/atom_rmse: 0.005005
2025-02-04 03:44:06.308 INFO: train_e/atom_rmse: 0.005005
train_f_mae: 0.043963
2025-02-04 03:44:06.311 INFO: train_f_mae: 0.043963
train_f_rmse: 0.067714
2025-02-04 03:44:06.311 INFO: train_f_rmse: 0.067714
val_e/atom_mae: 0.001431
2025-02-04 03:44:06.313 INFO: val_e/atom_mae: 0.001431
val_e/atom_rmse: 0.001939
2025-02-04 03:44:06.314 INFO: val_e/atom_rmse: 0.001939
val_f_mae: 0.043381
2025-02-04 03:44:06.314 INFO: val_f_mae: 0.043381
val_f_rmse: 0.071970
2025-02-04 03:44:06.314 INFO: val_f_rmse: 0.071970
##### Step: 71 Learning rate: 0.00125 #####
2025-02-04 03:45:06.310 INFO: ##### Step: 71 Learning rate: 0.00125 #####
Epoch 32, Train Loss: 4.9317, Val Loss: 5.3085
2025-02-04 03:45:06.310 INFO: Epoch 32, Train Loss: 4.9317, Val Loss: 5.3085
train_e/atom_mae: 0.002573
2025-02-04 03:45:06.311 INFO: train_e/atom_mae: 0.002573
train_e/atom_rmse: 0.003464
2025-02-04 03:45:06.311 INFO: train_e/atom_rmse: 0.003464
train_f_mae: 0.043113
2025-02-04 03:45:06.314 INFO: train_f_mae: 0.043113
train_f_rmse: 0.067003
2025-02-04 03:45:06.314 INFO: train_f_rmse: 0.067003
val_e/atom_mae: 0.001421
2025-02-04 03:45:06.316 INFO: val_e/atom_mae: 0.001421
val_e/atom_rmse: 0.001977
2025-02-04 03:45:06.317 INFO: val_e/atom_rmse: 0.001977
val_f_mae: 0.043290
2025-02-04 03:45:06.317 INFO: val_f_mae: 0.043290
val_f_rmse: 0.071881
2025-02-04 03:45:06.317 INFO: val_f_rmse: 0.071881
##### Step: 72 Learning rate: 0.00125 #####
2025-02-04 03:46:06.337 INFO: ##### Step: 72 Learning rate: 0.00125 #####
Epoch 33, Train Loss: 5.2697, Val Loss: 5.2637
2025-02-04 03:46:06.337 INFO: Epoch 33, Train Loss: 5.2697, Val Loss: 5.2637
train_e/atom_mae: 0.003653
2025-02-04 03:46:06.338 INFO: train_e/atom_mae: 0.003653
train_e/atom_rmse: 0.004651
2025-02-04 03:46:06.338 INFO: train_e/atom_rmse: 0.004651
train_f_mae: 0.042911
2025-02-04 03:46:06.341 INFO: train_f_mae: 0.042911
train_f_rmse: 0.066875
2025-02-04 03:46:06.341 INFO: train_f_rmse: 0.066875
val_e/atom_mae: 0.001408
2025-02-04 03:46:06.343 INFO: val_e/atom_mae: 0.001408
val_e/atom_rmse: 0.001967
2025-02-04 03:46:06.344 INFO: val_e/atom_rmse: 0.001967
val_f_mae: 0.043302
2025-02-04 03:46:06.344 INFO: val_f_mae: 0.043302
val_f_rmse: 0.071574
2025-02-04 03:46:06.344 INFO: val_f_rmse: 0.071574
##### Step: 73 Learning rate: 0.00125 #####
2025-02-04 03:47:06.376 INFO: ##### Step: 73 Learning rate: 0.00125 #####
Epoch 34, Train Loss: 4.9499, Val Loss: 5.3182
2025-02-04 03:47:06.377 INFO: Epoch 34, Train Loss: 4.9499, Val Loss: 5.3182
train_e/atom_mae: 0.002747
2025-02-04 03:47:06.377 INFO: train_e/atom_mae: 0.002747
train_e/atom_rmse: 0.003674
2025-02-04 03:47:06.378 INFO: train_e/atom_rmse: 0.003674
train_f_mae: 0.042826
2025-02-04 03:47:06.380 INFO: train_f_mae: 0.042826
train_f_rmse: 0.066726
2025-02-04 03:47:06.381 INFO: train_f_rmse: 0.066726
val_e/atom_mae: 0.001340
2025-02-04 03:47:06.383 INFO: val_e/atom_mae: 0.001340
val_e/atom_rmse: 0.001930
2025-02-04 03:47:06.383 INFO: val_e/atom_rmse: 0.001930
val_f_mae: 0.043309
2025-02-04 03:47:06.383 INFO: val_f_mae: 0.043309
val_f_rmse: 0.071993
2025-02-04 03:47:06.384 INFO: val_f_rmse: 0.071993
##### Step: 74 Learning rate: 0.00125 #####
2025-02-04 03:48:06.301 INFO: ##### Step: 74 Learning rate: 0.00125 #####
Epoch 35, Train Loss: 5.1628, Val Loss: 5.3056
2025-02-04 03:48:06.302 INFO: Epoch 35, Train Loss: 5.1628, Val Loss: 5.3056
train_e/atom_mae: 0.003508
2025-02-04 03:48:06.302 INFO: train_e/atom_mae: 0.003508
train_e/atom_rmse: 0.004419
2025-02-04 03:48:06.303 INFO: train_e/atom_rmse: 0.004419
train_f_mae: 0.042982
2025-02-04 03:48:06.305 INFO: train_f_mae: 0.042982
train_f_rmse: 0.066654
2025-02-04 03:48:06.305 INFO: train_f_rmse: 0.066654
val_e/atom_mae: 0.001368
2025-02-04 03:48:06.307 INFO: val_e/atom_mae: 0.001368
val_e/atom_rmse: 0.001924
2025-02-04 03:48:06.308 INFO: val_e/atom_rmse: 0.001924
val_f_mae: 0.043344
2025-02-04 03:48:06.308 INFO: val_f_mae: 0.043344
val_f_rmse: 0.071911
2025-02-04 03:48:06.309 INFO: val_f_rmse: 0.071911
##### Step: 75 Learning rate: 0.00125 #####
2025-02-04 03:49:06.209 INFO: ##### Step: 75 Learning rate: 0.00125 #####
Epoch 36, Train Loss: 5.2958, Val Loss: 5.2918
2025-02-04 03:49:06.210 INFO: Epoch 36, Train Loss: 5.2958, Val Loss: 5.2918
train_e/atom_mae: 0.003771
2025-02-04 03:49:06.211 INFO: train_e/atom_mae: 0.003771
train_e/atom_rmse: 0.004650
2025-02-04 03:49:06.211 INFO: train_e/atom_rmse: 0.004650
train_f_mae: 0.043293
2025-02-04 03:49:06.213 INFO: train_f_mae: 0.043293
train_f_rmse: 0.067072
2025-02-04 03:49:06.214 INFO: train_f_rmse: 0.067072
val_e/atom_mae: 0.001495
2025-02-04 03:49:06.216 INFO: val_e/atom_mae: 0.001495
val_e/atom_rmse: 0.001997
2025-02-04 03:49:06.216 INFO: val_e/atom_rmse: 0.001997
val_f_mae: 0.043314
2025-02-04 03:49:06.217 INFO: val_f_mae: 0.043314
val_f_rmse: 0.071741
2025-02-04 03:49:06.217 INFO: val_f_rmse: 0.071741
##### Step: 76 Learning rate: 0.00125 #####
2025-02-04 03:50:06.164 INFO: ##### Step: 76 Learning rate: 0.00125 #####
Epoch 37, Train Loss: 5.0621, Val Loss: 5.3251
2025-02-04 03:50:06.165 INFO: Epoch 37, Train Loss: 5.0621, Val Loss: 5.3251
train_e/atom_mae: 0.003073
2025-02-04 03:50:06.165 INFO: train_e/atom_mae: 0.003073
train_e/atom_rmse: 0.003931
2025-02-04 03:50:06.166 INFO: train_e/atom_rmse: 0.003931
train_f_mae: 0.043231
2025-02-04 03:50:06.168 INFO: train_f_mae: 0.043231
train_f_rmse: 0.067025
2025-02-04 03:50:06.169 INFO: train_f_rmse: 0.067025
val_e/atom_mae: 0.001347
2025-02-04 03:50:06.171 INFO: val_e/atom_mae: 0.001347
val_e/atom_rmse: 0.002117
2025-02-04 03:50:06.171 INFO: val_e/atom_rmse: 0.002117
val_f_mae: 0.043244
2025-02-04 03:50:06.171 INFO: val_f_mae: 0.043244
val_f_rmse: 0.071852
2025-02-04 03:50:06.172 INFO: val_f_rmse: 0.071852
##### Step: 77 Learning rate: 0.00125 #####
2025-02-04 03:51:05.930 INFO: ##### Step: 77 Learning rate: 0.00125 #####
Epoch 38, Train Loss: 5.1820, Val Loss: 5.3239
2025-02-04 03:51:05.931 INFO: Epoch 38, Train Loss: 5.1820, Val Loss: 5.3239
train_e/atom_mae: 0.003391
2025-02-04 03:51:05.932 INFO: train_e/atom_mae: 0.003391
train_e/atom_rmse: 0.004436
2025-02-04 03:51:05.932 INFO: train_e/atom_rmse: 0.004436
train_f_mae: 0.042914
2025-02-04 03:51:05.935 INFO: train_f_mae: 0.042914
train_f_rmse: 0.066757
2025-02-04 03:51:05.935 INFO: train_f_rmse: 0.066757
val_e/atom_mae: 0.001383
2025-02-04 03:51:05.937 INFO: val_e/atom_mae: 0.001383
val_e/atom_rmse: 0.001992
2025-02-04 03:51:05.937 INFO: val_e/atom_rmse: 0.001992
val_f_mae: 0.043248
2025-02-04 03:51:05.938 INFO: val_f_mae: 0.043248
val_f_rmse: 0.071969
2025-02-04 03:51:05.938 INFO: val_f_rmse: 0.071969
##### Step: 78 Learning rate: 0.00125 #####
2025-02-04 03:52:05.829 INFO: ##### Step: 78 Learning rate: 0.00125 #####
Epoch 39, Train Loss: 5.2830, Val Loss: 5.2668
2025-02-04 03:52:05.829 INFO: Epoch 39, Train Loss: 5.2830, Val Loss: 5.2668
train_e/atom_mae: 0.003666
2025-02-04 03:52:05.830 INFO: train_e/atom_mae: 0.003666
train_e/atom_rmse: 0.004662
2025-02-04 03:52:05.830 INFO: train_e/atom_rmse: 0.004662
train_f_mae: 0.043099
2025-02-04 03:52:05.832 INFO: train_f_mae: 0.043099
train_f_rmse: 0.066946
2025-02-04 03:52:05.833 INFO: train_f_rmse: 0.066946
val_e/atom_mae: 0.001284
2025-02-04 03:52:05.835 INFO: val_e/atom_mae: 0.001284
val_e/atom_rmse: 0.001912
2025-02-04 03:52:05.835 INFO: val_e/atom_rmse: 0.001912
val_f_mae: 0.043244
2025-02-04 03:52:05.836 INFO: val_f_mae: 0.043244
val_f_rmse: 0.071653
2025-02-04 03:52:05.836 INFO: val_f_rmse: 0.071653
##### Step: 79 Learning rate: 0.00125 #####
2025-02-04 03:53:05.792 INFO: ##### Step: 79 Learning rate: 0.00125 #####
Epoch 40, Train Loss: 4.7756, Val Loss: 5.2917
2025-02-04 03:53:05.793 INFO: Epoch 40, Train Loss: 4.7756, Val Loss: 5.2917
train_e/atom_mae: 0.002391
2025-02-04 03:53:05.794 INFO: train_e/atom_mae: 0.002391
train_e/atom_rmse: 0.003270
2025-02-04 03:53:05.794 INFO: train_e/atom_rmse: 0.003270
train_f_mae: 0.042326
2025-02-04 03:53:05.796 INFO: train_f_mae: 0.042326
train_f_rmse: 0.066192
2025-02-04 03:53:05.797 INFO: train_f_rmse: 0.066192
val_e/atom_mae: 0.001276
2025-02-04 03:53:05.799 INFO: val_e/atom_mae: 0.001276
val_e/atom_rmse: 0.001886
2025-02-04 03:53:05.799 INFO: val_e/atom_rmse: 0.001886
val_f_mae: 0.043282
2025-02-04 03:53:05.800 INFO: val_f_mae: 0.043282
val_f_rmse: 0.071852
2025-02-04 03:53:05.800 INFO: val_f_rmse: 0.071852
##### Step: 80 Learning rate: 0.000625 #####
2025-02-04 03:54:05.765 INFO: ##### Step: 80 Learning rate: 0.000625 #####
Epoch 41, Train Loss: 4.6423, Val Loss: 5.2753
2025-02-04 03:54:05.765 INFO: Epoch 41, Train Loss: 4.6423, Val Loss: 5.2753
train_e/atom_mae: 0.002255
2025-02-04 03:54:05.766 INFO: train_e/atom_mae: 0.002255
train_e/atom_rmse: 0.002963
2025-02-04 03:54:05.766 INFO: train_e/atom_rmse: 0.002963
train_f_mae: 0.041998
2025-02-04 03:54:05.769 INFO: train_f_mae: 0.041998
train_f_rmse: 0.065717
2025-02-04 03:54:05.769 INFO: train_f_rmse: 0.065717
val_e/atom_mae: 0.001389
2025-02-04 03:54:05.771 INFO: val_e/atom_mae: 0.001389
val_e/atom_rmse: 0.001960
2025-02-04 03:54:05.772 INFO: val_e/atom_rmse: 0.001960
val_f_mae: 0.043174
2025-02-04 03:54:05.772 INFO: val_f_mae: 0.043174
val_f_rmse: 0.071668
2025-02-04 03:54:05.772 INFO: val_f_rmse: 0.071668
##### Step: 81 Learning rate: 0.000625 #####
2025-02-04 03:55:05.701 INFO: ##### Step: 81 Learning rate: 0.000625 #####
Epoch 42, Train Loss: 4.8039, Val Loss: 5.2934
2025-02-04 03:55:05.701 INFO: Epoch 42, Train Loss: 4.8039, Val Loss: 5.2934
train_e/atom_mae: 0.002872
2025-02-04 03:55:05.702 INFO: train_e/atom_mae: 0.002872
train_e/atom_rmse: 0.003667
2025-02-04 03:55:05.702 INFO: train_e/atom_rmse: 0.003667
train_f_mae: 0.042067
2025-02-04 03:55:05.705 INFO: train_f_mae: 0.042067
train_f_rmse: 0.065638
2025-02-04 03:55:05.705 INFO: train_f_rmse: 0.065638
val_e/atom_mae: 0.001355
2025-02-04 03:55:05.707 INFO: val_e/atom_mae: 0.001355
val_e/atom_rmse: 0.001917
2025-02-04 03:55:05.708 INFO: val_e/atom_rmse: 0.001917
val_f_mae: 0.043251
2025-02-04 03:55:05.708 INFO: val_f_mae: 0.043251
val_f_rmse: 0.071836
2025-02-04 03:55:05.708 INFO: val_f_rmse: 0.071836
##### Step: 82 Learning rate: 0.000625 #####
2025-02-04 03:56:05.599 INFO: ##### Step: 82 Learning rate: 0.000625 #####
Epoch 43, Train Loss: 4.9491, Val Loss: 5.3128
2025-02-04 03:56:05.600 INFO: Epoch 43, Train Loss: 4.9491, Val Loss: 5.3128
train_e/atom_mae: 0.003236
2025-02-04 03:56:05.601 INFO: train_e/atom_mae: 0.003236
train_e/atom_rmse: 0.004115
2025-02-04 03:56:05.601 INFO: train_e/atom_rmse: 0.004115
train_f_mae: 0.042131
2025-02-04 03:56:05.604 INFO: train_f_mae: 0.042131
train_f_rmse: 0.065763
2025-02-04 03:56:05.604 INFO: train_f_rmse: 0.065763
val_e/atom_mae: 0.001449
2025-02-04 03:56:05.606 INFO: val_e/atom_mae: 0.001449
val_e/atom_rmse: 0.001962
2025-02-04 03:56:05.606 INFO: val_e/atom_rmse: 0.001962
val_f_mae: 0.043229
2025-02-04 03:56:05.607 INFO: val_f_mae: 0.043229
val_f_rmse: 0.071923
2025-02-04 03:56:05.607 INFO: val_f_rmse: 0.071923
##### Step: 83 Learning rate: 0.000625 #####
2025-02-04 03:57:05.490 INFO: ##### Step: 83 Learning rate: 0.000625 #####
Epoch 44, Train Loss: 4.8607, Val Loss: 5.2792
2025-02-04 03:57:05.491 INFO: Epoch 44, Train Loss: 4.8607, Val Loss: 5.2792
train_e/atom_mae: 0.003090
2025-02-04 03:57:05.491 INFO: train_e/atom_mae: 0.003090
train_e/atom_rmse: 0.003874
2025-02-04 03:57:05.492 INFO: train_e/atom_rmse: 0.003874
train_f_mae: 0.042094
2025-02-04 03:57:05.494 INFO: train_f_mae: 0.042094
train_f_rmse: 0.065630
2025-02-04 03:57:05.494 INFO: train_f_rmse: 0.065630
val_e/atom_mae: 0.001411
2025-02-04 03:57:05.496 INFO: val_e/atom_mae: 0.001411
val_e/atom_rmse: 0.001934
2025-02-04 03:57:05.497 INFO: val_e/atom_rmse: 0.001934
val_f_mae: 0.043279
2025-02-04 03:57:05.497 INFO: val_f_mae: 0.043279
val_f_rmse: 0.071715
2025-02-04 03:57:05.498 INFO: val_f_rmse: 0.071715
##### Step: 84 Learning rate: 0.000625 #####
2025-02-04 03:58:05.406 INFO: ##### Step: 84 Learning rate: 0.000625 #####
Epoch 45, Train Loss: 4.7738, Val Loss: 5.2840
2025-02-04 03:58:05.406 INFO: Epoch 45, Train Loss: 4.7738, Val Loss: 5.2840
train_e/atom_mae: 0.002735
2025-02-04 03:58:05.407 INFO: train_e/atom_mae: 0.002735
train_e/atom_rmse: 0.003497
2025-02-04 03:58:05.407 INFO: train_e/atom_rmse: 0.003497
train_f_mae: 0.042077
2025-02-04 03:58:05.410 INFO: train_f_mae: 0.042077
train_f_rmse: 0.065750
2025-02-04 03:58:05.410 INFO: train_f_rmse: 0.065750
val_e/atom_mae: 0.001337
2025-02-04 03:58:05.412 INFO: val_e/atom_mae: 0.001337
val_e/atom_rmse: 0.001909
2025-02-04 03:58:05.412 INFO: val_e/atom_rmse: 0.001909
val_f_mae: 0.043211
2025-02-04 03:58:05.413 INFO: val_f_mae: 0.043211
val_f_rmse: 0.071776
2025-02-04 03:58:05.413 INFO: val_f_rmse: 0.071776
##### Step: 85 Learning rate: 0.000625 #####
2025-02-04 03:59:05.337 INFO: ##### Step: 85 Learning rate: 0.000625 #####
Epoch 46, Train Loss: 4.6464, Val Loss: 5.2644
2025-02-04 03:59:05.337 INFO: Epoch 46, Train Loss: 4.6464, Val Loss: 5.2644
train_e/atom_mae: 0.002257
2025-02-04 03:59:05.338 INFO: train_e/atom_mae: 0.002257
train_e/atom_rmse: 0.003090
2025-02-04 03:59:05.338 INFO: train_e/atom_rmse: 0.003090
train_f_mae: 0.041831
2025-02-04 03:59:05.341 INFO: train_f_mae: 0.041831
train_f_rmse: 0.065531
2025-02-04 03:59:05.341 INFO: train_f_rmse: 0.065531
val_e/atom_mae: 0.001305
2025-02-04 03:59:05.343 INFO: val_e/atom_mae: 0.001305
val_e/atom_rmse: 0.001932
2025-02-04 03:59:05.343 INFO: val_e/atom_rmse: 0.001932
val_f_mae: 0.043201
2025-02-04 03:59:05.344 INFO: val_f_mae: 0.043201
val_f_rmse: 0.071615
2025-02-04 03:59:05.344 INFO: val_f_rmse: 0.071615
##### Step: 86 Learning rate: 0.000625 #####
2025-02-04 04:00:05.269 INFO: ##### Step: 86 Learning rate: 0.000625 #####
Epoch 47, Train Loss: 4.7727, Val Loss: 5.2744
2025-02-04 04:00:05.270 INFO: Epoch 47, Train Loss: 4.7727, Val Loss: 5.2744
train_e/atom_mae: 0.002472
2025-02-04 04:00:05.271 INFO: train_e/atom_mae: 0.002472
train_e/atom_rmse: 0.003441
2025-02-04 04:00:05.271 INFO: train_e/atom_rmse: 0.003441
train_f_mae: 0.042184
2025-02-04 04:00:05.273 INFO: train_f_mae: 0.042184
train_f_rmse: 0.065849
2025-02-04 04:00:05.274 INFO: train_f_rmse: 0.065849
val_e/atom_mae: 0.001349
2025-02-04 04:00:05.276 INFO: val_e/atom_mae: 0.001349
val_e/atom_rmse: 0.001906
2025-02-04 04:00:05.276 INFO: val_e/atom_rmse: 0.001906
val_f_mae: 0.043162
2025-02-04 04:00:05.276 INFO: val_f_mae: 0.043162
val_f_rmse: 0.071711
2025-02-04 04:00:05.277 INFO: val_f_rmse: 0.071711
##### Step: 87 Learning rate: 0.000625 #####
2025-02-04 04:01:05.120 INFO: ##### Step: 87 Learning rate: 0.000625 #####
Epoch 48, Train Loss: 4.8522, Val Loss: 5.2908
2025-02-04 04:01:05.121 INFO: Epoch 48, Train Loss: 4.8522, Val Loss: 5.2908
train_e/atom_mae: 0.003120
2025-02-04 04:01:05.122 INFO: train_e/atom_mae: 0.003120
train_e/atom_rmse: 0.003874
2025-02-04 04:01:05.122 INFO: train_e/atom_rmse: 0.003874
train_f_mae: 0.041984
2025-02-04 04:01:05.124 INFO: train_f_mae: 0.041984
train_f_rmse: 0.065566
2025-02-04 04:01:05.125 INFO: train_f_rmse: 0.065566
val_e/atom_mae: 0.001497
2025-02-04 04:01:05.127 INFO: val_e/atom_mae: 0.001497
val_e/atom_rmse: 0.001946
2025-02-04 04:01:05.127 INFO: val_e/atom_rmse: 0.001946
val_f_mae: 0.043199
2025-02-04 04:01:05.128 INFO: val_f_mae: 0.043199
val_f_rmse: 0.071784
2025-02-04 04:01:05.128 INFO: val_f_rmse: 0.071784
##### Step: 88 Learning rate: 0.000625 #####
2025-02-04 04:02:04.744 INFO: ##### Step: 88 Learning rate: 0.000625 #####
Epoch 49, Train Loss: 4.6158, Val Loss: 5.2804
2025-02-04 04:02:04.744 INFO: Epoch 49, Train Loss: 4.6158, Val Loss: 5.2804
train_e/atom_mae: 0.002116
2025-02-04 04:02:04.745 INFO: train_e/atom_mae: 0.002116
train_e/atom_rmse: 0.002996
2025-02-04 04:02:04.745 INFO: train_e/atom_rmse: 0.002996
train_f_mae: 0.041851
2025-02-04 04:02:04.748 INFO: train_f_mae: 0.041851
train_f_rmse: 0.065459
2025-02-04 04:02:04.748 INFO: train_f_rmse: 0.065459
val_e/atom_mae: 0.001382
2025-02-04 04:02:04.750 INFO: val_e/atom_mae: 0.001382
val_e/atom_rmse: 0.001948
2025-02-04 04:02:04.751 INFO: val_e/atom_rmse: 0.001948
val_f_mae: 0.043160
2025-02-04 04:02:04.751 INFO: val_f_mae: 0.043160
val_f_rmse: 0.071714
2025-02-04 04:02:04.751 INFO: val_f_rmse: 0.071714
##### Step: 89 Learning rate: 0.000625 #####
2025-02-04 04:03:04.371 INFO: ##### Step: 89 Learning rate: 0.000625 #####
Epoch 50, Train Loss: 4.6912, Val Loss: 5.2695
2025-02-04 04:03:04.371 INFO: Epoch 50, Train Loss: 4.6912, Val Loss: 5.2695
train_e/atom_mae: 0.002530
2025-02-04 04:03:04.372 INFO: train_e/atom_mae: 0.002530
train_e/atom_rmse: 0.003340
2025-02-04 04:03:04.372 INFO: train_e/atom_rmse: 0.003340
train_f_mae: 0.041848
2025-02-04 04:03:04.375 INFO: train_f_mae: 0.041848
train_f_rmse: 0.065421
2025-02-04 04:03:04.375 INFO: train_f_rmse: 0.065421
val_e/atom_mae: 0.001522
2025-02-04 04:03:04.377 INFO: val_e/atom_mae: 0.001522
val_e/atom_rmse: 0.002014
2025-02-04 04:03:04.377 INFO: val_e/atom_rmse: 0.002014
val_f_mae: 0.043161
2025-02-04 04:03:04.378 INFO: val_f_mae: 0.043161
val_f_rmse: 0.071569
2025-02-04 04:03:04.378 INFO: val_f_rmse: 0.071569
##### Step: 90 Learning rate: 0.000625 #####
2025-02-04 04:04:04.050 INFO: ##### Step: 90 Learning rate: 0.000625 #####
Epoch 51, Train Loss: 4.9706, Val Loss: 5.2473
2025-02-04 04:04:04.050 INFO: Epoch 51, Train Loss: 4.9706, Val Loss: 5.2473
train_e/atom_mae: 0.003440
2025-02-04 04:04:04.051 INFO: train_e/atom_mae: 0.003440
train_e/atom_rmse: 0.004220
2025-02-04 04:04:04.051 INFO: train_e/atom_rmse: 0.004220
train_f_mae: 0.042182
2025-02-04 04:04:04.054 INFO: train_f_mae: 0.042182
train_f_rmse: 0.065683
2025-02-04 04:04:04.054 INFO: train_f_rmse: 0.065683
val_e/atom_mae: 0.001437
2025-02-04 04:04:04.056 INFO: val_e/atom_mae: 0.001437
val_e/atom_rmse: 0.001975
2025-02-04 04:04:04.056 INFO: val_e/atom_rmse: 0.001975
val_f_mae: 0.043079
2025-02-04 04:04:04.057 INFO: val_f_mae: 0.043079
val_f_rmse: 0.071454
2025-02-04 04:04:04.057 INFO: val_f_rmse: 0.071454
##### Step: 91 Learning rate: 0.000625 #####
2025-02-04 04:05:03.680 INFO: ##### Step: 91 Learning rate: 0.000625 #####
Epoch 52, Train Loss: 4.6456, Val Loss: 5.2937
2025-02-04 04:05:03.680 INFO: Epoch 52, Train Loss: 4.6456, Val Loss: 5.2937
train_e/atom_mae: 0.002414
2025-02-04 04:05:03.681 INFO: train_e/atom_mae: 0.002414
train_e/atom_rmse: 0.003198
2025-02-04 04:05:03.681 INFO: train_e/atom_rmse: 0.003198
train_f_mae: 0.041746
2025-02-04 04:05:03.684 INFO: train_f_mae: 0.041746
train_f_rmse: 0.065334
2025-02-04 04:05:03.684 INFO: train_f_rmse: 0.065334
val_e/atom_mae: 0.001453
2025-02-04 04:05:03.686 INFO: val_e/atom_mae: 0.001453
val_e/atom_rmse: 0.001983
2025-02-04 04:05:03.686 INFO: val_e/atom_rmse: 0.001983
val_f_mae: 0.043169
2025-02-04 04:05:03.687 INFO: val_f_mae: 0.043169
val_f_rmse: 0.071771
2025-02-04 04:05:03.687 INFO: val_f_rmse: 0.071771
##### Step: 92 Learning rate: 0.000625 #####
2025-02-04 04:06:03.305 INFO: ##### Step: 92 Learning rate: 0.000625 #####
Epoch 53, Train Loss: 4.7143, Val Loss: 5.2455
2025-02-04 04:06:03.305 INFO: Epoch 53, Train Loss: 4.7143, Val Loss: 5.2455
train_e/atom_mae: 0.002550
2025-02-04 04:06:03.306 INFO: train_e/atom_mae: 0.002550
train_e/atom_rmse: 0.003427
2025-02-04 04:06:03.307 INFO: train_e/atom_rmse: 0.003427
train_f_mae: 0.041877
2025-02-04 04:06:03.309 INFO: train_f_mae: 0.041877
train_f_rmse: 0.065432
2025-02-04 04:06:03.309 INFO: train_f_rmse: 0.065432
val_e/atom_mae: 0.001273
2025-02-04 04:06:03.311 INFO: val_e/atom_mae: 0.001273
val_e/atom_rmse: 0.001914
2025-02-04 04:06:03.312 INFO: val_e/atom_rmse: 0.001914
val_f_mae: 0.043175
2025-02-04 04:06:03.312 INFO: val_f_mae: 0.043175
val_f_rmse: 0.071502
2025-02-04 04:06:03.312 INFO: val_f_rmse: 0.071502
##### Step: 93 Learning rate: 0.000625 #####
2025-02-04 04:07:02.998 INFO: ##### Step: 93 Learning rate: 0.000625 #####
Epoch 54, Train Loss: 4.8114, Val Loss: 5.2614
2025-02-04 04:07:02.999 INFO: Epoch 54, Train Loss: 4.8114, Val Loss: 5.2614
train_e/atom_mae: 0.002818
2025-02-04 04:07:03.000 INFO: train_e/atom_mae: 0.002818
train_e/atom_rmse: 0.003721
2025-02-04 04:07:03.000 INFO: train_e/atom_rmse: 0.003721
train_f_mae: 0.041983
2025-02-04 04:07:03.002 INFO: train_f_mae: 0.041983
train_f_rmse: 0.065583
2025-02-04 04:07:03.003 INFO: train_f_rmse: 0.065583
val_e/atom_mae: 0.001316
2025-02-04 04:07:03.005 INFO: val_e/atom_mae: 0.001316
val_e/atom_rmse: 0.001932
2025-02-04 04:07:03.005 INFO: val_e/atom_rmse: 0.001932
val_f_mae: 0.043097
2025-02-04 04:07:03.005 INFO: val_f_mae: 0.043097
val_f_rmse: 0.071597
2025-02-04 04:07:03.006 INFO: val_f_rmse: 0.071597
##### Step: 94 Learning rate: 0.000625 #####
2025-02-04 04:08:02.669 INFO: ##### Step: 94 Learning rate: 0.000625 #####
Epoch 55, Train Loss: 4.7751, Val Loss: 5.2789
2025-02-04 04:08:02.670 INFO: Epoch 55, Train Loss: 4.7751, Val Loss: 5.2789
train_e/atom_mae: 0.002566
2025-02-04 04:08:02.671 INFO: train_e/atom_mae: 0.002566
train_e/atom_rmse: 0.003573
2025-02-04 04:08:02.671 INFO: train_e/atom_rmse: 0.003573
train_f_mae: 0.041961
2025-02-04 04:08:02.674 INFO: train_f_mae: 0.041961
train_f_rmse: 0.065608
2025-02-04 04:08:02.674 INFO: train_f_rmse: 0.065608
val_e/atom_mae: 0.001317
2025-02-04 04:08:02.676 INFO: val_e/atom_mae: 0.001317
val_e/atom_rmse: 0.002006
2025-02-04 04:08:02.676 INFO: val_e/atom_rmse: 0.002006
val_f_mae: 0.043109
2025-02-04 04:08:02.677 INFO: val_f_mae: 0.043109
val_f_rmse: 0.071647
2025-02-04 04:08:02.677 INFO: val_f_rmse: 0.071647
##### Step: 95 Learning rate: 0.000625 #####
2025-02-04 04:09:02.328 INFO: ##### Step: 95 Learning rate: 0.000625 #####
Epoch 56, Train Loss: 4.6848, Val Loss: 5.2873
2025-02-04 04:09:02.329 INFO: Epoch 56, Train Loss: 4.6848, Val Loss: 5.2873
train_e/atom_mae: 0.002329
2025-02-04 04:09:02.330 INFO: train_e/atom_mae: 0.002329
train_e/atom_rmse: 0.003135
2025-02-04 04:09:02.330 INFO: train_e/atom_rmse: 0.003135
train_f_mae: 0.042200
2025-02-04 04:09:02.333 INFO: train_f_mae: 0.042200
train_f_rmse: 0.065746
2025-02-04 04:09:02.333 INFO: train_f_rmse: 0.065746
val_e/atom_mae: 0.001384
2025-02-04 04:09:02.335 INFO: val_e/atom_mae: 0.001384
val_e/atom_rmse: 0.002086
2025-02-04 04:09:02.335 INFO: val_e/atom_rmse: 0.002086
val_f_mae: 0.043060
2025-02-04 04:09:02.336 INFO: val_f_mae: 0.043060
val_f_rmse: 0.071621
2025-02-04 04:09:02.336 INFO: val_f_rmse: 0.071621
##### Step: 96 Learning rate: 0.000625 #####
2025-02-04 04:10:02.002 INFO: ##### Step: 96 Learning rate: 0.000625 #####
Epoch 57, Train Loss: 4.6359, Val Loss: 5.2575
2025-02-04 04:10:02.002 INFO: Epoch 57, Train Loss: 4.6359, Val Loss: 5.2575
train_e/atom_mae: 0.002335
2025-02-04 04:10:02.003 INFO: train_e/atom_mae: 0.002335
train_e/atom_rmse: 0.003164
2025-02-04 04:10:02.003 INFO: train_e/atom_rmse: 0.003164
train_f_mae: 0.041812
2025-02-04 04:10:02.006 INFO: train_f_mae: 0.041812
train_f_rmse: 0.065321
2025-02-04 04:10:02.006 INFO: train_f_rmse: 0.065321
val_e/atom_mae: 0.001242
2025-02-04 04:10:02.008 INFO: val_e/atom_mae: 0.001242
val_e/atom_rmse: 0.001877
2025-02-04 04:10:02.009 INFO: val_e/atom_rmse: 0.001877
val_f_mae: 0.043112
2025-02-04 04:10:02.009 INFO: val_f_mae: 0.043112
val_f_rmse: 0.071622
2025-02-04 04:10:02.009 INFO: val_f_rmse: 0.071622
##### Step: 97 Learning rate: 0.000625 #####
2025-02-04 04:11:01.666 INFO: ##### Step: 97 Learning rate: 0.000625 #####
Epoch 58, Train Loss: 4.6283, Val Loss: 5.2852
2025-02-04 04:11:01.666 INFO: Epoch 58, Train Loss: 4.6283, Val Loss: 5.2852
train_e/atom_mae: 0.002263
2025-02-04 04:11:01.667 INFO: train_e/atom_mae: 0.002263
train_e/atom_rmse: 0.003123
2025-02-04 04:11:01.668 INFO: train_e/atom_rmse: 0.003123
train_f_mae: 0.041684
2025-02-04 04:11:01.670 INFO: train_f_mae: 0.041684
train_f_rmse: 0.065336
2025-02-04 04:11:01.670 INFO: train_f_rmse: 0.065336
val_e/atom_mae: 0.001309
2025-02-04 04:11:01.672 INFO: val_e/atom_mae: 0.001309
val_e/atom_rmse: 0.001983
2025-02-04 04:11:01.673 INFO: val_e/atom_rmse: 0.001983
val_f_mae: 0.043106
2025-02-04 04:11:01.673 INFO: val_f_mae: 0.043106
val_f_rmse: 0.071715
2025-02-04 04:11:01.673 INFO: val_f_rmse: 0.071715
##### Step: 98 Learning rate: 0.000625 #####
2025-02-04 04:12:01.365 INFO: ##### Step: 98 Learning rate: 0.000625 #####
Epoch 59, Train Loss: 4.8461, Val Loss: 5.2639
2025-02-04 04:12:01.365 INFO: Epoch 59, Train Loss: 4.8461, Val Loss: 5.2639
train_e/atom_mae: 0.003064
2025-02-04 04:12:01.366 INFO: train_e/atom_mae: 0.003064
train_e/atom_rmse: 0.003856
2025-02-04 04:12:01.366 INFO: train_e/atom_rmse: 0.003856
train_f_mae: 0.041989
2025-02-04 04:12:01.369 INFO: train_f_mae: 0.041989
train_f_rmse: 0.065558
2025-02-04 04:12:01.369 INFO: train_f_rmse: 0.065558
val_e/atom_mae: 0.001361
2025-02-04 04:12:01.371 INFO: val_e/atom_mae: 0.001361
val_e/atom_rmse: 0.001907
2025-02-04 04:12:01.372 INFO: val_e/atom_rmse: 0.001907
val_f_mae: 0.043091
2025-02-04 04:12:01.372 INFO: val_f_mae: 0.043091
val_f_rmse: 0.071640
2025-02-04 04:12:01.372 INFO: val_f_rmse: 0.071640
##### Step: 99 Learning rate: 0.000625 #####
2025-02-04 04:13:01.045 INFO: ##### Step: 99 Learning rate: 0.000625 #####
Epoch 60, Train Loss: 4.8579, Val Loss: 5.2596
2025-02-04 04:13:01.045 INFO: Epoch 60, Train Loss: 4.8579, Val Loss: 5.2596
train_e/atom_mae: 0.003003
2025-02-04 04:13:01.046 INFO: train_e/atom_mae: 0.003003
train_e/atom_rmse: 0.003826
2025-02-04 04:13:01.047 INFO: train_e/atom_rmse: 0.003826
train_f_mae: 0.042205
2025-02-04 04:13:01.049 INFO: train_f_mae: 0.042205
train_f_rmse: 0.065713
2025-02-04 04:13:01.049 INFO: train_f_rmse: 0.065713
val_e/atom_mae: 0.001289
2025-02-04 04:13:01.051 INFO: val_e/atom_mae: 0.001289
val_e/atom_rmse: 0.001911
2025-02-04 04:13:01.052 INFO: val_e/atom_rmse: 0.001911
val_f_mae: 0.043166
2025-02-04 04:13:01.052 INFO: val_f_mae: 0.043166
val_f_rmse: 0.071604
2025-02-04 04:13:01.052 INFO: val_f_rmse: 0.071604
##### Step: 100 Learning rate: 0.0003125 #####
2025-02-04 04:14:00.767 INFO: ##### Step: 100 Learning rate: 0.0003125 #####
Epoch 61, Train Loss: 4.6747, Val Loss: 5.2564
2025-02-04 04:14:00.767 INFO: Epoch 61, Train Loss: 4.6747, Val Loss: 5.2564
train_e/atom_mae: 0.002621
2025-02-04 04:14:00.768 INFO: train_e/atom_mae: 0.002621
train_e/atom_rmse: 0.003447
2025-02-04 04:14:00.768 INFO: train_e/atom_rmse: 0.003447
train_f_mae: 0.041575
2025-02-04 04:14:00.771 INFO: train_f_mae: 0.041575
train_f_rmse: 0.065090
2025-02-04 04:14:00.771 INFO: train_f_rmse: 0.065090
val_e/atom_mae: 0.001407
2025-02-04 04:14:00.773 INFO: val_e/atom_mae: 0.001407
val_e/atom_rmse: 0.001917
2025-02-04 04:14:00.774 INFO: val_e/atom_rmse: 0.001917
val_f_mae: 0.043039
2025-02-04 04:14:00.774 INFO: val_f_mae: 0.043039
val_f_rmse: 0.071577
2025-02-04 04:14:00.774 INFO: val_f_rmse: 0.071577
##### Step: 101 Learning rate: 0.0003125 #####
2025-02-04 04:15:00.462 INFO: ##### Step: 101 Learning rate: 0.0003125 #####
Epoch 62, Train Loss: 4.4608, Val Loss: 5.2572
2025-02-04 04:15:00.462 INFO: Epoch 62, Train Loss: 4.4608, Val Loss: 5.2572
train_e/atom_mae: 0.001823
2025-02-04 04:15:00.463 INFO: train_e/atom_mae: 0.001823
train_e/atom_rmse: 0.002698
2025-02-04 04:15:00.463 INFO: train_e/atom_rmse: 0.002698
train_f_mae: 0.041215
2025-02-04 04:15:00.466 INFO: train_f_mae: 0.041215
train_f_rmse: 0.064750
2025-02-04 04:15:00.466 INFO: train_f_rmse: 0.064750
val_e/atom_mae: 0.001265
2025-02-04 04:15:00.468 INFO: val_e/atom_mae: 0.001265
val_e/atom_rmse: 0.001958
2025-02-04 04:15:00.468 INFO: val_e/atom_rmse: 0.001958
val_f_mae: 0.042994
2025-02-04 04:15:00.469 INFO: val_f_mae: 0.042994
val_f_rmse: 0.071546
2025-02-04 04:15:00.469 INFO: val_f_rmse: 0.071546
##### Step: 102 Learning rate: 0.0003125 #####
2025-02-04 04:16:00.150 INFO: ##### Step: 102 Learning rate: 0.0003125 #####
Epoch 63, Train Loss: 4.4317, Val Loss: 5.2472
2025-02-04 04:16:00.151 INFO: Epoch 63, Train Loss: 4.4317, Val Loss: 5.2472
train_e/atom_mae: 0.001776
2025-02-04 04:16:00.152 INFO: train_e/atom_mae: 0.001776
train_e/atom_rmse: 0.002597
2025-02-04 04:16:00.152 INFO: train_e/atom_rmse: 0.002597
train_f_mae: 0.041106
2025-02-04 04:16:00.154 INFO: train_f_mae: 0.041106
train_f_rmse: 0.064676
2025-02-04 04:16:00.155 INFO: train_f_rmse: 0.064676
val_e/atom_mae: 0.001294
2025-02-04 04:16:00.157 INFO: val_e/atom_mae: 0.001294
val_e/atom_rmse: 0.001873
2025-02-04 04:16:00.157 INFO: val_e/atom_rmse: 0.001873
val_f_mae: 0.043039
2025-02-04 04:16:00.157 INFO: val_f_mae: 0.043039
val_f_rmse: 0.071556
2025-02-04 04:16:00.158 INFO: val_f_rmse: 0.071556
##### Step: 103 Learning rate: 0.0003125 #####
2025-02-04 04:16:59.828 INFO: ##### Step: 103 Learning rate: 0.0003125 #####
Epoch 64, Train Loss: 4.4429, Val Loss: 5.2500
2025-02-04 04:16:59.828 INFO: Epoch 64, Train Loss: 4.4429, Val Loss: 5.2500
train_e/atom_mae: 0.001797
2025-02-04 04:16:59.829 INFO: train_e/atom_mae: 0.001797
train_e/atom_rmse: 0.002657
2025-02-04 04:16:59.829 INFO: train_e/atom_rmse: 0.002657
train_f_mae: 0.041085
2025-02-04 04:16:59.832 INFO: train_f_mae: 0.041085
train_f_rmse: 0.064673
2025-02-04 04:16:59.832 INFO: train_f_rmse: 0.064673
val_e/atom_mae: 0.001266
2025-02-04 04:16:59.834 INFO: val_e/atom_mae: 0.001266
val_e/atom_rmse: 0.001905
2025-02-04 04:16:59.835 INFO: val_e/atom_rmse: 0.001905
val_f_mae: 0.043002
2025-02-04 04:16:59.835 INFO: val_f_mae: 0.043002
val_f_rmse: 0.071544
2025-02-04 04:16:59.835 INFO: val_f_rmse: 0.071544
##### Step: 104 Learning rate: 0.0003125 #####
2025-02-04 04:17:59.494 INFO: ##### Step: 104 Learning rate: 0.0003125 #####
Epoch 65, Train Loss: 4.4698, Val Loss: 5.2329
2025-02-04 04:17:59.495 INFO: Epoch 65, Train Loss: 4.4698, Val Loss: 5.2329
train_e/atom_mae: 0.001869
2025-02-04 04:17:59.496 INFO: train_e/atom_mae: 0.001869
train_e/atom_rmse: 0.002727
2025-02-04 04:17:59.496 INFO: train_e/atom_rmse: 0.002727
train_f_mae: 0.041231
2025-02-04 04:17:59.499 INFO: train_f_mae: 0.041231
train_f_rmse: 0.064774
2025-02-04 04:17:59.499 INFO: train_f_rmse: 0.064774
val_e/atom_mae: 0.001284
2025-02-04 04:17:59.501 INFO: val_e/atom_mae: 0.001284
val_e/atom_rmse: 0.001857
2025-02-04 04:17:59.501 INFO: val_e/atom_rmse: 0.001857
val_f_mae: 0.043017
2025-02-04 04:17:59.502 INFO: val_f_mae: 0.043017
val_f_rmse: 0.071469
2025-02-04 04:17:59.502 INFO: val_f_rmse: 0.071469
##### Step: 105 Learning rate: 0.0003125 #####
2025-02-04 04:18:59.161 INFO: ##### Step: 105 Learning rate: 0.0003125 #####
Epoch 66, Train Loss: 4.4797, Val Loss: 5.2395
2025-02-04 04:18:59.162 INFO: Epoch 66, Train Loss: 4.4797, Val Loss: 5.2395
train_e/atom_mae: 0.001965
2025-02-04 04:18:59.162 INFO: train_e/atom_mae: 0.001965
train_e/atom_rmse: 0.002844
2025-02-04 04:18:59.163 INFO: train_e/atom_rmse: 0.002844
train_f_mae: 0.041064
2025-02-04 04:18:59.165 INFO: train_f_mae: 0.041064
train_f_rmse: 0.064665
2025-02-04 04:18:59.165 INFO: train_f_rmse: 0.064665
val_e/atom_mae: 0.001370
2025-02-04 04:18:59.168 INFO: val_e/atom_mae: 0.001370
val_e/atom_rmse: 0.001887
2025-02-04 04:18:59.168 INFO: val_e/atom_rmse: 0.001887
val_f_mae: 0.043008
2025-02-04 04:18:59.168 INFO: val_f_mae: 0.043008
val_f_rmse: 0.071488
2025-02-04 04:18:59.169 INFO: val_f_rmse: 0.071488
##### Step: 106 Learning rate: 0.0003125 #####
2025-02-04 04:19:58.853 INFO: ##### Step: 106 Learning rate: 0.0003125 #####
Epoch 67, Train Loss: 4.4739, Val Loss: 5.2329
2025-02-04 04:19:58.853 INFO: Epoch 67, Train Loss: 4.4739, Val Loss: 5.2329
train_e/atom_mae: 0.002000
2025-02-04 04:19:58.854 INFO: train_e/atom_mae: 0.002000
train_e/atom_rmse: 0.002794
2025-02-04 04:19:58.854 INFO: train_e/atom_rmse: 0.002794
train_f_mae: 0.041110
2025-02-04 04:19:58.857 INFO: train_f_mae: 0.041110
train_f_rmse: 0.064700
2025-02-04 04:19:58.857 INFO: train_f_rmse: 0.064700
val_e/atom_mae: 0.001434
2025-02-04 04:19:58.859 INFO: val_e/atom_mae: 0.001434
val_e/atom_rmse: 0.001905
2025-02-04 04:19:58.859 INFO: val_e/atom_rmse: 0.001905
val_f_mae: 0.042972
2025-02-04 04:19:58.860 INFO: val_f_mae: 0.042972
val_f_rmse: 0.071423
2025-02-04 04:19:58.860 INFO: val_f_rmse: 0.071423
##### Step: 107 Learning rate: 0.0003125 #####
2025-02-04 04:20:58.535 INFO: ##### Step: 107 Learning rate: 0.0003125 #####
Epoch 68, Train Loss: 4.5271, Val Loss: 5.2348
2025-02-04 04:20:58.536 INFO: Epoch 68, Train Loss: 4.5271, Val Loss: 5.2348
train_e/atom_mae: 0.002135
2025-02-04 04:20:58.537 INFO: train_e/atom_mae: 0.002135
train_e/atom_rmse: 0.002968
2025-02-04 04:20:58.537 INFO: train_e/atom_rmse: 0.002968
train_f_mae: 0.041301
2025-02-04 04:20:58.539 INFO: train_f_mae: 0.041301
train_f_rmse: 0.064825
2025-02-04 04:20:58.540 INFO: train_f_rmse: 0.064825
val_e/atom_mae: 0.001270
2025-02-04 04:20:58.542 INFO: val_e/atom_mae: 0.001270
val_e/atom_rmse: 0.001895
2025-02-04 04:20:58.542 INFO: val_e/atom_rmse: 0.001895
val_f_mae: 0.042993
2025-02-04 04:20:58.543 INFO: val_f_mae: 0.042993
val_f_rmse: 0.071449
2025-02-04 04:20:58.543 INFO: val_f_rmse: 0.071449
##### Step: 108 Learning rate: 0.0003125 #####
2025-02-04 04:21:58.181 INFO: ##### Step: 108 Learning rate: 0.0003125 #####
Epoch 69, Train Loss: 4.4394, Val Loss: 5.2294
2025-02-04 04:21:58.182 INFO: Epoch 69, Train Loss: 4.4394, Val Loss: 5.2294
train_e/atom_mae: 0.001787
2025-02-04 04:21:58.182 INFO: train_e/atom_mae: 0.001787
train_e/atom_rmse: 0.002629
2025-02-04 04:21:58.183 INFO: train_e/atom_rmse: 0.002629
train_f_mae: 0.041114
2025-02-04 04:21:58.185 INFO: train_f_mae: 0.041114
train_f_rmse: 0.064688
2025-02-04 04:21:58.185 INFO: train_f_rmse: 0.064688
val_e/atom_mae: 0.001296
2025-02-04 04:21:58.187 INFO: val_e/atom_mae: 0.001296
val_e/atom_rmse: 0.001890
2025-02-04 04:21:58.188 INFO: val_e/atom_rmse: 0.001890
val_f_mae: 0.043000
2025-02-04 04:21:58.188 INFO: val_f_mae: 0.043000
val_f_rmse: 0.071414
2025-02-04 04:21:58.189 INFO: val_f_rmse: 0.071414
##### Step: 109 Learning rate: 0.0003125 #####
2025-02-04 04:22:57.934 INFO: ##### Step: 109 Learning rate: 0.0003125 #####
Epoch 70, Train Loss: 4.4691, Val Loss: 5.2290
2025-02-04 04:22:57.935 INFO: Epoch 70, Train Loss: 4.4691, Val Loss: 5.2290
train_e/atom_mae: 0.001812
2025-02-04 04:22:57.936 INFO: train_e/atom_mae: 0.001812
train_e/atom_rmse: 0.002753
2025-02-04 04:22:57.936 INFO: train_e/atom_rmse: 0.002753
train_f_mae: 0.041123
2025-02-04 04:22:57.939 INFO: train_f_mae: 0.041123
train_f_rmse: 0.064728
2025-02-04 04:22:57.939 INFO: train_f_rmse: 0.064728
val_e/atom_mae: 0.001273
2025-02-04 04:22:57.941 INFO: val_e/atom_mae: 0.001273
val_e/atom_rmse: 0.001957
2025-02-04 04:22:57.941 INFO: val_e/atom_rmse: 0.001957
val_f_mae: 0.042960
2025-02-04 04:22:57.942 INFO: val_f_mae: 0.042960
val_f_rmse: 0.071347
2025-02-04 04:22:57.942 INFO: val_f_rmse: 0.071347
##### Step: 110 Learning rate: 0.0003125 #####
2025-02-04 04:23:57.696 INFO: ##### Step: 110 Learning rate: 0.0003125 #####
Epoch 71, Train Loss: 4.5076, Val Loss: 5.2381
2025-02-04 04:23:57.697 INFO: Epoch 71, Train Loss: 4.5076, Val Loss: 5.2381
train_e/atom_mae: 0.002043
2025-02-04 04:23:57.698 INFO: train_e/atom_mae: 0.002043
train_e/atom_rmse: 0.002914
2025-02-04 04:23:57.698 INFO: train_e/atom_rmse: 0.002914
train_f_mae: 0.041163
2025-02-04 04:23:57.700 INFO: train_f_mae: 0.041163
train_f_rmse: 0.064766
2025-02-04 04:23:57.701 INFO: train_f_rmse: 0.064766
val_e/atom_mae: 0.001288
2025-02-04 04:23:57.703 INFO: val_e/atom_mae: 0.001288
val_e/atom_rmse: 0.001955
2025-02-04 04:23:57.703 INFO: val_e/atom_rmse: 0.001955
val_f_mae: 0.042947
2025-02-04 04:23:57.704 INFO: val_f_mae: 0.042947
val_f_rmse: 0.071413
2025-02-04 04:23:57.704 INFO: val_f_rmse: 0.071413
##### Step: 111 Learning rate: 0.0003125 #####
2025-02-04 04:24:57.406 INFO: ##### Step: 111 Learning rate: 0.0003125 #####
Epoch 72, Train Loss: 4.5302, Val Loss: 5.2226
2025-02-04 04:24:57.407 INFO: Epoch 72, Train Loss: 4.5302, Val Loss: 5.2226
train_e/atom_mae: 0.002317
2025-02-04 04:24:57.407 INFO: train_e/atom_mae: 0.002317
train_e/atom_rmse: 0.003086
2025-02-04 04:24:57.408 INFO: train_e/atom_rmse: 0.003086
train_f_mae: 0.041147
2025-02-04 04:24:57.410 INFO: train_f_mae: 0.041147
train_f_rmse: 0.064646
2025-02-04 04:24:57.410 INFO: train_f_rmse: 0.064646
val_e/atom_mae: 0.001411
2025-02-04 04:24:57.413 INFO: val_e/atom_mae: 0.001411
val_e/atom_rmse: 0.001900
2025-02-04 04:24:57.413 INFO: val_e/atom_rmse: 0.001900
val_f_mae: 0.042979
2025-02-04 04:24:57.413 INFO: val_f_mae: 0.042979
val_f_rmse: 0.071357
2025-02-04 04:24:57.414 INFO: val_f_rmse: 0.071357
##### Step: 112 Learning rate: 0.0003125 #####
2025-02-04 04:25:57.124 INFO: ##### Step: 112 Learning rate: 0.0003125 #####
Epoch 73, Train Loss: 4.6908, Val Loss: 5.2120
2025-02-04 04:25:57.124 INFO: Epoch 73, Train Loss: 4.6908, Val Loss: 5.2120
train_e/atom_mae: 0.002658
2025-02-04 04:25:57.125 INFO: train_e/atom_mae: 0.002658
train_e/atom_rmse: 0.003424
2025-02-04 04:25:57.125 INFO: train_e/atom_rmse: 0.003424
train_f_mae: 0.041796
2025-02-04 04:25:57.128 INFO: train_f_mae: 0.041796
train_f_rmse: 0.065259
2025-02-04 04:25:57.128 INFO: train_f_rmse: 0.065259
val_e/atom_mae: 0.001327
2025-02-04 04:25:57.130 INFO: val_e/atom_mae: 0.001327
val_e/atom_rmse: 0.001893
2025-02-04 04:25:57.131 INFO: val_e/atom_rmse: 0.001893
val_f_mae: 0.042955
2025-02-04 04:25:57.131 INFO: val_f_mae: 0.042955
val_f_rmse: 0.071288
2025-02-04 04:25:57.131 INFO: val_f_rmse: 0.071288
##### Step: 113 Learning rate: 0.0003125 #####
2025-02-04 04:26:56.786 INFO: ##### Step: 113 Learning rate: 0.0003125 #####
Epoch 74, Train Loss: 4.4593, Val Loss: 5.2105
2025-02-04 04:26:56.787 INFO: Epoch 74, Train Loss: 4.4593, Val Loss: 5.2105
train_e/atom_mae: 0.002044
2025-02-04 04:26:56.788 INFO: train_e/atom_mae: 0.002044
train_e/atom_rmse: 0.002820
2025-02-04 04:26:56.788 INFO: train_e/atom_rmse: 0.002820
train_f_mae: 0.041055
2025-02-04 04:26:56.790 INFO: train_f_mae: 0.041055
train_f_rmse: 0.064546
2025-02-04 04:26:56.791 INFO: train_f_rmse: 0.064546
val_e/atom_mae: 0.001330
2025-02-04 04:26:56.793 INFO: val_e/atom_mae: 0.001330
val_e/atom_rmse: 0.001959
2025-02-04 04:26:56.793 INFO: val_e/atom_rmse: 0.001959
val_f_mae: 0.042892
2025-02-04 04:26:56.793 INFO: val_f_mae: 0.042892
val_f_rmse: 0.071215
2025-02-04 04:26:56.794 INFO: val_f_rmse: 0.071215
##### Step: 114 Learning rate: 0.0003125 #####
2025-02-04 04:27:56.447 INFO: ##### Step: 114 Learning rate: 0.0003125 #####
Epoch 75, Train Loss: 4.5457, Val Loss: 5.2387
2025-02-04 04:27:56.447 INFO: Epoch 75, Train Loss: 4.5457, Val Loss: 5.2387
train_e/atom_mae: 0.002278
2025-02-04 04:27:56.448 INFO: train_e/atom_mae: 0.002278
train_e/atom_rmse: 0.003052
2025-02-04 04:27:56.448 INFO: train_e/atom_rmse: 0.003052
train_f_mae: 0.041309
2025-02-04 04:27:56.451 INFO: train_f_mae: 0.041309
train_f_rmse: 0.064824
2025-02-04 04:27:56.451 INFO: train_f_rmse: 0.064824
val_e/atom_mae: 0.001359
2025-02-04 04:27:56.453 INFO: val_e/atom_mae: 0.001359
val_e/atom_rmse: 0.001899
2025-02-04 04:27:56.454 INFO: val_e/atom_rmse: 0.001899
val_f_mae: 0.042987
2025-02-04 04:27:56.454 INFO: val_f_mae: 0.042987
val_f_rmse: 0.071468
2025-02-04 04:27:56.454 INFO: val_f_rmse: 0.071468
##### Step: 115 Learning rate: 0.0003125 #####
2025-02-04 04:28:56.086 INFO: ##### Step: 115 Learning rate: 0.0003125 #####
Epoch 76, Train Loss: 4.5166, Val Loss: 5.2467
2025-02-04 04:28:56.086 INFO: Epoch 76, Train Loss: 4.5166, Val Loss: 5.2467
train_e/atom_mae: 0.002220
2025-02-04 04:28:56.087 INFO: train_e/atom_mae: 0.002220
train_e/atom_rmse: 0.003001
2025-02-04 04:28:56.087 INFO: train_e/atom_rmse: 0.003001
train_f_mae: 0.041213
2025-02-04 04:28:56.090 INFO: train_f_mae: 0.041213
train_f_rmse: 0.064688
2025-02-04 04:28:56.090 INFO: train_f_rmse: 0.064688
val_e/atom_mae: 0.001393
2025-02-04 04:28:56.092 INFO: val_e/atom_mae: 0.001393
val_e/atom_rmse: 0.001957
2025-02-04 04:28:56.092 INFO: val_e/atom_rmse: 0.001957
val_f_mae: 0.042914
2025-02-04 04:28:56.093 INFO: val_f_mae: 0.042914
val_f_rmse: 0.071470
2025-02-04 04:28:56.093 INFO: val_f_rmse: 0.071470
##### Step: 116 Learning rate: 0.0003125 #####
2025-02-04 04:29:55.717 INFO: ##### Step: 116 Learning rate: 0.0003125 #####
Epoch 77, Train Loss: 4.7495, Val Loss: 5.2169
2025-02-04 04:29:55.717 INFO: Epoch 77, Train Loss: 4.7495, Val Loss: 5.2169
train_e/atom_mae: 0.003068
2025-02-04 04:29:55.718 INFO: train_e/atom_mae: 0.003068
train_e/atom_rmse: 0.003839
2025-02-04 04:29:55.718 INFO: train_e/atom_rmse: 0.003839
train_f_mae: 0.041448
2025-02-04 04:29:55.721 INFO: train_f_mae: 0.041448
train_f_rmse: 0.064855
2025-02-04 04:29:55.721 INFO: train_f_rmse: 0.064855
val_e/atom_mae: 0.001364
2025-02-04 04:29:55.724 INFO: val_e/atom_mae: 0.001364
val_e/atom_rmse: 0.001903
2025-02-04 04:29:55.724 INFO: val_e/atom_rmse: 0.001903
val_f_mae: 0.042941
2025-02-04 04:29:55.724 INFO: val_f_mae: 0.042941
val_f_rmse: 0.071313
2025-02-04 04:29:55.725 INFO: val_f_rmse: 0.071313
##### Step: 117 Learning rate: 0.0003125 #####
2025-02-04 04:30:55.374 INFO: ##### Step: 117 Learning rate: 0.0003125 #####
Epoch 78, Train Loss: 4.4485, Val Loss: 5.2199
2025-02-04 04:30:55.374 INFO: Epoch 78, Train Loss: 4.4485, Val Loss: 5.2199
train_e/atom_mae: 0.001847
2025-02-04 04:30:55.375 INFO: train_e/atom_mae: 0.001847
train_e/atom_rmse: 0.002676
2025-02-04 04:30:55.375 INFO: train_e/atom_rmse: 0.002676
train_f_mae: 0.041189
2025-02-04 04:30:55.378 INFO: train_f_mae: 0.041189
train_f_rmse: 0.064688
2025-02-04 04:30:55.378 INFO: train_f_rmse: 0.064688
val_e/atom_mae: 0.001284
2025-02-04 04:30:55.380 INFO: val_e/atom_mae: 0.001284
val_e/atom_rmse: 0.001934
2025-02-04 04:30:55.381 INFO: val_e/atom_rmse: 0.001934
val_f_mae: 0.042949
2025-02-04 04:30:55.381 INFO: val_f_mae: 0.042949
val_f_rmse: 0.071306
2025-02-04 04:30:55.381 INFO: val_f_rmse: 0.071306
##### Step: 118 Learning rate: 0.0003125 #####
2025-02-04 04:31:55.035 INFO: ##### Step: 118 Learning rate: 0.0003125 #####
Epoch 79, Train Loss: 4.5853, Val Loss: 5.2378
2025-02-04 04:31:55.035 INFO: Epoch 79, Train Loss: 4.5853, Val Loss: 5.2378
train_e/atom_mae: 0.002422
2025-02-04 04:31:55.036 INFO: train_e/atom_mae: 0.002422
train_e/atom_rmse: 0.003195
2025-02-04 04:31:55.036 INFO: train_e/atom_rmse: 0.003195
train_f_mae: 0.041378
2025-02-04 04:31:55.039 INFO: train_f_mae: 0.041378
train_f_rmse: 0.064876
2025-02-04 04:31:55.039 INFO: train_f_rmse: 0.064876
val_e/atom_mae: 0.001368
2025-02-04 04:31:55.041 INFO: val_e/atom_mae: 0.001368
val_e/atom_rmse: 0.001869
2025-02-04 04:31:55.041 INFO: val_e/atom_rmse: 0.001869
val_f_mae: 0.043009
2025-02-04 04:31:55.042 INFO: val_f_mae: 0.043009
val_f_rmse: 0.071492
2025-02-04 04:31:55.042 INFO: val_f_rmse: 0.071492
##### Step: 119 Learning rate: 0.0003125 #####
2025-02-04 04:32:54.725 INFO: ##### Step: 119 Learning rate: 0.0003125 #####
Epoch 80, Train Loss: 4.4884, Val Loss: 5.2333
2025-02-04 04:32:54.725 INFO: Epoch 80, Train Loss: 4.4884, Val Loss: 5.2333
train_e/atom_mae: 0.002035
2025-02-04 04:32:54.726 INFO: train_e/atom_mae: 0.002035
train_e/atom_rmse: 0.002840
2025-02-04 04:32:54.726 INFO: train_e/atom_rmse: 0.002840
train_f_mae: 0.041190
2025-02-04 04:32:54.729 INFO: train_f_mae: 0.041190
train_f_rmse: 0.064739
2025-02-04 04:32:54.729 INFO: train_f_rmse: 0.064739
val_e/atom_mae: 0.001280
2025-02-04 04:32:54.731 INFO: val_e/atom_mae: 0.001280
val_e/atom_rmse: 0.001877
2025-02-04 04:32:54.732 INFO: val_e/atom_rmse: 0.001877
val_f_mae: 0.042982
2025-02-04 04:32:54.732 INFO: val_f_mae: 0.042982
val_f_rmse: 0.071455
2025-02-04 04:32:54.732 INFO: val_f_rmse: 0.071455
##### Step: 120 Learning rate: 0.00015625 #####
2025-02-04 04:33:54.409 INFO: ##### Step: 120 Learning rate: 0.00015625 #####
Epoch 81, Train Loss: 4.4300, Val Loss: 5.2263
2025-02-04 04:33:54.410 INFO: Epoch 81, Train Loss: 4.4300, Val Loss: 5.2263
train_e/atom_mae: 0.001952
2025-02-04 04:33:54.410 INFO: train_e/atom_mae: 0.001952
train_e/atom_rmse: 0.002775
2025-02-04 04:33:54.411 INFO: train_e/atom_rmse: 0.002775
train_f_mae: 0.040910
2025-02-04 04:33:54.413 INFO: train_f_mae: 0.040910
train_f_rmse: 0.064390
2025-02-04 04:33:54.413 INFO: train_f_rmse: 0.064390
val_e/atom_mae: 0.001248
2025-02-04 04:33:54.416 INFO: val_e/atom_mae: 0.001248
val_e/atom_rmse: 0.001949
2025-02-04 04:33:54.416 INFO: val_e/atom_rmse: 0.001949
val_f_mae: 0.042924
2025-02-04 04:33:54.416 INFO: val_f_mae: 0.042924
val_f_rmse: 0.071335
2025-02-04 04:33:54.417 INFO: val_f_rmse: 0.071335
##### Step: 121 Learning rate: 0.00015625 #####
2025-02-04 04:34:54.038 INFO: ##### Step: 121 Learning rate: 0.00015625 #####
Epoch 82, Train Loss: 4.4119, Val Loss: 5.2100
2025-02-04 04:34:54.038 INFO: Epoch 82, Train Loss: 4.4119, Val Loss: 5.2100
train_e/atom_mae: 0.001896
2025-02-04 04:34:54.039 INFO: train_e/atom_mae: 0.001896
train_e/atom_rmse: 0.002720
2025-02-04 04:34:54.039 INFO: train_e/atom_rmse: 0.002720
train_f_mae: 0.040892
2025-02-04 04:34:54.042 INFO: train_f_mae: 0.040892
train_f_rmse: 0.064336
2025-02-04 04:34:54.042 INFO: train_f_rmse: 0.064336
val_e/atom_mae: 0.001307
2025-02-04 04:34:54.044 INFO: val_e/atom_mae: 0.001307
val_e/atom_rmse: 0.001888
2025-02-04 04:34:54.044 INFO: val_e/atom_rmse: 0.001888
val_f_mae: 0.042907
2025-02-04 04:34:54.045 INFO: val_f_mae: 0.042907
val_f_rmse: 0.071280
2025-02-04 04:34:54.045 INFO: val_f_rmse: 0.071280
##### Step: 122 Learning rate: 0.00015625 #####
2025-02-04 04:35:53.658 INFO: ##### Step: 122 Learning rate: 0.00015625 #####
Epoch 83, Train Loss: 4.4430, Val Loss: 5.2047
2025-02-04 04:35:53.658 INFO: Epoch 83, Train Loss: 4.4430, Val Loss: 5.2047
train_e/atom_mae: 0.002003
2025-02-04 04:35:53.659 INFO: train_e/atom_mae: 0.002003
train_e/atom_rmse: 0.002812
2025-02-04 04:35:53.659 INFO: train_e/atom_rmse: 0.002812
train_f_mae: 0.040958
2025-02-04 04:35:53.662 INFO: train_f_mae: 0.040958
train_f_rmse: 0.064432
2025-02-04 04:35:53.662 INFO: train_f_rmse: 0.064432
val_e/atom_mae: 0.001347
2025-02-04 04:35:53.664 INFO: val_e/atom_mae: 0.001347
val_e/atom_rmse: 0.001889
2025-02-04 04:35:53.665 INFO: val_e/atom_rmse: 0.001889
val_f_mae: 0.042919
2025-02-04 04:35:53.665 INFO: val_f_mae: 0.042919
val_f_rmse: 0.071242
2025-02-04 04:35:53.665 INFO: val_f_rmse: 0.071242
##### Step: 123 Learning rate: 0.00015625 #####
2025-02-04 04:36:53.288 INFO: ##### Step: 123 Learning rate: 0.00015625 #####
Epoch 84, Train Loss: 4.3765, Val Loss: 5.2075
2025-02-04 04:36:53.288 INFO: Epoch 84, Train Loss: 4.3765, Val Loss: 5.2075
train_e/atom_mae: 0.001676
2025-02-04 04:36:53.289 INFO: train_e/atom_mae: 0.001676
train_e/atom_rmse: 0.002581
2025-02-04 04:36:53.289 INFO: train_e/atom_rmse: 0.002581
train_f_mae: 0.040770
2025-02-04 04:36:53.291 INFO: train_f_mae: 0.040770
train_f_rmse: 0.064272
2025-02-04 04:36:53.292 INFO: train_f_rmse: 0.064272
val_e/atom_mae: 0.001286
2025-02-04 04:36:53.294 INFO: val_e/atom_mae: 0.001286
val_e/atom_rmse: 0.001899
2025-02-04 04:36:53.294 INFO: val_e/atom_rmse: 0.001899
val_f_mae: 0.042886
2025-02-04 04:36:53.294 INFO: val_f_mae: 0.042886
val_f_rmse: 0.071254
2025-02-04 04:36:53.295 INFO: val_f_rmse: 0.071254
##### Step: 124 Learning rate: 0.00015625 #####
2025-02-04 04:37:52.924 INFO: ##### Step: 124 Learning rate: 0.00015625 #####
Epoch 85, Train Loss: 4.3958, Val Loss: 5.2140
2025-02-04 04:37:52.925 INFO: Epoch 85, Train Loss: 4.3958, Val Loss: 5.2140
train_e/atom_mae: 0.001768
2025-02-04 04:37:52.926 INFO: train_e/atom_mae: 0.001768
train_e/atom_rmse: 0.002622
2025-02-04 04:37:52.926 INFO: train_e/atom_rmse: 0.002622
train_f_mae: 0.040852
2025-02-04 04:37:52.929 INFO: train_f_mae: 0.040852
train_f_rmse: 0.064360
2025-02-04 04:37:52.929 INFO: train_f_rmse: 0.064360
val_e/atom_mae: 0.001411
2025-02-04 04:37:52.931 INFO: val_e/atom_mae: 0.001411
val_e/atom_rmse: 0.001944
2025-02-04 04:37:52.931 INFO: val_e/atom_rmse: 0.001944
val_f_mae: 0.042855
2025-02-04 04:37:52.932 INFO: val_f_mae: 0.042855
val_f_rmse: 0.071255
2025-02-04 04:37:52.932 INFO: val_f_rmse: 0.071255
##### Step: 125 Learning rate: 0.00015625 #####
2025-02-04 04:38:52.594 INFO: ##### Step: 125 Learning rate: 0.00015625 #####
Epoch 86, Train Loss: 4.4030, Val Loss: 5.2069
2025-02-04 04:38:52.594 INFO: Epoch 86, Train Loss: 4.4030, Val Loss: 5.2069
train_e/atom_mae: 0.001753
2025-02-04 04:38:52.595 INFO: train_e/atom_mae: 0.001753
train_e/atom_rmse: 0.002622
2025-02-04 04:38:52.595 INFO: train_e/atom_rmse: 0.002622
train_f_mae: 0.040951
2025-02-04 04:38:52.598 INFO: train_f_mae: 0.040951
train_f_rmse: 0.064418
2025-02-04 04:38:52.598 INFO: train_f_rmse: 0.064418
val_e/atom_mae: 0.001245
2025-02-04 04:38:52.600 INFO: val_e/atom_mae: 0.001245
val_e/atom_rmse: 0.001876
2025-02-04 04:38:52.601 INFO: val_e/atom_rmse: 0.001876
val_f_mae: 0.042903
2025-02-04 04:38:52.601 INFO: val_f_mae: 0.042903
val_f_rmse: 0.071272
2025-02-04 04:38:52.601 INFO: val_f_rmse: 0.071272
##### Step: 126 Learning rate: 0.00015625 #####
2025-02-04 04:39:52.257 INFO: ##### Step: 126 Learning rate: 0.00015625 #####
Epoch 87, Train Loss: 4.3889, Val Loss: 5.2028
2025-02-04 04:39:52.257 INFO: Epoch 87, Train Loss: 4.3889, Val Loss: 5.2028
train_e/atom_mae: 0.001729
2025-02-04 04:39:52.258 INFO: train_e/atom_mae: 0.001729
train_e/atom_rmse: 0.002644
2025-02-04 04:39:52.259 INFO: train_e/atom_rmse: 0.002644
train_f_mae: 0.040767
2025-02-04 04:39:52.261 INFO: train_f_mae: 0.040767
train_f_rmse: 0.064275
2025-02-04 04:39:52.261 INFO: train_f_rmse: 0.064275
val_e/atom_mae: 0.001289
2025-02-04 04:39:52.263 INFO: val_e/atom_mae: 0.001289
val_e/atom_rmse: 0.001905
2025-02-04 04:39:52.264 INFO: val_e/atom_rmse: 0.001905
val_f_mae: 0.042877
2025-02-04 04:39:52.264 INFO: val_f_mae: 0.042877
val_f_rmse: 0.071213
2025-02-04 04:39:52.264 INFO: val_f_rmse: 0.071213
##### Step: 127 Learning rate: 0.00015625 #####
2025-02-04 04:40:51.910 INFO: ##### Step: 127 Learning rate: 0.00015625 #####
Epoch 88, Train Loss: 4.4632, Val Loss: 5.2147
2025-02-04 04:40:51.910 INFO: Epoch 88, Train Loss: 4.4632, Val Loss: 5.2147
train_e/atom_mae: 0.002136
2025-02-04 04:40:51.911 INFO: train_e/atom_mae: 0.002136
train_e/atom_rmse: 0.002939
2025-02-04 04:40:51.911 INFO: train_e/atom_rmse: 0.002939
train_f_mae: 0.040942
2025-02-04 04:40:51.914 INFO: train_f_mae: 0.040942
train_f_rmse: 0.064380
2025-02-04 04:40:51.914 INFO: train_f_rmse: 0.064380
val_e/atom_mae: 0.001286
2025-02-04 04:40:51.916 INFO: val_e/atom_mae: 0.001286
val_e/atom_rmse: 0.001882
2025-02-04 04:40:51.917 INFO: val_e/atom_rmse: 0.001882
val_f_mae: 0.042886
2025-02-04 04:40:51.917 INFO: val_f_mae: 0.042886
val_f_rmse: 0.071321
2025-02-04 04:40:51.917 INFO: val_f_rmse: 0.071321
##### Step: 128 Learning rate: 0.00015625 #####
2025-02-04 04:41:51.705 INFO: ##### Step: 128 Learning rate: 0.00015625 #####
Epoch 89, Train Loss: 4.3719, Val Loss: 5.2178
2025-02-04 04:41:51.705 INFO: Epoch 89, Train Loss: 4.3719, Val Loss: 5.2178
train_e/atom_mae: 0.001695
2025-02-04 04:41:51.706 INFO: train_e/atom_mae: 0.001695
train_e/atom_rmse: 0.002567
2025-02-04 04:41:51.706 INFO: train_e/atom_rmse: 0.002567
train_f_mae: 0.040775
2025-02-04 04:41:51.709 INFO: train_f_mae: 0.040775
train_f_rmse: 0.064256
2025-02-04 04:41:51.709 INFO: train_f_rmse: 0.064256
val_e/atom_mae: 0.001257
2025-02-04 04:41:51.711 INFO: val_e/atom_mae: 0.001257
val_e/atom_rmse: 0.001927
2025-02-04 04:41:51.712 INFO: val_e/atom_rmse: 0.001927
val_f_mae: 0.042866
2025-02-04 04:41:51.712 INFO: val_f_mae: 0.042866
val_f_rmse: 0.071299
2025-02-04 04:41:51.712 INFO: val_f_rmse: 0.071299
##### Step: 129 Learning rate: 0.00015625 #####
2025-02-04 04:42:51.445 INFO: ##### Step: 129 Learning rate: 0.00015625 #####
Epoch 90, Train Loss: 4.4149, Val Loss: 5.2075
2025-02-04 04:42:51.446 INFO: Epoch 90, Train Loss: 4.4149, Val Loss: 5.2075
train_e/atom_mae: 0.001954
2025-02-04 04:42:51.447 INFO: train_e/atom_mae: 0.001954
train_e/atom_rmse: 0.002774
2025-02-04 04:42:51.447 INFO: train_e/atom_rmse: 0.002774
train_f_mae: 0.040802
2025-02-04 04:42:51.450 INFO: train_f_mae: 0.040802
train_f_rmse: 0.064275
2025-02-04 04:42:51.450 INFO: train_f_rmse: 0.064275
val_e/atom_mae: 0.001315
2025-02-04 04:42:51.452 INFO: val_e/atom_mae: 0.001315
val_e/atom_rmse: 0.001893
2025-02-04 04:42:51.452 INFO: val_e/atom_rmse: 0.001893
val_f_mae: 0.042875
2025-02-04 04:42:51.453 INFO: val_f_mae: 0.042875
val_f_rmse: 0.071260
2025-02-04 04:42:51.453 INFO: val_f_rmse: 0.071260
##### Step: 130 Learning rate: 0.00015625 #####
2025-02-04 04:43:51.177 INFO: ##### Step: 130 Learning rate: 0.00015625 #####
Epoch 91, Train Loss: 4.4165, Val Loss: 5.2187
2025-02-04 04:43:51.178 INFO: Epoch 91, Train Loss: 4.4165, Val Loss: 5.2187
train_e/atom_mae: 0.001950
2025-02-04 04:43:51.178 INFO: train_e/atom_mae: 0.001950
train_e/atom_rmse: 0.002753
2025-02-04 04:43:51.179 INFO: train_e/atom_rmse: 0.002753
train_f_mae: 0.040840
2025-02-04 04:43:51.181 INFO: train_f_mae: 0.040840
train_f_rmse: 0.064321
2025-02-04 04:43:51.181 INFO: train_f_rmse: 0.064321
val_e/atom_mae: 0.001228
2025-02-04 04:43:51.184 INFO: val_e/atom_mae: 0.001228
val_e/atom_rmse: 0.001898
2025-02-04 04:43:51.184 INFO: val_e/atom_rmse: 0.001898
val_f_mae: 0.042895
2025-02-04 04:43:51.184 INFO: val_f_mae: 0.042895
val_f_rmse: 0.071332
2025-02-04 04:43:51.185 INFO: val_f_rmse: 0.071332
##### Step: 131 Learning rate: 0.00015625 #####
2025-02-04 04:44:50.844 INFO: ##### Step: 131 Learning rate: 0.00015625 #####
Epoch 92, Train Loss: 4.3886, Val Loss: 5.2159
2025-02-04 04:44:50.844 INFO: Epoch 92, Train Loss: 4.3886, Val Loss: 5.2159
train_e/atom_mae: 0.001746
2025-02-04 04:44:50.845 INFO: train_e/atom_mae: 0.001746
train_e/atom_rmse: 0.002622
2025-02-04 04:44:50.845 INFO: train_e/atom_rmse: 0.002622
train_f_mae: 0.040779
2025-02-04 04:44:50.848 INFO: train_f_mae: 0.040779
train_f_rmse: 0.064305
2025-02-04 04:44:50.848 INFO: train_f_rmse: 0.064305
val_e/atom_mae: 0.001289
2025-02-04 04:44:50.850 INFO: val_e/atom_mae: 0.001289
val_e/atom_rmse: 0.001925
2025-02-04 04:44:50.850 INFO: val_e/atom_rmse: 0.001925
val_f_mae: 0.042854
2025-02-04 04:44:50.851 INFO: val_f_mae: 0.042854
val_f_rmse: 0.071288
2025-02-04 04:44:50.851 INFO: val_f_rmse: 0.071288
##### Step: 132 Learning rate: 0.00015625 #####
2025-02-04 04:45:50.566 INFO: ##### Step: 132 Learning rate: 0.00015625 #####
Epoch 93, Train Loss: 4.3880, Val Loss: 5.2061
2025-02-04 04:45:50.566 INFO: Epoch 93, Train Loss: 4.3880, Val Loss: 5.2061
train_e/atom_mae: 0.001700
2025-02-04 04:45:50.567 INFO: train_e/atom_mae: 0.001700
train_e/atom_rmse: 0.002598
2025-02-04 04:45:50.567 INFO: train_e/atom_rmse: 0.002598
train_f_mae: 0.040824
2025-02-04 04:45:50.570 INFO: train_f_mae: 0.040824
train_f_rmse: 0.064336
2025-02-04 04:45:50.570 INFO: train_f_rmse: 0.064336
val_e/atom_mae: 0.001350
2025-02-04 04:45:50.572 INFO: val_e/atom_mae: 0.001350
val_e/atom_rmse: 0.001893
2025-02-04 04:45:50.573 INFO: val_e/atom_rmse: 0.001893
val_f_mae: 0.042896
2025-02-04 04:45:50.573 INFO: val_f_mae: 0.042896
val_f_rmse: 0.071248
2025-02-04 04:45:50.573 INFO: val_f_rmse: 0.071248
##### Step: 133 Learning rate: 0.00015625 #####
2025-02-04 04:46:50.199 INFO: ##### Step: 133 Learning rate: 0.00015625 #####
Epoch 94, Train Loss: 4.3641, Val Loss: 5.2131
2025-02-04 04:46:50.200 INFO: Epoch 94, Train Loss: 4.3641, Val Loss: 5.2131
train_e/atom_mae: 0.001657
2025-02-04 04:46:50.201 INFO: train_e/atom_mae: 0.001657
train_e/atom_rmse: 0.002536
2025-02-04 04:46:50.201 INFO: train_e/atom_rmse: 0.002536
train_f_mae: 0.040752
2025-02-04 04:46:50.204 INFO: train_f_mae: 0.040752
train_f_rmse: 0.064242
2025-02-04 04:46:50.204 INFO: train_f_rmse: 0.064242
val_e/atom_mae: 0.001245
2025-02-04 04:46:50.206 INFO: val_e/atom_mae: 0.001245
val_e/atom_rmse: 0.001862
2025-02-04 04:46:50.206 INFO: val_e/atom_rmse: 0.001862
val_f_mae: 0.042890
2025-02-04 04:46:50.207 INFO: val_f_mae: 0.042890
val_f_rmse: 0.071329
2025-02-04 04:46:50.207 INFO: val_f_rmse: 0.071329
##### Step: 134 Learning rate: 0.00015625 #####
2025-02-04 04:47:49.848 INFO: ##### Step: 134 Learning rate: 0.00015625 #####
Epoch 95, Train Loss: 4.4067, Val Loss: 5.2080
2025-02-04 04:47:49.848 INFO: Epoch 95, Train Loss: 4.4067, Val Loss: 5.2080
train_e/atom_mae: 0.001867
2025-02-04 04:47:49.849 INFO: train_e/atom_mae: 0.001867
train_e/atom_rmse: 0.002666
2025-02-04 04:47:49.849 INFO: train_e/atom_rmse: 0.002666
train_f_mae: 0.040865
2025-02-04 04:47:49.852 INFO: train_f_mae: 0.040865
train_f_rmse: 0.064379
2025-02-04 04:47:49.852 INFO: train_f_rmse: 0.064379
val_e/atom_mae: 0.001273
2025-02-04 04:47:49.854 INFO: val_e/atom_mae: 0.001273
val_e/atom_rmse: 0.001854
2025-02-04 04:47:49.855 INFO: val_e/atom_rmse: 0.001854
val_f_mae: 0.042891
2025-02-04 04:47:49.855 INFO: val_f_mae: 0.042891
val_f_rmse: 0.071299
2025-02-04 04:47:49.855 INFO: val_f_rmse: 0.071299
##### Step: 135 Learning rate: 0.00015625 #####
2025-02-04 04:48:49.524 INFO: ##### Step: 135 Learning rate: 0.00015625 #####
Epoch 96, Train Loss: 4.4150, Val Loss: 5.2117
2025-02-04 04:48:49.525 INFO: Epoch 96, Train Loss: 4.4150, Val Loss: 5.2117
train_e/atom_mae: 0.001915
2025-02-04 04:48:49.526 INFO: train_e/atom_mae: 0.001915
train_e/atom_rmse: 0.002781
2025-02-04 04:48:49.526 INFO: train_e/atom_rmse: 0.002781
train_f_mae: 0.040755
2025-02-04 04:48:49.528 INFO: train_f_mae: 0.040755
train_f_rmse: 0.064265
2025-02-04 04:48:49.529 INFO: train_f_rmse: 0.064265
val_e/atom_mae: 0.001469
2025-02-04 04:48:49.531 INFO: val_e/atom_mae: 0.001469
val_e/atom_rmse: 0.001924
2025-02-04 04:48:49.531 INFO: val_e/atom_rmse: 0.001924
val_f_mae: 0.042884
2025-02-04 04:48:49.531 INFO: val_f_mae: 0.042884
val_f_rmse: 0.071257
2025-02-04 04:48:49.532 INFO: val_f_rmse: 0.071257
##### Step: 136 Learning rate: 0.00015625 #####
2025-02-04 04:49:49.227 INFO: ##### Step: 136 Learning rate: 0.00015625 #####
Epoch 97, Train Loss: 4.4103, Val Loss: 5.2080
2025-02-04 04:49:49.227 INFO: Epoch 97, Train Loss: 4.4103, Val Loss: 5.2080
train_e/atom_mae: 0.001953
2025-02-04 04:49:49.228 INFO: train_e/atom_mae: 0.001953
train_e/atom_rmse: 0.002705
2025-02-04 04:49:49.228 INFO: train_e/atom_rmse: 0.002705
train_f_mae: 0.040933
2025-02-04 04:49:49.231 INFO: train_f_mae: 0.040933
train_f_rmse: 0.064347
2025-02-04 04:49:49.231 INFO: train_f_rmse: 0.064347
val_e/atom_mae: 0.001370
2025-02-04 04:49:49.233 INFO: val_e/atom_mae: 0.001370
val_e/atom_rmse: 0.001879
2025-02-04 04:49:49.233 INFO: val_e/atom_rmse: 0.001879
val_f_mae: 0.042867
2025-02-04 04:49:49.234 INFO: val_f_mae: 0.042867
val_f_rmse: 0.071273
2025-02-04 04:49:49.234 INFO: val_f_rmse: 0.071273
##### Step: 137 Learning rate: 0.00015625 #####
2025-02-04 04:50:48.911 INFO: ##### Step: 137 Learning rate: 0.00015625 #####
Epoch 98, Train Loss: 4.3956, Val Loss: 5.2120
2025-02-04 04:50:48.911 INFO: Epoch 98, Train Loss: 4.3956, Val Loss: 5.2120
train_e/atom_mae: 0.001679
2025-02-04 04:50:48.912 INFO: train_e/atom_mae: 0.001679
train_e/atom_rmse: 0.002569
2025-02-04 04:50:48.913 INFO: train_e/atom_rmse: 0.002569
train_f_mae: 0.040959
2025-02-04 04:50:48.915 INFO: train_f_mae: 0.040959
train_f_rmse: 0.064438
2025-02-04 04:50:48.915 INFO: train_f_rmse: 0.064438
val_e/atom_mae: 0.001241
2025-02-04 04:50:48.917 INFO: val_e/atom_mae: 0.001241
val_e/atom_rmse: 0.001883
2025-02-04 04:50:48.918 INFO: val_e/atom_rmse: 0.001883
val_f_mae: 0.042845
2025-02-04 04:50:48.918 INFO: val_f_mae: 0.042845
val_f_rmse: 0.071302
2025-02-04 04:50:48.918 INFO: val_f_rmse: 0.071302
##### Step: 138 Learning rate: 0.00015625 #####
2025-02-04 04:51:48.582 INFO: ##### Step: 138 Learning rate: 0.00015625 #####
Epoch 99, Train Loss: 4.3787, Val Loss: 5.2170
2025-02-04 04:51:48.582 INFO: Epoch 99, Train Loss: 4.3787, Val Loss: 5.2170
train_e/atom_mae: 0.001732
2025-02-04 04:51:48.583 INFO: train_e/atom_mae: 0.001732
train_e/atom_rmse: 0.002601
2025-02-04 04:51:48.584 INFO: train_e/atom_rmse: 0.002601
train_f_mae: 0.040760
2025-02-04 04:51:48.586 INFO: train_f_mae: 0.040760
train_f_rmse: 0.064260
2025-02-04 04:51:48.586 INFO: train_f_rmse: 0.064260
val_e/atom_mae: 0.001251
2025-02-04 04:51:48.588 INFO: val_e/atom_mae: 0.001251
val_e/atom_rmse: 0.001916
2025-02-04 04:51:48.589 INFO: val_e/atom_rmse: 0.001916
val_f_mae: 0.042844
2025-02-04 04:51:48.589 INFO: val_f_mae: 0.042844
val_f_rmse: 0.071306
2025-02-04 04:51:48.589 INFO: val_f_rmse: 0.071306
##### Step: 139 Learning rate: 0.00015625 #####
2025-02-04 04:52:48.251 INFO: ##### Step: 139 Learning rate: 0.00015625 #####
Epoch 100, Train Loss: 4.3873, Val Loss: 5.2124
2025-02-04 04:52:48.252 INFO: Epoch 100, Train Loss: 4.3873, Val Loss: 5.2124
train_e/atom_mae: 0.001712
2025-02-04 04:52:48.252 INFO: train_e/atom_mae: 0.001712
train_e/atom_rmse: 0.002591
2025-02-04 04:52:48.253 INFO: train_e/atom_rmse: 0.002591
train_f_mae: 0.040869
2025-02-04 04:52:48.255 INFO: train_f_mae: 0.040869
train_f_rmse: 0.064341
2025-02-04 04:52:48.255 INFO: train_f_rmse: 0.064341
val_e/atom_mae: 0.001268
2025-02-04 04:52:48.258 INFO: val_e/atom_mae: 0.001268
val_e/atom_rmse: 0.001925
2025-02-04 04:52:48.258 INFO: val_e/atom_rmse: 0.001925
val_f_mae: 0.042813
2025-02-04 04:52:48.258 INFO: val_f_mae: 0.042813
val_f_rmse: 0.071265
2025-02-04 04:52:48.259 INFO: val_f_rmse: 0.071265
2025-02-04 04:52:49.103 INFO: Third train loop:
##### Step: 140 Learning rate: 7.8125e-05 #####
2025-02-04 04:53:55.358 INFO: ##### Step: 140 Learning rate: 7.8125e-05 #####
Epoch 1, Train Loss: 6.1836, Val Loss: 6.0903
2025-02-04 04:53:55.359 INFO: Epoch 1, Train Loss: 6.1836, Val Loss: 6.0903
train_e/atom_mae: 0.001439
2025-02-04 04:53:55.549 INFO: train_e/atom_mae: 0.001439
train_e/atom_rmse: 0.002331
2025-02-04 04:53:55.561 INFO: train_e/atom_rmse: 0.002331
train_f_mae: 0.040938
2025-02-04 04:53:55.563 INFO: train_f_mae: 0.040938
train_f_rmse: 0.064653
2025-02-04 04:53:55.564 INFO: train_f_rmse: 0.064653
val_e/atom_mae: 0.001054
2025-02-04 04:53:55.566 INFO: val_e/atom_mae: 0.001054
val_e/atom_rmse: 0.001576
2025-02-04 04:53:55.566 INFO: val_e/atom_rmse: 0.001576
val_f_mae: 0.043261
2025-02-04 04:53:55.566 INFO: val_f_mae: 0.043261
val_f_rmse: 0.071934
2025-02-04 04:53:55.567 INFO: val_f_rmse: 0.071934
##### Step: 141 Learning rate: 7.8125e-05 #####
2025-02-04 04:54:55.233 INFO: ##### Step: 141 Learning rate: 7.8125e-05 #####
Epoch 2, Train Loss: 6.0924, Val Loss: 6.0820
2025-02-04 04:54:55.234 INFO: Epoch 2, Train Loss: 6.0924, Val Loss: 6.0820
train_e/atom_mae: 0.001520
2025-02-04 04:54:55.235 INFO: train_e/atom_mae: 0.001520
train_e/atom_rmse: 0.002274
2025-02-04 04:54:55.235 INFO: train_e/atom_rmse: 0.002274
train_f_mae: 0.040994
2025-02-04 04:54:55.238 INFO: train_f_mae: 0.040994
train_f_rmse: 0.064705
2025-02-04 04:54:55.238 INFO: train_f_rmse: 0.064705
val_e/atom_mae: 0.001038
2025-02-04 04:54:55.240 INFO: val_e/atom_mae: 0.001038
val_e/atom_rmse: 0.001572
2025-02-04 04:54:55.240 INFO: val_e/atom_rmse: 0.001572
val_f_mae: 0.043264
2025-02-04 04:54:55.241 INFO: val_f_mae: 0.043264
val_f_rmse: 0.071910
2025-02-04 04:54:55.241 INFO: val_f_rmse: 0.071910
##### Step: 142 Learning rate: 7.8125e-05 #####
2025-02-04 04:55:54.980 INFO: ##### Step: 142 Learning rate: 7.8125e-05 #####
Epoch 3, Train Loss: 5.8570, Val Loss: 6.0802
2025-02-04 04:55:54.980 INFO: Epoch 3, Train Loss: 5.8570, Val Loss: 6.0802
train_e/atom_mae: 0.001287
2025-02-04 04:55:54.981 INFO: train_e/atom_mae: 0.001287
train_e/atom_rmse: 0.002118
2025-02-04 04:55:54.981 INFO: train_e/atom_rmse: 0.002118
train_f_mae: 0.041006
2025-02-04 04:55:54.984 INFO: train_f_mae: 0.041006
train_f_rmse: 0.064829
2025-02-04 04:55:54.984 INFO: train_f_rmse: 0.064829
val_e/atom_mae: 0.001032
2025-02-04 04:55:54.986 INFO: val_e/atom_mae: 0.001032
val_e/atom_rmse: 0.001560
2025-02-04 04:55:54.987 INFO: val_e/atom_rmse: 0.001560
val_f_mae: 0.043261
2025-02-04 04:55:54.987 INFO: val_f_mae: 0.043261
val_f_rmse: 0.072005
2025-02-04 04:55:54.987 INFO: val_f_rmse: 0.072005
##### Step: 143 Learning rate: 7.8125e-05 #####
2025-02-04 04:56:54.666 INFO: ##### Step: 143 Learning rate: 7.8125e-05 #####
Epoch 4, Train Loss: 6.0407, Val Loss: 6.0794
2025-02-04 04:56:54.667 INFO: Epoch 4, Train Loss: 6.0407, Val Loss: 6.0794
train_e/atom_mae: 0.001416
2025-02-04 04:56:54.668 INFO: train_e/atom_mae: 0.001416
train_e/atom_rmse: 0.002220
2025-02-04 04:56:54.668 INFO: train_e/atom_rmse: 0.002220
train_f_mae: 0.041126
2025-02-04 04:56:54.670 INFO: train_f_mae: 0.041126
train_f_rmse: 0.064987
2025-02-04 04:56:54.671 INFO: train_f_rmse: 0.064987
val_e/atom_mae: 0.001017
2025-02-04 04:56:54.673 INFO: val_e/atom_mae: 0.001017
val_e/atom_rmse: 0.001550
2025-02-04 04:56:54.673 INFO: val_e/atom_rmse: 0.001550
val_f_mae: 0.043330
2025-02-04 04:56:54.674 INFO: val_f_mae: 0.043330
val_f_rmse: 0.072062
2025-02-04 04:56:54.674 INFO: val_f_rmse: 0.072062
##### Step: 144 Learning rate: 7.8125e-05 #####
2025-02-04 04:57:54.336 INFO: ##### Step: 144 Learning rate: 7.8125e-05 #####
Epoch 5, Train Loss: 5.9646, Val Loss: 6.0468
2025-02-04 04:57:54.336 INFO: Epoch 5, Train Loss: 5.9646, Val Loss: 6.0468
train_e/atom_mae: 0.001389
2025-02-04 04:57:54.337 INFO: train_e/atom_mae: 0.001389
train_e/atom_rmse: 0.002164
2025-02-04 04:57:54.338 INFO: train_e/atom_rmse: 0.002164
train_f_mae: 0.041168
2025-02-04 04:57:54.340 INFO: train_f_mae: 0.041168
train_f_rmse: 0.065098
2025-02-04 04:57:54.340 INFO: train_f_rmse: 0.065098
val_e/atom_mae: 0.000990
2025-02-04 04:57:54.342 INFO: val_e/atom_mae: 0.000990
val_e/atom_rmse: 0.001509
2025-02-04 04:57:54.343 INFO: val_e/atom_rmse: 0.001509
val_f_mae: 0.043418
2025-02-04 04:57:54.343 INFO: val_f_mae: 0.043418
val_f_rmse: 0.072160
2025-02-04 04:57:54.343 INFO: val_f_rmse: 0.072160
##### Step: 145 Learning rate: 7.8125e-05 #####
2025-02-04 04:58:54.017 INFO: ##### Step: 145 Learning rate: 7.8125e-05 #####
Epoch 6, Train Loss: 5.8538, Val Loss: 6.1070
2025-02-04 04:58:54.017 INFO: Epoch 6, Train Loss: 5.8538, Val Loss: 6.1070
train_e/atom_mae: 0.001250
2025-02-04 04:58:54.018 INFO: train_e/atom_mae: 0.001250
train_e/atom_rmse: 0.002096
2025-02-04 04:58:54.019 INFO: train_e/atom_rmse: 0.002096
train_f_mae: 0.041091
2025-02-04 04:58:54.021 INFO: train_f_mae: 0.041091
train_f_rmse: 0.065067
2025-02-04 04:58:54.021 INFO: train_f_rmse: 0.065067
val_e/atom_mae: 0.000995
2025-02-04 04:58:54.023 INFO: val_e/atom_mae: 0.000995
val_e/atom_rmse: 0.001578
2025-02-04 04:58:54.024 INFO: val_e/atom_rmse: 0.001578
val_f_mae: 0.043275
2025-02-04 04:58:54.024 INFO: val_f_mae: 0.043275
val_f_rmse: 0.072048
2025-02-04 04:58:54.024 INFO: val_f_rmse: 0.072048
##### Step: 146 Learning rate: 7.8125e-05 #####
2025-02-04 04:59:53.717 INFO: ##### Step: 146 Learning rate: 7.8125e-05 #####
Epoch 7, Train Loss: 5.8147, Val Loss: 6.0926
2025-02-04 04:59:53.717 INFO: Epoch 7, Train Loss: 5.8147, Val Loss: 6.0926
train_e/atom_mae: 0.001218
2025-02-04 04:59:53.718 INFO: train_e/atom_mae: 0.001218
train_e/atom_rmse: 0.002072
2025-02-04 04:59:53.718 INFO: train_e/atom_rmse: 0.002072
train_f_mae: 0.041074
2025-02-04 04:59:53.721 INFO: train_f_mae: 0.041074
train_f_rmse: 0.065049
2025-02-04 04:59:53.721 INFO: train_f_rmse: 0.065049
val_e/atom_mae: 0.000995
2025-02-04 04:59:53.723 INFO: val_e/atom_mae: 0.000995
val_e/atom_rmse: 0.001562
2025-02-04 04:59:53.723 INFO: val_e/atom_rmse: 0.001562
val_f_mae: 0.043292
2025-02-04 04:59:53.724 INFO: val_f_mae: 0.043292
val_f_rmse: 0.072070
2025-02-04 04:59:53.724 INFO: val_f_rmse: 0.072070
##### Step: 147 Learning rate: 7.8125e-05 #####
2025-02-04 05:00:53.342 INFO: ##### Step: 147 Learning rate: 7.8125e-05 #####
Epoch 8, Train Loss: 6.1697, Val Loss: 6.0488
2025-02-04 05:00:53.343 INFO: Epoch 8, Train Loss: 6.1697, Val Loss: 6.0488
train_e/atom_mae: 0.001545
2025-02-04 05:00:53.344 INFO: train_e/atom_mae: 0.001545
train_e/atom_rmse: 0.002269
2025-02-04 05:00:53.344 INFO: train_e/atom_rmse: 0.002269
train_f_mae: 0.041346
2025-02-04 05:00:53.347 INFO: train_f_mae: 0.041346
train_f_rmse: 0.065354
2025-02-04 05:00:53.347 INFO: train_f_rmse: 0.065354
val_e/atom_mae: 0.000996
2025-02-04 05:00:53.349 INFO: val_e/atom_mae: 0.000996
val_e/atom_rmse: 0.001506
2025-02-04 05:00:53.349 INFO: val_e/atom_rmse: 0.001506
val_f_mae: 0.043391
2025-02-04 05:00:53.350 INFO: val_f_mae: 0.043391
val_f_rmse: 0.072194
2025-02-04 05:00:53.350 INFO: val_f_rmse: 0.072194
##### Step: 148 Learning rate: 7.8125e-05 #####
2025-02-04 05:02:02.992 INFO: ##### Step: 148 Learning rate: 7.8125e-05 #####
Epoch 9, Train Loss: 5.9929, Val Loss: 6.0536
2025-02-04 05:02:02.993 INFO: Epoch 9, Train Loss: 5.9929, Val Loss: 6.0536
train_e/atom_mae: 0.001382
2025-02-04 05:02:03.033 INFO: train_e/atom_mae: 0.001382
train_e/atom_rmse: 0.002174
2025-02-04 05:02:03.043 INFO: train_e/atom_rmse: 0.002174
train_f_mae: 0.041240
2025-02-04 05:02:03.046 INFO: train_f_mae: 0.041240
train_f_rmse: 0.065194
2025-02-04 05:02:03.046 INFO: train_f_rmse: 0.065194
val_e/atom_mae: 0.001007
2025-02-04 05:02:03.048 INFO: val_e/atom_mae: 0.001007
val_e/atom_rmse: 0.001519
2025-02-04 05:02:03.048 INFO: val_e/atom_rmse: 0.001519
val_f_mae: 0.043351
2025-02-04 05:02:03.049 INFO: val_f_mae: 0.043351
val_f_rmse: 0.072138
2025-02-04 05:02:03.049 INFO: val_f_rmse: 0.072138
##### Step: 149 Learning rate: 7.8125e-05 #####
2025-02-04 05:03:02.660 INFO: ##### Step: 149 Learning rate: 7.8125e-05 #####
Epoch 10, Train Loss: 6.0718, Val Loss: 6.0604
2025-02-04 05:03:02.660 INFO: Epoch 10, Train Loss: 6.0718, Val Loss: 6.0604
train_e/atom_mae: 0.001441
2025-02-04 05:03:02.661 INFO: train_e/atom_mae: 0.001441
train_e/atom_rmse: 0.002225
2025-02-04 05:03:02.661 INFO: train_e/atom_rmse: 0.002225
train_f_mae: 0.041211
2025-02-04 05:03:02.664 INFO: train_f_mae: 0.041211
train_f_rmse: 0.065168
2025-02-04 05:03:02.664 INFO: train_f_rmse: 0.065168
val_e/atom_mae: 0.001000
2025-02-04 05:03:02.666 INFO: val_e/atom_mae: 0.001000
val_e/atom_rmse: 0.001534
2025-02-04 05:03:02.667 INFO: val_e/atom_rmse: 0.001534
val_f_mae: 0.043312
2025-02-04 05:03:02.667 INFO: val_f_mae: 0.043312
val_f_rmse: 0.072067
2025-02-04 05:03:02.667 INFO: val_f_rmse: 0.072067
##### Step: 150 Learning rate: 7.8125e-05 #####
2025-02-04 05:04:02.557 INFO: ##### Step: 150 Learning rate: 7.8125e-05 #####
Epoch 11, Train Loss: 5.9693, Val Loss: 6.0592
2025-02-04 05:04:02.558 INFO: Epoch 11, Train Loss: 5.9693, Val Loss: 6.0592
train_e/atom_mae: 0.001420
2025-02-04 05:04:02.559 INFO: train_e/atom_mae: 0.001420
train_e/atom_rmse: 0.002149
2025-02-04 05:04:02.559 INFO: train_e/atom_rmse: 0.002149
train_f_mae: 0.041247
2025-02-04 05:04:02.562 INFO: train_f_mae: 0.041247
train_f_rmse: 0.065325
2025-02-04 05:04:02.562 INFO: train_f_rmse: 0.065325
val_e/atom_mae: 0.001015
2025-02-04 05:04:02.564 INFO: val_e/atom_mae: 0.001015
val_e/atom_rmse: 0.001506
2025-02-04 05:04:02.564 INFO: val_e/atom_rmse: 0.001506
val_f_mae: 0.043418
2025-02-04 05:04:02.565 INFO: val_f_mae: 0.043418
val_f_rmse: 0.072278
2025-02-04 05:04:02.565 INFO: val_f_rmse: 0.072278
##### Step: 151 Learning rate: 7.8125e-05 #####
2025-02-04 05:05:02.399 INFO: ##### Step: 151 Learning rate: 7.8125e-05 #####
Epoch 12, Train Loss: 5.9189, Val Loss: 6.0213
2025-02-04 05:05:02.400 INFO: Epoch 12, Train Loss: 5.9189, Val Loss: 6.0213
train_e/atom_mae: 0.001345
2025-02-04 05:05:02.401 INFO: train_e/atom_mae: 0.001345
train_e/atom_rmse: 0.002126
2025-02-04 05:05:02.401 INFO: train_e/atom_rmse: 0.002126
train_f_mae: 0.041189
2025-02-04 05:05:02.403 INFO: train_f_mae: 0.041189
train_f_rmse: 0.065217
2025-02-04 05:05:02.404 INFO: train_f_rmse: 0.065217
val_e/atom_mae: 0.001006
2025-02-04 05:05:02.406 INFO: val_e/atom_mae: 0.001006
val_e/atom_rmse: 0.001485
2025-02-04 05:05:02.406 INFO: val_e/atom_rmse: 0.001485
val_f_mae: 0.043427
2025-02-04 05:05:02.407 INFO: val_f_mae: 0.043427
val_f_rmse: 0.072166
2025-02-04 05:05:02.407 INFO: val_f_rmse: 0.072166
##### Step: 152 Learning rate: 7.8125e-05 #####
2025-02-04 05:06:02.349 INFO: ##### Step: 152 Learning rate: 7.8125e-05 #####
Epoch 13, Train Loss: 6.0941, Val Loss: 6.0594
2025-02-04 05:06:02.349 INFO: Epoch 13, Train Loss: 6.0941, Val Loss: 6.0594
train_e/atom_mae: 0.001572
2025-02-04 05:06:02.350 INFO: train_e/atom_mae: 0.001572
train_e/atom_rmse: 0.002236
2025-02-04 05:06:02.350 INFO: train_e/atom_rmse: 0.002236
train_f_mae: 0.041239
2025-02-04 05:06:02.353 INFO: train_f_mae: 0.041239
train_f_rmse: 0.065206
2025-02-04 05:06:02.353 INFO: train_f_rmse: 0.065206
val_e/atom_mae: 0.001034
2025-02-04 05:06:02.355 INFO: val_e/atom_mae: 0.001034
val_e/atom_rmse: 0.001505
2025-02-04 05:06:02.356 INFO: val_e/atom_rmse: 0.001505
val_f_mae: 0.043462
2025-02-04 05:06:02.356 INFO: val_f_mae: 0.043462
val_f_rmse: 0.072271
2025-02-04 05:06:02.356 INFO: val_f_rmse: 0.072271
##### Step: 153 Learning rate: 7.8125e-05 #####
2025-02-04 05:07:02.266 INFO: ##### Step: 153 Learning rate: 7.8125e-05 #####
Epoch 14, Train Loss: 5.9554, Val Loss: 6.0974
2025-02-04 05:07:02.267 INFO: Epoch 14, Train Loss: 5.9554, Val Loss: 6.0974
train_e/atom_mae: 0.001365
2025-02-04 05:07:02.268 INFO: train_e/atom_mae: 0.001365
train_e/atom_rmse: 0.002141
2025-02-04 05:07:02.268 INFO: train_e/atom_rmse: 0.002141
train_f_mae: 0.041287
2025-02-04 05:07:02.270 INFO: train_f_mae: 0.041287
train_f_rmse: 0.065310
2025-02-04 05:07:02.271 INFO: train_f_rmse: 0.065310
val_e/atom_mae: 0.000998
2025-02-04 05:07:02.273 INFO: val_e/atom_mae: 0.000998
val_e/atom_rmse: 0.001559
2025-02-04 05:07:02.273 INFO: val_e/atom_rmse: 0.001559
val_f_mae: 0.043376
2025-02-04 05:07:02.274 INFO: val_f_mae: 0.043376
val_f_rmse: 0.072132
2025-02-04 05:07:02.274 INFO: val_f_rmse: 0.072132
##### Step: 154 Learning rate: 7.8125e-05 #####
2025-02-04 05:08:02.222 INFO: ##### Step: 154 Learning rate: 7.8125e-05 #####
Epoch 15, Train Loss: 6.0640, Val Loss: 6.0994
2025-02-04 05:08:02.223 INFO: Epoch 15, Train Loss: 6.0640, Val Loss: 6.0994
train_e/atom_mae: 0.001455
2025-02-04 05:08:02.223 INFO: train_e/atom_mae: 0.001455
train_e/atom_rmse: 0.002208
2025-02-04 05:08:02.224 INFO: train_e/atom_rmse: 0.002208
train_f_mae: 0.041301
2025-02-04 05:08:02.226 INFO: train_f_mae: 0.041301
train_f_rmse: 0.065325
2025-02-04 05:08:02.227 INFO: train_f_rmse: 0.065325
val_e/atom_mae: 0.001104
2025-02-04 05:08:02.229 INFO: val_e/atom_mae: 0.001104
val_e/atom_rmse: 0.001534
2025-02-04 05:08:02.229 INFO: val_e/atom_rmse: 0.001534
val_f_mae: 0.043475
2025-02-04 05:08:02.229 INFO: val_f_mae: 0.043475
val_f_rmse: 0.072302
2025-02-04 05:08:02.230 INFO: val_f_rmse: 0.072302
##### Step: 155 Learning rate: 7.8125e-05 #####
2025-02-04 05:09:02.150 INFO: ##### Step: 155 Learning rate: 7.8125e-05 #####
Epoch 16, Train Loss: 6.4471, Val Loss: 6.0825
2025-02-04 05:09:02.151 INFO: Epoch 16, Train Loss: 6.4471, Val Loss: 6.0825
train_e/atom_mae: 0.001766
2025-02-04 05:09:02.152 INFO: train_e/atom_mae: 0.001766
train_e/atom_rmse: 0.002418
2025-02-04 05:09:02.152 INFO: train_e/atom_rmse: 0.002418
train_f_mae: 0.041403
2025-02-04 05:09:02.155 INFO: train_f_mae: 0.041403
train_f_rmse: 0.065506
2025-02-04 05:09:02.155 INFO: train_f_rmse: 0.065506
val_e/atom_mae: 0.001118
2025-02-04 05:09:02.157 INFO: val_e/atom_mae: 0.001118
val_e/atom_rmse: 0.001520
2025-02-04 05:09:02.157 INFO: val_e/atom_rmse: 0.001520
val_f_mae: 0.043492
2025-02-04 05:09:02.158 INFO: val_f_mae: 0.043492
val_f_rmse: 0.072313
2025-02-04 05:09:02.158 INFO: val_f_rmse: 0.072313
##### Step: 156 Learning rate: 7.8125e-05 #####
2025-02-04 05:10:02.077 INFO: ##### Step: 156 Learning rate: 7.8125e-05 #####
Epoch 17, Train Loss: 6.1479, Val Loss: 6.0345
2025-02-04 05:10:02.077 INFO: Epoch 17, Train Loss: 6.1479, Val Loss: 6.0345
train_e/atom_mae: 0.001526
2025-02-04 05:10:02.078 INFO: train_e/atom_mae: 0.001526
train_e/atom_rmse: 0.002260
2025-02-04 05:10:02.078 INFO: train_e/atom_rmse: 0.002260
train_f_mae: 0.041337
2025-02-04 05:10:02.081 INFO: train_f_mae: 0.041337
train_f_rmse: 0.065313
2025-02-04 05:10:02.081 INFO: train_f_rmse: 0.065313
val_e/atom_mae: 0.000981
2025-02-04 05:10:02.083 INFO: val_e/atom_mae: 0.000981
val_e/atom_rmse: 0.001488
2025-02-04 05:10:02.084 INFO: val_e/atom_rmse: 0.001488
val_f_mae: 0.043465
2025-02-04 05:10:02.084 INFO: val_f_mae: 0.043465
val_f_rmse: 0.072236
2025-02-04 05:10:02.084 INFO: val_f_rmse: 0.072236
##### Step: 157 Learning rate: 7.8125e-05 #####
2025-02-04 05:11:01.983 INFO: ##### Step: 157 Learning rate: 7.8125e-05 #####
Epoch 18, Train Loss: 5.8184, Val Loss: 6.0333
2025-02-04 05:11:01.984 INFO: Epoch 18, Train Loss: 5.8184, Val Loss: 6.0333
train_e/atom_mae: 0.001279
2025-02-04 05:11:01.985 INFO: train_e/atom_mae: 0.001279
train_e/atom_rmse: 0.002049
2025-02-04 05:11:01.985 INFO: train_e/atom_rmse: 0.002049
train_f_mae: 0.041324
2025-02-04 05:11:01.987 INFO: train_f_mae: 0.041324
train_f_rmse: 0.065348
2025-02-04 05:11:01.988 INFO: train_f_rmse: 0.065348
val_e/atom_mae: 0.000981
2025-02-04 05:11:01.990 INFO: val_e/atom_mae: 0.000981
val_e/atom_rmse: 0.001469
2025-02-04 05:11:01.990 INFO: val_e/atom_rmse: 0.001469
val_f_mae: 0.043468
2025-02-04 05:11:01.991 INFO: val_f_mae: 0.043468
val_f_rmse: 0.072362
2025-02-04 05:11:01.991 INFO: val_f_rmse: 0.072362
##### Step: 158 Learning rate: 7.8125e-05 #####
2025-02-04 05:12:01.904 INFO: ##### Step: 158 Learning rate: 7.8125e-05 #####
Epoch 19, Train Loss: 5.9069, Val Loss: 6.0932
2025-02-04 05:12:01.904 INFO: Epoch 19, Train Loss: 5.9069, Val Loss: 6.0932
train_e/atom_mae: 0.001307
2025-02-04 05:12:01.905 INFO: train_e/atom_mae: 0.001307
train_e/atom_rmse: 0.002110
2025-02-04 05:12:01.905 INFO: train_e/atom_rmse: 0.002110
train_f_mae: 0.041246
2025-02-04 05:12:01.908 INFO: train_f_mae: 0.041246
train_f_rmse: 0.065313
2025-02-04 05:12:01.908 INFO: train_f_rmse: 0.065313
val_e/atom_mae: 0.000988
2025-02-04 05:12:01.910 INFO: val_e/atom_mae: 0.000988
val_e/atom_rmse: 0.001545
2025-02-04 05:12:01.911 INFO: val_e/atom_rmse: 0.001545
val_f_mae: 0.043350
2025-02-04 05:12:01.911 INFO: val_f_mae: 0.043350
val_f_rmse: 0.072216
2025-02-04 05:12:01.911 INFO: val_f_rmse: 0.072216
##### Step: 159 Learning rate: 7.8125e-05 #####
2025-02-04 05:13:13.086 INFO: ##### Step: 159 Learning rate: 7.8125e-05 #####
Epoch 20, Train Loss: 5.8233, Val Loss: 6.0306
2025-02-04 05:13:13.087 INFO: Epoch 20, Train Loss: 5.8233, Val Loss: 6.0306
train_e/atom_mae: 0.001292
2025-02-04 05:13:13.255 INFO: train_e/atom_mae: 0.001292
train_e/atom_rmse: 0.002053
2025-02-04 05:13:13.325 INFO: train_e/atom_rmse: 0.002053
train_f_mae: 0.041288
2025-02-04 05:13:13.328 INFO: train_f_mae: 0.041288
train_f_rmse: 0.065337
2025-02-04 05:13:13.328 INFO: train_f_rmse: 0.065337
val_e/atom_mae: 0.000974
2025-02-04 05:13:13.330 INFO: val_e/atom_mae: 0.000974
val_e/atom_rmse: 0.001478
2025-02-04 05:13:13.331 INFO: val_e/atom_rmse: 0.001478
val_f_mae: 0.043454
2025-02-04 05:13:13.331 INFO: val_f_mae: 0.043454
val_f_rmse: 0.072290
2025-02-04 05:13:13.331 INFO: val_f_rmse: 0.072290
##### Step: 160 Learning rate: 3.90625e-05 #####
2025-02-04 05:14:15.789 INFO: ##### Step: 160 Learning rate: 3.90625e-05 #####
Epoch 21, Train Loss: 5.6784, Val Loss: 6.0446
2025-02-04 05:14:15.789 INFO: Epoch 21, Train Loss: 5.6784, Val Loss: 6.0446
train_e/atom_mae: 0.001130
2025-02-04 05:14:15.790 INFO: train_e/atom_mae: 0.001130
train_e/atom_rmse: 0.001974
2025-02-04 05:14:15.790 INFO: train_e/atom_rmse: 0.001974
train_f_mae: 0.041151
2025-02-04 05:14:15.793 INFO: train_f_mae: 0.041151
train_f_rmse: 0.065128
2025-02-04 05:14:15.793 INFO: train_f_rmse: 0.065128
val_e/atom_mae: 0.000985
2025-02-04 05:14:15.795 INFO: val_e/atom_mae: 0.000985
val_e/atom_rmse: 0.001500
2025-02-04 05:14:15.796 INFO: val_e/atom_rmse: 0.001500
val_f_mae: 0.043358
2025-02-04 05:14:15.796 INFO: val_f_mae: 0.043358
val_f_rmse: 0.072218
2025-02-04 05:14:15.796 INFO: val_f_rmse: 0.072218
##### Step: 161 Learning rate: 3.90625e-05 #####
2025-02-04 05:15:15.417 INFO: ##### Step: 161 Learning rate: 3.90625e-05 #####
Epoch 22, Train Loss: 5.7342, Val Loss: 6.0494
2025-02-04 05:15:15.418 INFO: Epoch 22, Train Loss: 5.7342, Val Loss: 6.0494
train_e/atom_mae: 0.001157
2025-02-04 05:15:15.418 INFO: train_e/atom_mae: 0.001157
train_e/atom_rmse: 0.002004
2025-02-04 05:15:15.419 INFO: train_e/atom_rmse: 0.002004
train_f_mae: 0.041163
2025-02-04 05:15:15.421 INFO: train_f_mae: 0.041163
train_f_rmse: 0.065223
2025-02-04 05:15:15.421 INFO: train_f_rmse: 0.065223
val_e/atom_mae: 0.000969
2025-02-04 05:15:15.423 INFO: val_e/atom_mae: 0.000969
val_e/atom_rmse: 0.001511
2025-02-04 05:15:15.424 INFO: val_e/atom_rmse: 0.001511
val_f_mae: 0.043358
2025-02-04 05:15:15.424 INFO: val_f_mae: 0.043358
val_f_rmse: 0.072168
2025-02-04 05:15:15.424 INFO: val_f_rmse: 0.072168
##### Step: 162 Learning rate: 3.90625e-05 #####
2025-02-04 05:16:15.108 INFO: ##### Step: 162 Learning rate: 3.90625e-05 #####
Epoch 23, Train Loss: 5.7969, Val Loss: 6.0426
2025-02-04 05:16:15.108 INFO: Epoch 23, Train Loss: 5.7969, Val Loss: 6.0426
train_e/atom_mae: 0.001263
2025-02-04 05:16:15.109 INFO: train_e/atom_mae: 0.001263
train_e/atom_rmse: 0.002052
2025-02-04 05:16:15.109 INFO: train_e/atom_rmse: 0.002052
train_f_mae: 0.041138
2025-02-04 05:16:15.112 INFO: train_f_mae: 0.041138
train_f_rmse: 0.065147
2025-02-04 05:16:15.112 INFO: train_f_rmse: 0.065147
val_e/atom_mae: 0.000963
2025-02-04 05:16:15.114 INFO: val_e/atom_mae: 0.000963
val_e/atom_rmse: 0.001497
2025-02-04 05:16:15.114 INFO: val_e/atom_rmse: 0.001497
val_f_mae: 0.043445
2025-02-04 05:16:15.115 INFO: val_f_mae: 0.043445
val_f_rmse: 0.072228
2025-02-04 05:16:15.115 INFO: val_f_rmse: 0.072228
##### Step: 163 Learning rate: 3.90625e-05 #####
2025-02-04 05:17:14.797 INFO: ##### Step: 163 Learning rate: 3.90625e-05 #####
Epoch 24, Train Loss: 5.6860, Val Loss: 6.0425
2025-02-04 05:17:14.798 INFO: Epoch 24, Train Loss: 5.6860, Val Loss: 6.0425
train_e/atom_mae: 0.001148
2025-02-04 05:17:14.799 INFO: train_e/atom_mae: 0.001148
train_e/atom_rmse: 0.001966
2025-02-04 05:17:14.799 INFO: train_e/atom_rmse: 0.001966
train_f_mae: 0.041208
2025-02-04 05:17:14.801 INFO: train_f_mae: 0.041208
train_f_rmse: 0.065279
2025-02-04 05:17:14.802 INFO: train_f_rmse: 0.065279
val_e/atom_mae: 0.000957
2025-02-04 05:17:14.804 INFO: val_e/atom_mae: 0.000957
val_e/atom_rmse: 0.001491
2025-02-04 05:17:14.804 INFO: val_e/atom_rmse: 0.001491
val_f_mae: 0.043404
2025-02-04 05:17:14.805 INFO: val_f_mae: 0.043404
val_f_rmse: 0.072279
2025-02-04 05:17:14.805 INFO: val_f_rmse: 0.072279
##### Step: 164 Learning rate: 3.90625e-05 #####
2025-02-04 05:18:14.545 INFO: ##### Step: 164 Learning rate: 3.90625e-05 #####
Epoch 25, Train Loss: 5.7836, Val Loss: 6.0278
2025-02-04 05:18:14.546 INFO: Epoch 25, Train Loss: 5.7836, Val Loss: 6.0278
train_e/atom_mae: 0.001235
2025-02-04 05:18:14.547 INFO: train_e/atom_mae: 0.001235
train_e/atom_rmse: 0.002040
2025-02-04 05:18:14.547 INFO: train_e/atom_rmse: 0.002040
train_f_mae: 0.041122
2025-02-04 05:18:14.549 INFO: train_f_mae: 0.041122
train_f_rmse: 0.065182
2025-02-04 05:18:14.550 INFO: train_f_rmse: 0.065182
val_e/atom_mae: 0.000990
2025-02-04 05:18:14.552 INFO: val_e/atom_mae: 0.000990
val_e/atom_rmse: 0.001488
2025-02-04 05:18:14.552 INFO: val_e/atom_rmse: 0.001488
val_f_mae: 0.043376
2025-02-04 05:18:14.553 INFO: val_f_mae: 0.043376
val_f_rmse: 0.072189
2025-02-04 05:18:14.553 INFO: val_f_rmse: 0.072189
##### Step: 165 Learning rate: 3.90625e-05 #####
2025-02-04 05:19:14.245 INFO: ##### Step: 165 Learning rate: 3.90625e-05 #####
Epoch 26, Train Loss: 5.7814, Val Loss: 6.0390
2025-02-04 05:19:14.245 INFO: Epoch 26, Train Loss: 5.7814, Val Loss: 6.0390
train_e/atom_mae: 0.001221
2025-02-04 05:19:14.246 INFO: train_e/atom_mae: 0.001221
train_e/atom_rmse: 0.002041
2025-02-04 05:19:14.247 INFO: train_e/atom_rmse: 0.002041
train_f_mae: 0.041135
2025-02-04 05:19:14.249 INFO: train_f_mae: 0.041135
train_f_rmse: 0.065161
2025-02-04 05:19:14.249 INFO: train_f_rmse: 0.065161
val_e/atom_mae: 0.000980
2025-02-04 05:19:14.251 INFO: val_e/atom_mae: 0.000980
val_e/atom_rmse: 0.001492
2025-02-04 05:19:14.252 INFO: val_e/atom_rmse: 0.001492
val_f_mae: 0.043388
2025-02-04 05:19:14.252 INFO: val_f_mae: 0.043388
val_f_rmse: 0.072236
2025-02-04 05:19:14.253 INFO: val_f_rmse: 0.072236
##### Step: 166 Learning rate: 3.90625e-05 #####
2025-02-04 05:20:19.719 INFO: ##### Step: 166 Learning rate: 3.90625e-05 #####
Epoch 27, Train Loss: 5.7697, Val Loss: 6.0830
2025-02-04 05:20:19.719 INFO: Epoch 27, Train Loss: 5.7697, Val Loss: 6.0830
train_e/atom_mae: 0.001223
2025-02-04 05:20:19.799 INFO: train_e/atom_mae: 0.001223
train_e/atom_rmse: 0.002028
2025-02-04 05:20:19.808 INFO: train_e/atom_rmse: 0.002028
train_f_mae: 0.041178
2025-02-04 05:20:19.810 INFO: train_f_mae: 0.041178
train_f_rmse: 0.065219
2025-02-04 05:20:19.811 INFO: train_f_rmse: 0.065219
val_e/atom_mae: 0.000958
2025-02-04 05:20:19.813 INFO: val_e/atom_mae: 0.000958
val_e/atom_rmse: 0.001535
2025-02-04 05:20:19.813 INFO: val_e/atom_rmse: 0.001535
val_f_mae: 0.043368
2025-02-04 05:20:19.814 INFO: val_f_mae: 0.043368
val_f_rmse: 0.072222
2025-02-04 05:20:19.814 INFO: val_f_rmse: 0.072222
##### Step: 167 Learning rate: 3.90625e-05 #####
2025-02-04 05:21:19.470 INFO: ##### Step: 167 Learning rate: 3.90625e-05 #####
Epoch 28, Train Loss: 5.7759, Val Loss: 6.0508
2025-02-04 05:21:19.470 INFO: Epoch 28, Train Loss: 5.7759, Val Loss: 6.0508
train_e/atom_mae: 0.001236
2025-02-04 05:21:19.471 INFO: train_e/atom_mae: 0.001236
train_e/atom_rmse: 0.002038
2025-02-04 05:21:19.471 INFO: train_e/atom_rmse: 0.002038
train_f_mae: 0.041120
2025-02-04 05:21:19.474 INFO: train_f_mae: 0.041120
train_f_rmse: 0.065154
2025-02-04 05:21:19.474 INFO: train_f_rmse: 0.065154
val_e/atom_mae: 0.000975
2025-02-04 05:21:19.476 INFO: val_e/atom_mae: 0.000975
val_e/atom_rmse: 0.001500
2025-02-04 05:21:19.477 INFO: val_e/atom_rmse: 0.001500
val_f_mae: 0.043355
2025-02-04 05:21:19.477 INFO: val_f_mae: 0.043355
val_f_rmse: 0.072260
2025-02-04 05:21:19.477 INFO: val_f_rmse: 0.072260
##### Step: 168 Learning rate: 3.90625e-05 #####
2025-02-04 05:22:19.087 INFO: ##### Step: 168 Learning rate: 3.90625e-05 #####
Epoch 29, Train Loss: 5.7475, Val Loss: 6.0787
2025-02-04 05:22:19.088 INFO: Epoch 29, Train Loss: 5.7475, Val Loss: 6.0787
train_e/atom_mae: 0.001187
2025-02-04 05:22:19.089 INFO: train_e/atom_mae: 0.001187
train_e/atom_rmse: 0.002013
2025-02-04 05:22:19.089 INFO: train_e/atom_rmse: 0.002013
train_f_mae: 0.041147
2025-02-04 05:22:19.092 INFO: train_f_mae: 0.041147
train_f_rmse: 0.065217
2025-02-04 05:22:19.092 INFO: train_f_rmse: 0.065217
val_e/atom_mae: 0.000965
2025-02-04 05:22:19.094 INFO: val_e/atom_mae: 0.000965
val_e/atom_rmse: 0.001535
2025-02-04 05:22:19.094 INFO: val_e/atom_rmse: 0.001535
val_f_mae: 0.043341
2025-02-04 05:22:19.095 INFO: val_f_mae: 0.043341
val_f_rmse: 0.072189
2025-02-04 05:22:19.095 INFO: val_f_rmse: 0.072189
##### Step: 169 Learning rate: 3.90625e-05 #####
2025-02-04 05:23:18.714 INFO: ##### Step: 169 Learning rate: 3.90625e-05 #####
Epoch 30, Train Loss: 5.7111, Val Loss: 6.0617
2025-02-04 05:23:18.715 INFO: Epoch 30, Train Loss: 5.7111, Val Loss: 6.0617
train_e/atom_mae: 0.001147
2025-02-04 05:23:18.715 INFO: train_e/atom_mae: 0.001147
train_e/atom_rmse: 0.001996
2025-02-04 05:23:18.716 INFO: train_e/atom_rmse: 0.001996
train_f_mae: 0.041102
2025-02-04 05:23:18.718 INFO: train_f_mae: 0.041102
train_f_rmse: 0.065136
2025-02-04 05:23:18.718 INFO: train_f_rmse: 0.065136
val_e/atom_mae: 0.000981
2025-02-04 05:23:18.720 INFO: val_e/atom_mae: 0.000981
val_e/atom_rmse: 0.001528
2025-02-04 05:23:18.721 INFO: val_e/atom_rmse: 0.001528
val_f_mae: 0.043312
2025-02-04 05:23:18.721 INFO: val_f_mae: 0.043312
val_f_rmse: 0.072129
2025-02-04 05:23:18.721 INFO: val_f_rmse: 0.072129
##### Step: 170 Learning rate: 3.90625e-05 #####
2025-02-04 05:24:18.423 INFO: ##### Step: 170 Learning rate: 3.90625e-05 #####
Epoch 31, Train Loss: 5.7662, Val Loss: 6.0668
2025-02-04 05:24:18.423 INFO: Epoch 31, Train Loss: 5.7662, Val Loss: 6.0668
train_e/atom_mae: 0.001200
2025-02-04 05:24:18.424 INFO: train_e/atom_mae: 0.001200
train_e/atom_rmse: 0.002025
2025-02-04 05:24:18.424 INFO: train_e/atom_rmse: 0.002025
train_f_mae: 0.041173
2025-02-04 05:24:18.427 INFO: train_f_mae: 0.041173
train_f_rmse: 0.065227
2025-02-04 05:24:18.427 INFO: train_f_rmse: 0.065227
val_e/atom_mae: 0.000965
2025-02-04 05:24:18.429 INFO: val_e/atom_mae: 0.000965
val_e/atom_rmse: 0.001526
2025-02-04 05:24:18.430 INFO: val_e/atom_rmse: 0.001526
val_f_mae: 0.043345
2025-02-04 05:24:18.430 INFO: val_f_mae: 0.043345
val_f_rmse: 0.072177
2025-02-04 05:24:18.430 INFO: val_f_rmse: 0.072177
##### Step: 171 Learning rate: 3.90625e-05 #####
2025-02-04 05:25:18.055 INFO: ##### Step: 171 Learning rate: 3.90625e-05 #####
Epoch 32, Train Loss: 5.7036, Val Loss: 6.0461
2025-02-04 05:25:18.055 INFO: Epoch 32, Train Loss: 5.7036, Val Loss: 6.0461
train_e/atom_mae: 0.001151
2025-02-04 05:25:18.056 INFO: train_e/atom_mae: 0.001151
train_e/atom_rmse: 0.001984
2025-02-04 05:25:18.057 INFO: train_e/atom_rmse: 0.001984
train_f_mae: 0.041134
2025-02-04 05:25:18.059 INFO: train_f_mae: 0.041134
train_f_rmse: 0.065214
2025-02-04 05:25:18.059 INFO: train_f_rmse: 0.065214
val_e/atom_mae: 0.000970
2025-02-04 05:25:18.061 INFO: val_e/atom_mae: 0.000970
val_e/atom_rmse: 0.001496
2025-02-04 05:25:18.062 INFO: val_e/atom_rmse: 0.001496
val_f_mae: 0.043381
2025-02-04 05:25:18.062 INFO: val_f_mae: 0.043381
val_f_rmse: 0.072250
2025-02-04 05:25:18.062 INFO: val_f_rmse: 0.072250
##### Step: 172 Learning rate: 3.90625e-05 #####
2025-02-04 05:26:17.672 INFO: ##### Step: 172 Learning rate: 3.90625e-05 #####
Epoch 33, Train Loss: 5.7697, Val Loss: 6.0329
2025-02-04 05:26:17.672 INFO: Epoch 33, Train Loss: 5.7697, Val Loss: 6.0329
train_e/atom_mae: 0.001236
2025-02-04 05:26:17.673 INFO: train_e/atom_mae: 0.001236
train_e/atom_rmse: 0.002023
2025-02-04 05:26:17.673 INFO: train_e/atom_rmse: 0.002023
train_f_mae: 0.041216
2025-02-04 05:26:17.676 INFO: train_f_mae: 0.041216
train_f_rmse: 0.065278
2025-02-04 05:26:17.676 INFO: train_f_rmse: 0.065278
val_e/atom_mae: 0.000974
2025-02-04 05:26:17.678 INFO: val_e/atom_mae: 0.000974
val_e/atom_rmse: 0.001484
2025-02-04 05:26:17.678 INFO: val_e/atom_rmse: 0.001484
val_f_mae: 0.043380
2025-02-04 05:26:17.679 INFO: val_f_mae: 0.043380
val_f_rmse: 0.072252
2025-02-04 05:26:17.679 INFO: val_f_rmse: 0.072252
##### Step: 173 Learning rate: 3.90625e-05 #####
2025-02-04 05:27:17.575 INFO: ##### Step: 173 Learning rate: 3.90625e-05 #####
Epoch 34, Train Loss: 5.7076, Val Loss: 6.0540
2025-02-04 05:27:17.575 INFO: Epoch 34, Train Loss: 5.7076, Val Loss: 6.0540
train_e/atom_mae: 0.001159
2025-02-04 05:27:17.576 INFO: train_e/atom_mae: 0.001159
train_e/atom_rmse: 0.001990
2025-02-04 05:27:17.576 INFO: train_e/atom_rmse: 0.001990
train_f_mae: 0.041141
2025-02-04 05:27:17.579 INFO: train_f_mae: 0.041141
train_f_rmse: 0.065173
2025-02-04 05:27:17.579 INFO: train_f_rmse: 0.065173
val_e/atom_mae: 0.000973
2025-02-04 05:27:17.581 INFO: val_e/atom_mae: 0.000973
val_e/atom_rmse: 0.001511
2025-02-04 05:27:17.582 INFO: val_e/atom_rmse: 0.001511
val_f_mae: 0.043345
2025-02-04 05:27:17.582 INFO: val_f_mae: 0.043345
val_f_rmse: 0.072199
2025-02-04 05:27:17.582 INFO: val_f_rmse: 0.072199
##### Step: 174 Learning rate: 3.90625e-05 #####
2025-02-04 05:28:17.419 INFO: ##### Step: 174 Learning rate: 3.90625e-05 #####
Epoch 35, Train Loss: 5.7040, Val Loss: 6.0536
2025-02-04 05:28:17.419 INFO: Epoch 35, Train Loss: 5.7040, Val Loss: 6.0536
train_e/atom_mae: 0.001172
2025-02-04 05:28:17.420 INFO: train_e/atom_mae: 0.001172
train_e/atom_rmse: 0.001995
2025-02-04 05:28:17.420 INFO: train_e/atom_rmse: 0.001995
train_f_mae: 0.041084
2025-02-04 05:28:17.423 INFO: train_f_mae: 0.041084
train_f_rmse: 0.065090
2025-02-04 05:28:17.423 INFO: train_f_rmse: 0.065090
val_e/atom_mae: 0.000971
2025-02-04 05:28:17.425 INFO: val_e/atom_mae: 0.000971
val_e/atom_rmse: 0.001509
2025-02-04 05:28:17.425 INFO: val_e/atom_rmse: 0.001509
val_f_mae: 0.043345
2025-02-04 05:28:17.426 INFO: val_f_mae: 0.043345
val_f_rmse: 0.072213
2025-02-04 05:28:17.426 INFO: val_f_rmse: 0.072213
##### Step: 175 Learning rate: 3.90625e-05 #####
2025-02-04 05:29:17.043 INFO: ##### Step: 175 Learning rate: 3.90625e-05 #####
Epoch 36, Train Loss: 5.7476, Val Loss: 6.0708
2025-02-04 05:29:17.044 INFO: Epoch 36, Train Loss: 5.7476, Val Loss: 6.0708
train_e/atom_mae: 0.001255
2025-02-04 05:29:17.044 INFO: train_e/atom_mae: 0.001255
train_e/atom_rmse: 0.002018
2025-02-04 05:29:17.045 INFO: train_e/atom_rmse: 0.002018
train_f_mae: 0.041151
2025-02-04 05:29:17.047 INFO: train_f_mae: 0.041151
train_f_rmse: 0.065163
2025-02-04 05:29:17.048 INFO: train_f_rmse: 0.065163
val_e/atom_mae: 0.000971
2025-02-04 05:29:17.050 INFO: val_e/atom_mae: 0.000971
val_e/atom_rmse: 0.001528
2025-02-04 05:29:17.050 INFO: val_e/atom_rmse: 0.001528
val_f_mae: 0.043360
2025-02-04 05:29:17.050 INFO: val_f_mae: 0.043360
val_f_rmse: 0.072187
2025-02-04 05:29:17.051 INFO: val_f_rmse: 0.072187
##### Step: 176 Learning rate: 3.90625e-05 #####
2025-02-04 05:30:16.676 INFO: ##### Step: 176 Learning rate: 3.90625e-05 #####
Epoch 37, Train Loss: 5.7216, Val Loss: 6.0420
2025-02-04 05:30:16.676 INFO: Epoch 37, Train Loss: 5.7216, Val Loss: 6.0420
train_e/atom_mae: 0.001195
2025-02-04 05:30:16.677 INFO: train_e/atom_mae: 0.001195
train_e/atom_rmse: 0.001997
2025-02-04 05:30:16.678 INFO: train_e/atom_rmse: 0.001997
train_f_mae: 0.041148
2025-02-04 05:30:16.680 INFO: train_f_mae: 0.041148
train_f_rmse: 0.065206
2025-02-04 05:30:16.680 INFO: train_f_rmse: 0.065206
val_e/atom_mae: 0.000977
2025-02-04 05:30:16.682 INFO: val_e/atom_mae: 0.000977
val_e/atom_rmse: 0.001487
2025-02-04 05:30:16.683 INFO: val_e/atom_rmse: 0.001487
val_f_mae: 0.043411
2025-02-04 05:30:16.683 INFO: val_f_mae: 0.043411
val_f_rmse: 0.072294
2025-02-04 05:30:16.683 INFO: val_f_rmse: 0.072294
##### Step: 177 Learning rate: 3.90625e-05 #####
2025-02-04 05:31:16.281 INFO: ##### Step: 177 Learning rate: 3.90625e-05 #####
Epoch 38, Train Loss: 5.7889, Val Loss: 6.0460
2025-02-04 05:31:16.281 INFO: Epoch 38, Train Loss: 5.7889, Val Loss: 6.0460
train_e/atom_mae: 0.001310
2025-02-04 05:31:16.282 INFO: train_e/atom_mae: 0.001310
train_e/atom_rmse: 0.002039
2025-02-04 05:31:16.282 INFO: train_e/atom_rmse: 0.002039
train_f_mae: 0.041195
2025-02-04 05:31:16.285 INFO: train_f_mae: 0.041195
train_f_rmse: 0.065239
2025-02-04 05:31:16.285 INFO: train_f_rmse: 0.065239
val_e/atom_mae: 0.000964
2025-02-04 05:31:16.287 INFO: val_e/atom_mae: 0.000964
val_e/atom_rmse: 0.001489
2025-02-04 05:31:16.288 INFO: val_e/atom_rmse: 0.001489
val_f_mae: 0.043402
2025-02-04 05:31:16.288 INFO: val_f_mae: 0.043402
val_f_rmse: 0.072315
2025-02-04 05:31:16.288 INFO: val_f_rmse: 0.072315
##### Step: 178 Learning rate: 3.90625e-05 #####
2025-02-04 05:32:15.979 INFO: ##### Step: 178 Learning rate: 3.90625e-05 #####
Epoch 39, Train Loss: 5.7764, Val Loss: 6.0380
2025-02-04 05:32:15.980 INFO: Epoch 39, Train Loss: 5.7764, Val Loss: 6.0380
train_e/atom_mae: 0.001295
2025-02-04 05:32:15.980 INFO: train_e/atom_mae: 0.001295
train_e/atom_rmse: 0.002024
2025-02-04 05:32:15.981 INFO: train_e/atom_rmse: 0.002024
train_f_mae: 0.041211
2025-02-04 05:32:15.983 INFO: train_f_mae: 0.041211
train_f_rmse: 0.065319
2025-02-04 05:32:15.983 INFO: train_f_rmse: 0.065319
val_e/atom_mae: 0.000966
2025-02-04 05:32:15.986 INFO: val_e/atom_mae: 0.000966
val_e/atom_rmse: 0.001481
2025-02-04 05:32:15.986 INFO: val_e/atom_rmse: 0.001481
val_f_mae: 0.043412
2025-02-04 05:32:15.986 INFO: val_f_mae: 0.043412
val_f_rmse: 0.072312
2025-02-04 05:32:15.987 INFO: val_f_rmse: 0.072312
##### Step: 179 Learning rate: 3.90625e-05 #####
2025-02-04 05:33:15.851 INFO: ##### Step: 179 Learning rate: 3.90625e-05 #####
Epoch 40, Train Loss: 5.7099, Val Loss: 6.0427
2025-02-04 05:33:15.851 INFO: Epoch 40, Train Loss: 5.7099, Val Loss: 6.0427
train_e/atom_mae: 0.001212
2025-02-04 05:33:15.852 INFO: train_e/atom_mae: 0.001212
train_e/atom_rmse: 0.001992
2025-02-04 05:33:15.852 INFO: train_e/atom_rmse: 0.001992
train_f_mae: 0.041167
2025-02-04 05:33:15.855 INFO: train_f_mae: 0.041167
train_f_rmse: 0.065175
2025-02-04 05:33:15.855 INFO: train_f_rmse: 0.065175
val_e/atom_mae: 0.000979
2025-02-04 05:33:15.857 INFO: val_e/atom_mae: 0.000979
val_e/atom_rmse: 0.001480
2025-02-04 05:33:15.857 INFO: val_e/atom_rmse: 0.001480
val_f_mae: 0.043426
2025-02-04 05:33:15.858 INFO: val_f_mae: 0.043426
val_f_rmse: 0.072345
2025-02-04 05:33:15.858 INFO: val_f_rmse: 0.072345
##### Step: 180 Learning rate: 1.953125e-05 #####
2025-02-04 05:34:15.856 INFO: ##### Step: 180 Learning rate: 1.953125e-05 #####
Epoch 41, Train Loss: 5.6815, Val Loss: 6.0488
2025-02-04 05:34:15.856 INFO: Epoch 41, Train Loss: 5.6815, Val Loss: 6.0488
train_e/atom_mae: 0.001125
2025-02-04 05:34:15.857 INFO: train_e/atom_mae: 0.001125
train_e/atom_rmse: 0.001969
2025-02-04 05:34:15.857 INFO: train_e/atom_rmse: 0.001969
train_f_mae: 0.041134
2025-02-04 05:34:15.860 INFO: train_f_mae: 0.041134
train_f_rmse: 0.065211
2025-02-04 05:34:15.860 INFO: train_f_rmse: 0.065211
val_e/atom_mae: 0.000956
2025-02-04 05:34:15.862 INFO: val_e/atom_mae: 0.000956
val_e/atom_rmse: 0.001496
2025-02-04 05:34:15.862 INFO: val_e/atom_rmse: 0.001496
val_f_mae: 0.043371
2025-02-04 05:34:15.863 INFO: val_f_mae: 0.043371
val_f_rmse: 0.072279
2025-02-04 05:34:15.863 INFO: val_f_rmse: 0.072279
##### Step: 181 Learning rate: 1.953125e-05 #####
2025-02-04 05:35:15.753 INFO: ##### Step: 181 Learning rate: 1.953125e-05 #####
Epoch 42, Train Loss: 5.6495, Val Loss: 6.0426
2025-02-04 05:35:15.754 INFO: Epoch 42, Train Loss: 5.6495, Val Loss: 6.0426
train_e/atom_mae: 0.001099
2025-02-04 05:35:15.754 INFO: train_e/atom_mae: 0.001099
train_e/atom_rmse: 0.001947
2025-02-04 05:35:15.755 INFO: train_e/atom_rmse: 0.001947
train_f_mae: 0.041131
2025-02-04 05:35:15.757 INFO: train_f_mae: 0.041131
train_f_rmse: 0.065210
2025-02-04 05:35:15.757 INFO: train_f_rmse: 0.065210
val_e/atom_mae: 0.000966
2025-02-04 05:35:15.760 INFO: val_e/atom_mae: 0.000966
val_e/atom_rmse: 0.001498
2025-02-04 05:35:15.760 INFO: val_e/atom_rmse: 0.001498
val_f_mae: 0.043369
2025-02-04 05:35:15.760 INFO: val_f_mae: 0.043369
val_f_rmse: 0.072215
2025-02-04 05:35:15.761 INFO: val_f_rmse: 0.072215
##### Step: 182 Learning rate: 1.953125e-05 #####
2025-02-04 05:36:15.658 INFO: ##### Step: 182 Learning rate: 1.953125e-05 #####
Epoch 43, Train Loss: 5.6800, Val Loss: 6.0498
2025-02-04 05:36:15.659 INFO: Epoch 43, Train Loss: 5.6800, Val Loss: 6.0498
train_e/atom_mae: 0.001153
2025-02-04 05:36:15.660 INFO: train_e/atom_mae: 0.001153
train_e/atom_rmse: 0.001972
2025-02-04 05:36:15.660 INFO: train_e/atom_rmse: 0.001972
train_f_mae: 0.041107
2025-02-04 05:36:15.662 INFO: train_f_mae: 0.041107
train_f_rmse: 0.065162
2025-02-04 05:36:15.663 INFO: train_f_rmse: 0.065162
val_e/atom_mae: 0.000960
2025-02-04 05:36:15.665 INFO: val_e/atom_mae: 0.000960
val_e/atom_rmse: 0.001508
2025-02-04 05:36:15.665 INFO: val_e/atom_rmse: 0.001508
val_f_mae: 0.043345
2025-02-04 05:36:15.666 INFO: val_f_mae: 0.043345
val_f_rmse: 0.072195
2025-02-04 05:36:15.666 INFO: val_f_rmse: 0.072195
##### Step: 183 Learning rate: 1.953125e-05 #####
2025-02-04 05:37:15.607 INFO: ##### Step: 183 Learning rate: 1.953125e-05 #####
Epoch 44, Train Loss: 5.6599, Val Loss: 6.0362
2025-02-04 05:37:15.607 INFO: Epoch 44, Train Loss: 5.6599, Val Loss: 6.0362
train_e/atom_mae: 0.001154
2025-02-04 05:37:15.608 INFO: train_e/atom_mae: 0.001154
train_e/atom_rmse: 0.001955
2025-02-04 05:37:15.608 INFO: train_e/atom_rmse: 0.001955
train_f_mae: 0.041123
2025-02-04 05:37:15.611 INFO: train_f_mae: 0.041123
train_f_rmse: 0.065196
2025-02-04 05:37:15.611 INFO: train_f_rmse: 0.065196
val_e/atom_mae: 0.000969
2025-02-04 05:37:15.613 INFO: val_e/atom_mae: 0.000969
val_e/atom_rmse: 0.001482
2025-02-04 05:37:15.614 INFO: val_e/atom_rmse: 0.001482
val_f_mae: 0.043395
2025-02-04 05:37:15.614 INFO: val_f_mae: 0.043395
val_f_rmse: 0.072291
2025-02-04 05:37:15.614 INFO: val_f_rmse: 0.072291
##### Step: 184 Learning rate: 1.953125e-05 #####
2025-02-04 05:38:15.331 INFO: ##### Step: 184 Learning rate: 1.953125e-05 #####
Epoch 45, Train Loss: 5.7229, Val Loss: 6.0451
2025-02-04 05:38:15.332 INFO: Epoch 45, Train Loss: 5.7229, Val Loss: 6.0451
train_e/atom_mae: 0.001212
2025-02-04 05:38:15.333 INFO: train_e/atom_mae: 0.001212
train_e/atom_rmse: 0.001996
2025-02-04 05:38:15.333 INFO: train_e/atom_rmse: 0.001996
train_f_mae: 0.041147
2025-02-04 05:38:15.335 INFO: train_f_mae: 0.041147
train_f_rmse: 0.065223
2025-02-04 05:38:15.336 INFO: train_f_rmse: 0.065223
val_e/atom_mae: 0.000953
2025-02-04 05:38:15.338 INFO: val_e/atom_mae: 0.000953
val_e/atom_rmse: 0.001496
2025-02-04 05:38:15.338 INFO: val_e/atom_rmse: 0.001496
val_f_mae: 0.043380
2025-02-04 05:38:15.338 INFO: val_f_mae: 0.043380
val_f_rmse: 0.072255
2025-02-04 05:38:15.339 INFO: val_f_rmse: 0.072255
##### Step: 185 Learning rate: 1.953125e-05 #####
2025-02-04 05:39:15.251 INFO: ##### Step: 185 Learning rate: 1.953125e-05 #####
Epoch 46, Train Loss: 5.7054, Val Loss: 6.0590
2025-02-04 05:39:15.251 INFO: Epoch 46, Train Loss: 5.7054, Val Loss: 6.0590
train_e/atom_mae: 0.001144
2025-02-04 05:39:15.252 INFO: train_e/atom_mae: 0.001144
train_e/atom_rmse: 0.001984
2025-02-04 05:39:15.252 INFO: train_e/atom_rmse: 0.001984
train_f_mae: 0.041149
2025-02-04 05:39:15.255 INFO: train_f_mae: 0.041149
train_f_rmse: 0.065221
2025-02-04 05:39:15.255 INFO: train_f_rmse: 0.065221
val_e/atom_mae: 0.000971
2025-02-04 05:39:15.257 INFO: val_e/atom_mae: 0.000971
val_e/atom_rmse: 0.001518
2025-02-04 05:39:15.257 INFO: val_e/atom_rmse: 0.001518
val_f_mae: 0.043349
2025-02-04 05:39:15.258 INFO: val_f_mae: 0.043349
val_f_rmse: 0.072184
2025-02-04 05:39:15.258 INFO: val_f_rmse: 0.072184
##### Step: 186 Learning rate: 1.953125e-05 #####
2025-02-04 05:40:15.195 INFO: ##### Step: 186 Learning rate: 1.953125e-05 #####
Epoch 47, Train Loss: 5.6768, Val Loss: 6.0602
2025-02-04 05:40:15.196 INFO: Epoch 47, Train Loss: 5.6768, Val Loss: 6.0602
train_e/atom_mae: 0.001145
2025-02-04 05:40:15.196 INFO: train_e/atom_mae: 0.001145
train_e/atom_rmse: 0.001973
2025-02-04 05:40:15.197 INFO: train_e/atom_rmse: 0.001973
train_f_mae: 0.041090
2025-02-04 05:40:15.199 INFO: train_f_mae: 0.041090
train_f_rmse: 0.065128
2025-02-04 05:40:15.200 INFO: train_f_rmse: 0.065128
val_e/atom_mae: 0.000952
2025-02-04 05:40:15.202 INFO: val_e/atom_mae: 0.000952
val_e/atom_rmse: 0.001513
2025-02-04 05:40:15.202 INFO: val_e/atom_rmse: 0.001513
val_f_mae: 0.043355
2025-02-04 05:40:15.202 INFO: val_f_mae: 0.043355
val_f_rmse: 0.072226
2025-02-04 05:40:15.203 INFO: val_f_rmse: 0.072226
##### Step: 187 Learning rate: 1.953125e-05 #####
2025-02-04 05:41:14.848 INFO: ##### Step: 187 Learning rate: 1.953125e-05 #####
Epoch 48, Train Loss: 5.6976, Val Loss: 6.0645
2025-02-04 05:41:14.849 INFO: Epoch 48, Train Loss: 5.6976, Val Loss: 6.0645
train_e/atom_mae: 0.001173
2025-02-04 05:41:14.850 INFO: train_e/atom_mae: 0.001173
train_e/atom_rmse: 0.001981
2025-02-04 05:41:14.850 INFO: train_e/atom_rmse: 0.001981
train_f_mae: 0.041132
2025-02-04 05:41:14.852 INFO: train_f_mae: 0.041132
train_f_rmse: 0.065201
2025-02-04 05:41:14.853 INFO: train_f_rmse: 0.065201
val_e/atom_mae: 0.000947
2025-02-04 05:41:14.855 INFO: val_e/atom_mae: 0.000947
val_e/atom_rmse: 0.001519
2025-02-04 05:41:14.855 INFO: val_e/atom_rmse: 0.001519
val_f_mae: 0.043374
2025-02-04 05:41:14.856 INFO: val_f_mae: 0.043374
val_f_rmse: 0.072215
2025-02-04 05:41:14.856 INFO: val_f_rmse: 0.072215
##### Step: 188 Learning rate: 1.953125e-05 #####
2025-02-04 05:42:14.470 INFO: ##### Step: 188 Learning rate: 1.953125e-05 #####
Epoch 49, Train Loss: 5.6683, Val Loss: 6.0682
2025-02-04 05:42:14.470 INFO: Epoch 49, Train Loss: 5.6683, Val Loss: 6.0682
train_e/atom_mae: 0.001126
2025-02-04 05:42:14.471 INFO: train_e/atom_mae: 0.001126
train_e/atom_rmse: 0.001959
2025-02-04 05:42:14.472 INFO: train_e/atom_rmse: 0.001959
train_f_mae: 0.041142
2025-02-04 05:42:14.474 INFO: train_f_mae: 0.041142
train_f_rmse: 0.065222
2025-02-04 05:42:14.474 INFO: train_f_rmse: 0.065222
val_e/atom_mae: 0.000967
2025-02-04 05:42:14.476 INFO: val_e/atom_mae: 0.000967
val_e/atom_rmse: 0.001511
2025-02-04 05:42:14.477 INFO: val_e/atom_rmse: 0.001511
val_f_mae: 0.043393
2025-02-04 05:42:14.477 INFO: val_f_mae: 0.043393
val_f_rmse: 0.072301
2025-02-04 05:42:14.477 INFO: val_f_rmse: 0.072301
##### Step: 189 Learning rate: 1.953125e-05 #####
2025-02-04 05:43:14.098 INFO: ##### Step: 189 Learning rate: 1.953125e-05 #####
Epoch 50, Train Loss: 5.6860, Val Loss: 6.0496
2025-02-04 05:43:14.098 INFO: Epoch 50, Train Loss: 5.6860, Val Loss: 6.0496
train_e/atom_mae: 0.001209
2025-02-04 05:43:14.099 INFO: train_e/atom_mae: 0.001209
train_e/atom_rmse: 0.001988
2025-02-04 05:43:14.099 INFO: train_e/atom_rmse: 0.001988
train_f_mae: 0.041045
2025-02-04 05:43:14.102 INFO: train_f_mae: 0.041045
train_f_rmse: 0.065032
2025-02-04 05:43:14.102 INFO: train_f_rmse: 0.065032
val_e/atom_mae: 0.000966
2025-02-04 05:43:14.104 INFO: val_e/atom_mae: 0.000966
val_e/atom_rmse: 0.001498
2025-02-04 05:43:14.105 INFO: val_e/atom_rmse: 0.001498
val_f_mae: 0.043352
2025-02-04 05:43:14.105 INFO: val_f_mae: 0.043352
val_f_rmse: 0.072269
2025-02-04 05:43:14.105 INFO: val_f_rmse: 0.072269
##### Step: 190 Learning rate: 1.953125e-05 #####
2025-02-04 05:44:13.778 INFO: ##### Step: 190 Learning rate: 1.953125e-05 #####
Epoch 51, Train Loss: 5.6751, Val Loss: 6.0436
2025-02-04 05:44:13.779 INFO: Epoch 51, Train Loss: 5.6751, Val Loss: 6.0436
train_e/atom_mae: 0.001129
2025-02-04 05:44:13.780 INFO: train_e/atom_mae: 0.001129
train_e/atom_rmse: 0.001965
2025-02-04 05:44:13.780 INFO: train_e/atom_rmse: 0.001965
train_f_mae: 0.041113
2025-02-04 05:44:13.782 INFO: train_f_mae: 0.041113
train_f_rmse: 0.065200
2025-02-04 05:44:13.783 INFO: train_f_rmse: 0.065200
val_e/atom_mae: 0.000968
2025-02-04 05:44:13.785 INFO: val_e/atom_mae: 0.000968
val_e/atom_rmse: 0.001492
2025-02-04 05:44:13.785 INFO: val_e/atom_rmse: 0.001492
val_f_mae: 0.043371
2025-02-04 05:44:13.786 INFO: val_f_mae: 0.043371
val_f_rmse: 0.072266
2025-02-04 05:44:13.786 INFO: val_f_rmse: 0.072266
##### Step: 191 Learning rate: 1.953125e-05 #####
2025-02-04 05:45:13.430 INFO: ##### Step: 191 Learning rate: 1.953125e-05 #####
Epoch 52, Train Loss: 5.6620, Val Loss: 6.0521
2025-02-04 05:45:13.431 INFO: Epoch 52, Train Loss: 5.6620, Val Loss: 6.0521
train_e/atom_mae: 0.001128
2025-02-04 05:45:13.432 INFO: train_e/atom_mae: 0.001128
train_e/atom_rmse: 0.001954
2025-02-04 05:45:13.432 INFO: train_e/atom_rmse: 0.001954
train_f_mae: 0.041127
2025-02-04 05:45:13.435 INFO: train_f_mae: 0.041127
train_f_rmse: 0.065229
2025-02-04 05:45:13.435 INFO: train_f_rmse: 0.065229
val_e/atom_mae: 0.000956
2025-02-04 05:45:13.437 INFO: val_e/atom_mae: 0.000956
val_e/atom_rmse: 0.001504
2025-02-04 05:45:13.437 INFO: val_e/atom_rmse: 0.001504
val_f_mae: 0.043352
2025-02-04 05:45:13.438 INFO: val_f_mae: 0.043352
val_f_rmse: 0.072240
2025-02-04 05:45:13.438 INFO: val_f_rmse: 0.072240
##### Step: 192 Learning rate: 1.953125e-05 #####
2025-02-04 05:46:13.091 INFO: ##### Step: 192 Learning rate: 1.953125e-05 #####
Epoch 53, Train Loss: 5.6816, Val Loss: 6.0636
2025-02-04 05:46:13.091 INFO: Epoch 53, Train Loss: 5.6816, Val Loss: 6.0636
train_e/atom_mae: 0.001189
2025-02-04 05:46:13.092 INFO: train_e/atom_mae: 0.001189
train_e/atom_rmse: 0.001971
2025-02-04 05:46:13.092 INFO: train_e/atom_rmse: 0.001971
train_f_mae: 0.041101
2025-02-04 05:46:13.095 INFO: train_f_mae: 0.041101
train_f_rmse: 0.065191
2025-02-04 05:46:13.095 INFO: train_f_rmse: 0.065191
val_e/atom_mae: 0.000959
2025-02-04 05:46:13.097 INFO: val_e/atom_mae: 0.000959
val_e/atom_rmse: 0.001518
2025-02-04 05:46:13.097 INFO: val_e/atom_rmse: 0.001518
val_f_mae: 0.043340
2025-02-04 05:46:13.098 INFO: val_f_mae: 0.043340
val_f_rmse: 0.072216
2025-02-04 05:46:13.098 INFO: val_f_rmse: 0.072216
##### Step: 193 Learning rate: 1.953125e-05 #####
2025-02-04 05:47:12.741 INFO: ##### Step: 193 Learning rate: 1.953125e-05 #####
Epoch 54, Train Loss: 5.6452, Val Loss: 6.0565
2025-02-04 05:47:12.741 INFO: Epoch 54, Train Loss: 5.6452, Val Loss: 6.0565
train_e/atom_mae: 0.001102
2025-02-04 05:47:12.742 INFO: train_e/atom_mae: 0.001102
train_e/atom_rmse: 0.001944
2025-02-04 05:47:12.742 INFO: train_e/atom_rmse: 0.001944
train_f_mae: 0.041131
2025-02-04 05:47:12.745 INFO: train_f_mae: 0.041131
train_f_rmse: 0.065206
2025-02-04 05:47:12.745 INFO: train_f_rmse: 0.065206
val_e/atom_mae: 0.000948
2025-02-04 05:47:12.747 INFO: val_e/atom_mae: 0.000948
val_e/atom_rmse: 0.001504
2025-02-04 05:47:12.747 INFO: val_e/atom_rmse: 0.001504
val_f_mae: 0.043367
2025-02-04 05:47:12.748 INFO: val_f_mae: 0.043367
val_f_rmse: 0.072276
2025-02-04 05:47:12.748 INFO: val_f_rmse: 0.072276
##### Step: 194 Learning rate: 1.953125e-05 #####
2025-02-04 05:48:12.395 INFO: ##### Step: 194 Learning rate: 1.953125e-05 #####
Epoch 55, Train Loss: 5.6752, Val Loss: 6.0415
2025-02-04 05:48:12.395 INFO: Epoch 55, Train Loss: 5.6752, Val Loss: 6.0415
train_e/atom_mae: 0.001159
2025-02-04 05:48:12.396 INFO: train_e/atom_mae: 0.001159
train_e/atom_rmse: 0.001966
2025-02-04 05:48:12.397 INFO: train_e/atom_rmse: 0.001966
train_f_mae: 0.041119
2025-02-04 05:48:12.399 INFO: train_f_mae: 0.041119
train_f_rmse: 0.065199
2025-02-04 05:48:12.399 INFO: train_f_rmse: 0.065199
val_e/atom_mae: 0.000964
2025-02-04 05:48:12.401 INFO: val_e/atom_mae: 0.000964
val_e/atom_rmse: 0.001489
2025-02-04 05:48:12.402 INFO: val_e/atom_rmse: 0.001489
val_f_mae: 0.043360
2025-02-04 05:48:12.402 INFO: val_f_mae: 0.043360
val_f_rmse: 0.072280
2025-02-04 05:48:12.402 INFO: val_f_rmse: 0.072280
##### Step: 195 Learning rate: 1.953125e-05 #####
2025-02-04 05:49:12.069 INFO: ##### Step: 195 Learning rate: 1.953125e-05 #####
Epoch 56, Train Loss: 5.6638, Val Loss: 6.0481
2025-02-04 05:49:12.069 INFO: Epoch 56, Train Loss: 5.6638, Val Loss: 6.0481
train_e/atom_mae: 0.001142
2025-02-04 05:49:12.070 INFO: train_e/atom_mae: 0.001142
train_e/atom_rmse: 0.001959
2025-02-04 05:49:12.070 INFO: train_e/atom_rmse: 0.001959
train_f_mae: 0.041090
2025-02-04 05:49:12.073 INFO: train_f_mae: 0.041090
train_f_rmse: 0.065190
2025-02-04 05:49:12.073 INFO: train_f_rmse: 0.065190
val_e/atom_mae: 0.000954
2025-02-04 05:49:12.075 INFO: val_e/atom_mae: 0.000954
val_e/atom_rmse: 0.001493
2025-02-04 05:49:12.076 INFO: val_e/atom_rmse: 0.001493
val_f_mae: 0.043382
2025-02-04 05:49:12.076 INFO: val_f_mae: 0.043382
val_f_rmse: 0.072296
2025-02-04 05:49:12.076 INFO: val_f_rmse: 0.072296
##### Step: 196 Learning rate: 1.953125e-05 #####
2025-02-04 05:50:11.723 INFO: ##### Step: 196 Learning rate: 1.953125e-05 #####
Epoch 57, Train Loss: 5.7109, Val Loss: 6.0458
2025-02-04 05:50:11.723 INFO: Epoch 57, Train Loss: 5.7109, Val Loss: 6.0458
train_e/atom_mae: 0.001225
2025-02-04 05:50:11.724 INFO: train_e/atom_mae: 0.001225
train_e/atom_rmse: 0.001996
2025-02-04 05:50:11.724 INFO: train_e/atom_rmse: 0.001996
train_f_mae: 0.041072
2025-02-04 05:50:11.727 INFO: train_f_mae: 0.041072
train_f_rmse: 0.065132
2025-02-04 05:50:11.727 INFO: train_f_rmse: 0.065132
val_e/atom_mae: 0.000990
2025-02-04 05:50:11.729 INFO: val_e/atom_mae: 0.000990
val_e/atom_rmse: 0.001497
2025-02-04 05:50:11.730 INFO: val_e/atom_rmse: 0.001497
val_f_mae: 0.043357
2025-02-04 05:50:11.730 INFO: val_f_mae: 0.043357
val_f_rmse: 0.072242
2025-02-04 05:50:11.730 INFO: val_f_rmse: 0.072242
##### Step: 197 Learning rate: 1.953125e-05 #####
2025-02-04 05:51:11.375 INFO: ##### Step: 197 Learning rate: 1.953125e-05 #####
Epoch 58, Train Loss: 5.7890, Val Loss: 6.0421
2025-02-04 05:51:11.376 INFO: Epoch 58, Train Loss: 5.7890, Val Loss: 6.0421
train_e/atom_mae: 0.001290
2025-02-04 05:51:11.377 INFO: train_e/atom_mae: 0.001290
train_e/atom_rmse: 0.002045
2025-02-04 05:51:11.377 INFO: train_e/atom_rmse: 0.002045
train_f_mae: 0.041113
2025-02-04 05:51:11.379 INFO: train_f_mae: 0.041113
train_f_rmse: 0.065172
2025-02-04 05:51:11.380 INFO: train_f_rmse: 0.065172
val_e/atom_mae: 0.000973
2025-02-04 05:51:11.382 INFO: val_e/atom_mae: 0.000973
val_e/atom_rmse: 0.001491
2025-02-04 05:51:11.382 INFO: val_e/atom_rmse: 0.001491
val_f_mae: 0.043362
2025-02-04 05:51:11.383 INFO: val_f_mae: 0.043362
val_f_rmse: 0.072268
2025-02-04 05:51:11.383 INFO: val_f_rmse: 0.072268
##### Step: 198 Learning rate: 1.953125e-05 #####
2025-02-04 05:52:11.031 INFO: ##### Step: 198 Learning rate: 1.953125e-05 #####
Epoch 59, Train Loss: 5.6431, Val Loss: 6.0445
2025-02-04 05:52:11.031 INFO: Epoch 59, Train Loss: 5.6431, Val Loss: 6.0445
train_e/atom_mae: 0.001124
2025-02-04 05:52:11.032 INFO: train_e/atom_mae: 0.001124
train_e/atom_rmse: 0.001946
2025-02-04 05:52:11.032 INFO: train_e/atom_rmse: 0.001946
train_f_mae: 0.041095
2025-02-04 05:52:11.035 INFO: train_f_mae: 0.041095
train_f_rmse: 0.065172
2025-02-04 05:52:11.035 INFO: train_f_rmse: 0.065172
val_e/atom_mae: 0.000984
2025-02-04 05:52:11.037 INFO: val_e/atom_mae: 0.000984
val_e/atom_rmse: 0.001495
2025-02-04 05:52:11.037 INFO: val_e/atom_rmse: 0.001495
val_f_mae: 0.043353
2025-02-04 05:52:11.038 INFO: val_f_mae: 0.043353
val_f_rmse: 0.072255
2025-02-04 05:52:11.038 INFO: val_f_rmse: 0.072255
##### Step: 199 Learning rate: 1.953125e-05 #####
2025-02-04 05:53:10.677 INFO: ##### Step: 199 Learning rate: 1.953125e-05 #####
Epoch 60, Train Loss: 5.6321, Val Loss: 6.0456
2025-02-04 05:53:10.677 INFO: Epoch 60, Train Loss: 5.6321, Val Loss: 6.0456
train_e/atom_mae: 0.001119
2025-02-04 05:53:10.678 INFO: train_e/atom_mae: 0.001119
train_e/atom_rmse: 0.001938
2025-02-04 05:53:10.679 INFO: train_e/atom_rmse: 0.001938
train_f_mae: 0.041105
2025-02-04 05:53:10.681 INFO: train_f_mae: 0.041105
train_f_rmse: 0.065175
2025-02-04 05:53:10.681 INFO: train_f_rmse: 0.065175
val_e/atom_mae: 0.001008
2025-02-04 05:53:10.683 INFO: val_e/atom_mae: 0.001008
val_e/atom_rmse: 0.001498
2025-02-04 05:53:10.684 INFO: val_e/atom_rmse: 0.001498
val_f_mae: 0.043351
2025-02-04 05:53:10.684 INFO: val_f_mae: 0.043351
val_f_rmse: 0.072232
2025-02-04 05:53:10.684 INFO: val_f_rmse: 0.072232
##### Step: 200 Learning rate: 9.765625e-06 #####
2025-02-04 05:54:10.420 INFO: ##### Step: 200 Learning rate: 9.765625e-06 #####
Epoch 61, Train Loss: 5.6402, Val Loss: 6.0467
2025-02-04 05:54:10.421 INFO: Epoch 61, Train Loss: 5.6402, Val Loss: 6.0467
train_e/atom_mae: 0.001109
2025-02-04 05:54:10.421 INFO: train_e/atom_mae: 0.001109
train_e/atom_rmse: 0.001944
2025-02-04 05:54:10.422 INFO: train_e/atom_rmse: 0.001944
train_f_mae: 0.041089
2025-02-04 05:54:10.424 INFO: train_f_mae: 0.041089
train_f_rmse: 0.065171
2025-02-04 05:54:10.424 INFO: train_f_rmse: 0.065171
val_e/atom_mae: 0.000960
2025-02-04 05:54:10.426 INFO: val_e/atom_mae: 0.000960
val_e/atom_rmse: 0.001494
2025-02-04 05:54:10.427 INFO: val_e/atom_rmse: 0.001494
val_f_mae: 0.043368
2025-02-04 05:54:10.427 INFO: val_f_mae: 0.043368
val_f_rmse: 0.072281
2025-02-04 05:54:10.428 INFO: val_f_rmse: 0.072281
##### Step: 201 Learning rate: 9.765625e-06 #####
2025-02-04 05:55:10.049 INFO: ##### Step: 201 Learning rate: 9.765625e-06 #####
Epoch 62, Train Loss: 5.6440, Val Loss: 6.0475
2025-02-04 05:55:10.049 INFO: Epoch 62, Train Loss: 5.6440, Val Loss: 6.0475
train_e/atom_mae: 0.001099
2025-02-04 05:55:10.050 INFO: train_e/atom_mae: 0.001099
train_e/atom_rmse: 0.001945
2025-02-04 05:55:10.050 INFO: train_e/atom_rmse: 0.001945
train_f_mae: 0.041109
2025-02-04 05:55:10.053 INFO: train_f_mae: 0.041109
train_f_rmse: 0.065192
2025-02-04 05:55:10.053 INFO: train_f_rmse: 0.065192
val_e/atom_mae: 0.000962
2025-02-04 05:55:10.055 INFO: val_e/atom_mae: 0.000962
val_e/atom_rmse: 0.001499
2025-02-04 05:55:10.056 INFO: val_e/atom_rmse: 0.001499
val_f_mae: 0.043350
2025-02-04 05:55:10.056 INFO: val_f_mae: 0.043350
val_f_rmse: 0.072243
2025-02-04 05:55:10.056 INFO: val_f_rmse: 0.072243
##### Step: 202 Learning rate: 9.765625e-06 #####
2025-02-04 05:56:09.717 INFO: ##### Step: 202 Learning rate: 9.765625e-06 #####
Epoch 63, Train Loss: 5.6509, Val Loss: 6.0662
2025-02-04 05:56:09.718 INFO: Epoch 63, Train Loss: 5.6509, Val Loss: 6.0662
train_e/atom_mae: 0.001108
2025-02-04 05:56:09.719 INFO: train_e/atom_mae: 0.001108
train_e/atom_rmse: 0.001951
2025-02-04 05:56:09.719 INFO: train_e/atom_rmse: 0.001951
train_f_mae: 0.041090
2025-02-04 05:56:09.721 INFO: train_f_mae: 0.041090
train_f_rmse: 0.065172
2025-02-04 05:56:09.722 INFO: train_f_rmse: 0.065172
val_e/atom_mae: 0.000950
2025-02-04 05:56:09.724 INFO: val_e/atom_mae: 0.000950
val_e/atom_rmse: 0.001516
2025-02-04 05:56:09.724 INFO: val_e/atom_rmse: 0.001516
val_f_mae: 0.043344
2025-02-04 05:56:09.724 INFO: val_f_mae: 0.043344
val_f_rmse: 0.072252
2025-02-04 05:56:09.725 INFO: val_f_rmse: 0.072252
##### Step: 203 Learning rate: 9.765625e-06 #####
2025-02-04 05:57:09.360 INFO: ##### Step: 203 Learning rate: 9.765625e-06 #####
Epoch 64, Train Loss: 5.6490, Val Loss: 6.0601
2025-02-04 05:57:09.361 INFO: Epoch 64, Train Loss: 5.6490, Val Loss: 6.0601
train_e/atom_mae: 0.001097
2025-02-04 05:57:09.362 INFO: train_e/atom_mae: 0.001097
train_e/atom_rmse: 0.001950
2025-02-04 05:57:09.362 INFO: train_e/atom_rmse: 0.001950
train_f_mae: 0.041085
2025-02-04 05:57:09.364 INFO: train_f_mae: 0.041085
train_f_rmse: 0.065176
2025-02-04 05:57:09.365 INFO: train_f_rmse: 0.065176
val_e/atom_mae: 0.000959
2025-02-04 05:57:09.367 INFO: val_e/atom_mae: 0.000959
val_e/atom_rmse: 0.001512
2025-02-04 05:57:09.367 INFO: val_e/atom_rmse: 0.001512
val_f_mae: 0.043331
2025-02-04 05:57:09.367 INFO: val_f_mae: 0.043331
val_f_rmse: 0.072239
2025-02-04 05:57:09.368 INFO: val_f_rmse: 0.072239
##### Step: 204 Learning rate: 9.765625e-06 #####
2025-02-04 05:58:09.006 INFO: ##### Step: 204 Learning rate: 9.765625e-06 #####
Epoch 65, Train Loss: 5.6399, Val Loss: 6.0456
2025-02-04 05:58:09.006 INFO: Epoch 65, Train Loss: 5.6399, Val Loss: 6.0456
train_e/atom_mae: 0.001096
2025-02-04 05:58:09.007 INFO: train_e/atom_mae: 0.001096
train_e/atom_rmse: 0.001943
2025-02-04 05:58:09.007 INFO: train_e/atom_rmse: 0.001943
train_f_mae: 0.041090
2025-02-04 05:58:09.010 INFO: train_f_mae: 0.041090
train_f_rmse: 0.065180
2025-02-04 05:58:09.010 INFO: train_f_rmse: 0.065180
val_e/atom_mae: 0.000975
2025-02-04 05:58:09.012 INFO: val_e/atom_mae: 0.000975
val_e/atom_rmse: 0.001495
2025-02-04 05:58:09.012 INFO: val_e/atom_rmse: 0.001495
val_f_mae: 0.043340
2025-02-04 05:58:09.013 INFO: val_f_mae: 0.043340
val_f_rmse: 0.072259
2025-02-04 05:58:09.013 INFO: val_f_rmse: 0.072259
##### Step: 205 Learning rate: 9.765625e-06 #####
2025-02-04 05:59:08.835 INFO: ##### Step: 205 Learning rate: 9.765625e-06 #####
Epoch 66, Train Loss: 5.6457, Val Loss: 6.0500
2025-02-04 05:59:08.836 INFO: Epoch 66, Train Loss: 5.6457, Val Loss: 6.0500
train_e/atom_mae: 0.001115
2025-02-04 05:59:08.837 INFO: train_e/atom_mae: 0.001115
train_e/atom_rmse: 0.001951
2025-02-04 05:59:08.837 INFO: train_e/atom_rmse: 0.001951
train_f_mae: 0.041060
2025-02-04 05:59:08.840 INFO: train_f_mae: 0.041060
train_f_rmse: 0.065139
2025-02-04 05:59:08.840 INFO: train_f_rmse: 0.065139
val_e/atom_mae: 0.000981
2025-02-04 05:59:08.842 INFO: val_e/atom_mae: 0.000981
val_e/atom_rmse: 0.001501
2025-02-04 05:59:08.842 INFO: val_e/atom_rmse: 0.001501
val_f_mae: 0.043337
2025-02-04 05:59:08.843 INFO: val_f_mae: 0.043337
val_f_rmse: 0.072249
2025-02-04 05:59:08.843 INFO: val_f_rmse: 0.072249
##### Step: 206 Learning rate: 9.765625e-06 #####
2025-02-04 06:00:08.651 INFO: ##### Step: 206 Learning rate: 9.765625e-06 #####
Epoch 67, Train Loss: 5.6524, Val Loss: 6.0673
2025-02-04 06:00:08.652 INFO: Epoch 67, Train Loss: 5.6524, Val Loss: 6.0673
train_e/atom_mae: 0.001101
2025-02-04 06:00:08.653 INFO: train_e/atom_mae: 0.001101
train_e/atom_rmse: 0.001952
2025-02-04 06:00:08.653 INFO: train_e/atom_rmse: 0.001952
train_f_mae: 0.041088
2025-02-04 06:00:08.656 INFO: train_f_mae: 0.041088
train_f_rmse: 0.065171
2025-02-04 06:00:08.656 INFO: train_f_rmse: 0.065171
val_e/atom_mae: 0.000956
2025-02-04 06:00:08.658 INFO: val_e/atom_mae: 0.000956
val_e/atom_rmse: 0.001517
2025-02-04 06:00:08.658 INFO: val_e/atom_rmse: 0.001517
val_f_mae: 0.043336
2025-02-04 06:00:08.659 INFO: val_f_mae: 0.043336
val_f_rmse: 0.072254
2025-02-04 06:00:08.659 INFO: val_f_rmse: 0.072254
##### Step: 207 Learning rate: 9.765625e-06 #####
2025-02-04 06:01:08.354 INFO: ##### Step: 207 Learning rate: 9.765625e-06 #####
Epoch 68, Train Loss: 5.6460, Val Loss: 6.0756
2025-02-04 06:01:08.355 INFO: Epoch 68, Train Loss: 5.6460, Val Loss: 6.0756
train_e/atom_mae: 0.001110
2025-02-04 06:01:08.355 INFO: train_e/atom_mae: 0.001110
train_e/atom_rmse: 0.001950
2025-02-04 06:01:08.356 INFO: train_e/atom_rmse: 0.001950
train_f_mae: 0.041071
2025-02-04 06:01:08.358 INFO: train_f_mae: 0.041071
train_f_rmse: 0.065153
2025-02-04 06:01:08.358 INFO: train_f_rmse: 0.065153
val_e/atom_mae: 0.000952
2025-02-04 06:01:08.361 INFO: val_e/atom_mae: 0.000952
val_e/atom_rmse: 0.001521
2025-02-04 06:01:08.361 INFO: val_e/atom_rmse: 0.001521
val_f_mae: 0.043351
2025-02-04 06:01:08.361 INFO: val_f_mae: 0.043351
val_f_rmse: 0.072279
2025-02-04 06:01:08.362 INFO: val_f_rmse: 0.072279
##### Step: 208 Learning rate: 9.765625e-06 #####
2025-02-04 06:02:08.055 INFO: ##### Step: 208 Learning rate: 9.765625e-06 #####
Epoch 69, Train Loss: 5.6626, Val Loss: 6.0722
2025-02-04 06:02:08.055 INFO: Epoch 69, Train Loss: 5.6626, Val Loss: 6.0722
train_e/atom_mae: 0.001091
2025-02-04 06:02:08.056 INFO: train_e/atom_mae: 0.001091
train_e/atom_rmse: 0.001961
2025-02-04 06:02:08.056 INFO: train_e/atom_rmse: 0.001961
train_f_mae: 0.041069
2025-02-04 06:02:08.059 INFO: train_f_mae: 0.041069
train_f_rmse: 0.065153
2025-02-04 06:02:08.059 INFO: train_f_rmse: 0.065153
val_e/atom_mae: 0.000960
2025-02-04 06:02:08.061 INFO: val_e/atom_mae: 0.000960
val_e/atom_rmse: 0.001523
2025-02-04 06:02:08.061 INFO: val_e/atom_rmse: 0.001523
val_f_mae: 0.043321
2025-02-04 06:02:08.062 INFO: val_f_mae: 0.043321
val_f_rmse: 0.072241
2025-02-04 06:02:08.062 INFO: val_f_rmse: 0.072241
##### Step: 209 Learning rate: 9.765625e-06 #####
2025-02-04 06:03:07.837 INFO: ##### Step: 209 Learning rate: 9.765625e-06 #####
Epoch 70, Train Loss: 5.6429, Val Loss: 6.0822
2025-02-04 06:03:07.837 INFO: Epoch 70, Train Loss: 5.6429, Val Loss: 6.0822
train_e/atom_mae: 0.001103
2025-02-04 06:03:07.838 INFO: train_e/atom_mae: 0.001103
train_e/atom_rmse: 0.001944
2025-02-04 06:03:07.838 INFO: train_e/atom_rmse: 0.001944
train_f_mae: 0.041089
2025-02-04 06:03:07.841 INFO: train_f_mae: 0.041089
train_f_rmse: 0.065186
2025-02-04 06:03:07.841 INFO: train_f_rmse: 0.065186
val_e/atom_mae: 0.000958
2025-02-04 06:03:07.843 INFO: val_e/atom_mae: 0.000958
val_e/atom_rmse: 0.001534
2025-02-04 06:03:07.843 INFO: val_e/atom_rmse: 0.001534
val_f_mae: 0.043322
2025-02-04 06:03:07.844 INFO: val_f_mae: 0.043322
val_f_rmse: 0.072226
2025-02-04 06:03:07.844 INFO: val_f_rmse: 0.072226
##### Step: 210 Learning rate: 9.765625e-06 #####
2025-02-04 06:04:07.531 INFO: ##### Step: 210 Learning rate: 9.765625e-06 #####
Epoch 71, Train Loss: 5.6651, Val Loss: 6.0822
2025-02-04 06:04:07.532 INFO: Epoch 71, Train Loss: 5.6651, Val Loss: 6.0822
train_e/atom_mae: 0.001116
2025-02-04 06:04:07.533 INFO: train_e/atom_mae: 0.001116
train_e/atom_rmse: 0.001963
2025-02-04 06:04:07.533 INFO: train_e/atom_rmse: 0.001963
train_f_mae: 0.041072
2025-02-04 06:04:07.536 INFO: train_f_mae: 0.041072
train_f_rmse: 0.065151
2025-02-04 06:04:07.536 INFO: train_f_rmse: 0.065151
val_e/atom_mae: 0.000964
2025-02-04 06:04:07.538 INFO: val_e/atom_mae: 0.000964
val_e/atom_rmse: 0.001536
2025-02-04 06:04:07.538 INFO: val_e/atom_rmse: 0.001536
val_f_mae: 0.043316
2025-02-04 06:04:07.539 INFO: val_f_mae: 0.043316
val_f_rmse: 0.072204
2025-02-04 06:04:07.539 INFO: val_f_rmse: 0.072204
##### Step: 211 Learning rate: 9.765625e-06 #####
2025-02-04 06:05:07.159 INFO: ##### Step: 211 Learning rate: 9.765625e-06 #####
Epoch 72, Train Loss: 5.6537, Val Loss: 6.0523
2025-02-04 06:05:07.159 INFO: Epoch 72, Train Loss: 5.6537, Val Loss: 6.0523
train_e/atom_mae: 0.001143
2025-02-04 06:05:07.160 INFO: train_e/atom_mae: 0.001143
train_e/atom_rmse: 0.001968
2025-02-04 06:05:07.160 INFO: train_e/atom_rmse: 0.001968
train_f_mae: 0.041017
2025-02-04 06:05:07.163 INFO: train_f_mae: 0.041017
train_f_rmse: 0.065007
2025-02-04 06:05:07.163 INFO: train_f_rmse: 0.065007
val_e/atom_mae: 0.000967
2025-02-04 06:05:07.165 INFO: val_e/atom_mae: 0.000967
val_e/atom_rmse: 0.001506
2025-02-04 06:05:07.166 INFO: val_e/atom_rmse: 0.001506
val_f_mae: 0.043334
2025-02-04 06:05:07.166 INFO: val_f_mae: 0.043334
val_f_rmse: 0.072226
2025-02-04 06:05:07.166 INFO: val_f_rmse: 0.072226
##### Step: 212 Learning rate: 9.765625e-06 #####
2025-02-04 06:06:06.838 INFO: ##### Step: 212 Learning rate: 9.765625e-06 #####
Epoch 73, Train Loss: 5.6591, Val Loss: 6.0760
2025-02-04 06:06:06.838 INFO: Epoch 73, Train Loss: 5.6591, Val Loss: 6.0760
train_e/atom_mae: 0.001119
2025-02-04 06:06:06.839 INFO: train_e/atom_mae: 0.001119
train_e/atom_rmse: 0.001959
2025-02-04 06:06:06.839 INFO: train_e/atom_rmse: 0.001959
train_f_mae: 0.041080
2025-02-04 06:06:06.842 INFO: train_f_mae: 0.041080
train_f_rmse: 0.065150
2025-02-04 06:06:06.842 INFO: train_f_rmse: 0.065150
val_e/atom_mae: 0.000959
2025-02-04 06:06:06.844 INFO: val_e/atom_mae: 0.000959
val_e/atom_rmse: 0.001530
2025-02-04 06:06:06.844 INFO: val_e/atom_rmse: 0.001530
val_f_mae: 0.043316
2025-02-04 06:06:06.845 INFO: val_f_mae: 0.043316
val_f_rmse: 0.072214
2025-02-04 06:06:06.845 INFO: val_f_rmse: 0.072214
##### Step: 213 Learning rate: 9.765625e-06 #####
2025-02-04 06:07:06.504 INFO: ##### Step: 213 Learning rate: 9.765625e-06 #####
Epoch 74, Train Loss: 5.6663, Val Loss: 6.0770
2025-02-04 06:07:06.505 INFO: Epoch 74, Train Loss: 5.6663, Val Loss: 6.0770
train_e/atom_mae: 0.001107
2025-02-04 06:07:06.505 INFO: train_e/atom_mae: 0.001107
train_e/atom_rmse: 0.001966
2025-02-04 06:07:06.506 INFO: train_e/atom_rmse: 0.001966
train_f_mae: 0.041054
2025-02-04 06:07:06.508 INFO: train_f_mae: 0.041054
train_f_rmse: 0.065121
2025-02-04 06:07:06.509 INFO: train_f_rmse: 0.065121
val_e/atom_mae: 0.000960
2025-02-04 06:07:06.511 INFO: val_e/atom_mae: 0.000960
val_e/atom_rmse: 0.001534
2025-02-04 06:07:06.511 INFO: val_e/atom_rmse: 0.001534
val_f_mae: 0.043305
2025-02-04 06:07:06.511 INFO: val_f_mae: 0.043305
val_f_rmse: 0.072183
2025-02-04 06:07:06.512 INFO: val_f_rmse: 0.072183
##### Step: 214 Learning rate: 9.765625e-06 #####
2025-02-04 06:08:06.151 INFO: ##### Step: 214 Learning rate: 9.765625e-06 #####
Epoch 75, Train Loss: 5.6606, Val Loss: 6.0925
2025-02-04 06:08:06.151 INFO: Epoch 75, Train Loss: 5.6606, Val Loss: 6.0925
train_e/atom_mae: 0.001085
2025-02-04 06:08:06.152 INFO: train_e/atom_mae: 0.001085
train_e/atom_rmse: 0.001962
2025-02-04 06:08:06.153 INFO: train_e/atom_rmse: 0.001962
train_f_mae: 0.041051
2025-02-04 06:08:06.155 INFO: train_f_mae: 0.041051
train_f_rmse: 0.065125
2025-02-04 06:08:06.155 INFO: train_f_rmse: 0.065125
val_e/atom_mae: 0.000955
2025-02-04 06:08:06.157 INFO: val_e/atom_mae: 0.000955
val_e/atom_rmse: 0.001545
2025-02-04 06:08:06.158 INFO: val_e/atom_rmse: 0.001545
val_f_mae: 0.043317
2025-02-04 06:08:06.158 INFO: val_f_mae: 0.043317
val_f_rmse: 0.072211
2025-02-04 06:08:06.158 INFO: val_f_rmse: 0.072211
##### Step: 215 Learning rate: 9.765625e-06 #####
2025-02-04 06:09:05.915 INFO: ##### Step: 215 Learning rate: 9.765625e-06 #####
Epoch 76, Train Loss: 5.6851, Val Loss: 6.0634
2025-02-04 06:09:05.916 INFO: Epoch 76, Train Loss: 5.6851, Val Loss: 6.0634
train_e/atom_mae: 0.001131
2025-02-04 06:09:05.917 INFO: train_e/atom_mae: 0.001131
train_e/atom_rmse: 0.001980
2025-02-04 06:09:05.917 INFO: train_e/atom_rmse: 0.001980
train_f_mae: 0.041051
2025-02-04 06:09:05.920 INFO: train_f_mae: 0.041051
train_f_rmse: 0.065116
2025-02-04 06:09:05.920 INFO: train_f_rmse: 0.065116
val_e/atom_mae: 0.001011
2025-02-04 06:09:05.922 INFO: val_e/atom_mae: 0.001011
val_e/atom_rmse: 0.001522
2025-02-04 06:09:05.922 INFO: val_e/atom_rmse: 0.001522
val_f_mae: 0.043303
2025-02-04 06:09:05.923 INFO: val_f_mae: 0.043303
val_f_rmse: 0.072179
2025-02-04 06:09:05.923 INFO: val_f_rmse: 0.072179
##### Step: 216 Learning rate: 9.765625e-06 #####
2025-02-04 06:10:05.710 INFO: ##### Step: 216 Learning rate: 9.765625e-06 #####
Epoch 77, Train Loss: 5.6709, Val Loss: 6.0544
2025-02-04 06:10:05.711 INFO: Epoch 77, Train Loss: 5.6709, Val Loss: 6.0544
train_e/atom_mae: 0.001113
2025-02-04 06:10:05.712 INFO: train_e/atom_mae: 0.001113
train_e/atom_rmse: 0.001966
2025-02-04 06:10:05.712 INFO: train_e/atom_rmse: 0.001966
train_f_mae: 0.041092
2025-02-04 06:10:05.715 INFO: train_f_mae: 0.041092
train_f_rmse: 0.065167
2025-02-04 06:10:05.715 INFO: train_f_rmse: 0.065167
val_e/atom_mae: 0.000962
2025-02-04 06:10:05.717 INFO: val_e/atom_mae: 0.000962
val_e/atom_rmse: 0.001504
2025-02-04 06:10:05.717 INFO: val_e/atom_rmse: 0.001504
val_f_mae: 0.043349
2025-02-04 06:10:05.718 INFO: val_f_mae: 0.043349
val_f_rmse: 0.072257
2025-02-04 06:10:05.718 INFO: val_f_rmse: 0.072257
##### Step: 217 Learning rate: 9.765625e-06 #####
2025-02-04 06:11:05.512 INFO: ##### Step: 217 Learning rate: 9.765625e-06 #####
Epoch 78, Train Loss: 5.6715, Val Loss: 6.0680
2025-02-04 06:11:05.512 INFO: Epoch 78, Train Loss: 5.6715, Val Loss: 6.0680
train_e/atom_mae: 0.001143
2025-02-04 06:11:05.513 INFO: train_e/atom_mae: 0.001143
train_e/atom_rmse: 0.001969
2025-02-04 06:11:05.513 INFO: train_e/atom_rmse: 0.001969
train_f_mae: 0.041055
2025-02-04 06:11:05.516 INFO: train_f_mae: 0.041055
train_f_rmse: 0.065130
2025-02-04 06:11:05.516 INFO: train_f_rmse: 0.065130
val_e/atom_mae: 0.000959
2025-02-04 06:11:05.518 INFO: val_e/atom_mae: 0.000959
val_e/atom_rmse: 0.001522
2025-02-04 06:11:05.519 INFO: val_e/atom_rmse: 0.001522
val_f_mae: 0.043319
2025-02-04 06:11:05.519 INFO: val_f_mae: 0.043319
val_f_rmse: 0.072220
2025-02-04 06:11:05.519 INFO: val_f_rmse: 0.072220
##### Step: 218 Learning rate: 9.765625e-06 #####
2025-02-04 06:12:05.263 INFO: ##### Step: 218 Learning rate: 9.765625e-06 #####
Epoch 79, Train Loss: 5.6408, Val Loss: 6.0656
2025-02-04 06:12:05.263 INFO: Epoch 79, Train Loss: 5.6408, Val Loss: 6.0656
train_e/atom_mae: 0.001087
2025-02-04 06:12:05.264 INFO: train_e/atom_mae: 0.001087
train_e/atom_rmse: 0.001947
2025-02-04 06:12:05.264 INFO: train_e/atom_rmse: 0.001947
train_f_mae: 0.041066
2025-02-04 06:12:05.267 INFO: train_f_mae: 0.041066
train_f_rmse: 0.065139
2025-02-04 06:12:05.267 INFO: train_f_rmse: 0.065139
val_e/atom_mae: 0.000957
2025-02-04 06:12:05.269 INFO: val_e/atom_mae: 0.000957
val_e/atom_rmse: 0.001522
2025-02-04 06:12:05.270 INFO: val_e/atom_rmse: 0.001522
val_f_mae: 0.043312
2025-02-04 06:12:05.270 INFO: val_f_mae: 0.043312
val_f_rmse: 0.072202
2025-02-04 06:12:05.270 INFO: val_f_rmse: 0.072202
##### Step: 219 Learning rate: 9.765625e-06 #####
2025-02-04 06:13:05.062 INFO: ##### Step: 219 Learning rate: 9.765625e-06 #####
Epoch 80, Train Loss: 5.6697, Val Loss: 6.0645
2025-02-04 06:13:05.063 INFO: Epoch 80, Train Loss: 5.6697, Val Loss: 6.0645
train_e/atom_mae: 0.001117
2025-02-04 06:13:05.063 INFO: train_e/atom_mae: 0.001117
train_e/atom_rmse: 0.001964
2025-02-04 06:13:05.064 INFO: train_e/atom_rmse: 0.001964
train_f_mae: 0.041065
2025-02-04 06:13:05.066 INFO: train_f_mae: 0.041065
train_f_rmse: 0.065178
2025-02-04 06:13:05.066 INFO: train_f_rmse: 0.065178
val_e/atom_mae: 0.000955
2025-02-04 06:13:05.069 INFO: val_e/atom_mae: 0.000955
val_e/atom_rmse: 0.001514
2025-02-04 06:13:05.069 INFO: val_e/atom_rmse: 0.001514
val_f_mae: 0.043335
2025-02-04 06:13:05.069 INFO: val_f_mae: 0.043335
val_f_rmse: 0.072255
2025-02-04 06:13:05.070 INFO: val_f_rmse: 0.072255
##### Step: 220 Learning rate: 4.8828125e-06 #####
2025-02-04 06:14:04.816 INFO: ##### Step: 220 Learning rate: 4.8828125e-06 #####
Epoch 81, Train Loss: 5.6298, Val Loss: 6.0539
2025-02-04 06:14:04.816 INFO: Epoch 81, Train Loss: 5.6298, Val Loss: 6.0539
train_e/atom_mae: 0.001081
2025-02-04 06:14:04.817 INFO: train_e/atom_mae: 0.001081
train_e/atom_rmse: 0.001938
2025-02-04 06:14:04.817 INFO: train_e/atom_rmse: 0.001938
train_f_mae: 0.041051
2025-02-04 06:14:04.820 INFO: train_f_mae: 0.041051
train_f_rmse: 0.065152
2025-02-04 06:14:04.820 INFO: train_f_rmse: 0.065152
val_e/atom_mae: 0.000958
2025-02-04 06:14:04.822 INFO: val_e/atom_mae: 0.000958
val_e/atom_rmse: 0.001503
2025-02-04 06:14:04.823 INFO: val_e/atom_rmse: 0.001503
val_f_mae: 0.043333
2025-02-04 06:14:04.823 INFO: val_f_mae: 0.043333
val_f_rmse: 0.072266
2025-02-04 06:14:04.823 INFO: val_f_rmse: 0.072266
##### Step: 221 Learning rate: 4.8828125e-06 #####
2025-02-04 06:15:04.566 INFO: ##### Step: 221 Learning rate: 4.8828125e-06 #####
Epoch 82, Train Loss: 5.6402, Val Loss: 6.0579
2025-02-04 06:15:04.566 INFO: Epoch 82, Train Loss: 5.6402, Val Loss: 6.0579
train_e/atom_mae: 0.001076
2025-02-04 06:15:04.567 INFO: train_e/atom_mae: 0.001076
train_e/atom_rmse: 0.001942
2025-02-04 06:15:04.567 INFO: train_e/atom_rmse: 0.001942
train_f_mae: 0.041079
2025-02-04 06:15:04.570 INFO: train_f_mae: 0.041079
train_f_rmse: 0.065196
2025-02-04 06:15:04.570 INFO: train_f_rmse: 0.065196
val_e/atom_mae: 0.000958
2025-02-04 06:15:04.572 INFO: val_e/atom_mae: 0.000958
val_e/atom_rmse: 0.001509
2025-02-04 06:15:04.572 INFO: val_e/atom_rmse: 0.001509
val_f_mae: 0.043322
2025-02-04 06:15:04.573 INFO: val_f_mae: 0.043322
val_f_rmse: 0.072249
2025-02-04 06:15:04.573 INFO: val_f_rmse: 0.072249
##### Step: 222 Learning rate: 4.8828125e-06 #####
2025-02-04 06:16:04.322 INFO: ##### Step: 222 Learning rate: 4.8828125e-06 #####
Epoch 83, Train Loss: 5.6428, Val Loss: 6.0551
2025-02-04 06:16:04.322 INFO: Epoch 83, Train Loss: 5.6428, Val Loss: 6.0551
train_e/atom_mae: 0.001077
2025-02-04 06:16:04.323 INFO: train_e/atom_mae: 0.001077
train_e/atom_rmse: 0.001945
2025-02-04 06:16:04.323 INFO: train_e/atom_rmse: 0.001945
train_f_mae: 0.041071
2025-02-04 06:16:04.326 INFO: train_f_mae: 0.041071
train_f_rmse: 0.065175
2025-02-04 06:16:04.326 INFO: train_f_rmse: 0.065175
val_e/atom_mae: 0.000963
2025-02-04 06:16:04.328 INFO: val_e/atom_mae: 0.000963
val_e/atom_rmse: 0.001509
2025-02-04 06:16:04.328 INFO: val_e/atom_rmse: 0.001509
val_f_mae: 0.043318
2025-02-04 06:16:04.329 INFO: val_f_mae: 0.043318
val_f_rmse: 0.072228
2025-02-04 06:16:04.329 INFO: val_f_rmse: 0.072228
##### Step: 223 Learning rate: 4.8828125e-06 #####
2025-02-04 06:17:04.075 INFO: ##### Step: 223 Learning rate: 4.8828125e-06 #####
Epoch 84, Train Loss: 5.6421, Val Loss: 6.0626
2025-02-04 06:17:04.076 INFO: Epoch 84, Train Loss: 5.6421, Val Loss: 6.0626
train_e/atom_mae: 0.001091
2025-02-04 06:17:04.077 INFO: train_e/atom_mae: 0.001091
train_e/atom_rmse: 0.001946
2025-02-04 06:17:04.077 INFO: train_e/atom_rmse: 0.001946
train_f_mae: 0.041064
2025-02-04 06:17:04.079 INFO: train_f_mae: 0.041064
train_f_rmse: 0.065161
2025-02-04 06:17:04.080 INFO: train_f_rmse: 0.065161
val_e/atom_mae: 0.000955
2025-02-04 06:17:04.082 INFO: val_e/atom_mae: 0.000955
val_e/atom_rmse: 0.001513
2025-02-04 06:17:04.082 INFO: val_e/atom_rmse: 0.001513
val_f_mae: 0.043326
2025-02-04 06:17:04.083 INFO: val_f_mae: 0.043326
val_f_rmse: 0.072245
2025-02-04 06:17:04.083 INFO: val_f_rmse: 0.072245
##### Step: 224 Learning rate: 4.8828125e-06 #####
2025-02-04 06:18:03.902 INFO: ##### Step: 224 Learning rate: 4.8828125e-06 #####
Epoch 85, Train Loss: 5.6377, Val Loss: 6.0542
2025-02-04 06:18:03.902 INFO: Epoch 85, Train Loss: 5.6377, Val Loss: 6.0542
train_e/atom_mae: 0.001077
2025-02-04 06:18:03.903 INFO: train_e/atom_mae: 0.001077
train_e/atom_rmse: 0.001944
2025-02-04 06:18:03.904 INFO: train_e/atom_rmse: 0.001944
train_f_mae: 0.041053
2025-02-04 06:18:03.906 INFO: train_f_mae: 0.041053
train_f_rmse: 0.065153
2025-02-04 06:18:03.906 INFO: train_f_rmse: 0.065153
val_e/atom_mae: 0.000966
2025-02-04 06:18:03.908 INFO: val_e/atom_mae: 0.000966
val_e/atom_rmse: 0.001508
2025-02-04 06:18:03.909 INFO: val_e/atom_rmse: 0.001508
val_f_mae: 0.043315
2025-02-04 06:18:03.909 INFO: val_f_mae: 0.043315
val_f_rmse: 0.072222
2025-02-04 06:18:03.909 INFO: val_f_rmse: 0.072222
##### Step: 225 Learning rate: 4.8828125e-06 #####
2025-02-04 06:19:03.769 INFO: ##### Step: 225 Learning rate: 4.8828125e-06 #####
Epoch 86, Train Loss: 5.6419, Val Loss: 6.0571
2025-02-04 06:19:03.769 INFO: Epoch 86, Train Loss: 5.6419, Val Loss: 6.0571
train_e/atom_mae: 0.001079
2025-02-04 06:19:03.770 INFO: train_e/atom_mae: 0.001079
train_e/atom_rmse: 0.001946
2025-02-04 06:19:03.770 INFO: train_e/atom_rmse: 0.001946
train_f_mae: 0.041062
2025-02-04 06:19:03.773 INFO: train_f_mae: 0.041062
train_f_rmse: 0.065157
2025-02-04 06:19:03.773 INFO: train_f_rmse: 0.065157
val_e/atom_mae: 0.000956
2025-02-04 06:19:03.775 INFO: val_e/atom_mae: 0.000956
val_e/atom_rmse: 0.001509
2025-02-04 06:19:03.776 INFO: val_e/atom_rmse: 0.001509
val_f_mae: 0.043330
2025-02-04 06:19:03.776 INFO: val_f_mae: 0.043330
val_f_rmse: 0.072237
2025-02-04 06:19:03.776 INFO: val_f_rmse: 0.072237
##### Step: 226 Learning rate: 4.8828125e-06 #####
2025-02-04 06:20:03.602 INFO: ##### Step: 226 Learning rate: 4.8828125e-06 #####
Epoch 87, Train Loss: 5.6416, Val Loss: 6.0657
2025-02-04 06:20:03.602 INFO: Epoch 87, Train Loss: 5.6416, Val Loss: 6.0657
train_e/atom_mae: 0.001090
2025-02-04 06:20:03.603 INFO: train_e/atom_mae: 0.001090
train_e/atom_rmse: 0.001944
2025-02-04 06:20:03.603 INFO: train_e/atom_rmse: 0.001944
train_f_mae: 0.041064
2025-02-04 06:20:03.606 INFO: train_f_mae: 0.041064
train_f_rmse: 0.065177
2025-02-04 06:20:03.606 INFO: train_f_rmse: 0.065177
val_e/atom_mae: 0.000956
2025-02-04 06:20:03.608 INFO: val_e/atom_mae: 0.000956
val_e/atom_rmse: 0.001518
2025-02-04 06:20:03.608 INFO: val_e/atom_rmse: 0.001518
val_f_mae: 0.043314
2025-02-04 06:20:03.609 INFO: val_f_mae: 0.043314
val_f_rmse: 0.072235
2025-02-04 06:20:03.609 INFO: val_f_rmse: 0.072235
##### Step: 227 Learning rate: 4.8828125e-06 #####
2025-02-04 06:21:03.486 INFO: ##### Step: 227 Learning rate: 4.8828125e-06 #####
Epoch 88, Train Loss: 5.6352, Val Loss: 6.0654
2025-02-04 06:21:03.487 INFO: Epoch 88, Train Loss: 5.6352, Val Loss: 6.0654
train_e/atom_mae: 0.001084
2025-02-04 06:21:03.487 INFO: train_e/atom_mae: 0.001084
train_e/atom_rmse: 0.001944
2025-02-04 06:21:03.488 INFO: train_e/atom_rmse: 0.001944
train_f_mae: 0.041037
2025-02-04 06:21:03.490 INFO: train_f_mae: 0.041037
train_f_rmse: 0.065136
2025-02-04 06:21:03.490 INFO: train_f_rmse: 0.065136
val_e/atom_mae: 0.000954
2025-02-04 06:21:03.493 INFO: val_e/atom_mae: 0.000954
val_e/atom_rmse: 0.001519
2025-02-04 06:21:03.493 INFO: val_e/atom_rmse: 0.001519
val_f_mae: 0.043311
2025-02-04 06:21:03.493 INFO: val_f_mae: 0.043311
val_f_rmse: 0.072224
2025-02-04 06:21:03.494 INFO: val_f_rmse: 0.072224
##### Step: 228 Learning rate: 4.8828125e-06 #####
2025-02-04 06:22:03.342 INFO: ##### Step: 228 Learning rate: 4.8828125e-06 #####
Epoch 89, Train Loss: 5.6485, Val Loss: 6.0787
2025-02-04 06:22:03.342 INFO: Epoch 89, Train Loss: 5.6485, Val Loss: 6.0787
train_e/atom_mae: 0.001089
2025-02-04 06:22:03.343 INFO: train_e/atom_mae: 0.001089
train_e/atom_rmse: 0.001951
2025-02-04 06:22:03.343 INFO: train_e/atom_rmse: 0.001951
train_f_mae: 0.041059
2025-02-04 06:22:03.346 INFO: train_f_mae: 0.041059
train_f_rmse: 0.065157
2025-02-04 06:22:03.346 INFO: train_f_rmse: 0.065157
val_e/atom_mae: 0.000955
2025-02-04 06:22:03.348 INFO: val_e/atom_mae: 0.000955
val_e/atom_rmse: 0.001531
2025-02-04 06:22:03.349 INFO: val_e/atom_rmse: 0.001531
val_f_mae: 0.043310
2025-02-04 06:22:03.349 INFO: val_f_mae: 0.043310
val_f_rmse: 0.072220
2025-02-04 06:22:03.349 INFO: val_f_rmse: 0.072220
##### Step: 229 Learning rate: 4.8828125e-06 #####
2025-02-04 06:23:03.198 INFO: ##### Step: 229 Learning rate: 4.8828125e-06 #####
Epoch 90, Train Loss: 5.6440, Val Loss: 6.0797
2025-02-04 06:23:03.199 INFO: Epoch 90, Train Loss: 5.6440, Val Loss: 6.0797
train_e/atom_mae: 0.001081
2025-02-04 06:23:03.200 INFO: train_e/atom_mae: 0.001081
train_e/atom_rmse: 0.001955
2025-02-04 06:23:03.200 INFO: train_e/atom_rmse: 0.001955
train_f_mae: 0.041012
2025-02-04 06:23:03.203 INFO: train_f_mae: 0.041012
train_f_rmse: 0.065075
2025-02-04 06:23:03.203 INFO: train_f_rmse: 0.065075
val_e/atom_mae: 0.000952
2025-02-04 06:23:03.205 INFO: val_e/atom_mae: 0.000952
val_e/atom_rmse: 0.001534
2025-02-04 06:23:03.205 INFO: val_e/atom_rmse: 0.001534
val_f_mae: 0.043298
2025-02-04 06:23:03.206 INFO: val_f_mae: 0.043298
val_f_rmse: 0.072205
2025-02-04 06:23:03.206 INFO: val_f_rmse: 0.072205
##### Step: 230 Learning rate: 4.8828125e-06 #####
2025-02-04 06:24:03.070 INFO: ##### Step: 230 Learning rate: 4.8828125e-06 #####
Epoch 91, Train Loss: 5.6473, Val Loss: 6.0738
2025-02-04 06:24:03.071 INFO: Epoch 91, Train Loss: 5.6473, Val Loss: 6.0738
train_e/atom_mae: 0.001082
2025-02-04 06:24:03.072 INFO: train_e/atom_mae: 0.001082
train_e/atom_rmse: 0.001950
2025-02-04 06:24:03.072 INFO: train_e/atom_rmse: 0.001950
train_f_mae: 0.041050
2025-02-04 06:24:03.074 INFO: train_f_mae: 0.041050
train_f_rmse: 0.065154
2025-02-04 06:24:03.075 INFO: train_f_rmse: 0.065154
val_e/atom_mae: 0.000955
2025-02-04 06:24:03.077 INFO: val_e/atom_mae: 0.000955
val_e/atom_rmse: 0.001528
2025-02-04 06:24:03.077 INFO: val_e/atom_rmse: 0.001528
val_f_mae: 0.043298
2025-02-04 06:24:03.077 INFO: val_f_mae: 0.043298
val_f_rmse: 0.072210
2025-02-04 06:24:03.078 INFO: val_f_rmse: 0.072210
##### Step: 231 Learning rate: 4.8828125e-06 #####
2025-02-04 06:25:02.949 INFO: ##### Step: 231 Learning rate: 4.8828125e-06 #####
Epoch 92, Train Loss: 5.6345, Val Loss: 6.0627
2025-02-04 06:25:02.949 INFO: Epoch 92, Train Loss: 5.6345, Val Loss: 6.0627
train_e/atom_mae: 0.001080
2025-02-04 06:25:02.950 INFO: train_e/atom_mae: 0.001080
train_e/atom_rmse: 0.001948
2025-02-04 06:25:02.950 INFO: train_e/atom_rmse: 0.001948
train_f_mae: 0.041015
2025-02-04 06:25:02.953 INFO: train_f_mae: 0.041015
train_f_rmse: 0.065078
2025-02-04 06:25:02.953 INFO: train_f_rmse: 0.065078
val_e/atom_mae: 0.000960
2025-02-04 06:25:02.955 INFO: val_e/atom_mae: 0.000960
val_e/atom_rmse: 0.001520
2025-02-04 06:25:02.955 INFO: val_e/atom_rmse: 0.001520
val_f_mae: 0.043301
2025-02-04 06:25:02.956 INFO: val_f_mae: 0.043301
val_f_rmse: 0.072197
2025-02-04 06:25:02.956 INFO: val_f_rmse: 0.072197
##### Step: 232 Learning rate: 4.8828125e-06 #####
2025-02-04 06:26:02.808 INFO: ##### Step: 232 Learning rate: 4.8828125e-06 #####
Epoch 93, Train Loss: 5.6445, Val Loss: 6.0491
2025-02-04 06:26:02.808 INFO: Epoch 93, Train Loss: 5.6445, Val Loss: 6.0491
train_e/atom_mae: 0.001096
2025-02-04 06:26:02.809 INFO: train_e/atom_mae: 0.001096
train_e/atom_rmse: 0.001948
2025-02-04 06:26:02.809 INFO: train_e/atom_rmse: 0.001948
train_f_mae: 0.041064
2025-02-04 06:26:02.812 INFO: train_f_mae: 0.041064
train_f_rmse: 0.065153
2025-02-04 06:26:02.812 INFO: train_f_rmse: 0.065153
val_e/atom_mae: 0.000973
2025-02-04 06:26:02.814 INFO: val_e/atom_mae: 0.000973
val_e/atom_rmse: 0.001505
2025-02-04 06:26:02.814 INFO: val_e/atom_rmse: 0.001505
val_f_mae: 0.043318
2025-02-04 06:26:02.815 INFO: val_f_mae: 0.043318
val_f_rmse: 0.072213
2025-02-04 06:26:02.815 INFO: val_f_rmse: 0.072213
##### Step: 233 Learning rate: 4.8828125e-06 #####
2025-02-04 06:27:02.651 INFO: ##### Step: 233 Learning rate: 4.8828125e-06 #####
Epoch 94, Train Loss: 5.6410, Val Loss: 6.0598
2025-02-04 06:27:02.652 INFO: Epoch 94, Train Loss: 5.6410, Val Loss: 6.0598
train_e/atom_mae: 0.001084
2025-02-04 06:27:02.653 INFO: train_e/atom_mae: 0.001084
train_e/atom_rmse: 0.001946
2025-02-04 06:27:02.653 INFO: train_e/atom_rmse: 0.001946
train_f_mae: 0.041057
2025-02-04 06:27:02.655 INFO: train_f_mae: 0.041057
train_f_rmse: 0.065149
2025-02-04 06:27:02.656 INFO: train_f_rmse: 0.065149
val_e/atom_mae: 0.000963
2025-02-04 06:27:02.658 INFO: val_e/atom_mae: 0.000963
val_e/atom_rmse: 0.001516
2025-02-04 06:27:02.658 INFO: val_e/atom_rmse: 0.001516
val_f_mae: 0.043302
2025-02-04 06:27:02.659 INFO: val_f_mae: 0.043302
val_f_rmse: 0.072205
2025-02-04 06:27:02.659 INFO: val_f_rmse: 0.072205
##### Step: 234 Learning rate: 4.8828125e-06 #####
2025-02-04 06:28:02.470 INFO: ##### Step: 234 Learning rate: 4.8828125e-06 #####
Epoch 95, Train Loss: 5.6615, Val Loss: 6.0740
2025-02-04 06:28:02.471 INFO: Epoch 95, Train Loss: 5.6615, Val Loss: 6.0740
train_e/atom_mae: 0.001102
2025-02-04 06:28:02.471 INFO: train_e/atom_mae: 0.001102
train_e/atom_rmse: 0.001963
2025-02-04 06:28:02.472 INFO: train_e/atom_rmse: 0.001963
train_f_mae: 0.041039
2025-02-04 06:28:02.474 INFO: train_f_mae: 0.041039
train_f_rmse: 0.065121
2025-02-04 06:28:02.474 INFO: train_f_rmse: 0.065121
val_e/atom_mae: 0.000957
2025-02-04 06:28:02.477 INFO: val_e/atom_mae: 0.000957
val_e/atom_rmse: 0.001529
2025-02-04 06:28:02.477 INFO: val_e/atom_rmse: 0.001529
val_f_mae: 0.043305
2025-02-04 06:28:02.477 INFO: val_f_mae: 0.043305
val_f_rmse: 0.072208
2025-02-04 06:28:02.478 INFO: val_f_rmse: 0.072208
##### Step: 235 Learning rate: 4.8828125e-06 #####
2025-02-04 06:29:02.299 INFO: ##### Step: 235 Learning rate: 4.8828125e-06 #####
Epoch 96, Train Loss: 5.6409, Val Loss: 6.0729
2025-02-04 06:29:02.299 INFO: Epoch 96, Train Loss: 5.6409, Val Loss: 6.0729
train_e/atom_mae: 0.001093
2025-02-04 06:29:02.300 INFO: train_e/atom_mae: 0.001093
train_e/atom_rmse: 0.001947
2025-02-04 06:29:02.300 INFO: train_e/atom_rmse: 0.001947
train_f_mae: 0.041052
2025-02-04 06:29:02.303 INFO: train_f_mae: 0.041052
train_f_rmse: 0.065142
2025-02-04 06:29:02.303 INFO: train_f_rmse: 0.065142
val_e/atom_mae: 0.000958
2025-02-04 06:29:02.305 INFO: val_e/atom_mae: 0.000958
val_e/atom_rmse: 0.001530
2025-02-04 06:29:02.305 INFO: val_e/atom_rmse: 0.001530
val_f_mae: 0.043297
2025-02-04 06:29:02.306 INFO: val_f_mae: 0.043297
val_f_rmse: 0.072191
2025-02-04 06:29:02.306 INFO: val_f_rmse: 0.072191
##### Step: 236 Learning rate: 4.8828125e-06 #####
2025-02-04 06:30:08.151 INFO: ##### Step: 236 Learning rate: 4.8828125e-06 #####
Epoch 97, Train Loss: 5.6431, Val Loss: 6.0883
2025-02-04 06:30:08.151 INFO: Epoch 97, Train Loss: 5.6431, Val Loss: 6.0883
train_e/atom_mae: 0.001081
2025-02-04 06:30:08.232 INFO: train_e/atom_mae: 0.001081
train_e/atom_rmse: 0.001959
2025-02-04 06:30:08.241 INFO: train_e/atom_rmse: 0.001959
train_f_mae: 0.040992
2025-02-04 06:30:08.244 INFO: train_f_mae: 0.040992
train_f_rmse: 0.065028
2025-02-04 06:30:08.244 INFO: train_f_rmse: 0.065028
val_e/atom_mae: 0.000959
2025-02-04 06:30:08.246 INFO: val_e/atom_mae: 0.000959
val_e/atom_rmse: 0.001543
2025-02-04 06:30:08.247 INFO: val_e/atom_rmse: 0.001543
val_f_mae: 0.043289
2025-02-04 06:30:08.247 INFO: val_f_mae: 0.043289
val_f_rmse: 0.072197
2025-02-04 06:30:08.247 INFO: val_f_rmse: 0.072197
##### Step: 237 Learning rate: 4.8828125e-06 #####
2025-02-04 06:31:08.229 INFO: ##### Step: 237 Learning rate: 4.8828125e-06 #####
Epoch 98, Train Loss: 5.6478, Val Loss: 6.0742
2025-02-04 06:31:08.229 INFO: Epoch 98, Train Loss: 5.6478, Val Loss: 6.0742
train_e/atom_mae: 0.001087
2025-02-04 06:31:08.230 INFO: train_e/atom_mae: 0.001087
train_e/atom_rmse: 0.001954
2025-02-04 06:31:08.230 INFO: train_e/atom_rmse: 0.001954
train_f_mae: 0.041026
2025-02-04 06:31:08.233 INFO: train_f_mae: 0.041026
train_f_rmse: 0.065122
2025-02-04 06:31:08.233 INFO: train_f_rmse: 0.065122
val_e/atom_mae: 0.000958
2025-02-04 06:31:08.235 INFO: val_e/atom_mae: 0.000958
val_e/atom_rmse: 0.001528
2025-02-04 06:31:08.236 INFO: val_e/atom_rmse: 0.001528
val_f_mae: 0.043294
2025-02-04 06:31:08.236 INFO: val_f_mae: 0.043294
val_f_rmse: 0.072212
2025-02-04 06:31:08.236 INFO: val_f_rmse: 0.072212
##### Step: 238 Learning rate: 4.8828125e-06 #####
2025-02-04 06:32:08.125 INFO: ##### Step: 238 Learning rate: 4.8828125e-06 #####
Epoch 99, Train Loss: 5.6424, Val Loss: 6.0852
2025-02-04 06:32:08.126 INFO: Epoch 99, Train Loss: 5.6424, Val Loss: 6.0852
train_e/atom_mae: 0.001086
2025-02-04 06:32:08.126 INFO: train_e/atom_mae: 0.001086
train_e/atom_rmse: 0.001947
2025-02-04 06:32:08.127 INFO: train_e/atom_rmse: 0.001947
train_f_mae: 0.041056
2025-02-04 06:32:08.129 INFO: train_f_mae: 0.041056
train_f_rmse: 0.065156
2025-02-04 06:32:08.129 INFO: train_f_rmse: 0.065156
val_e/atom_mae: 0.000957
2025-02-04 06:32:08.132 INFO: val_e/atom_mae: 0.000957
val_e/atom_rmse: 0.001535
2025-02-04 06:32:08.132 INFO: val_e/atom_rmse: 0.001535
val_f_mae: 0.043306
2025-02-04 06:32:08.132 INFO: val_f_mae: 0.043306
val_f_rmse: 0.072239
2025-02-04 06:32:08.133 INFO: val_f_rmse: 0.072239
##### Step: 239 Learning rate: 4.8828125e-06 #####
2025-02-04 06:33:08.069 INFO: ##### Step: 239 Learning rate: 4.8828125e-06 #####
Epoch 100, Train Loss: 5.6443, Val Loss: 6.0621
2025-02-04 06:33:08.069 INFO: Epoch 100, Train Loss: 5.6443, Val Loss: 6.0621
train_e/atom_mae: 0.001104
2025-02-04 06:33:08.070 INFO: train_e/atom_mae: 0.001104
train_e/atom_rmse: 0.001946
2025-02-04 06:33:08.070 INFO: train_e/atom_rmse: 0.001946
train_f_mae: 0.041064
2025-02-04 06:33:08.073 INFO: train_f_mae: 0.041064
train_f_rmse: 0.065176
2025-02-04 06:33:08.073 INFO: train_f_rmse: 0.065176
val_e/atom_mae: 0.000957
2025-02-04 06:33:08.075 INFO: val_e/atom_mae: 0.000957
val_e/atom_rmse: 0.001515
2025-02-04 06:33:08.075 INFO: val_e/atom_rmse: 0.001515
val_f_mae: 0.043308
2025-02-04 06:33:08.076 INFO: val_f_mae: 0.043308
val_f_rmse: 0.072229
2025-02-04 06:33:08.076 INFO: val_f_rmse: 0.072229
2025-02-04 06:33:09.094 INFO: Fourth train loop:
##### Step: 240 Learning rate: 2.44140625e-06 #####
2025-02-04 06:34:15.521 INFO: ##### Step: 240 Learning rate: 2.44140625e-06 #####
Epoch 1, Train Loss: 141.7059, Val Loss: 89.5971
2025-02-04 06:34:15.521 INFO: Epoch 1, Train Loss: 141.7059, Val Loss: 89.5971
train_e/atom_mae: 0.001084
2025-02-04 06:34:15.582 INFO: train_e/atom_mae: 0.001084
train_e/atom_rmse: 0.001931
2025-02-04 06:34:15.585 INFO: train_e/atom_rmse: 0.001931
train_f_mae: 0.041105
2025-02-04 06:34:15.588 INFO: train_f_mae: 0.041105
train_f_rmse: 0.065216
2025-02-04 06:34:15.588 INFO: train_f_rmse: 0.065216
val_e/atom_mae: 0.000947
2025-02-04 06:34:15.590 INFO: val_e/atom_mae: 0.000947
val_e/atom_rmse: 0.001512
2025-02-04 06:34:15.591 INFO: val_e/atom_rmse: 0.001512
val_f_mae: 0.043382
2025-02-04 06:34:15.591 INFO: val_f_mae: 0.043382
val_f_rmse: 0.072304
2025-02-04 06:34:15.591 INFO: val_f_rmse: 0.072304
##### Step: 241 Learning rate: 2.44140625e-06 #####
2025-02-04 06:35:15.536 INFO: ##### Step: 241 Learning rate: 2.44140625e-06 #####
Epoch 2, Train Loss: 138.4840, Val Loss: 84.9979
2025-02-04 06:35:15.537 INFO: Epoch 2, Train Loss: 138.4840, Val Loss: 84.9979
train_e/atom_mae: 0.001055
2025-02-04 06:35:15.538 INFO: train_e/atom_mae: 0.001055
train_e/atom_rmse: 0.001908
2025-02-04 06:35:15.538 INFO: train_e/atom_rmse: 0.001908
train_f_mae: 0.041322
2025-02-04 06:35:15.540 INFO: train_f_mae: 0.041322
train_f_rmse: 0.065412
2025-02-04 06:35:15.541 INFO: train_f_rmse: 0.065412
val_e/atom_mae: 0.000930
2025-02-04 06:35:15.543 INFO: val_e/atom_mae: 0.000930
val_e/atom_rmse: 0.001469
2025-02-04 06:35:15.543 INFO: val_e/atom_rmse: 0.001469
val_f_mae: 0.043614
2025-02-04 06:35:15.543 INFO: val_f_mae: 0.043614
val_f_rmse: 0.072508
2025-02-04 06:35:15.544 INFO: val_f_rmse: 0.072508
##### Step: 242 Learning rate: 2.44140625e-06 #####
2025-02-04 06:36:19.575 INFO: ##### Step: 242 Learning rate: 2.44140625e-06 #####
Epoch 3, Train Loss: 136.3282, Val Loss: 82.8234
2025-02-04 06:36:19.576 INFO: Epoch 3, Train Loss: 136.3282, Val Loss: 82.8234
train_e/atom_mae: 0.001042
2025-02-04 06:36:19.591 INFO: train_e/atom_mae: 0.001042
train_e/atom_rmse: 0.001892
2025-02-04 06:36:19.591 INFO: train_e/atom_rmse: 0.001892
train_f_mae: 0.041489
2025-02-04 06:36:19.594 INFO: train_f_mae: 0.041489
train_f_rmse: 0.065601
2025-02-04 06:36:19.594 INFO: train_f_rmse: 0.065601
val_e/atom_mae: 0.000926
2025-02-04 06:36:19.596 INFO: val_e/atom_mae: 0.000926
val_e/atom_rmse: 0.001448
2025-02-04 06:36:19.596 INFO: val_e/atom_rmse: 0.001448
val_f_mae: 0.043831
2025-02-04 06:36:19.597 INFO: val_f_mae: 0.043831
val_f_rmse: 0.072711
2025-02-04 06:36:19.597 INFO: val_f_rmse: 0.072711
##### Step: 243 Learning rate: 2.44140625e-06 #####
2025-02-04 06:37:19.468 INFO: ##### Step: 243 Learning rate: 2.44140625e-06 #####
Epoch 4, Train Loss: 134.4274, Val Loss: 81.2837
2025-02-04 06:37:19.468 INFO: Epoch 4, Train Loss: 134.4274, Val Loss: 81.2837
train_e/atom_mae: 0.001044
2025-02-04 06:37:19.469 INFO: train_e/atom_mae: 0.001044
train_e/atom_rmse: 0.001879
2025-02-04 06:37:19.469 INFO: train_e/atom_rmse: 0.001879
train_f_mae: 0.041798
2025-02-04 06:37:19.472 INFO: train_f_mae: 0.041798
train_f_rmse: 0.065890
2025-02-04 06:37:19.472 INFO: train_f_rmse: 0.065890
val_e/atom_mae: 0.000923
2025-02-04 06:37:19.474 INFO: val_e/atom_mae: 0.000923
val_e/atom_rmse: 0.001434
2025-02-04 06:37:19.475 INFO: val_e/atom_rmse: 0.001434
val_f_mae: 0.044025
2025-02-04 06:37:19.475 INFO: val_f_mae: 0.044025
val_f_rmse: 0.072895
2025-02-04 06:37:19.475 INFO: val_f_rmse: 0.072895
##### Step: 244 Learning rate: 2.44140625e-06 #####
2025-02-04 06:38:19.367 INFO: ##### Step: 244 Learning rate: 2.44140625e-06 #####
Epoch 5, Train Loss: 133.8676, Val Loss: 79.3263
2025-02-04 06:38:19.368 INFO: Epoch 5, Train Loss: 133.8676, Val Loss: 79.3263
train_e/atom_mae: 0.001048
2025-02-04 06:38:19.368 INFO: train_e/atom_mae: 0.001048
train_e/atom_rmse: 0.001874
2025-02-04 06:38:19.369 INFO: train_e/atom_rmse: 0.001874
train_f_mae: 0.041911
2025-02-04 06:38:19.371 INFO: train_f_mae: 0.041911
train_f_rmse: 0.066023
2025-02-04 06:38:19.371 INFO: train_f_rmse: 0.066023
val_e/atom_mae: 0.000921
2025-02-04 06:38:19.373 INFO: val_e/atom_mae: 0.000921
val_e/atom_rmse: 0.001414
2025-02-04 06:38:19.374 INFO: val_e/atom_rmse: 0.001414
val_f_mae: 0.044217
2025-02-04 06:38:19.374 INFO: val_f_mae: 0.044217
val_f_rmse: 0.073065
2025-02-04 06:38:19.374 INFO: val_f_rmse: 0.073065
##### Step: 245 Learning rate: 2.44140625e-06 #####
2025-02-04 06:39:19.240 INFO: ##### Step: 245 Learning rate: 2.44140625e-06 #####
Epoch 6, Train Loss: 131.5741, Val Loss: 78.3490
2025-02-04 06:39:19.240 INFO: Epoch 6, Train Loss: 131.5741, Val Loss: 78.3490
train_e/atom_mae: 0.001030
2025-02-04 06:39:19.241 INFO: train_e/atom_mae: 0.001030
train_e/atom_rmse: 0.001858
2025-02-04 06:39:19.241 INFO: train_e/atom_rmse: 0.001858
train_f_mae: 0.041929
2025-02-04 06:39:19.244 INFO: train_f_mae: 0.041929
train_f_rmse: 0.066040
2025-02-04 06:39:19.244 INFO: train_f_rmse: 0.066040
val_e/atom_mae: 0.000926
2025-02-04 06:39:19.246 INFO: val_e/atom_mae: 0.000926
val_e/atom_rmse: 0.001404
2025-02-04 06:39:19.247 INFO: val_e/atom_rmse: 0.001404
val_f_mae: 0.044130
2025-02-04 06:39:19.247 INFO: val_f_mae: 0.044130
val_f_rmse: 0.073038
2025-02-04 06:39:19.247 INFO: val_f_rmse: 0.073038
##### Step: 246 Learning rate: 2.44140625e-06 #####
2025-02-04 06:40:19.115 INFO: ##### Step: 246 Learning rate: 2.44140625e-06 #####
Epoch 7, Train Loss: 130.8237, Val Loss: 78.3877
2025-02-04 06:40:19.115 INFO: Epoch 7, Train Loss: 130.8237, Val Loss: 78.3877
train_e/atom_mae: 0.001021
2025-02-04 06:40:19.116 INFO: train_e/atom_mae: 0.001021
train_e/atom_rmse: 0.001852
2025-02-04 06:40:19.116 INFO: train_e/atom_rmse: 0.001852
train_f_mae: 0.041852
2025-02-04 06:40:19.119 INFO: train_f_mae: 0.041852
train_f_rmse: 0.066050
2025-02-04 06:40:19.119 INFO: train_f_rmse: 0.066050
val_e/atom_mae: 0.000919
2025-02-04 06:40:19.121 INFO: val_e/atom_mae: 0.000919
val_e/atom_rmse: 0.001405
2025-02-04 06:40:19.121 INFO: val_e/atom_rmse: 0.001405
val_f_mae: 0.044083
2025-02-04 06:40:19.122 INFO: val_f_mae: 0.044083
val_f_rmse: 0.073043
2025-02-04 06:40:19.122 INFO: val_f_rmse: 0.073043
##### Step: 247 Learning rate: 2.44140625e-06 #####
2025-02-04 06:41:19.012 INFO: ##### Step: 247 Learning rate: 2.44140625e-06 #####
Epoch 8, Train Loss: 129.1465, Val Loss: 77.4000
2025-02-04 06:41:19.013 INFO: Epoch 8, Train Loss: 129.1465, Val Loss: 77.4000
train_e/atom_mae: 0.001019
2025-02-04 06:41:19.014 INFO: train_e/atom_mae: 0.001019
train_e/atom_rmse: 0.001840
2025-02-04 06:41:19.014 INFO: train_e/atom_rmse: 0.001840
train_f_mae: 0.042032
2025-02-04 06:41:19.016 INFO: train_f_mae: 0.042032
train_f_rmse: 0.066238
2025-02-04 06:41:19.017 INFO: train_f_rmse: 0.066238
val_e/atom_mae: 0.000916
2025-02-04 06:41:19.019 INFO: val_e/atom_mae: 0.000916
val_e/atom_rmse: 0.001395
2025-02-04 06:41:19.019 INFO: val_e/atom_rmse: 0.001395
val_f_mae: 0.044271
2025-02-04 06:41:19.019 INFO: val_f_mae: 0.044271
val_f_rmse: 0.073233
2025-02-04 06:41:19.020 INFO: val_f_rmse: 0.073233
##### Step: 248 Learning rate: 2.44140625e-06 #####
2025-02-04 06:42:19.009 INFO: ##### Step: 248 Learning rate: 2.44140625e-06 #####
Epoch 9, Train Loss: 128.4594, Val Loss: 76.2933
2025-02-04 06:42:19.010 INFO: Epoch 9, Train Loss: 128.4594, Val Loss: 76.2933
train_e/atom_mae: 0.001017
2025-02-04 06:42:19.011 INFO: train_e/atom_mae: 0.001017
train_e/atom_rmse: 0.001834
2025-02-04 06:42:19.011 INFO: train_e/atom_rmse: 0.001834
train_f_mae: 0.042094
2025-02-04 06:42:19.013 INFO: train_f_mae: 0.042094
train_f_rmse: 0.066357
2025-02-04 06:42:19.014 INFO: train_f_rmse: 0.066357
val_e/atom_mae: 0.000918
2025-02-04 06:42:19.016 INFO: val_e/atom_mae: 0.000918
val_e/atom_rmse: 0.001384
2025-02-04 06:42:19.016 INFO: val_e/atom_rmse: 0.001384
val_f_mae: 0.044330
2025-02-04 06:42:19.016 INFO: val_f_mae: 0.044330
val_f_rmse: 0.073308
2025-02-04 06:42:19.017 INFO: val_f_rmse: 0.073308
##### Step: 249 Learning rate: 2.44140625e-06 #####
2025-02-04 06:43:18.933 INFO: ##### Step: 249 Learning rate: 2.44140625e-06 #####
Epoch 10, Train Loss: 127.4017, Val Loss: 75.9778
2025-02-04 06:43:18.934 INFO: Epoch 10, Train Loss: 127.4017, Val Loss: 75.9778
train_e/atom_mae: 0.001010
2025-02-04 06:43:18.935 INFO: train_e/atom_mae: 0.001010
train_e/atom_rmse: 0.001827
2025-02-04 06:43:18.935 INFO: train_e/atom_rmse: 0.001827
train_f_mae: 0.042143
2025-02-04 06:43:18.937 INFO: train_f_mae: 0.042143
train_f_rmse: 0.066449
2025-02-04 06:43:18.938 INFO: train_f_rmse: 0.066449
val_e/atom_mae: 0.000917
2025-02-04 06:43:18.940 INFO: val_e/atom_mae: 0.000917
val_e/atom_rmse: 0.001380
2025-02-04 06:43:18.940 INFO: val_e/atom_rmse: 0.001380
val_f_mae: 0.044305
2025-02-04 06:43:18.941 INFO: val_f_mae: 0.044305
val_f_rmse: 0.073333
2025-02-04 06:43:18.941 INFO: val_f_rmse: 0.073333
##### Step: 250 Learning rate: 2.44140625e-06 #####
2025-02-04 06:44:18.810 INFO: ##### Step: 250 Learning rate: 2.44140625e-06 #####
Epoch 11, Train Loss: 126.4967, Val Loss: 75.7144
2025-02-04 06:44:18.811 INFO: Epoch 11, Train Loss: 126.4967, Val Loss: 75.7144
train_e/atom_mae: 0.001009
2025-02-04 06:44:18.811 INFO: train_e/atom_mae: 0.001009
train_e/atom_rmse: 0.001820
2025-02-04 06:44:18.812 INFO: train_e/atom_rmse: 0.001820
train_f_mae: 0.042146
2025-02-04 06:44:18.814 INFO: train_f_mae: 0.042146
train_f_rmse: 0.066477
2025-02-04 06:44:18.814 INFO: train_f_rmse: 0.066477
val_e/atom_mae: 0.000926
2025-02-04 06:44:18.817 INFO: val_e/atom_mae: 0.000926
val_e/atom_rmse: 0.001377
2025-02-04 06:44:18.817 INFO: val_e/atom_rmse: 0.001377
val_f_mae: 0.044411
2025-02-04 06:44:18.817 INFO: val_f_mae: 0.044411
val_f_rmse: 0.073426
2025-02-04 06:44:18.818 INFO: val_f_rmse: 0.073426
##### Step: 251 Learning rate: 2.44140625e-06 #####
2025-02-04 06:45:18.595 INFO: ##### Step: 251 Learning rate: 2.44140625e-06 #####
Epoch 12, Train Loss: 127.0325, Val Loss: 75.4381
2025-02-04 06:45:18.595 INFO: Epoch 12, Train Loss: 127.0325, Val Loss: 75.4381
train_e/atom_mae: 0.001009
2025-02-04 06:45:18.596 INFO: train_e/atom_mae: 0.001009
train_e/atom_rmse: 0.001824
2025-02-04 06:45:18.596 INFO: train_e/atom_rmse: 0.001824
train_f_mae: 0.042053
2025-02-04 06:45:18.599 INFO: train_f_mae: 0.042053
train_f_rmse: 0.066433
2025-02-04 06:45:18.599 INFO: train_f_rmse: 0.066433
val_e/atom_mae: 0.000919
2025-02-04 06:45:18.601 INFO: val_e/atom_mae: 0.000919
val_e/atom_rmse: 0.001375
2025-02-04 06:45:18.601 INFO: val_e/atom_rmse: 0.001375
val_f_mae: 0.044277
2025-02-04 06:45:18.602 INFO: val_f_mae: 0.044277
val_f_rmse: 0.073343
2025-02-04 06:45:18.602 INFO: val_f_rmse: 0.073343
##### Step: 252 Learning rate: 2.44140625e-06 #####
2025-02-04 06:46:18.405 INFO: ##### Step: 252 Learning rate: 2.44140625e-06 #####
Epoch 13, Train Loss: 125.7787, Val Loss: 75.1200
2025-02-04 06:46:18.405 INFO: Epoch 13, Train Loss: 125.7787, Val Loss: 75.1200
train_e/atom_mae: 0.001006
2025-02-04 06:46:18.406 INFO: train_e/atom_mae: 0.001006
train_e/atom_rmse: 0.001814
2025-02-04 06:46:18.406 INFO: train_e/atom_rmse: 0.001814
train_f_mae: 0.042139
2025-02-04 06:46:18.409 INFO: train_f_mae: 0.042139
train_f_rmse: 0.066520
2025-02-04 06:46:18.409 INFO: train_f_rmse: 0.066520
val_e/atom_mae: 0.000919
2025-02-04 06:46:18.411 INFO: val_e/atom_mae: 0.000919
val_e/atom_rmse: 0.001372
2025-02-04 06:46:18.412 INFO: val_e/atom_rmse: 0.001372
val_f_mae: 0.044315
2025-02-04 06:46:18.412 INFO: val_f_mae: 0.044315
val_f_rmse: 0.073388
2025-02-04 06:46:18.412 INFO: val_f_rmse: 0.073388
##### Step: 253 Learning rate: 2.44140625e-06 #####
2025-02-04 06:47:18.192 INFO: ##### Step: 253 Learning rate: 2.44140625e-06 #####
Epoch 14, Train Loss: 125.3025, Val Loss: 75.0097
2025-02-04 06:47:18.193 INFO: Epoch 14, Train Loss: 125.3025, Val Loss: 75.0097
train_e/atom_mae: 0.001003
2025-02-04 06:47:18.194 INFO: train_e/atom_mae: 0.001003
train_e/atom_rmse: 0.001811
2025-02-04 06:47:18.194 INFO: train_e/atom_rmse: 0.001811
train_f_mae: 0.042016
2025-02-04 06:47:18.196 INFO: train_f_mae: 0.042016
train_f_rmse: 0.066443
2025-02-04 06:47:18.197 INFO: train_f_rmse: 0.066443
val_e/atom_mae: 0.000915
2025-02-04 06:47:18.199 INFO: val_e/atom_mae: 0.000915
val_e/atom_rmse: 0.001371
2025-02-04 06:47:18.199 INFO: val_e/atom_rmse: 0.001371
val_f_mae: 0.044272
2025-02-04 06:47:18.199 INFO: val_f_mae: 0.044272
val_f_rmse: 0.073365
2025-02-04 06:47:18.200 INFO: val_f_rmse: 0.073365
##### Step: 254 Learning rate: 2.44140625e-06 #####
2025-02-04 06:48:17.971 INFO: ##### Step: 254 Learning rate: 2.44140625e-06 #####
Epoch 15, Train Loss: 125.7819, Val Loss: 74.7437
2025-02-04 06:48:17.971 INFO: Epoch 15, Train Loss: 125.7819, Val Loss: 74.7437
train_e/atom_mae: 0.001006
2025-02-04 06:48:17.972 INFO: train_e/atom_mae: 0.001006
train_e/atom_rmse: 0.001814
2025-02-04 06:48:17.972 INFO: train_e/atom_rmse: 0.001814
train_f_mae: 0.042166
2025-02-04 06:48:17.975 INFO: train_f_mae: 0.042166
train_f_rmse: 0.066602
2025-02-04 06:48:17.975 INFO: train_f_rmse: 0.066602
val_e/atom_mae: 0.000912
2025-02-04 06:48:17.977 INFO: val_e/atom_mae: 0.000912
val_e/atom_rmse: 0.001368
2025-02-04 06:48:17.977 INFO: val_e/atom_rmse: 0.001368
val_f_mae: 0.044323
2025-02-04 06:48:17.978 INFO: val_f_mae: 0.044323
val_f_rmse: 0.073441
2025-02-04 06:48:17.978 INFO: val_f_rmse: 0.073441
##### Step: 255 Learning rate: 2.44140625e-06 #####
2025-02-04 06:49:17.785 INFO: ##### Step: 255 Learning rate: 2.44140625e-06 #####
Epoch 16, Train Loss: 124.5711, Val Loss: 74.2872
2025-02-04 06:49:17.785 INFO: Epoch 16, Train Loss: 124.5711, Val Loss: 74.2872
train_e/atom_mae: 0.001004
2025-02-04 06:49:17.786 INFO: train_e/atom_mae: 0.001004
train_e/atom_rmse: 0.001805
2025-02-04 06:49:17.787 INFO: train_e/atom_rmse: 0.001805
train_f_mae: 0.042103
2025-02-04 06:49:17.789 INFO: train_f_mae: 0.042103
train_f_rmse: 0.066552
2025-02-04 06:49:17.789 INFO: train_f_rmse: 0.066552
val_e/atom_mae: 0.000911
2025-02-04 06:49:17.791 INFO: val_e/atom_mae: 0.000911
val_e/atom_rmse: 0.001364
2025-02-04 06:49:17.792 INFO: val_e/atom_rmse: 0.001364
val_f_mae: 0.044394
2025-02-04 06:49:17.792 INFO: val_f_mae: 0.044394
val_f_rmse: 0.073513
2025-02-04 06:49:17.792 INFO: val_f_rmse: 0.073513
##### Step: 256 Learning rate: 2.44140625e-06 #####
2025-02-04 06:50:17.586 INFO: ##### Step: 256 Learning rate: 2.44140625e-06 #####
Epoch 17, Train Loss: 124.8117, Val Loss: 73.9299
2025-02-04 06:50:17.587 INFO: Epoch 17, Train Loss: 124.8117, Val Loss: 73.9299
train_e/atom_mae: 0.001007
2025-02-04 06:50:17.587 INFO: train_e/atom_mae: 0.001007
train_e/atom_rmse: 0.001807
2025-02-04 06:50:17.588 INFO: train_e/atom_rmse: 0.001807
train_f_mae: 0.042229
2025-02-04 06:50:17.590 INFO: train_f_mae: 0.042229
train_f_rmse: 0.066717
2025-02-04 06:50:17.591 INFO: train_f_rmse: 0.066717
val_e/atom_mae: 0.000917
2025-02-04 06:50:17.593 INFO: val_e/atom_mae: 0.000917
val_e/atom_rmse: 0.001360
2025-02-04 06:50:17.593 INFO: val_e/atom_rmse: 0.001360
val_f_mae: 0.044417
2025-02-04 06:50:17.593 INFO: val_f_mae: 0.044417
val_f_rmse: 0.073550
2025-02-04 06:50:17.594 INFO: val_f_rmse: 0.073550
##### Step: 257 Learning rate: 2.44140625e-06 #####
2025-02-04 06:51:17.330 INFO: ##### Step: 257 Learning rate: 2.44140625e-06 #####
Epoch 18, Train Loss: 124.6041, Val Loss: 74.0479
2025-02-04 06:51:17.331 INFO: Epoch 18, Train Loss: 124.6041, Val Loss: 74.0479
train_e/atom_mae: 0.001003
2025-02-04 06:51:17.331 INFO: train_e/atom_mae: 0.001003
train_e/atom_rmse: 0.001805
2025-02-04 06:51:17.332 INFO: train_e/atom_rmse: 0.001805
train_f_mae: 0.042080
2025-02-04 06:51:17.334 INFO: train_f_mae: 0.042080
train_f_rmse: 0.066617
2025-02-04 06:51:17.334 INFO: train_f_rmse: 0.066617
val_e/atom_mae: 0.000916
2025-02-04 06:51:17.337 INFO: val_e/atom_mae: 0.000916
val_e/atom_rmse: 0.001361
2025-02-04 06:51:17.337 INFO: val_e/atom_rmse: 0.001361
val_f_mae: 0.044252
2025-02-04 06:51:17.337 INFO: val_f_mae: 0.044252
val_f_rmse: 0.073432
2025-02-04 06:51:17.338 INFO: val_f_rmse: 0.073432
##### Step: 258 Learning rate: 2.44140625e-06 #####
2025-02-04 06:52:16.993 INFO: ##### Step: 258 Learning rate: 2.44140625e-06 #####
Epoch 19, Train Loss: 123.6588, Val Loss: 74.0287
2025-02-04 06:52:16.993 INFO: Epoch 19, Train Loss: 123.6588, Val Loss: 74.0287
train_e/atom_mae: 0.000994
2025-02-04 06:52:16.994 INFO: train_e/atom_mae: 0.000994
train_e/atom_rmse: 0.001798
2025-02-04 06:52:16.994 INFO: train_e/atom_rmse: 0.001798
train_f_mae: 0.042057
2025-02-04 06:52:16.997 INFO: train_f_mae: 0.042057
train_f_rmse: 0.066604
2025-02-04 06:52:16.997 INFO: train_f_rmse: 0.066604
val_e/atom_mae: 0.000914
2025-02-04 06:52:16.999 INFO: val_e/atom_mae: 0.000914
val_e/atom_rmse: 0.001361
2025-02-04 06:52:16.999 INFO: val_e/atom_rmse: 0.001361
val_f_mae: 0.044212
2025-02-04 06:52:17.000 INFO: val_f_mae: 0.044212
val_f_rmse: 0.073407
2025-02-04 06:52:17.000 INFO: val_f_rmse: 0.073407
##### Step: 259 Learning rate: 2.44140625e-06 #####
2025-02-04 06:53:16.695 INFO: ##### Step: 259 Learning rate: 2.44140625e-06 #####
Epoch 20, Train Loss: 123.5422, Val Loss: 74.6192
2025-02-04 06:53:16.695 INFO: Epoch 20, Train Loss: 123.5422, Val Loss: 74.6192
train_e/atom_mae: 0.000996
2025-02-04 06:53:16.696 INFO: train_e/atom_mae: 0.000996
train_e/atom_rmse: 0.001798
2025-02-04 06:53:16.696 INFO: train_e/atom_rmse: 0.001798
train_f_mae: 0.041873
2025-02-04 06:53:16.699 INFO: train_f_mae: 0.041873
train_f_rmse: 0.066460
2025-02-04 06:53:16.699 INFO: train_f_rmse: 0.066460
val_e/atom_mae: 0.000915
2025-02-04 06:53:16.701 INFO: val_e/atom_mae: 0.000915
val_e/atom_rmse: 0.001367
2025-02-04 06:53:16.702 INFO: val_e/atom_rmse: 0.001367
val_f_mae: 0.044075
2025-02-04 06:53:16.702 INFO: val_f_mae: 0.044075
val_f_rmse: 0.073314
2025-02-04 06:53:16.702 INFO: val_f_rmse: 0.073314
##### Step: 260 Learning rate: 1.220703125e-06 #####
2025-02-04 06:54:16.480 INFO: ##### Step: 260 Learning rate: 1.220703125e-06 #####
Epoch 21, Train Loss: 123.3667, Val Loss: 74.3941
2025-02-04 06:54:16.480 INFO: Epoch 21, Train Loss: 123.3667, Val Loss: 74.3941
train_e/atom_mae: 0.000992
2025-02-04 06:54:16.481 INFO: train_e/atom_mae: 0.000992
train_e/atom_rmse: 0.001796
2025-02-04 06:54:16.481 INFO: train_e/atom_rmse: 0.001796
train_f_mae: 0.041852
2025-02-04 06:54:16.484 INFO: train_f_mae: 0.041852
train_f_rmse: 0.066466
2025-02-04 06:54:16.484 INFO: train_f_rmse: 0.066466
val_e/atom_mae: 0.000914
2025-02-04 06:54:16.486 INFO: val_e/atom_mae: 0.000914
val_e/atom_rmse: 0.001365
2025-02-04 06:54:16.487 INFO: val_e/atom_rmse: 0.001365
val_f_mae: 0.044087
2025-02-04 06:54:16.487 INFO: val_f_mae: 0.044087
val_f_rmse: 0.073326
2025-02-04 06:54:16.487 INFO: val_f_rmse: 0.073326
##### Step: 261 Learning rate: 1.220703125e-06 #####
2025-02-04 06:55:16.225 INFO: ##### Step: 261 Learning rate: 1.220703125e-06 #####
Epoch 22, Train Loss: 122.6978, Val Loss: 74.0325
2025-02-04 06:55:16.225 INFO: Epoch 22, Train Loss: 122.6978, Val Loss: 74.0325
train_e/atom_mae: 0.000986
2025-02-04 06:55:16.226 INFO: train_e/atom_mae: 0.000986
train_e/atom_rmse: 0.001791
2025-02-04 06:55:16.226 INFO: train_e/atom_rmse: 0.001791
train_f_mae: 0.041935
2025-02-04 06:55:16.229 INFO: train_f_mae: 0.041935
train_f_rmse: 0.066539
2025-02-04 06:55:16.229 INFO: train_f_rmse: 0.066539
val_e/atom_mae: 0.000913
2025-02-04 06:55:16.231 INFO: val_e/atom_mae: 0.000913
val_e/atom_rmse: 0.001361
2025-02-04 06:55:16.231 INFO: val_e/atom_rmse: 0.001361
val_f_mae: 0.044142
2025-02-04 06:55:16.232 INFO: val_f_mae: 0.044142
val_f_rmse: 0.073368
2025-02-04 06:55:16.232 INFO: val_f_rmse: 0.073368
##### Step: 262 Learning rate: 1.220703125e-06 #####
2025-02-04 06:56:15.976 INFO: ##### Step: 262 Learning rate: 1.220703125e-06 #####
Epoch 23, Train Loss: 122.8942, Val Loss: 73.9322
2025-02-04 06:56:15.977 INFO: Epoch 23, Train Loss: 122.8942, Val Loss: 73.9322
train_e/atom_mae: 0.000986
2025-02-04 06:56:15.977 INFO: train_e/atom_mae: 0.000986
train_e/atom_rmse: 0.001793
2025-02-04 06:56:15.978 INFO: train_e/atom_rmse: 0.001793
train_f_mae: 0.041960
2025-02-04 06:56:15.980 INFO: train_f_mae: 0.041960
train_f_rmse: 0.066567
2025-02-04 06:56:15.981 INFO: train_f_rmse: 0.066567
val_e/atom_mae: 0.000911
2025-02-04 06:56:15.983 INFO: val_e/atom_mae: 0.000911
val_e/atom_rmse: 0.001360
2025-02-04 06:56:15.983 INFO: val_e/atom_rmse: 0.001360
val_f_mae: 0.044176
2025-02-04 06:56:15.983 INFO: val_f_mae: 0.044176
val_f_rmse: 0.073403
2025-02-04 06:56:15.984 INFO: val_f_rmse: 0.073403
##### Step: 263 Learning rate: 1.220703125e-06 #####
2025-02-04 06:57:15.723 INFO: ##### Step: 263 Learning rate: 1.220703125e-06 #####
Epoch 24, Train Loss: 122.7020, Val Loss: 74.1105
2025-02-04 06:57:15.724 INFO: Epoch 24, Train Loss: 122.7020, Val Loss: 74.1105
train_e/atom_mae: 0.000990
2025-02-04 06:57:15.725 INFO: train_e/atom_mae: 0.000990
train_e/atom_rmse: 0.001791
2025-02-04 06:57:15.725 INFO: train_e/atom_rmse: 0.001791
train_f_mae: 0.041960
2025-02-04 06:57:15.728 INFO: train_f_mae: 0.041960
train_f_rmse: 0.066587
2025-02-04 06:57:15.728 INFO: train_f_rmse: 0.066587
val_e/atom_mae: 0.000915
2025-02-04 06:57:15.730 INFO: val_e/atom_mae: 0.000915
val_e/atom_rmse: 0.001362
2025-02-04 06:57:15.730 INFO: val_e/atom_rmse: 0.001362
val_f_mae: 0.044162
2025-02-04 06:57:15.731 INFO: val_f_mae: 0.044162
val_f_rmse: 0.073414
2025-02-04 06:57:15.731 INFO: val_f_rmse: 0.073414
##### Step: 264 Learning rate: 1.220703125e-06 #####
2025-02-04 06:58:15.503 INFO: ##### Step: 264 Learning rate: 1.220703125e-06 #####
Epoch 25, Train Loss: 122.9614, Val Loss: 73.9599
2025-02-04 06:58:15.504 INFO: Epoch 25, Train Loss: 122.9614, Val Loss: 73.9599
train_e/atom_mae: 0.000993
2025-02-04 06:58:15.505 INFO: train_e/atom_mae: 0.000993
train_e/atom_rmse: 0.001793
2025-02-04 06:58:15.505 INFO: train_e/atom_rmse: 0.001793
train_f_mae: 0.042039
2025-02-04 06:58:15.508 INFO: train_f_mae: 0.042039
train_f_rmse: 0.066669
2025-02-04 06:58:15.508 INFO: train_f_rmse: 0.066669
val_e/atom_mae: 0.000914
2025-02-04 06:58:15.510 INFO: val_e/atom_mae: 0.000914
val_e/atom_rmse: 0.001360
2025-02-04 06:58:15.510 INFO: val_e/atom_rmse: 0.001360
val_f_mae: 0.044262
2025-02-04 06:58:15.511 INFO: val_f_mae: 0.044262
val_f_rmse: 0.073505
2025-02-04 06:58:15.511 INFO: val_f_rmse: 0.073505
##### Step: 265 Learning rate: 1.220703125e-06 #####
2025-02-04 06:59:15.265 INFO: ##### Step: 265 Learning rate: 1.220703125e-06 #####
Epoch 26, Train Loss: 122.3491, Val Loss: 73.5373
2025-02-04 06:59:15.266 INFO: Epoch 26, Train Loss: 122.3491, Val Loss: 73.5373
train_e/atom_mae: 0.000986
2025-02-04 06:59:15.267 INFO: train_e/atom_mae: 0.000986
train_e/atom_rmse: 0.001788
2025-02-04 06:59:15.267 INFO: train_e/atom_rmse: 0.001788
train_f_mae: 0.042060
2025-02-04 06:59:15.269 INFO: train_f_mae: 0.042060
train_f_rmse: 0.066694
2025-02-04 06:59:15.270 INFO: train_f_rmse: 0.066694
val_e/atom_mae: 0.000910
2025-02-04 06:59:15.272 INFO: val_e/atom_mae: 0.000910
val_e/atom_rmse: 0.001356
2025-02-04 06:59:15.272 INFO: val_e/atom_rmse: 0.001356
val_f_mae: 0.044270
2025-02-04 06:59:15.273 INFO: val_f_mae: 0.044270
val_f_rmse: 0.073512
2025-02-04 06:59:15.273 INFO: val_f_rmse: 0.073512
##### Step: 266 Learning rate: 1.220703125e-06 #####
2025-02-04 07:00:15.035 INFO: ##### Step: 266 Learning rate: 1.220703125e-06 #####
Epoch 27, Train Loss: 122.1114, Val Loss: 73.5807
2025-02-04 07:00:15.035 INFO: Epoch 27, Train Loss: 122.1114, Val Loss: 73.5807
train_e/atom_mae: 0.000984
2025-02-04 07:00:15.036 INFO: train_e/atom_mae: 0.000984
train_e/atom_rmse: 0.001787
2025-02-04 07:00:15.036 INFO: train_e/atom_rmse: 0.001787
train_f_mae: 0.041997
2025-02-04 07:00:15.039 INFO: train_f_mae: 0.041997
train_f_rmse: 0.066647
2025-02-04 07:00:15.039 INFO: train_f_rmse: 0.066647
val_e/atom_mae: 0.000911
2025-02-04 07:00:15.041 INFO: val_e/atom_mae: 0.000911
val_e/atom_rmse: 0.001356
2025-02-04 07:00:15.041 INFO: val_e/atom_rmse: 0.001356
val_f_mae: 0.044207
2025-02-04 07:00:15.042 INFO: val_f_mae: 0.044207
val_f_rmse: 0.073458
2025-02-04 07:00:15.042 INFO: val_f_rmse: 0.073458
##### Step: 267 Learning rate: 1.220703125e-06 #####
2025-02-04 07:01:14.790 INFO: ##### Step: 267 Learning rate: 1.220703125e-06 #####
Epoch 28, Train Loss: 121.9546, Val Loss: 73.8154
2025-02-04 07:01:14.790 INFO: Epoch 28, Train Loss: 121.9546, Val Loss: 73.8154
train_e/atom_mae: 0.000984
2025-02-04 07:01:14.791 INFO: train_e/atom_mae: 0.000984
train_e/atom_rmse: 0.001786
2025-02-04 07:01:14.791 INFO: train_e/atom_rmse: 0.001786
train_f_mae: 0.041886
2025-02-04 07:01:14.794 INFO: train_f_mae: 0.041886
train_f_rmse: 0.066540
2025-02-04 07:01:14.794 INFO: train_f_rmse: 0.066540
val_e/atom_mae: 0.000911
2025-02-04 07:01:14.796 INFO: val_e/atom_mae: 0.000911
val_e/atom_rmse: 0.001359
2025-02-04 07:01:14.797 INFO: val_e/atom_rmse: 0.001359
val_f_mae: 0.044130
2025-02-04 07:01:14.797 INFO: val_f_mae: 0.044130
val_f_rmse: 0.073403
2025-02-04 07:01:14.797 INFO: val_f_rmse: 0.073403
##### Step: 268 Learning rate: 1.220703125e-06 #####
2025-02-04 07:02:14.547 INFO: ##### Step: 268 Learning rate: 1.220703125e-06 #####
Epoch 29, Train Loss: 121.7823, Val Loss: 74.2293
2025-02-04 07:02:14.547 INFO: Epoch 29, Train Loss: 121.7823, Val Loss: 74.2293
train_e/atom_mae: 0.000981
2025-02-04 07:02:14.548 INFO: train_e/atom_mae: 0.000981
train_e/atom_rmse: 0.001784
2025-02-04 07:02:14.548 INFO: train_e/atom_rmse: 0.001784
train_f_mae: 0.041898
2025-02-04 07:02:14.551 INFO: train_f_mae: 0.041898
train_f_rmse: 0.066557
2025-02-04 07:02:14.551 INFO: train_f_rmse: 0.066557
val_e/atom_mae: 0.000915
2025-02-04 07:02:14.553 INFO: val_e/atom_mae: 0.000915
val_e/atom_rmse: 0.001363
2025-02-04 07:02:14.553 INFO: val_e/atom_rmse: 0.001363
val_f_mae: 0.044157
2025-02-04 07:02:14.554 INFO: val_f_mae: 0.044157
val_f_rmse: 0.073434
2025-02-04 07:02:14.554 INFO: val_f_rmse: 0.073434
##### Step: 269 Learning rate: 1.220703125e-06 #####
2025-02-04 07:03:14.242 INFO: ##### Step: 269 Learning rate: 1.220703125e-06 #####
Epoch 30, Train Loss: 121.7524, Val Loss: 73.4575
2025-02-04 07:03:14.242 INFO: Epoch 30, Train Loss: 121.7524, Val Loss: 73.4575
train_e/atom_mae: 0.000982
2025-02-04 07:03:14.243 INFO: train_e/atom_mae: 0.000982
train_e/atom_rmse: 0.001784
2025-02-04 07:03:14.244 INFO: train_e/atom_rmse: 0.001784
train_f_mae: 0.041993
2025-02-04 07:03:14.246 INFO: train_f_mae: 0.041993
train_f_rmse: 0.066661
2025-02-04 07:03:14.246 INFO: train_f_rmse: 0.066661
val_e/atom_mae: 0.000910
2025-02-04 07:03:14.248 INFO: val_e/atom_mae: 0.000910
val_e/atom_rmse: 0.001355
2025-02-04 07:03:14.249 INFO: val_e/atom_rmse: 0.001355
val_f_mae: 0.044212
2025-02-04 07:03:14.249 INFO: val_f_mae: 0.044212
val_f_rmse: 0.073473
2025-02-04 07:03:14.249 INFO: val_f_rmse: 0.073473
##### Step: 270 Learning rate: 1.220703125e-06 #####
2025-02-04 07:04:14.019 INFO: ##### Step: 270 Learning rate: 1.220703125e-06 #####
Epoch 31, Train Loss: 121.0896, Val Loss: 74.0064
2025-02-04 07:04:14.020 INFO: Epoch 31, Train Loss: 121.0896, Val Loss: 74.0064
train_e/atom_mae: 0.000981
2025-02-04 07:04:14.021 INFO: train_e/atom_mae: 0.000981
train_e/atom_rmse: 0.001779
2025-02-04 07:04:14.021 INFO: train_e/atom_rmse: 0.001779
train_f_mae: 0.042008
2025-02-04 07:04:14.024 INFO: train_f_mae: 0.042008
train_f_rmse: 0.066681
2025-02-04 07:04:14.024 INFO: train_f_rmse: 0.066681
val_e/atom_mae: 0.000914
2025-02-04 07:04:14.026 INFO: val_e/atom_mae: 0.000914
val_e/atom_rmse: 0.001361
2025-02-04 07:04:14.027 INFO: val_e/atom_rmse: 0.001361
val_f_mae: 0.044201
2025-02-04 07:04:14.027 INFO: val_f_mae: 0.044201
val_f_rmse: 0.073476
2025-02-04 07:04:14.027 INFO: val_f_rmse: 0.073476
##### Step: 271 Learning rate: 1.220703125e-06 #####
2025-02-04 07:05:13.749 INFO: ##### Step: 271 Learning rate: 1.220703125e-06 #####
Epoch 32, Train Loss: 121.6467, Val Loss: 73.3898
2025-02-04 07:05:13.750 INFO: Epoch 32, Train Loss: 121.6467, Val Loss: 73.3898
train_e/atom_mae: 0.000980
2025-02-04 07:05:13.751 INFO: train_e/atom_mae: 0.000980
train_e/atom_rmse: 0.001783
2025-02-04 07:05:13.751 INFO: train_e/atom_rmse: 0.001783
train_f_mae: 0.041988
2025-02-04 07:05:13.753 INFO: train_f_mae: 0.041988
train_f_rmse: 0.066612
2025-02-04 07:05:13.754 INFO: train_f_rmse: 0.066612
val_e/atom_mae: 0.000909
2025-02-04 07:05:13.756 INFO: val_e/atom_mae: 0.000909
val_e/atom_rmse: 0.001355
2025-02-04 07:05:13.756 INFO: val_e/atom_rmse: 0.001355
val_f_mae: 0.044263
2025-02-04 07:05:13.756 INFO: val_f_mae: 0.044263
val_f_rmse: 0.073522
2025-02-04 07:05:13.757 INFO: val_f_rmse: 0.073522
##### Step: 272 Learning rate: 1.220703125e-06 #####
2025-02-04 07:06:13.467 INFO: ##### Step: 272 Learning rate: 1.220703125e-06 #####
Epoch 33, Train Loss: 120.3943, Val Loss: 73.4556
2025-02-04 07:06:13.467 INFO: Epoch 33, Train Loss: 120.3943, Val Loss: 73.4556
train_e/atom_mae: 0.000974
2025-02-04 07:06:13.468 INFO: train_e/atom_mae: 0.000974
train_e/atom_rmse: 0.001773
2025-02-04 07:06:13.468 INFO: train_e/atom_rmse: 0.001773
train_f_mae: 0.042013
2025-02-04 07:06:13.471 INFO: train_f_mae: 0.042013
train_f_rmse: 0.066690
2025-02-04 07:06:13.471 INFO: train_f_rmse: 0.066690
val_e/atom_mae: 0.000910
2025-02-04 07:06:13.473 INFO: val_e/atom_mae: 0.000910
val_e/atom_rmse: 0.001355
2025-02-04 07:06:13.473 INFO: val_e/atom_rmse: 0.001355
val_f_mae: 0.044215
2025-02-04 07:06:13.474 INFO: val_f_mae: 0.044215
val_f_rmse: 0.073494
2025-02-04 07:06:13.474 INFO: val_f_rmse: 0.073494
##### Step: 273 Learning rate: 1.220703125e-06 #####
2025-02-04 07:07:13.167 INFO: ##### Step: 273 Learning rate: 1.220703125e-06 #####
Epoch 34, Train Loss: 121.7546, Val Loss: 73.5023
2025-02-04 07:07:13.168 INFO: Epoch 34, Train Loss: 121.7546, Val Loss: 73.5023
train_e/atom_mae: 0.000982
2025-02-04 07:07:13.168 INFO: train_e/atom_mae: 0.000982
train_e/atom_rmse: 0.001784
2025-02-04 07:07:13.169 INFO: train_e/atom_rmse: 0.001784
train_f_mae: 0.042011
2025-02-04 07:07:13.171 INFO: train_f_mae: 0.042011
train_f_rmse: 0.066708
2025-02-04 07:07:13.171 INFO: train_f_rmse: 0.066708
val_e/atom_mae: 0.000911
2025-02-04 07:07:13.174 INFO: val_e/atom_mae: 0.000911
val_e/atom_rmse: 0.001356
2025-02-04 07:07:13.174 INFO: val_e/atom_rmse: 0.001356
val_f_mae: 0.044219
2025-02-04 07:07:13.174 INFO: val_f_mae: 0.044219
val_f_rmse: 0.073504
2025-02-04 07:07:13.175 INFO: val_f_rmse: 0.073504
##### Step: 274 Learning rate: 1.220703125e-06 #####
2025-02-04 07:08:12.871 INFO: ##### Step: 274 Learning rate: 1.220703125e-06 #####
Epoch 35, Train Loss: 120.5012, Val Loss: 73.2544
2025-02-04 07:08:12.871 INFO: Epoch 35, Train Loss: 120.5012, Val Loss: 73.2544
train_e/atom_mae: 0.000974
2025-02-04 07:08:12.872 INFO: train_e/atom_mae: 0.000974
train_e/atom_rmse: 0.001774
2025-02-04 07:08:12.872 INFO: train_e/atom_rmse: 0.001774
train_f_mae: 0.041963
2025-02-04 07:08:12.875 INFO: train_f_mae: 0.041963
train_f_rmse: 0.066639
2025-02-04 07:08:12.875 INFO: train_f_rmse: 0.066639
val_e/atom_mae: 0.000910
2025-02-04 07:08:12.877 INFO: val_e/atom_mae: 0.000910
val_e/atom_rmse: 0.001353
2025-02-04 07:08:12.878 INFO: val_e/atom_rmse: 0.001353
val_f_mae: 0.044204
2025-02-04 07:08:12.878 INFO: val_f_mae: 0.044204
val_f_rmse: 0.073513
2025-02-04 07:08:12.878 INFO: val_f_rmse: 0.073513
##### Step: 275 Learning rate: 1.220703125e-06 #####
2025-02-04 07:09:12.775 INFO: ##### Step: 275 Learning rate: 1.220703125e-06 #####
Epoch 36, Train Loss: 120.8750, Val Loss: 72.9105
2025-02-04 07:09:12.776 INFO: Epoch 36, Train Loss: 120.8750, Val Loss: 72.9105
train_e/atom_mae: 0.000980
2025-02-04 07:09:12.776 INFO: train_e/atom_mae: 0.000980
train_e/atom_rmse: 0.001777
2025-02-04 07:09:12.777 INFO: train_e/atom_rmse: 0.001777
train_f_mae: 0.042022
2025-02-04 07:09:12.779 INFO: train_f_mae: 0.042022
train_f_rmse: 0.066753
2025-02-04 07:09:12.779 INFO: train_f_rmse: 0.066753
val_e/atom_mae: 0.000910
2025-02-04 07:09:12.781 INFO: val_e/atom_mae: 0.000910
val_e/atom_rmse: 0.001349
2025-02-04 07:09:12.782 INFO: val_e/atom_rmse: 0.001349
val_f_mae: 0.044263
2025-02-04 07:09:12.782 INFO: val_f_mae: 0.044263
val_f_rmse: 0.073565
2025-02-04 07:09:12.782 INFO: val_f_rmse: 0.073565
##### Step: 276 Learning rate: 1.220703125e-06 #####
2025-02-04 07:10:12.776 INFO: ##### Step: 276 Learning rate: 1.220703125e-06 #####
Epoch 37, Train Loss: 121.3164, Val Loss: 72.8942
2025-02-04 07:10:12.776 INFO: Epoch 37, Train Loss: 121.3164, Val Loss: 72.8942
train_e/atom_mae: 0.000986
2025-02-04 07:10:12.777 INFO: train_e/atom_mae: 0.000986
train_e/atom_rmse: 0.001780
2025-02-04 07:10:12.777 INFO: train_e/atom_rmse: 0.001780
train_f_mae: 0.042061
2025-02-04 07:10:12.780 INFO: train_f_mae: 0.042061
train_f_rmse: 0.066792
2025-02-04 07:10:12.780 INFO: train_f_rmse: 0.066792
val_e/atom_mae: 0.000908
2025-02-04 07:10:12.782 INFO: val_e/atom_mae: 0.000908
val_e/atom_rmse: 0.001349
2025-02-04 07:10:12.783 INFO: val_e/atom_rmse: 0.001349
val_f_mae: 0.044306
2025-02-04 07:10:12.783 INFO: val_f_mae: 0.044306
val_f_rmse: 0.073606
2025-02-04 07:10:12.783 INFO: val_f_rmse: 0.073606
##### Step: 277 Learning rate: 1.220703125e-06 #####
2025-02-04 07:11:12.704 INFO: ##### Step: 277 Learning rate: 1.220703125e-06 #####
Epoch 38, Train Loss: 121.0859, Val Loss: 72.7845
2025-02-04 07:11:12.705 INFO: Epoch 38, Train Loss: 121.0859, Val Loss: 72.7845
train_e/atom_mae: 0.000978
2025-02-04 07:11:12.706 INFO: train_e/atom_mae: 0.000978
train_e/atom_rmse: 0.001779
2025-02-04 07:11:12.706 INFO: train_e/atom_rmse: 0.001779
train_f_mae: 0.042135
2025-02-04 07:11:12.709 INFO: train_f_mae: 0.042135
train_f_rmse: 0.066857
2025-02-04 07:11:12.709 INFO: train_f_rmse: 0.066857
val_e/atom_mae: 0.000907
2025-02-04 07:11:12.711 INFO: val_e/atom_mae: 0.000907
val_e/atom_rmse: 0.001348
2025-02-04 07:11:12.711 INFO: val_e/atom_rmse: 0.001348
val_f_mae: 0.044343
2025-02-04 07:11:12.712 INFO: val_f_mae: 0.044343
val_f_rmse: 0.073637
2025-02-04 07:11:12.712 INFO: val_f_rmse: 0.073637
##### Step: 278 Learning rate: 1.220703125e-06 #####
2025-02-04 07:12:12.647 INFO: ##### Step: 278 Learning rate: 1.220703125e-06 #####
Epoch 39, Train Loss: 120.4603, Val Loss: 72.5666
2025-02-04 07:12:12.647 INFO: Epoch 39, Train Loss: 120.4603, Val Loss: 72.5666
train_e/atom_mae: 0.000981
2025-02-04 07:12:12.648 INFO: train_e/atom_mae: 0.000981
train_e/atom_rmse: 0.001774
2025-02-04 07:12:12.648 INFO: train_e/atom_rmse: 0.001774
train_f_mae: 0.042160
2025-02-04 07:12:12.651 INFO: train_f_mae: 0.042160
train_f_rmse: 0.066881
2025-02-04 07:12:12.651 INFO: train_f_rmse: 0.066881
val_e/atom_mae: 0.000909
2025-02-04 07:12:12.653 INFO: val_e/atom_mae: 0.000909
val_e/atom_rmse: 0.001346
2025-02-04 07:12:12.654 INFO: val_e/atom_rmse: 0.001346
val_f_mae: 0.044394
2025-02-04 07:12:12.654 INFO: val_f_mae: 0.044394
val_f_rmse: 0.073677
2025-02-04 07:12:12.654 INFO: val_f_rmse: 0.073677
##### Step: 279 Learning rate: 1.220703125e-06 #####
2025-02-04 07:13:12.495 INFO: ##### Step: 279 Learning rate: 1.220703125e-06 #####
Epoch 40, Train Loss: 120.5992, Val Loss: 72.4845
2025-02-04 07:13:12.496 INFO: Epoch 40, Train Loss: 120.5992, Val Loss: 72.4845
train_e/atom_mae: 0.000980
2025-02-04 07:13:12.496 INFO: train_e/atom_mae: 0.000980
train_e/atom_rmse: 0.001775
2025-02-04 07:13:12.497 INFO: train_e/atom_rmse: 0.001775
train_f_mae: 0.042173
2025-02-04 07:13:12.499 INFO: train_f_mae: 0.042173
train_f_rmse: 0.066901
2025-02-04 07:13:12.499 INFO: train_f_rmse: 0.066901
val_e/atom_mae: 0.000907
2025-02-04 07:13:12.502 INFO: val_e/atom_mae: 0.000907
val_e/atom_rmse: 0.001345
2025-02-04 07:13:12.502 INFO: val_e/atom_rmse: 0.001345
val_f_mae: 0.044381
2025-02-04 07:13:12.502 INFO: val_f_mae: 0.044381
val_f_rmse: 0.073675
2025-02-04 07:13:12.503 INFO: val_f_rmse: 0.073675
##### Step: 280 Learning rate: 6.103515625e-07 #####
2025-02-04 07:14:12.436 INFO: ##### Step: 280 Learning rate: 6.103515625e-07 #####
Epoch 41, Train Loss: 120.3809, Val Loss: 72.4833
2025-02-04 07:14:12.436 INFO: Epoch 41, Train Loss: 120.3809, Val Loss: 72.4833
train_e/atom_mae: 0.000973
2025-02-04 07:14:12.437 INFO: train_e/atom_mae: 0.000973
train_e/atom_rmse: 0.001773
2025-02-04 07:14:12.437 INFO: train_e/atom_rmse: 0.001773
train_f_mae: 0.042181
2025-02-04 07:14:12.440 INFO: train_f_mae: 0.042181
train_f_rmse: 0.066910
2025-02-04 07:14:12.440 INFO: train_f_rmse: 0.066910
val_e/atom_mae: 0.000906
2025-02-04 07:14:12.442 INFO: val_e/atom_mae: 0.000906
val_e/atom_rmse: 0.001345
2025-02-04 07:14:12.443 INFO: val_e/atom_rmse: 0.001345
val_f_mae: 0.044385
2025-02-04 07:14:12.443 INFO: val_f_mae: 0.044385
val_f_rmse: 0.073679
2025-02-04 07:14:12.443 INFO: val_f_rmse: 0.073679
##### Step: 281 Learning rate: 6.103515625e-07 #####
2025-02-04 07:15:12.269 INFO: ##### Step: 281 Learning rate: 6.103515625e-07 #####
Epoch 42, Train Loss: 120.5187, Val Loss: 72.5296
2025-02-04 07:15:12.269 INFO: Epoch 42, Train Loss: 120.5187, Val Loss: 72.5296
train_e/atom_mae: 0.000974
2025-02-04 07:15:12.270 INFO: train_e/atom_mae: 0.000974
train_e/atom_rmse: 0.001774
2025-02-04 07:15:12.270 INFO: train_e/atom_rmse: 0.001774
train_f_mae: 0.042186
2025-02-04 07:15:12.273 INFO: train_f_mae: 0.042186
train_f_rmse: 0.066917
2025-02-04 07:15:12.273 INFO: train_f_rmse: 0.066917
val_e/atom_mae: 0.000906
2025-02-04 07:15:12.275 INFO: val_e/atom_mae: 0.000906
val_e/atom_rmse: 0.001346
2025-02-04 07:15:12.275 INFO: val_e/atom_rmse: 0.001346
val_f_mae: 0.044377
2025-02-04 07:15:12.276 INFO: val_f_mae: 0.044377
val_f_rmse: 0.073675
2025-02-04 07:15:12.276 INFO: val_f_rmse: 0.073675
##### Step: 282 Learning rate: 6.103515625e-07 #####
2025-02-04 07:16:12.020 INFO: ##### Step: 282 Learning rate: 6.103515625e-07 #####
Epoch 43, Train Loss: 120.2508, Val Loss: 72.5649
2025-02-04 07:16:12.021 INFO: Epoch 43, Train Loss: 120.2508, Val Loss: 72.5649
train_e/atom_mae: 0.000974
2025-02-04 07:16:12.022 INFO: train_e/atom_mae: 0.000974
train_e/atom_rmse: 0.001772
2025-02-04 07:16:12.022 INFO: train_e/atom_rmse: 0.001772
train_f_mae: 0.042152
2025-02-04 07:16:12.024 INFO: train_f_mae: 0.042152
train_f_rmse: 0.066899
2025-02-04 07:16:12.025 INFO: train_f_rmse: 0.066899
val_e/atom_mae: 0.000907
2025-02-04 07:16:12.027 INFO: val_e/atom_mae: 0.000907
val_e/atom_rmse: 0.001346
2025-02-04 07:16:12.027 INFO: val_e/atom_rmse: 0.001346
val_f_mae: 0.044354
2025-02-04 07:16:12.027 INFO: val_f_mae: 0.044354
val_f_rmse: 0.073668
2025-02-04 07:16:12.028 INFO: val_f_rmse: 0.073668
##### Step: 283 Learning rate: 6.103515625e-07 #####
2025-02-04 07:17:11.753 INFO: ##### Step: 283 Learning rate: 6.103515625e-07 #####
Epoch 44, Train Loss: 120.3557, Val Loss: 72.7360
2025-02-04 07:17:11.753 INFO: Epoch 44, Train Loss: 120.3557, Val Loss: 72.7360
train_e/atom_mae: 0.000975
2025-02-04 07:17:11.754 INFO: train_e/atom_mae: 0.000975
train_e/atom_rmse: 0.001773
2025-02-04 07:17:11.754 INFO: train_e/atom_rmse: 0.001773
train_f_mae: 0.042146
2025-02-04 07:17:11.757 INFO: train_f_mae: 0.042146
train_f_rmse: 0.066901
2025-02-04 07:17:11.757 INFO: train_f_rmse: 0.066901
val_e/atom_mae: 0.000908
2025-02-04 07:17:11.759 INFO: val_e/atom_mae: 0.000908
val_e/atom_rmse: 0.001348
2025-02-04 07:17:11.759 INFO: val_e/atom_rmse: 0.001348
val_f_mae: 0.044345
2025-02-04 07:17:11.760 INFO: val_f_mae: 0.044345
val_f_rmse: 0.073664
2025-02-04 07:17:11.760 INFO: val_f_rmse: 0.073664
##### Step: 284 Learning rate: 6.103515625e-07 #####
2025-02-04 07:18:11.657 INFO: ##### Step: 284 Learning rate: 6.103515625e-07 #####
Epoch 45, Train Loss: 120.3512, Val Loss: 72.6001
2025-02-04 07:18:11.658 INFO: Epoch 45, Train Loss: 120.3512, Val Loss: 72.6001
train_e/atom_mae: 0.000973
2025-02-04 07:18:11.659 INFO: train_e/atom_mae: 0.000973
train_e/atom_rmse: 0.001773
2025-02-04 07:18:11.659 INFO: train_e/atom_rmse: 0.001773
train_f_mae: 0.042130
2025-02-04 07:18:11.662 INFO: train_f_mae: 0.042130
train_f_rmse: 0.066890
2025-02-04 07:18:11.662 INFO: train_f_rmse: 0.066890
val_e/atom_mae: 0.000907
2025-02-04 07:18:11.664 INFO: val_e/atom_mae: 0.000907
val_e/atom_rmse: 0.001347
2025-02-04 07:18:11.664 INFO: val_e/atom_rmse: 0.001347
val_f_mae: 0.044339
2025-02-04 07:18:11.665 INFO: val_f_mae: 0.044339
val_f_rmse: 0.073661
2025-02-04 07:18:11.665 INFO: val_f_rmse: 0.073661
##### Step: 285 Learning rate: 6.103515625e-07 #####
2025-02-04 07:19:11.592 INFO: ##### Step: 285 Learning rate: 6.103515625e-07 #####
Epoch 46, Train Loss: 120.3149, Val Loss: 72.4464
2025-02-04 07:19:11.592 INFO: Epoch 46, Train Loss: 120.3149, Val Loss: 72.4464
train_e/atom_mae: 0.000974
2025-02-04 07:19:11.593 INFO: train_e/atom_mae: 0.000974
train_e/atom_rmse: 0.001773
2025-02-04 07:19:11.593 INFO: train_e/atom_rmse: 0.001773
train_f_mae: 0.042135
2025-02-04 07:19:11.596 INFO: train_f_mae: 0.042135
train_f_rmse: 0.066898
2025-02-04 07:19:11.596 INFO: train_f_rmse: 0.066898
val_e/atom_mae: 0.000907
2025-02-04 07:19:11.598 INFO: val_e/atom_mae: 0.000907
val_e/atom_rmse: 0.001345
2025-02-04 07:19:11.598 INFO: val_e/atom_rmse: 0.001345
val_f_mae: 0.044351
2025-02-04 07:19:11.599 INFO: val_f_mae: 0.044351
val_f_rmse: 0.073673
2025-02-04 07:19:11.599 INFO: val_f_rmse: 0.073673
##### Step: 286 Learning rate: 6.103515625e-07 #####
2025-02-04 07:20:11.422 INFO: ##### Step: 286 Learning rate: 6.103515625e-07 #####
Epoch 47, Train Loss: 120.1260, Val Loss: 72.3652
2025-02-04 07:20:11.422 INFO: Epoch 47, Train Loss: 120.1260, Val Loss: 72.3652
train_e/atom_mae: 0.000972
2025-02-04 07:20:11.423 INFO: train_e/atom_mae: 0.000972
train_e/atom_rmse: 0.001771
2025-02-04 07:20:11.424 INFO: train_e/atom_rmse: 0.001771
train_f_mae: 0.042097
2025-02-04 07:20:11.426 INFO: train_f_mae: 0.042097
train_f_rmse: 0.066841
2025-02-04 07:20:11.426 INFO: train_f_rmse: 0.066841
val_e/atom_mae: 0.000907
2025-02-04 07:20:11.429 INFO: val_e/atom_mae: 0.000907
val_e/atom_rmse: 0.001344
2025-02-04 07:20:11.429 INFO: val_e/atom_rmse: 0.001344
val_f_mae: 0.044348
2025-02-04 07:20:11.429 INFO: val_f_mae: 0.044348
val_f_rmse: 0.073675
2025-02-04 07:20:11.430 INFO: val_f_rmse: 0.073675
##### Step: 287 Learning rate: 6.103515625e-07 #####
2025-02-04 07:21:11.131 INFO: ##### Step: 287 Learning rate: 6.103515625e-07 #####
Epoch 48, Train Loss: 120.0320, Val Loss: 72.3522
2025-02-04 07:21:11.132 INFO: Epoch 48, Train Loss: 120.0320, Val Loss: 72.3522
train_e/atom_mae: 0.000972
2025-02-04 07:21:11.133 INFO: train_e/atom_mae: 0.000972
train_e/atom_rmse: 0.001770
2025-02-04 07:21:11.133 INFO: train_e/atom_rmse: 0.001770
train_f_mae: 0.042140
2025-02-04 07:21:11.135 INFO: train_f_mae: 0.042140
train_f_rmse: 0.066913
2025-02-04 07:21:11.136 INFO: train_f_rmse: 0.066913
val_e/atom_mae: 0.000906
2025-02-04 07:21:11.138 INFO: val_e/atom_mae: 0.000906
val_e/atom_rmse: 0.001344
2025-02-04 07:21:11.138 INFO: val_e/atom_rmse: 0.001344
val_f_mae: 0.044360
2025-02-04 07:21:11.139 INFO: val_f_mae: 0.044360
val_f_rmse: 0.073690
2025-02-04 07:21:11.139 INFO: val_f_rmse: 0.073690
##### Step: 288 Learning rate: 6.103515625e-07 #####
2025-02-04 07:22:10.836 INFO: ##### Step: 288 Learning rate: 6.103515625e-07 #####
Epoch 49, Train Loss: 120.2521, Val Loss: 72.5024
2025-02-04 07:22:10.837 INFO: Epoch 49, Train Loss: 120.2521, Val Loss: 72.5024
train_e/atom_mae: 0.000973
2025-02-04 07:22:10.838 INFO: train_e/atom_mae: 0.000973
train_e/atom_rmse: 0.001772
2025-02-04 07:22:10.838 INFO: train_e/atom_rmse: 0.001772
train_f_mae: 0.042149
2025-02-04 07:22:10.840 INFO: train_f_mae: 0.042149
train_f_rmse: 0.066923
2025-02-04 07:22:10.841 INFO: train_f_rmse: 0.066923
val_e/atom_mae: 0.000907
2025-02-04 07:22:10.843 INFO: val_e/atom_mae: 0.000907
val_e/atom_rmse: 0.001345
2025-02-04 07:22:10.843 INFO: val_e/atom_rmse: 0.001345
val_f_mae: 0.044361
2025-02-04 07:22:10.844 INFO: val_f_mae: 0.044361
val_f_rmse: 0.073692
2025-02-04 07:22:10.844 INFO: val_f_rmse: 0.073692
##### Step: 289 Learning rate: 6.103515625e-07 #####
2025-02-04 07:23:10.567 INFO: ##### Step: 289 Learning rate: 6.103515625e-07 #####
Epoch 50, Train Loss: 120.1972, Val Loss: 72.7573
2025-02-04 07:23:10.567 INFO: Epoch 50, Train Loss: 120.1972, Val Loss: 72.7573
train_e/atom_mae: 0.000974
2025-02-04 07:23:10.568 INFO: train_e/atom_mae: 0.000974
train_e/atom_rmse: 0.001772
2025-02-04 07:23:10.568 INFO: train_e/atom_rmse: 0.001772
train_f_mae: 0.042159
2025-02-04 07:23:10.571 INFO: train_f_mae: 0.042159
train_f_rmse: 0.066930
2025-02-04 07:23:10.571 INFO: train_f_rmse: 0.066930
val_e/atom_mae: 0.000909
2025-02-04 07:23:10.573 INFO: val_e/atom_mae: 0.000909
val_e/atom_rmse: 0.001348
2025-02-04 07:23:10.574 INFO: val_e/atom_rmse: 0.001348
val_f_mae: 0.044361
2025-02-04 07:23:10.574 INFO: val_f_mae: 0.044361
val_f_rmse: 0.073690
2025-02-04 07:23:10.574 INFO: val_f_rmse: 0.073690
##### Step: 290 Learning rate: 6.103515625e-07 #####
2025-02-04 07:24:10.267 INFO: ##### Step: 290 Learning rate: 6.103515625e-07 #####
Epoch 51, Train Loss: 120.2320, Val Loss: 72.3735
2025-02-04 07:24:10.267 INFO: Epoch 51, Train Loss: 120.2320, Val Loss: 72.3735
train_e/atom_mae: 0.000973
2025-02-04 07:24:10.268 INFO: train_e/atom_mae: 0.000973
train_e/atom_rmse: 0.001772
2025-02-04 07:24:10.269 INFO: train_e/atom_rmse: 0.001772
train_f_mae: 0.042177
2025-02-04 07:24:10.271 INFO: train_f_mae: 0.042177
train_f_rmse: 0.066946
2025-02-04 07:24:10.271 INFO: train_f_rmse: 0.066946
val_e/atom_mae: 0.000905
2025-02-04 07:24:10.273 INFO: val_e/atom_mae: 0.000905
val_e/atom_rmse: 0.001344
2025-02-04 07:24:10.274 INFO: val_e/atom_rmse: 0.001344
val_f_mae: 0.044386
2025-02-04 07:24:10.274 INFO: val_f_mae: 0.044386
val_f_rmse: 0.073710
2025-02-04 07:24:10.274 INFO: val_f_rmse: 0.073710
##### Step: 291 Learning rate: 6.103515625e-07 #####
2025-02-04 07:25:09.937 INFO: ##### Step: 291 Learning rate: 6.103515625e-07 #####
Epoch 52, Train Loss: 119.8903, Val Loss: 72.2896
2025-02-04 07:25:09.938 INFO: Epoch 52, Train Loss: 119.8903, Val Loss: 72.2896
train_e/atom_mae: 0.000970
2025-02-04 07:25:09.939 INFO: train_e/atom_mae: 0.000970
train_e/atom_rmse: 0.001769
2025-02-04 07:25:09.939 INFO: train_e/atom_rmse: 0.001769
train_f_mae: 0.042179
2025-02-04 07:25:09.941 INFO: train_f_mae: 0.042179
train_f_rmse: 0.066952
2025-02-04 07:25:09.942 INFO: train_f_rmse: 0.066952
val_e/atom_mae: 0.000906
2025-02-04 07:25:09.944 INFO: val_e/atom_mae: 0.000906
val_e/atom_rmse: 0.001343
2025-02-04 07:25:09.944 INFO: val_e/atom_rmse: 0.001343
val_f_mae: 0.044374
2025-02-04 07:25:09.945 INFO: val_f_mae: 0.044374
val_f_rmse: 0.073705
2025-02-04 07:25:09.945 INFO: val_f_rmse: 0.073705
##### Step: 292 Learning rate: 6.103515625e-07 #####
2025-02-04 07:26:09.682 INFO: ##### Step: 292 Learning rate: 6.103515625e-07 #####
Epoch 53, Train Loss: 119.8014, Val Loss: 72.4746
2025-02-04 07:26:09.682 INFO: Epoch 53, Train Loss: 119.8014, Val Loss: 72.4746
train_e/atom_mae: 0.000970
2025-02-04 07:26:09.683 INFO: train_e/atom_mae: 0.000970
train_e/atom_rmse: 0.001769
2025-02-04 07:26:09.683 INFO: train_e/atom_rmse: 0.001769
train_f_mae: 0.042141
2025-02-04 07:26:09.686 INFO: train_f_mae: 0.042141
train_f_rmse: 0.066929
2025-02-04 07:26:09.686 INFO: train_f_rmse: 0.066929
val_e/atom_mae: 0.000907
2025-02-04 07:26:09.688 INFO: val_e/atom_mae: 0.000907
val_e/atom_rmse: 0.001345
2025-02-04 07:26:09.689 INFO: val_e/atom_rmse: 0.001345
val_f_mae: 0.044345
2025-02-04 07:26:09.689 INFO: val_f_mae: 0.044345
val_f_rmse: 0.073688
2025-02-04 07:26:09.689 INFO: val_f_rmse: 0.073688
##### Step: 293 Learning rate: 6.103515625e-07 #####
2025-02-04 07:27:09.348 INFO: ##### Step: 293 Learning rate: 6.103515625e-07 #####
Epoch 54, Train Loss: 119.7809, Val Loss: 72.2578
2025-02-04 07:27:09.349 INFO: Epoch 54, Train Loss: 119.7809, Val Loss: 72.2578
train_e/atom_mae: 0.000971
2025-02-04 07:27:09.350 INFO: train_e/atom_mae: 0.000971
train_e/atom_rmse: 0.001769
2025-02-04 07:27:09.350 INFO: train_e/atom_rmse: 0.001769
train_f_mae: 0.042152
2025-02-04 07:27:09.352 INFO: train_f_mae: 0.042152
train_f_rmse: 0.066938
2025-02-04 07:27:09.353 INFO: train_f_rmse: 0.066938
val_e/atom_mae: 0.000907
2025-02-04 07:27:09.355 INFO: val_e/atom_mae: 0.000907
val_e/atom_rmse: 0.001343
2025-02-04 07:27:09.355 INFO: val_e/atom_rmse: 0.001343
val_f_mae: 0.044357
2025-02-04 07:27:09.355 INFO: val_f_mae: 0.044357
val_f_rmse: 0.073693
2025-02-04 07:27:09.356 INFO: val_f_rmse: 0.073693
##### Step: 294 Learning rate: 6.103515625e-07 #####
2025-02-04 07:28:09.078 INFO: ##### Step: 294 Learning rate: 6.103515625e-07 #####
Epoch 55, Train Loss: 119.6258, Val Loss: 72.2564
2025-02-04 07:28:09.079 INFO: Epoch 55, Train Loss: 119.6258, Val Loss: 72.2564
train_e/atom_mae: 0.000971
2025-02-04 07:28:09.080 INFO: train_e/atom_mae: 0.000971
train_e/atom_rmse: 0.001767
2025-02-04 07:28:09.080 INFO: train_e/atom_rmse: 0.001767
train_f_mae: 0.042137
2025-02-04 07:28:09.083 INFO: train_f_mae: 0.042137
train_f_rmse: 0.066934
2025-02-04 07:28:09.083 INFO: train_f_rmse: 0.066934
val_e/atom_mae: 0.000906
2025-02-04 07:28:09.085 INFO: val_e/atom_mae: 0.000906
val_e/atom_rmse: 0.001343
2025-02-04 07:28:09.085 INFO: val_e/atom_rmse: 0.001343
val_f_mae: 0.044354
2025-02-04 07:28:09.086 INFO: val_f_mae: 0.044354
val_f_rmse: 0.073700
2025-02-04 07:28:09.086 INFO: val_f_rmse: 0.073700
##### Step: 295 Learning rate: 6.103515625e-07 #####
2025-02-04 07:29:08.927 INFO: ##### Step: 295 Learning rate: 6.103515625e-07 #####
Epoch 56, Train Loss: 119.7582, Val Loss: 72.2106
2025-02-04 07:29:08.927 INFO: Epoch 56, Train Loss: 119.7582, Val Loss: 72.2106
train_e/atom_mae: 0.000971
2025-02-04 07:29:08.928 INFO: train_e/atom_mae: 0.000971
train_e/atom_rmse: 0.001768
2025-02-04 07:29:08.928 INFO: train_e/atom_rmse: 0.001768
train_f_mae: 0.042154
2025-02-04 07:29:08.931 INFO: train_f_mae: 0.042154
train_f_rmse: 0.066954
2025-02-04 07:29:08.931 INFO: train_f_rmse: 0.066954
val_e/atom_mae: 0.000907
2025-02-04 07:29:08.933 INFO: val_e/atom_mae: 0.000907
val_e/atom_rmse: 0.001342
2025-02-04 07:29:08.934 INFO: val_e/atom_rmse: 0.001342
val_f_mae: 0.044364
2025-02-04 07:29:08.934 INFO: val_f_mae: 0.044364
val_f_rmse: 0.073713
2025-02-04 07:29:08.934 INFO: val_f_rmse: 0.073713
##### Step: 296 Learning rate: 6.103515625e-07 #####
2025-02-04 07:30:08.882 INFO: ##### Step: 296 Learning rate: 6.103515625e-07 #####
Epoch 57, Train Loss: 119.6717, Val Loss: 72.3923
2025-02-04 07:30:08.882 INFO: Epoch 57, Train Loss: 119.6717, Val Loss: 72.3923
train_e/atom_mae: 0.000970
2025-02-04 07:30:08.883 INFO: train_e/atom_mae: 0.000970
train_e/atom_rmse: 0.001768
2025-02-04 07:30:08.883 INFO: train_e/atom_rmse: 0.001768
train_f_mae: 0.042123
2025-02-04 07:30:08.886 INFO: train_f_mae: 0.042123
train_f_rmse: 0.066920
2025-02-04 07:30:08.886 INFO: train_f_rmse: 0.066920
val_e/atom_mae: 0.000907
2025-02-04 07:30:08.888 INFO: val_e/atom_mae: 0.000907
val_e/atom_rmse: 0.001344
2025-02-04 07:30:08.889 INFO: val_e/atom_rmse: 0.001344
val_f_mae: 0.044352
2025-02-04 07:30:08.889 INFO: val_f_mae: 0.044352
val_f_rmse: 0.073703
2025-02-04 07:30:08.889 INFO: val_f_rmse: 0.073703
##### Step: 297 Learning rate: 6.103515625e-07 #####
2025-02-04 07:31:08.817 INFO: ##### Step: 297 Learning rate: 6.103515625e-07 #####
Epoch 58, Train Loss: 119.5302, Val Loss: 72.2848
2025-02-04 07:31:08.817 INFO: Epoch 58, Train Loss: 119.5302, Val Loss: 72.2848
train_e/atom_mae: 0.000967
2025-02-04 07:31:08.818 INFO: train_e/atom_mae: 0.000967
train_e/atom_rmse: 0.001767
2025-02-04 07:31:08.818 INFO: train_e/atom_rmse: 0.001767
train_f_mae: 0.042135
2025-02-04 07:31:08.821 INFO: train_f_mae: 0.042135
train_f_rmse: 0.066941
2025-02-04 07:31:08.821 INFO: train_f_rmse: 0.066941
val_e/atom_mae: 0.000906
2025-02-04 07:31:08.823 INFO: val_e/atom_mae: 0.000906
val_e/atom_rmse: 0.001343
2025-02-04 07:31:08.823 INFO: val_e/atom_rmse: 0.001343
val_f_mae: 0.044347
2025-02-04 07:31:08.824 INFO: val_f_mae: 0.044347
val_f_rmse: 0.073700
2025-02-04 07:31:08.824 INFO: val_f_rmse: 0.073700
##### Step: 298 Learning rate: 6.103515625e-07 #####
2025-02-04 07:32:08.751 INFO: ##### Step: 298 Learning rate: 6.103515625e-07 #####
Epoch 59, Train Loss: 119.8938, Val Loss: 72.4437
2025-02-04 07:32:08.752 INFO: Epoch 59, Train Loss: 119.8938, Val Loss: 72.4437
train_e/atom_mae: 0.000973
2025-02-04 07:32:08.752 INFO: train_e/atom_mae: 0.000973
train_e/atom_rmse: 0.001769
2025-02-04 07:32:08.753 INFO: train_e/atom_rmse: 0.001769
train_f_mae: 0.042141
2025-02-04 07:32:08.755 INFO: train_f_mae: 0.042141
train_f_rmse: 0.066946
2025-02-04 07:32:08.755 INFO: train_f_rmse: 0.066946
val_e/atom_mae: 0.000907
2025-02-04 07:32:08.758 INFO: val_e/atom_mae: 0.000907
val_e/atom_rmse: 0.001345
2025-02-04 07:32:08.758 INFO: val_e/atom_rmse: 0.001345
val_f_mae: 0.044354
2025-02-04 07:32:08.758 INFO: val_f_mae: 0.044354
val_f_rmse: 0.073704
2025-02-04 07:32:08.759 INFO: val_f_rmse: 0.073704
##### Step: 299 Learning rate: 6.103515625e-07 #####
2025-02-04 07:33:08.527 INFO: ##### Step: 299 Learning rate: 6.103515625e-07 #####
Epoch 60, Train Loss: 119.8019, Val Loss: 72.2593
2025-02-04 07:33:08.527 INFO: Epoch 60, Train Loss: 119.8019, Val Loss: 72.2593
train_e/atom_mae: 0.000971
2025-02-04 07:33:08.528 INFO: train_e/atom_mae: 0.000971
train_e/atom_rmse: 0.001769
2025-02-04 07:33:08.528 INFO: train_e/atom_rmse: 0.001769
train_f_mae: 0.042140
2025-02-04 07:33:08.531 INFO: train_f_mae: 0.042140
train_f_rmse: 0.066937
2025-02-04 07:33:08.531 INFO: train_f_rmse: 0.066937
val_e/atom_mae: 0.000906
2025-02-04 07:33:08.533 INFO: val_e/atom_mae: 0.000906
val_e/atom_rmse: 0.001343
2025-02-04 07:33:08.534 INFO: val_e/atom_rmse: 0.001343
val_f_mae: 0.044353
2025-02-04 07:33:08.534 INFO: val_f_mae: 0.044353
val_f_rmse: 0.073702
2025-02-04 07:33:08.534 INFO: val_f_rmse: 0.073702
##### Step: 300 Learning rate: 3.0517578125e-07 #####
2025-02-04 07:34:08.250 INFO: ##### Step: 300 Learning rate: 3.0517578125e-07 #####
Epoch 61, Train Loss: 119.6334, Val Loss: 72.3640
2025-02-04 07:34:08.251 INFO: Epoch 61, Train Loss: 119.6334, Val Loss: 72.3640
train_e/atom_mae: 0.000969
2025-02-04 07:34:08.252 INFO: train_e/atom_mae: 0.000969
train_e/atom_rmse: 0.001767
2025-02-04 07:34:08.252 INFO: train_e/atom_rmse: 0.001767
train_f_mae: 0.042135
2025-02-04 07:34:08.255 INFO: train_f_mae: 0.042135
train_f_rmse: 0.066941
2025-02-04 07:34:08.255 INFO: train_f_rmse: 0.066941
val_e/atom_mae: 0.000906
2025-02-04 07:34:08.257 INFO: val_e/atom_mae: 0.000906
val_e/atom_rmse: 0.001344
2025-02-04 07:34:08.257 INFO: val_e/atom_rmse: 0.001344
val_f_mae: 0.044343
2025-02-04 07:34:08.258 INFO: val_f_mae: 0.044343
val_f_rmse: 0.073694
2025-02-04 07:34:08.258 INFO: val_f_rmse: 0.073694
##### Step: 301 Learning rate: 3.0517578125e-07 #####
2025-02-04 07:35:07.963 INFO: ##### Step: 301 Learning rate: 3.0517578125e-07 #####
Epoch 62, Train Loss: 119.6660, Val Loss: 72.3354
2025-02-04 07:35:07.963 INFO: Epoch 62, Train Loss: 119.6660, Val Loss: 72.3354
train_e/atom_mae: 0.000969
2025-02-04 07:35:07.964 INFO: train_e/atom_mae: 0.000969
train_e/atom_rmse: 0.001768
2025-02-04 07:35:07.964 INFO: train_e/atom_rmse: 0.001768
train_f_mae: 0.042089
2025-02-04 07:35:07.967 INFO: train_f_mae: 0.042089
train_f_rmse: 0.066871
2025-02-04 07:35:07.967 INFO: train_f_rmse: 0.066871
val_e/atom_mae: 0.000906
2025-02-04 07:35:07.969 INFO: val_e/atom_mae: 0.000906
val_e/atom_rmse: 0.001344
2025-02-04 07:35:07.970 INFO: val_e/atom_rmse: 0.001344
val_f_mae: 0.044340
2025-02-04 07:35:07.970 INFO: val_f_mae: 0.044340
val_f_rmse: 0.073693
2025-02-04 07:35:07.970 INFO: val_f_rmse: 0.073693
##### Step: 302 Learning rate: 3.0517578125e-07 #####
2025-02-04 07:36:07.617 INFO: ##### Step: 302 Learning rate: 3.0517578125e-07 #####
Epoch 63, Train Loss: 119.6601, Val Loss: 72.3405
2025-02-04 07:36:07.618 INFO: Epoch 63, Train Loss: 119.6601, Val Loss: 72.3405
train_e/atom_mae: 0.000968
2025-02-04 07:36:07.618 INFO: train_e/atom_mae: 0.000968
train_e/atom_rmse: 0.001768
2025-02-04 07:36:07.619 INFO: train_e/atom_rmse: 0.001768
train_f_mae: 0.042099
2025-02-04 07:36:07.621 INFO: train_f_mae: 0.042099
train_f_rmse: 0.066882
2025-02-04 07:36:07.621 INFO: train_f_rmse: 0.066882
val_e/atom_mae: 0.000906
2025-02-04 07:36:07.623 INFO: val_e/atom_mae: 0.000906
val_e/atom_rmse: 0.001344
2025-02-04 07:36:07.624 INFO: val_e/atom_rmse: 0.001344
val_f_mae: 0.044352
2025-02-04 07:36:07.624 INFO: val_f_mae: 0.044352
val_f_rmse: 0.073702
2025-02-04 07:36:07.624 INFO: val_f_rmse: 0.073702
##### Step: 303 Learning rate: 3.0517578125e-07 #####
2025-02-04 07:37:07.341 INFO: ##### Step: 303 Learning rate: 3.0517578125e-07 #####
Epoch 64, Train Loss: 119.5943, Val Loss: 72.1532
2025-02-04 07:37:07.342 INFO: Epoch 64, Train Loss: 119.5943, Val Loss: 72.1532
train_e/atom_mae: 0.000968
2025-02-04 07:37:07.343 INFO: train_e/atom_mae: 0.000968
train_e/atom_rmse: 0.001767
2025-02-04 07:37:07.343 INFO: train_e/atom_rmse: 0.001767
train_f_mae: 0.042168
2025-02-04 07:37:07.346 INFO: train_f_mae: 0.042168
train_f_rmse: 0.066968
2025-02-04 07:37:07.346 INFO: train_f_rmse: 0.066968
val_e/atom_mae: 0.000905
2025-02-04 07:37:07.348 INFO: val_e/atom_mae: 0.000905
val_e/atom_rmse: 0.001342
2025-02-04 07:37:07.348 INFO: val_e/atom_rmse: 0.001342
val_f_mae: 0.044380
2025-02-04 07:37:07.349 INFO: val_f_mae: 0.044380
val_f_rmse: 0.073724
2025-02-04 07:37:07.349 INFO: val_f_rmse: 0.073724
##### Step: 304 Learning rate: 3.0517578125e-07 #####
2025-02-04 07:38:07.095 INFO: ##### Step: 304 Learning rate: 3.0517578125e-07 #####
Epoch 65, Train Loss: 119.5576, Val Loss: 72.1648
2025-02-04 07:38:07.095 INFO: Epoch 65, Train Loss: 119.5576, Val Loss: 72.1648
train_e/atom_mae: 0.000968
2025-02-04 07:38:07.096 INFO: train_e/atom_mae: 0.000968
train_e/atom_rmse: 0.001767
2025-02-04 07:38:07.096 INFO: train_e/atom_rmse: 0.001767
train_f_mae: 0.042163
2025-02-04 07:38:07.099 INFO: train_f_mae: 0.042163
train_f_rmse: 0.066967
2025-02-04 07:38:07.099 INFO: train_f_rmse: 0.066967
val_e/atom_mae: 0.000905
2025-02-04 07:38:07.101 INFO: val_e/atom_mae: 0.000905
val_e/atom_rmse: 0.001342
2025-02-04 07:38:07.102 INFO: val_e/atom_rmse: 0.001342
val_f_mae: 0.044373
2025-02-04 07:38:07.102 INFO: val_f_mae: 0.044373
val_f_rmse: 0.073720
2025-02-04 07:38:07.102 INFO: val_f_rmse: 0.073720
##### Step: 305 Learning rate: 3.0517578125e-07 #####
2025-02-04 07:39:06.901 INFO: ##### Step: 305 Learning rate: 3.0517578125e-07 #####
Epoch 66, Train Loss: 119.4841, Val Loss: 72.1929
2025-02-04 07:39:06.902 INFO: Epoch 66, Train Loss: 119.4841, Val Loss: 72.1929
train_e/atom_mae: 0.000967
2025-02-04 07:39:06.903 INFO: train_e/atom_mae: 0.000967
train_e/atom_rmse: 0.001766
2025-02-04 07:39:06.903 INFO: train_e/atom_rmse: 0.001766
train_f_mae: 0.042149
2025-02-04 07:39:06.905 INFO: train_f_mae: 0.042149
train_f_rmse: 0.066954
2025-02-04 07:39:06.906 INFO: train_f_rmse: 0.066954
val_e/atom_mae: 0.000905
2025-02-04 07:39:06.908 INFO: val_e/atom_mae: 0.000905
val_e/atom_rmse: 0.001342
2025-02-04 07:39:06.908 INFO: val_e/atom_rmse: 0.001342
val_f_mae: 0.044369
2025-02-04 07:39:06.909 INFO: val_f_mae: 0.044369
val_f_rmse: 0.073719
2025-02-04 07:39:06.909 INFO: val_f_rmse: 0.073719
##### Step: 306 Learning rate: 3.0517578125e-07 #####
2025-02-04 07:40:06.652 INFO: ##### Step: 306 Learning rate: 3.0517578125e-07 #####
Epoch 67, Train Loss: 119.5833, Val Loss: 72.2710
2025-02-04 07:40:06.652 INFO: Epoch 67, Train Loss: 119.5833, Val Loss: 72.2710
train_e/atom_mae: 0.000968
2025-02-04 07:40:06.653 INFO: train_e/atom_mae: 0.000968
train_e/atom_rmse: 0.001767
2025-02-04 07:40:06.653 INFO: train_e/atom_rmse: 0.001767
train_f_mae: 0.042154
2025-02-04 07:40:06.656 INFO: train_f_mae: 0.042154
train_f_rmse: 0.066963
2025-02-04 07:40:06.656 INFO: train_f_rmse: 0.066963
val_e/atom_mae: 0.000906
2025-02-04 07:40:06.658 INFO: val_e/atom_mae: 0.000906
val_e/atom_rmse: 0.001343
2025-02-04 07:40:06.658 INFO: val_e/atom_rmse: 0.001343
val_f_mae: 0.044364
2025-02-04 07:40:06.659 INFO: val_f_mae: 0.044364
val_f_rmse: 0.073718
2025-02-04 07:40:06.659 INFO: val_f_rmse: 0.073718
##### Step: 307 Learning rate: 3.0517578125e-07 #####
2025-02-04 07:41:06.400 INFO: ##### Step: 307 Learning rate: 3.0517578125e-07 #####
Epoch 68, Train Loss: 119.3804, Val Loss: 72.1681
2025-02-04 07:41:06.400 INFO: Epoch 68, Train Loss: 119.3804, Val Loss: 72.1681
train_e/atom_mae: 0.000966
2025-02-04 07:41:06.401 INFO: train_e/atom_mae: 0.000966
train_e/atom_rmse: 0.001765
2025-02-04 07:41:06.401 INFO: train_e/atom_rmse: 0.001765
train_f_mae: 0.042122
2025-02-04 07:41:06.404 INFO: train_f_mae: 0.042122
train_f_rmse: 0.066906
2025-02-04 07:41:06.404 INFO: train_f_rmse: 0.066906
val_e/atom_mae: 0.000905
2025-02-04 07:41:06.406 INFO: val_e/atom_mae: 0.000905
val_e/atom_rmse: 0.001342
2025-02-04 07:41:06.406 INFO: val_e/atom_rmse: 0.001342
val_f_mae: 0.044370
2025-02-04 07:41:06.407 INFO: val_f_mae: 0.044370
val_f_rmse: 0.073723
2025-02-04 07:41:06.407 INFO: val_f_rmse: 0.073723
##### Step: 308 Learning rate: 3.0517578125e-07 #####
2025-02-04 07:42:06.114 INFO: ##### Step: 308 Learning rate: 3.0517578125e-07 #####
Epoch 69, Train Loss: 119.5103, Val Loss: 72.3695
2025-02-04 07:42:06.114 INFO: Epoch 69, Train Loss: 119.5103, Val Loss: 72.3695
train_e/atom_mae: 0.000969
2025-02-04 07:42:06.115 INFO: train_e/atom_mae: 0.000969
train_e/atom_rmse: 0.001766
2025-02-04 07:42:06.116 INFO: train_e/atom_rmse: 0.001766
train_f_mae: 0.042145
2025-02-04 07:42:06.118 INFO: train_f_mae: 0.042145
train_f_rmse: 0.066959
2025-02-04 07:42:06.118 INFO: train_f_rmse: 0.066959
val_e/atom_mae: 0.000907
2025-02-04 07:42:06.120 INFO: val_e/atom_mae: 0.000907
val_e/atom_rmse: 0.001344
2025-02-04 07:42:06.121 INFO: val_e/atom_rmse: 0.001344
val_f_mae: 0.044364
2025-02-04 07:42:06.121 INFO: val_f_mae: 0.044364
val_f_rmse: 0.073721
2025-02-04 07:42:06.121 INFO: val_f_rmse: 0.073721
##### Step: 309 Learning rate: 3.0517578125e-07 #####
2025-02-04 07:43:05.829 INFO: ##### Step: 309 Learning rate: 3.0517578125e-07 #####
Epoch 70, Train Loss: 119.3397, Val Loss: 72.2354
2025-02-04 07:43:05.829 INFO: Epoch 70, Train Loss: 119.3397, Val Loss: 72.2354
train_e/atom_mae: 0.000969
2025-02-04 07:43:05.830 INFO: train_e/atom_mae: 0.000969
train_e/atom_rmse: 0.001765
2025-02-04 07:43:05.830 INFO: train_e/atom_rmse: 0.001765
train_f_mae: 0.042149
2025-02-04 07:43:05.833 INFO: train_f_mae: 0.042149
train_f_rmse: 0.066959
2025-02-04 07:43:05.833 INFO: train_f_rmse: 0.066959
val_e/atom_mae: 0.000905
2025-02-04 07:43:05.835 INFO: val_e/atom_mae: 0.000905
val_e/atom_rmse: 0.001343
2025-02-04 07:43:05.836 INFO: val_e/atom_rmse: 0.001343
val_f_mae: 0.044374
2025-02-04 07:43:05.836 INFO: val_f_mae: 0.044374
val_f_rmse: 0.073729
2025-02-04 07:43:05.836 INFO: val_f_rmse: 0.073729
##### Step: 310 Learning rate: 3.0517578125e-07 #####
2025-02-04 07:44:05.557 INFO: ##### Step: 310 Learning rate: 3.0517578125e-07 #####
Epoch 71, Train Loss: 119.5193, Val Loss: 72.4291
2025-02-04 07:44:05.557 INFO: Epoch 71, Train Loss: 119.5193, Val Loss: 72.4291
train_e/atom_mae: 0.000969
2025-02-04 07:44:05.558 INFO: train_e/atom_mae: 0.000969
train_e/atom_rmse: 0.001766
2025-02-04 07:44:05.558 INFO: train_e/atom_rmse: 0.001766
train_f_mae: 0.042185
2025-02-04 07:44:05.561 INFO: train_f_mae: 0.042185
train_f_rmse: 0.066995
2025-02-04 07:44:05.561 INFO: train_f_rmse: 0.066995
val_e/atom_mae: 0.000907
2025-02-04 07:44:05.563 INFO: val_e/atom_mae: 0.000907
val_e/atom_rmse: 0.001345
2025-02-04 07:44:05.564 INFO: val_e/atom_rmse: 0.001345
val_f_mae: 0.044387
2025-02-04 07:44:05.564 INFO: val_f_mae: 0.044387
val_f_rmse: 0.073741
2025-02-04 07:44:05.564 INFO: val_f_rmse: 0.073741
##### Step: 311 Learning rate: 3.0517578125e-07 #####
2025-02-04 07:45:05.247 INFO: ##### Step: 311 Learning rate: 3.0517578125e-07 #####
Epoch 72, Train Loss: 119.5926, Val Loss: 72.2085
2025-02-04 07:45:05.247 INFO: Epoch 72, Train Loss: 119.5926, Val Loss: 72.2085
train_e/atom_mae: 0.000968
2025-02-04 07:45:05.248 INFO: train_e/atom_mae: 0.000968
train_e/atom_rmse: 0.001767
2025-02-04 07:45:05.248 INFO: train_e/atom_rmse: 0.001767
train_f_mae: 0.042188
2025-02-04 07:45:05.251 INFO: train_f_mae: 0.042188
train_f_rmse: 0.066999
2025-02-04 07:45:05.251 INFO: train_f_rmse: 0.066999
val_e/atom_mae: 0.000905
2025-02-04 07:45:05.253 INFO: val_e/atom_mae: 0.000905
val_e/atom_rmse: 0.001342
2025-02-04 07:45:05.254 INFO: val_e/atom_rmse: 0.001342
val_f_mae: 0.044399
2025-02-04 07:45:05.254 INFO: val_f_mae: 0.044399
val_f_rmse: 0.073749
2025-02-04 07:45:05.254 INFO: val_f_rmse: 0.073749
##### Step: 312 Learning rate: 3.0517578125e-07 #####
2025-02-04 07:46:04.941 INFO: ##### Step: 312 Learning rate: 3.0517578125e-07 #####
Epoch 73, Train Loss: 119.1138, Val Loss: 72.1328
2025-02-04 07:46:04.941 INFO: Epoch 73, Train Loss: 119.1138, Val Loss: 72.1328
train_e/atom_mae: 0.000965
2025-02-04 07:46:04.942 INFO: train_e/atom_mae: 0.000965
train_e/atom_rmse: 0.001763
2025-02-04 07:46:04.942 INFO: train_e/atom_rmse: 0.001763
train_f_mae: 0.042128
2025-02-04 07:46:04.945 INFO: train_f_mae: 0.042128
train_f_rmse: 0.066881
2025-02-04 07:46:04.945 INFO: train_f_rmse: 0.066881
val_e/atom_mae: 0.000904
2025-02-04 07:46:04.947 INFO: val_e/atom_mae: 0.000904
val_e/atom_rmse: 0.001342
2025-02-04 07:46:04.948 INFO: val_e/atom_rmse: 0.001342
val_f_mae: 0.044403
2025-02-04 07:46:04.948 INFO: val_f_mae: 0.044403
val_f_rmse: 0.073753
2025-02-04 07:46:04.948 INFO: val_f_rmse: 0.073753
##### Step: 313 Learning rate: 3.0517578125e-07 #####
2025-02-04 07:47:04.755 INFO: ##### Step: 313 Learning rate: 3.0517578125e-07 #####
Epoch 74, Train Loss: 119.4722, Val Loss: 72.1367
2025-02-04 07:47:04.756 INFO: Epoch 74, Train Loss: 119.4722, Val Loss: 72.1367
train_e/atom_mae: 0.000967
2025-02-04 07:47:04.757 INFO: train_e/atom_mae: 0.000967
train_e/atom_rmse: 0.001766
2025-02-04 07:47:04.757 INFO: train_e/atom_rmse: 0.001766
train_f_mae: 0.042180
2025-02-04 07:47:04.759 INFO: train_f_mae: 0.042180
train_f_rmse: 0.066992
2025-02-04 07:47:04.760 INFO: train_f_rmse: 0.066992
val_e/atom_mae: 0.000904
2025-02-04 07:47:04.762 INFO: val_e/atom_mae: 0.000904
val_e/atom_rmse: 0.001342
2025-02-04 07:47:04.762 INFO: val_e/atom_rmse: 0.001342
val_f_mae: 0.044400
2025-02-04 07:47:04.762 INFO: val_f_mae: 0.044400
val_f_rmse: 0.073752
2025-02-04 07:47:04.763 INFO: val_f_rmse: 0.073752
##### Step: 314 Learning rate: 3.0517578125e-07 #####
2025-02-04 07:48:04.687 INFO: ##### Step: 314 Learning rate: 3.0517578125e-07 #####
Epoch 75, Train Loss: 119.3555, Val Loss: 72.1794
2025-02-04 07:48:04.688 INFO: Epoch 75, Train Loss: 119.3555, Val Loss: 72.1794
train_e/atom_mae: 0.000968
2025-02-04 07:48:04.688 INFO: train_e/atom_mae: 0.000968
train_e/atom_rmse: 0.001765
2025-02-04 07:48:04.689 INFO: train_e/atom_rmse: 0.001765
train_f_mae: 0.042172
2025-02-04 07:48:04.691 INFO: train_f_mae: 0.042172
train_f_rmse: 0.066973
2025-02-04 07:48:04.692 INFO: train_f_rmse: 0.066973
val_e/atom_mae: 0.000904
2025-02-04 07:48:04.694 INFO: val_e/atom_mae: 0.000904
val_e/atom_rmse: 0.001342
2025-02-04 07:48:04.694 INFO: val_e/atom_rmse: 0.001342
val_f_mae: 0.044406
2025-02-04 07:48:04.694 INFO: val_f_mae: 0.044406
val_f_rmse: 0.073758
2025-02-04 07:48:04.695 INFO: val_f_rmse: 0.073758
##### Step: 315 Learning rate: 3.0517578125e-07 #####
2025-02-04 07:49:04.611 INFO: ##### Step: 315 Learning rate: 3.0517578125e-07 #####
Epoch 76, Train Loss: 119.3307, Val Loss: 72.1189
2025-02-04 07:49:04.611 INFO: Epoch 76, Train Loss: 119.3307, Val Loss: 72.1189
train_e/atom_mae: 0.000967
2025-02-04 07:49:04.612 INFO: train_e/atom_mae: 0.000967
train_e/atom_rmse: 0.001765
2025-02-04 07:49:04.612 INFO: train_e/atom_rmse: 0.001765
train_f_mae: 0.042191
2025-02-04 07:49:04.615 INFO: train_f_mae: 0.042191
train_f_rmse: 0.067007
2025-02-04 07:49:04.615 INFO: train_f_rmse: 0.067007
val_e/atom_mae: 0.000904
2025-02-04 07:49:04.617 INFO: val_e/atom_mae: 0.000904
val_e/atom_rmse: 0.001341
2025-02-04 07:49:04.618 INFO: val_e/atom_rmse: 0.001341
val_f_mae: 0.044396
2025-02-04 07:49:04.618 INFO: val_f_mae: 0.044396
val_f_rmse: 0.073751
2025-02-04 07:49:04.618 INFO: val_f_rmse: 0.073751
##### Step: 316 Learning rate: 3.0517578125e-07 #####
2025-02-04 07:50:04.589 INFO: ##### Step: 316 Learning rate: 3.0517578125e-07 #####
Epoch 77, Train Loss: 119.0860, Val Loss: 72.0869
2025-02-04 07:50:04.589 INFO: Epoch 77, Train Loss: 119.0860, Val Loss: 72.0869
train_e/atom_mae: 0.000964
2025-02-04 07:50:04.590 INFO: train_e/atom_mae: 0.000964
train_e/atom_rmse: 0.001763
2025-02-04 07:50:04.590 INFO: train_e/atom_rmse: 0.001763
train_f_mae: 0.042162
2025-02-04 07:50:04.593 INFO: train_f_mae: 0.042162
train_f_rmse: 0.066973
2025-02-04 07:50:04.593 INFO: train_f_rmse: 0.066973
val_e/atom_mae: 0.000904
2025-02-04 07:50:04.595 INFO: val_e/atom_mae: 0.000904
val_e/atom_rmse: 0.001341
2025-02-04 07:50:04.596 INFO: val_e/atom_rmse: 0.001341
val_f_mae: 0.044394
2025-02-04 07:50:04.596 INFO: val_f_mae: 0.044394
val_f_rmse: 0.073750
2025-02-04 07:50:04.596 INFO: val_f_rmse: 0.073750
##### Step: 317 Learning rate: 3.0517578125e-07 #####
2025-02-04 07:51:04.545 INFO: ##### Step: 317 Learning rate: 3.0517578125e-07 #####
Epoch 78, Train Loss: 119.4853, Val Loss: 72.0947
2025-02-04 07:51:04.546 INFO: Epoch 78, Train Loss: 119.4853, Val Loss: 72.0947
train_e/atom_mae: 0.000968
2025-02-04 07:51:04.547 INFO: train_e/atom_mae: 0.000968
train_e/atom_rmse: 0.001766
2025-02-04 07:51:04.547 INFO: train_e/atom_rmse: 0.001766
train_f_mae: 0.042177
2025-02-04 07:51:04.550 INFO: train_f_mae: 0.042177
train_f_rmse: 0.066999
2025-02-04 07:51:04.550 INFO: train_f_rmse: 0.066999
val_e/atom_mae: 0.000904
2025-02-04 07:51:04.552 INFO: val_e/atom_mae: 0.000904
val_e/atom_rmse: 0.001341
2025-02-04 07:51:04.552 INFO: val_e/atom_rmse: 0.001341
val_f_mae: 0.044388
2025-02-04 07:51:04.553 INFO: val_f_mae: 0.044388
val_f_rmse: 0.073748
2025-02-04 07:51:04.553 INFO: val_f_rmse: 0.073748
##### Step: 318 Learning rate: 3.0517578125e-07 #####
2025-02-04 07:52:04.474 INFO: ##### Step: 318 Learning rate: 3.0517578125e-07 #####
Epoch 79, Train Loss: 119.3460, Val Loss: 72.1387
2025-02-04 07:52:04.474 INFO: Epoch 79, Train Loss: 119.3460, Val Loss: 72.1387
train_e/atom_mae: 0.000967
2025-02-04 07:52:04.475 INFO: train_e/atom_mae: 0.000967
train_e/atom_rmse: 0.001765
2025-02-04 07:52:04.475 INFO: train_e/atom_rmse: 0.001765
train_f_mae: 0.042177
2025-02-04 07:52:04.478 INFO: train_f_mae: 0.042177
train_f_rmse: 0.067000
2025-02-04 07:52:04.478 INFO: train_f_rmse: 0.067000
val_e/atom_mae: 0.000904
2025-02-04 07:52:04.480 INFO: val_e/atom_mae: 0.000904
val_e/atom_rmse: 0.001342
2025-02-04 07:52:04.480 INFO: val_e/atom_rmse: 0.001342
val_f_mae: 0.044384
2025-02-04 07:52:04.481 INFO: val_f_mae: 0.044384
val_f_rmse: 0.073745
2025-02-04 07:52:04.481 INFO: val_f_rmse: 0.073745
##### Step: 319 Learning rate: 3.0517578125e-07 #####
2025-02-04 07:53:04.396 INFO: ##### Step: 319 Learning rate: 3.0517578125e-07 #####
Epoch 80, Train Loss: 119.0107, Val Loss: 72.0843
2025-02-04 07:53:04.396 INFO: Epoch 80, Train Loss: 119.0107, Val Loss: 72.0843
train_e/atom_mae: 0.000964
2025-02-04 07:53:04.397 INFO: train_e/atom_mae: 0.000964
train_e/atom_rmse: 0.001763
2025-02-04 07:53:04.397 INFO: train_e/atom_rmse: 0.001763
train_f_mae: 0.042117
2025-02-04 07:53:04.400 INFO: train_f_mae: 0.042117
train_f_rmse: 0.066882
2025-02-04 07:53:04.400 INFO: train_f_rmse: 0.066882
val_e/atom_mae: 0.000904
2025-02-04 07:53:04.402 INFO: val_e/atom_mae: 0.000904
val_e/atom_rmse: 0.001341
2025-02-04 07:53:04.403 INFO: val_e/atom_rmse: 0.001341
val_f_mae: 0.044382
2025-02-04 07:53:04.403 INFO: val_f_mae: 0.044382
val_f_rmse: 0.073744
2025-02-04 07:53:04.403 INFO: val_f_rmse: 0.073744
##### Step: 320 Learning rate: 1.52587890625e-07 #####
2025-02-04 07:54:04.328 INFO: ##### Step: 320 Learning rate: 1.52587890625e-07 #####
Epoch 81, Train Loss: 119.1733, Val Loss: 72.1091
2025-02-04 07:54:04.328 INFO: Epoch 81, Train Loss: 119.1733, Val Loss: 72.1091
train_e/atom_mae: 0.000965
2025-02-04 07:54:04.329 INFO: train_e/atom_mae: 0.000965
train_e/atom_rmse: 0.001764
2025-02-04 07:54:04.329 INFO: train_e/atom_rmse: 0.001764
train_f_mae: 0.042134
2025-02-04 07:54:04.332 INFO: train_f_mae: 0.042134
train_f_rmse: 0.066925
2025-02-04 07:54:04.332 INFO: train_f_rmse: 0.066925
val_e/atom_mae: 0.000904
2025-02-04 07:54:04.334 INFO: val_e/atom_mae: 0.000904
val_e/atom_rmse: 0.001341
2025-02-04 07:54:04.334 INFO: val_e/atom_rmse: 0.001341
val_f_mae: 0.044379
2025-02-04 07:54:04.335 INFO: val_f_mae: 0.044379
val_f_rmse: 0.073742
2025-02-04 07:54:04.335 INFO: val_f_rmse: 0.073742
##### Step: 321 Learning rate: 1.52587890625e-07 #####
2025-02-04 07:55:04.277 INFO: ##### Step: 321 Learning rate: 1.52587890625e-07 #####
Epoch 82, Train Loss: 119.2743, Val Loss: 72.1662
2025-02-04 07:55:04.277 INFO: Epoch 82, Train Loss: 119.2743, Val Loss: 72.1662
train_e/atom_mae: 0.000966
2025-02-04 07:55:04.278 INFO: train_e/atom_mae: 0.000966
train_e/atom_rmse: 0.001765
2025-02-04 07:55:04.278 INFO: train_e/atom_rmse: 0.001765
train_f_mae: 0.042161
2025-02-04 07:55:04.281 INFO: train_f_mae: 0.042161
train_f_rmse: 0.066989
2025-02-04 07:55:04.281 INFO: train_f_rmse: 0.066989
val_e/atom_mae: 0.000904
2025-02-04 07:55:04.283 INFO: val_e/atom_mae: 0.000904
val_e/atom_rmse: 0.001342
2025-02-04 07:55:04.284 INFO: val_e/atom_rmse: 0.001342
val_f_mae: 0.044378
2025-02-04 07:55:04.284 INFO: val_f_mae: 0.044378
val_f_rmse: 0.073741
2025-02-04 07:55:04.284 INFO: val_f_rmse: 0.073741
##### Step: 322 Learning rate: 1.52587890625e-07 #####
2025-02-04 07:56:04.072 INFO: ##### Step: 322 Learning rate: 1.52587890625e-07 #####
Epoch 83, Train Loss: 119.3264, Val Loss: 72.1431
2025-02-04 07:56:04.072 INFO: Epoch 83, Train Loss: 119.3264, Val Loss: 72.1431
train_e/atom_mae: 0.000966
2025-02-04 07:56:04.073 INFO: train_e/atom_mae: 0.000966
train_e/atom_rmse: 0.001765
2025-02-04 07:56:04.073 INFO: train_e/atom_rmse: 0.001765
train_f_mae: 0.042143
2025-02-04 07:56:04.076 INFO: train_f_mae: 0.042143
train_f_rmse: 0.066957
2025-02-04 07:56:04.076 INFO: train_f_rmse: 0.066957
val_e/atom_mae: 0.000904
2025-02-04 07:56:04.078 INFO: val_e/atom_mae: 0.000904
val_e/atom_rmse: 0.001342
2025-02-04 07:56:04.079 INFO: val_e/atom_rmse: 0.001342
val_f_mae: 0.044380
2025-02-04 07:56:04.079 INFO: val_f_mae: 0.044380
val_f_rmse: 0.073742
2025-02-04 07:56:04.079 INFO: val_f_rmse: 0.073742
##### Step: 323 Learning rate: 1.52587890625e-07 #####
2025-02-04 07:57:03.788 INFO: ##### Step: 323 Learning rate: 1.52587890625e-07 #####
Epoch 84, Train Loss: 119.1798, Val Loss: 72.1760
2025-02-04 07:57:03.788 INFO: Epoch 84, Train Loss: 119.1798, Val Loss: 72.1760
train_e/atom_mae: 0.000965
2025-02-04 07:57:03.789 INFO: train_e/atom_mae: 0.000965
train_e/atom_rmse: 0.001764
2025-02-04 07:57:03.789 INFO: train_e/atom_rmse: 0.001764
train_f_mae: 0.042151
2025-02-04 07:57:03.792 INFO: train_f_mae: 0.042151
train_f_rmse: 0.066966
2025-02-04 07:57:03.792 INFO: train_f_rmse: 0.066966
val_e/atom_mae: 0.000905
2025-02-04 07:57:03.794 INFO: val_e/atom_mae: 0.000905
val_e/atom_rmse: 0.001342
2025-02-04 07:57:03.795 INFO: val_e/atom_rmse: 0.001342
val_f_mae: 0.044381
2025-02-04 07:57:03.795 INFO: val_f_mae: 0.044381
val_f_rmse: 0.073743
2025-02-04 07:57:03.795 INFO: val_f_rmse: 0.073743
##### Step: 324 Learning rate: 1.52587890625e-07 #####
2025-02-04 07:58:03.502 INFO: ##### Step: 324 Learning rate: 1.52587890625e-07 #####
Epoch 85, Train Loss: 119.1810, Val Loss: 72.1806
2025-02-04 07:58:03.502 INFO: Epoch 85, Train Loss: 119.1810, Val Loss: 72.1806
train_e/atom_mae: 0.000965
2025-02-04 07:58:03.503 INFO: train_e/atom_mae: 0.000965
train_e/atom_rmse: 0.001764
2025-02-04 07:58:03.503 INFO: train_e/atom_rmse: 0.001764
train_f_mae: 0.042162
2025-02-04 07:58:03.506 INFO: train_f_mae: 0.042162
train_f_rmse: 0.066977
2025-02-04 07:58:03.506 INFO: train_f_rmse: 0.066977
val_e/atom_mae: 0.000904
2025-02-04 07:58:03.508 INFO: val_e/atom_mae: 0.000904
val_e/atom_rmse: 0.001342
2025-02-04 07:58:03.508 INFO: val_e/atom_rmse: 0.001342
val_f_mae: 0.044384
2025-02-04 07:58:03.509 INFO: val_f_mae: 0.044384
val_f_rmse: 0.073746
2025-02-04 07:58:03.509 INFO: val_f_rmse: 0.073746
##### Step: 325 Learning rate: 1.52587890625e-07 #####
2025-02-04 07:59:03.232 INFO: ##### Step: 325 Learning rate: 1.52587890625e-07 #####
Epoch 86, Train Loss: 119.1450, Val Loss: 72.1516
2025-02-04 07:59:03.233 INFO: Epoch 86, Train Loss: 119.1450, Val Loss: 72.1516
train_e/atom_mae: 0.000965
2025-02-04 07:59:03.234 INFO: train_e/atom_mae: 0.000965
train_e/atom_rmse: 0.001764
2025-02-04 07:59:03.234 INFO: train_e/atom_rmse: 0.001764
train_f_mae: 0.042153
2025-02-04 07:59:03.236 INFO: train_f_mae: 0.042153
train_f_rmse: 0.066971
2025-02-04 07:59:03.237 INFO: train_f_rmse: 0.066971
val_e/atom_mae: 0.000904
2025-02-04 07:59:03.239 INFO: val_e/atom_mae: 0.000904
val_e/atom_rmse: 0.001342
2025-02-04 07:59:03.239 INFO: val_e/atom_rmse: 0.001342
val_f_mae: 0.044385
2025-02-04 07:59:03.240 INFO: val_f_mae: 0.044385
val_f_rmse: 0.073747
2025-02-04 07:59:03.240 INFO: val_f_rmse: 0.073747
##### Step: 326 Learning rate: 1.52587890625e-07 #####
2025-02-04 08:00:02.923 INFO: ##### Step: 326 Learning rate: 1.52587890625e-07 #####
Epoch 87, Train Loss: 119.2793, Val Loss: 72.1212
2025-02-04 08:00:02.923 INFO: Epoch 87, Train Loss: 119.2793, Val Loss: 72.1212
train_e/atom_mae: 0.000966
2025-02-04 08:00:02.924 INFO: train_e/atom_mae: 0.000966
train_e/atom_rmse: 0.001765
2025-02-04 08:00:02.924 INFO: train_e/atom_rmse: 0.001765
train_f_mae: 0.042178
2025-02-04 08:00:02.927 INFO: train_f_mae: 0.042178
train_f_rmse: 0.067003
2025-02-04 08:00:02.927 INFO: train_f_rmse: 0.067003
val_e/atom_mae: 0.000904
2025-02-04 08:00:02.929 INFO: val_e/atom_mae: 0.000904
val_e/atom_rmse: 0.001341
2025-02-04 08:00:02.930 INFO: val_e/atom_rmse: 0.001341
val_f_mae: 0.044384
2025-02-04 08:00:02.930 INFO: val_f_mae: 0.044384
val_f_rmse: 0.073746
2025-02-04 08:00:02.930 INFO: val_f_rmse: 0.073746
##### Step: 327 Learning rate: 1.52587890625e-07 #####
2025-02-04 08:01:02.662 INFO: ##### Step: 327 Learning rate: 1.52587890625e-07 #####
Epoch 88, Train Loss: 119.3158, Val Loss: 72.2593
2025-02-04 08:01:02.663 INFO: Epoch 88, Train Loss: 119.3158, Val Loss: 72.2593
train_e/atom_mae: 0.000967
2025-02-04 08:01:02.664 INFO: train_e/atom_mae: 0.000967
train_e/atom_rmse: 0.001765
2025-02-04 08:01:02.664 INFO: train_e/atom_rmse: 0.001765
train_f_mae: 0.042132
2025-02-04 08:01:02.666 INFO: train_f_mae: 0.042132
train_f_rmse: 0.066916
2025-02-04 08:01:02.667 INFO: train_f_rmse: 0.066916
val_e/atom_mae: 0.000905
2025-02-04 08:01:02.669 INFO: val_e/atom_mae: 0.000905
val_e/atom_rmse: 0.001343
2025-02-04 08:01:02.669 INFO: val_e/atom_rmse: 0.001343
val_f_mae: 0.044382
2025-02-04 08:01:02.670 INFO: val_f_mae: 0.044382
val_f_rmse: 0.073745
2025-02-04 08:01:02.670 INFO: val_f_rmse: 0.073745
##### Step: 328 Learning rate: 1.52587890625e-07 #####
2025-02-04 08:02:02.413 INFO: ##### Step: 328 Learning rate: 1.52587890625e-07 #####
Epoch 89, Train Loss: 119.3244, Val Loss: 72.0790
2025-02-04 08:02:02.414 INFO: Epoch 89, Train Loss: 119.3244, Val Loss: 72.0790
train_e/atom_mae: 0.000967
2025-02-04 08:02:02.415 INFO: train_e/atom_mae: 0.000967
train_e/atom_rmse: 0.001765
2025-02-04 08:02:02.415 INFO: train_e/atom_rmse: 0.001765
train_f_mae: 0.042187
2025-02-04 08:02:02.417 INFO: train_f_mae: 0.042187
train_f_rmse: 0.067010
2025-02-04 08:02:02.418 INFO: train_f_rmse: 0.067010
val_e/atom_mae: 0.000904
2025-02-04 08:02:02.420 INFO: val_e/atom_mae: 0.000904
val_e/atom_rmse: 0.001341
2025-02-04 08:02:02.420 INFO: val_e/atom_rmse: 0.001341
val_f_mae: 0.044395
2025-02-04 08:02:02.420 INFO: val_f_mae: 0.044395
val_f_rmse: 0.073754
2025-02-04 08:02:02.421 INFO: val_f_rmse: 0.073754
##### Step: 329 Learning rate: 1.52587890625e-07 #####
2025-02-04 08:03:02.161 INFO: ##### Step: 329 Learning rate: 1.52587890625e-07 #####
Epoch 90, Train Loss: 119.1726, Val Loss: 72.1488
2025-02-04 08:03:02.161 INFO: Epoch 90, Train Loss: 119.1726, Val Loss: 72.1488
train_e/atom_mae: 0.000964
2025-02-04 08:03:02.162 INFO: train_e/atom_mae: 0.000964
train_e/atom_rmse: 0.001764
2025-02-04 08:03:02.162 INFO: train_e/atom_rmse: 0.001764
train_f_mae: 0.042156
2025-02-04 08:03:02.165 INFO: train_f_mae: 0.042156
train_f_rmse: 0.066955
2025-02-04 08:03:02.165 INFO: train_f_rmse: 0.066955
val_e/atom_mae: 0.000904
2025-02-04 08:03:02.167 INFO: val_e/atom_mae: 0.000904
val_e/atom_rmse: 0.001342
2025-02-04 08:03:02.168 INFO: val_e/atom_rmse: 0.001342
val_f_mae: 0.044395
2025-02-04 08:03:02.168 INFO: val_f_mae: 0.044395
val_f_rmse: 0.073754
2025-02-04 08:03:02.168 INFO: val_f_rmse: 0.073754
##### Step: 330 Learning rate: 1.52587890625e-07 #####
2025-02-04 08:04:01.877 INFO: ##### Step: 330 Learning rate: 1.52587890625e-07 #####
Epoch 91, Train Loss: 119.3029, Val Loss: 72.1383
2025-02-04 08:04:01.877 INFO: Epoch 91, Train Loss: 119.3029, Val Loss: 72.1383
train_e/atom_mae: 0.000966
2025-02-04 08:04:01.878 INFO: train_e/atom_mae: 0.000966
train_e/atom_rmse: 0.001765
2025-02-04 08:04:01.878 INFO: train_e/atom_rmse: 0.001765
train_f_mae: 0.042187
2025-02-04 08:04:01.881 INFO: train_f_mae: 0.042187
train_f_rmse: 0.067010
2025-02-04 08:04:01.881 INFO: train_f_rmse: 0.067010
val_e/atom_mae: 0.000904
2025-02-04 08:04:01.883 INFO: val_e/atom_mae: 0.000904
val_e/atom_rmse: 0.001342
2025-02-04 08:04:01.883 INFO: val_e/atom_rmse: 0.001342
val_f_mae: 0.044399
2025-02-04 08:04:01.884 INFO: val_f_mae: 0.044399
val_f_rmse: 0.073757
2025-02-04 08:04:01.884 INFO: val_f_rmse: 0.073757
##### Step: 331 Learning rate: 1.52587890625e-07 #####
2025-02-04 08:05:01.597 INFO: ##### Step: 331 Learning rate: 1.52587890625e-07 #####
Epoch 92, Train Loss: 119.2464, Val Loss: 72.0890
2025-02-04 08:05:01.597 INFO: Epoch 92, Train Loss: 119.2464, Val Loss: 72.0890
train_e/atom_mae: 0.000965
2025-02-04 08:05:01.598 INFO: train_e/atom_mae: 0.000965
train_e/atom_rmse: 0.001764
2025-02-04 08:05:01.599 INFO: train_e/atom_rmse: 0.001764
train_f_mae: 0.042157
2025-02-04 08:05:01.601 INFO: train_f_mae: 0.042157
train_f_rmse: 0.066955
2025-02-04 08:05:01.601 INFO: train_f_rmse: 0.066955
val_e/atom_mae: 0.000904
2025-02-04 08:05:01.603 INFO: val_e/atom_mae: 0.000904
val_e/atom_rmse: 0.001341
2025-02-04 08:05:01.604 INFO: val_e/atom_rmse: 0.001341
val_f_mae: 0.044398
2025-02-04 08:05:01.604 INFO: val_f_mae: 0.044398
val_f_rmse: 0.073756
2025-02-04 08:05:01.604 INFO: val_f_rmse: 0.073756
##### Step: 332 Learning rate: 1.52587890625e-07 #####
2025-02-04 08:06:01.293 INFO: ##### Step: 332 Learning rate: 1.52587890625e-07 #####
Epoch 93, Train Loss: 119.3264, Val Loss: 72.1820
2025-02-04 08:06:01.294 INFO: Epoch 93, Train Loss: 119.3264, Val Loss: 72.1820
train_e/atom_mae: 0.000967
2025-02-04 08:06:01.295 INFO: train_e/atom_mae: 0.000967
train_e/atom_rmse: 0.001765
2025-02-04 08:06:01.295 INFO: train_e/atom_rmse: 0.001765
train_f_mae: 0.042185
2025-02-04 08:06:01.297 INFO: train_f_mae: 0.042185
train_f_rmse: 0.067009
2025-02-04 08:06:01.298 INFO: train_f_rmse: 0.067009
val_e/atom_mae: 0.000904
2025-02-04 08:06:01.300 INFO: val_e/atom_mae: 0.000904
val_e/atom_rmse: 0.001342
2025-02-04 08:06:01.300 INFO: val_e/atom_rmse: 0.001342
val_f_mae: 0.044391
2025-02-04 08:06:01.301 INFO: val_f_mae: 0.044391
val_f_rmse: 0.073752
2025-02-04 08:06:01.301 INFO: val_f_rmse: 0.073752
##### Step: 333 Learning rate: 1.52587890625e-07 #####
2025-02-04 08:07:00.977 INFO: ##### Step: 333 Learning rate: 1.52587890625e-07 #####
Epoch 94, Train Loss: 119.2998, Val Loss: 72.0461
2025-02-04 08:07:00.977 INFO: Epoch 94, Train Loss: 119.2998, Val Loss: 72.0461
train_e/atom_mae: 0.000967
2025-02-04 08:07:00.978 INFO: train_e/atom_mae: 0.000967
train_e/atom_rmse: 0.001765
2025-02-04 08:07:00.979 INFO: train_e/atom_rmse: 0.001765
train_f_mae: 0.042184
2025-02-04 08:07:00.981 INFO: train_f_mae: 0.042184
train_f_rmse: 0.067009
2025-02-04 08:07:00.981 INFO: train_f_rmse: 0.067009
val_e/atom_mae: 0.000904
2025-02-04 08:07:00.983 INFO: val_e/atom_mae: 0.000904
val_e/atom_rmse: 0.001341
2025-02-04 08:07:00.984 INFO: val_e/atom_rmse: 0.001341
val_f_mae: 0.044392
2025-02-04 08:07:00.984 INFO: val_f_mae: 0.044392
val_f_rmse: 0.073753
2025-02-04 08:07:00.984 INFO: val_f_rmse: 0.073753
##### Step: 334 Learning rate: 1.52587890625e-07 #####
2025-02-04 08:08:00.705 INFO: ##### Step: 334 Learning rate: 1.52587890625e-07 #####
Epoch 95, Train Loss: 119.2985, Val Loss: 72.2048
2025-02-04 08:08:00.706 INFO: Epoch 95, Train Loss: 119.2985, Val Loss: 72.2048
train_e/atom_mae: 0.000966
2025-02-04 08:08:00.706 INFO: train_e/atom_mae: 0.000966
train_e/atom_rmse: 0.001765
2025-02-04 08:08:00.707 INFO: train_e/atom_rmse: 0.001765
train_f_mae: 0.042177
2025-02-04 08:08:00.709 INFO: train_f_mae: 0.042177
train_f_rmse: 0.067003
2025-02-04 08:08:00.710 INFO: train_f_rmse: 0.067003
val_e/atom_mae: 0.000905
2025-02-04 08:08:00.712 INFO: val_e/atom_mae: 0.000905
val_e/atom_rmse: 0.001342
2025-02-04 08:08:00.712 INFO: val_e/atom_rmse: 0.001342
val_f_mae: 0.044383
2025-02-04 08:08:00.712 INFO: val_f_mae: 0.044383
val_f_rmse: 0.073747
2025-02-04 08:08:00.713 INFO: val_f_rmse: 0.073747
##### Step: 335 Learning rate: 1.52587890625e-07 #####
2025-02-04 08:09:00.481 INFO: ##### Step: 335 Learning rate: 1.52587890625e-07 #####
Epoch 96, Train Loss: 119.3336, Val Loss: 72.1779
2025-02-04 08:09:00.481 INFO: Epoch 96, Train Loss: 119.3336, Val Loss: 72.1779
train_e/atom_mae: 0.000967
2025-02-04 08:09:00.482 INFO: train_e/atom_mae: 0.000967
train_e/atom_rmse: 0.001765
2025-02-04 08:09:00.482 INFO: train_e/atom_rmse: 0.001765
train_f_mae: 0.042174
2025-02-04 08:09:00.485 INFO: train_f_mae: 0.042174
train_f_rmse: 0.067003
2025-02-04 08:09:00.485 INFO: train_f_rmse: 0.067003
val_e/atom_mae: 0.000904
2025-02-04 08:09:00.487 INFO: val_e/atom_mae: 0.000904
val_e/atom_rmse: 0.001342
2025-02-04 08:09:00.488 INFO: val_e/atom_rmse: 0.001342
val_f_mae: 0.044386
2025-02-04 08:09:00.488 INFO: val_f_mae: 0.044386
val_f_rmse: 0.073749
2025-02-04 08:09:00.488 INFO: val_f_rmse: 0.073749
##### Step: 336 Learning rate: 1.52587890625e-07 #####
2025-02-04 08:10:00.168 INFO: ##### Step: 336 Learning rate: 1.52587890625e-07 #####
Epoch 97, Train Loss: 119.2782, Val Loss: 72.0695
2025-02-04 08:10:00.169 INFO: Epoch 97, Train Loss: 119.2782, Val Loss: 72.0695
train_e/atom_mae: 0.000966
2025-02-04 08:10:00.169 INFO: train_e/atom_mae: 0.000966
train_e/atom_rmse: 0.001765
2025-02-04 08:10:00.170 INFO: train_e/atom_rmse: 0.001765
train_f_mae: 0.042183
2025-02-04 08:10:00.172 INFO: train_f_mae: 0.042183
train_f_rmse: 0.067010
2025-02-04 08:10:00.173 INFO: train_f_rmse: 0.067010
val_e/atom_mae: 0.000904
2025-02-04 08:10:00.175 INFO: val_e/atom_mae: 0.000904
val_e/atom_rmse: 0.001341
2025-02-04 08:10:00.175 INFO: val_e/atom_rmse: 0.001341
val_f_mae: 0.044389
2025-02-04 08:10:00.175 INFO: val_f_mae: 0.044389
val_f_rmse: 0.073752
2025-02-04 08:10:00.176 INFO: val_f_rmse: 0.073752
##### Step: 337 Learning rate: 1.52587890625e-07 #####
2025-02-04 08:10:59.880 INFO: ##### Step: 337 Learning rate: 1.52587890625e-07 #####
Epoch 98, Train Loss: 119.2621, Val Loss: 72.0971
2025-02-04 08:10:59.880 INFO: Epoch 98, Train Loss: 119.2621, Val Loss: 72.0971
train_e/atom_mae: 0.000966
2025-02-04 08:10:59.881 INFO: train_e/atom_mae: 0.000966
train_e/atom_rmse: 0.001764
2025-02-04 08:10:59.881 INFO: train_e/atom_rmse: 0.001764
train_f_mae: 0.042176
2025-02-04 08:10:59.884 INFO: train_f_mae: 0.042176
train_f_rmse: 0.067006
2025-02-04 08:10:59.884 INFO: train_f_rmse: 0.067006
val_e/atom_mae: 0.000904
2025-02-04 08:10:59.886 INFO: val_e/atom_mae: 0.000904
val_e/atom_rmse: 0.001341
2025-02-04 08:10:59.886 INFO: val_e/atom_rmse: 0.001341
val_f_mae: 0.044385
2025-02-04 08:10:59.887 INFO: val_f_mae: 0.044385
val_f_rmse: 0.073750
2025-02-04 08:10:59.887 INFO: val_f_rmse: 0.073750
##### Step: 338 Learning rate: 1.52587890625e-07 #####
2025-02-04 08:11:59.579 INFO: ##### Step: 338 Learning rate: 1.52587890625e-07 #####
Epoch 99, Train Loss: 119.2364, Val Loss: 72.0363
2025-02-04 08:11:59.580 INFO: Epoch 99, Train Loss: 119.2364, Val Loss: 72.0363
train_e/atom_mae: 0.000966
2025-02-04 08:11:59.580 INFO: train_e/atom_mae: 0.000966
train_e/atom_rmse: 0.001764
2025-02-04 08:11:59.581 INFO: train_e/atom_rmse: 0.001764
train_f_mae: 0.042178
2025-02-04 08:11:59.583 INFO: train_f_mae: 0.042178
train_f_rmse: 0.067008
2025-02-04 08:11:59.583 INFO: train_f_rmse: 0.067008
val_e/atom_mae: 0.000904
2025-02-04 08:11:59.586 INFO: val_e/atom_mae: 0.000904
val_e/atom_rmse: 0.001341
2025-02-04 08:11:59.586 INFO: val_e/atom_rmse: 0.001341
val_f_mae: 0.044385
2025-02-04 08:11:59.586 INFO: val_f_mae: 0.044385
val_f_rmse: 0.073751
2025-02-04 08:11:59.587 INFO: val_f_rmse: 0.073751
##### Step: 339 Learning rate: 1.52587890625e-07 #####
2025-02-04 08:12:59.493 INFO: ##### Step: 339 Learning rate: 1.52587890625e-07 #####
Epoch 100, Train Loss: 118.4712, Val Loss: 72.1680
2025-02-04 08:12:59.493 INFO: Epoch 100, Train Loss: 118.4712, Val Loss: 72.1680
train_e/atom_mae: 0.000962
2025-02-04 08:12:59.494 INFO: train_e/atom_mae: 0.000962
train_e/atom_rmse: 0.001758
2025-02-04 08:12:59.494 INFO: train_e/atom_rmse: 0.001758
train_f_mae: 0.042119
2025-02-04 08:12:59.497 INFO: train_f_mae: 0.042119
train_f_rmse: 0.066913
2025-02-04 08:12:59.497 INFO: train_f_rmse: 0.066913
val_e/atom_mae: 0.000904
2025-02-04 08:12:59.499 INFO: val_e/atom_mae: 0.000904
val_e/atom_rmse: 0.001342
2025-02-04 08:12:59.499 INFO: val_e/atom_rmse: 0.001342
val_f_mae: 0.044381
2025-02-04 08:12:59.500 INFO: val_f_mae: 0.044381
val_f_rmse: 0.073749
2025-02-04 08:12:59.500 INFO: val_f_rmse: 0.073749
2025-02-04 08:13:00.182 INFO: Finished
2025-02-04 08:13:00.182 INFO: Number of trainable parameters: 24572
